{
  "item_type": "chapter",
  "chapter_id": "analysis-guide",
  "chapter_title": "Analysis Guide",
  "chapter_file": "book/src/analysis-guide.md",
  "drift_detected": true,
  "severity": "medium",
  "quality_assessment": "Comprehensive and well-structured guide covering all major analysis features. The documentation is excellent overall but contains several factual inaccuracies regarding debt type counts and enum naming that need correction. The guide excels at explaining complex concepts with clear examples and provides valuable strategic guidance for users.",
  "issues": [
    {
      "type": "incorrect_information",
      "severity": "medium",
      "section": "Debt Patterns - Introduction",
      "line_number": 298,
      "description": "Documentation states 'Debtmap detects 25 types of technical debt' but codebase analysis reveals 24 enum variants in DebtType",
      "current_content": "Debtmap detects 25 types of technical debt, organized into 4 strategic categories.",
      "should_be": "Debtmap detects 24 types of technical debt, organized into 4 strategic categories.",
      "fix_suggestion": "Update count from 25 to 24. Verify the complete DebtType enum list (lines 300-336) matches the actual implementation. Count: Testing (7) + Architecture (7) + Performance (8) + CodeQuality (3) = 25 listed, but implementation has 24 variants.",
      "source_reference": "src/priority/mod.rs:152-262 (DebtType enum with 24 variants: TestingGap, ComplexityHotspot, DeadCode, Duplication, Risk, TestComplexityHotspot, TestTodo, TestDuplication, ErrorSwallowing, AllocationInefficiency, StringConcatenation, NestedLoops, BlockingIO, SuboptimalDataStructure, GodObject, GodModule, FeatureEnvy, PrimitiveObsession, MagicValues, AssertionComplexity, FlakyTestPattern, AsyncMisuse, ResourceLeak, CollectionInefficiency)"
    },
    {
      "type": "incorrect_information",
      "severity": "medium",
      "section": "Legacy Risk Scoring - RiskLevel Enum",
      "line_number": 846,
      "description": "Documentation uses 'Medium' for risk level enum variant, but implementation uses 'Moderate'",
      "current_content": "pub enum RiskLevel {\n    Low,       // Score < 10\n    Medium,    // Score 10-24\n    High,      // Score 25-49\n    Critical,  // Score ≥ 50\n}",
      "should_be": "pub enum RiskLevel {\n    Low,\n    Moderate,  // Not 'Medium'\n    High,\n    Critical,\n}",
      "fix_suggestion": "Replace all occurrences of 'Medium' risk level with 'Moderate' throughout the document. Search for 'Medium' in risk scoring sections (lines 846-898) and update to match actual enum variant name.",
      "source_reference": "src/io/writers/enhanced_markdown/executive_summary.rs:106-133 (RiskLevel enum uses 'Moderate' variant)"
    },
    {
      "type": "incomplete_explanation",
      "severity": "low",
      "section": "Tiered Prioritization",
      "line_number": 1212,
      "description": "Documentation describes single Tier enum but implementation has two distinct tier systems serving different purposes",
      "current_content": "The Tier enum defines four priority levels based on score thresholds: Critical, High, Moderate, Low",
      "should_be": "Clarify that Debtmap uses two complementary tier systems: 1) Tier enum for score-based prioritization (Critical ≥90, High 70-89.9, Moderate 50-69.9, Low <50), and 2) RecommendationTier for strategic batch remediation (T1CriticalArchitecture, T2ComplexUntested, T3TestingGaps, T4Maintenance)",
      "fix_suggestion": "Add clarification section after line 1240 explaining the relationship between Tier (numerical score-based) and RecommendationTier (strategic category-based). Example:\n\n**Two Tier Systems:**\n- **Tier enum**: Score-based classification for function-level prioritization (Critical/High/Moderate/Low)\n- **RecommendationTier**: Strategic grouping for batch remediation planning (T1/T2/T3/T4)\n\nUse Tier for sprint planning individual items, use RecommendationTier for identifying patterns and batch refactoring opportunities.",
      "source_reference": "src/priority/mod.rs:430-464 (Tier enum), src/priority/tiers.rs:10-26 (RecommendationTier enum)"
    },
    {
      "type": "incomplete_explanation",
      "severity": "low",
      "section": "Unified Scoring System - Default Weights",
      "line_number": 721,
      "description": "Documentation states weights are configurable but doesn't reference configuration chapter or explain how to customize them",
      "current_content": "The scoring formula uses configurable weights (default values shown):\n\n- **Complexity: 40%** - How difficult the code is to understand and test\n- **Coverage: 40%** - How well the code is tested\n- **Dependency: 20%** - How many other functions depend on this code\n\nThese weights can be adjusted in `.debtmap.toml` to match your team's priorities.",
      "should_be": "Add cross-reference to configuration chapter showing exactly where and how to customize weights",
      "fix_suggestion": "Update line 728 to include cross-reference:\n\n'These weights can be adjusted in `.debtmap.toml` to match your team's priorities. See [Configuration Guide](./configuration.md#unified-scoring-weights) for details on customizing the scoring formula.'",
      "source_reference": "src/priority/unified_scorer.rs:24-26 (comments mention weights are configurable)"
    },
    {
      "type": "incomplete_explanation",
      "severity": "low",
      "section": "Unified Scoring - Dependency Factor",
      "line_number": 714,
      "description": "Documentation explains dependency factor concept but lacks specific numeric thresholds for caller counts",
      "current_content": "**Dependency Factor** (0-10 scale):\nBased on call graph analysis:\n- High upstream caller count (many functions depend on this): 8-10\n- On critical paths from entry points: 7-9\n- Moderate dependencies: 4-6\n- Isolated utilities: 1-3",
      "should_be": "Provide concrete thresholds: 'High impact = 5+ upstream callers (score 8-10), Moderate = 2-4 callers (score 4-6), Low = 0-1 callers (score 1-3), Critical path bonus = +2-3 points'",
      "fix_suggestion": "Update Dependency Factor description around line 714:\n\n**Dependency Factor** (0-10 scale):\nBased on call graph analysis with specific thresholds:\n- **High impact** (score 8-10): 5+ upstream callers, or on critical path from entry point (adds 2-3 points)\n- **Moderate impact** (score 4-6): 2-4 upstream callers\n- **Low impact** (score 1-3): 0-1 upstream callers\n- **Critical path bonus**: Being on a critical path from an entry point adds 2-3 points to the base dependency score",
      "source_reference": "src/priority/unified_scorer.rs or src/risk/scoring.rs (dependency factor calculation)"
    },
    {
      "type": "missing_content",
      "severity": "low",
      "section": "Debt Patterns - Language-Specific Patterns",
      "line_number": 359,
      "description": "Documentation lists all debt types but doesn't clarify which patterns are language-specific",
      "current_content": "Lists all debt types under categories without noting language-specific applicability",
      "should_add": "Add subsection explaining which debt patterns only apply to languages with specific features (async, manual memory management, etc.)",
      "fix_suggestion": "Add new subsection after line 366 titled 'Language-Specific Debt Patterns':\n\n**Language-Specific Debt Patterns:**\n\nSome debt patterns only apply to languages with specific features:\n- **BlockingIO, AsyncMisuse**: Async-capable languages (Rust, JavaScript, TypeScript)\n- **AllocationInefficiency, ResourceLeak**: Languages with manual memory management (Rust)\n- **Error handling patterns**: Vary by language error model (Result in Rust, exceptions in Python/JS)\n\nDebtmap automatically applies only the relevant debt patterns for each language during analysis.",
      "source_reference": "features.json:debt_detection and language-specific analyzer implementations"
    },
    {
      "type": "incomplete_explanation",
      "severity": "low",
      "section": "Risk Distribution - minimal_count field",
      "line_number": 965,
      "description": "Example JSON shows minimal_count field but this isn't a standard risk category and the connection to unified scoring could be clearer",
      "current_content": "\"minimal_count\": 234,",
      "should_be": "Clarify that minimal_count is specific to unified scoring (functions scoring 0-2.9), not a separate risk category",
      "fix_suggestion": "Add prominent note after risk distribution example (around line 976):\n\n**Note on minimal_count:**\n\nIn unified scoring (0-10 scale), `minimal_count` represents functions scoring 0-2.9, which includes:\n- Simple utility functions\n- Helper functions with low complexity\n- Well-tested complex code that scores low due to coverage dampening\n\nThis is not a separate risk category but an **outcome** of the unified scoring system. Complex business logic with 95% test coverage appropriately receives a minimal score, reflecting that good testing mitigates complexity risk.\n\n**Important:** `minimal_count` does not appear in the standard `risk_categories` from features.json (Low, Medium, High, Critical, WellTested). It's specific to unified scoring's 0-10 scale priority classifications (Minimal, Low, Medium, High, Critical).",
      "source_reference": "features.json:risk_assessment.risk_categories (does not list minimal_count)"
    },
    {
      "type": "missing_content",
      "severity": "low",
      "section": "JSON Structure - est_branches field",
      "line_number": 1056,
      "description": "The est_branches field is documented in features.json as an estimated metric but not shown in JSON structure example or Reading Function Metrics section",
      "current_content": "JSON structure example showing complexity metrics without est_branches",
      "should_add": "Include est_branches in JSON examples and explain it's a formula-based estimate for test case planning",
      "fix_suggestion": "Add est_branches to JSON structure example around line 1056:\n\n\"cyclomatic\": 15,\n\"cognitive\": 22,\n\"est_branches\": 20,  // Formula: max(nesting_depth, 1) × cyclomatic ÷ 3\n\"nesting\": 4,\n\nAnd update 'Reading Function Metrics' section (line 1133) to add:\n- `est_branches`: Estimated execution paths (formula: max(nesting_depth, 1) × cyclomatic ÷ 3) - approximates test cases needed for branch coverage",
      "source_reference": "features.json:complexity_metrics.estimated_metrics.est_branches"
    },
    {
      "type": "incomplete_explanation",
      "severity": "low",
      "section": "Coverage Integration - Performance Guidance",
      "line_number": 2396,
      "description": "Documentation provides detailed performance metrics (2.5x overhead) but lacks practical guidance on when to skip coverage for faster iteration",
      "current_content": "**Analysis Overhead:**\n- Coverage integration overhead: ~2.5x baseline analysis time\n- Target overhead: ≤3x (maintained through optimizations)\n- Example timing: 53ms baseline → 130ms with coverage (2.45x overhead)",
      "should_add": "Add guidance on when the 2.5x overhead is acceptable versus when to omit coverage for faster feedback",
      "fix_suggestion": "Add subsection after line 2402 titled 'When to use coverage integration':\n\n**When to use coverage integration:**\n- **Skip coverage** (faster iteration): For rapid development iteration or quick local checks, omit `--lcov` to get baseline results 2.5x faster\n- **Include coverage** (comprehensive analysis): Use coverage integration for final validation, sprint planning, and CI/CD gates where comprehensive risk analysis is needed",
      "source_reference": "Coverage integration performance documentation and user workflow best practices"
    },
    {
      "type": "missing_content",
      "severity": "low",
      "section": "JSON Output Format Variants",
      "line_number": 1107,
      "description": "Documentation mentions unified format (spec 108) as future enhancement but doesn't clearly state it's not available to users",
      "current_content": "**Note:** The unified format is currently an internal representation. If you need this format exposed as a CLI option, please open a feature request on GitHub.",
      "should_be": "Make it explicitly clear this is internal-only and not a user-facing option",
      "fix_suggestion": "Update note around line 1128 to be more prominent:\n\n**Note:** The unified format is currently an internal representation and is **not available** as a user-facing CLI option. The legacy format remains the stable default for all current integrations. If you need the unified format exposed as a CLI option (`--format json-unified`), please open a feature request on GitHub.",
      "source_reference": "features.json:output_formats.json.variants"
    }
  ],
  "positive_aspects": [
    "Extremely comprehensive coverage of all analysis capabilities - one of the most thorough technical guides",
    "Excellent use of examples throughout (code samples, JSON output, terminal output, before/after comparisons)",
    "Clear distinction between unified scoring (0-10, recommended) and legacy scoring systems",
    "Outstanding explanation of entropy-based complexity with Shannon entropy formulas and pattern detection",
    "Strong integration of theory (formulas, algorithms) with practice (CLI examples, workflows)",
    "Comprehensive debt type documentation with when/why/action guidance for each pattern",
    "Coverage integration performance characteristics well-documented with specific timing metrics",
    "Advanced features (DataFlowGraph, purity detection, call graph) thoroughly explained with use cases",
    "Strategic guidance for different prioritization approaches (unified score, risk category, debt type, ROI)",
    "Unified scoring system exceptionally well-documented: all three factors (complexity 40%, coverage 40%, dependency 20%), role multipliers, and priority classifications explained",
    "Role-based prioritization comprehensive: all six role types (Entry Points 1.5×, Business Logic 1.2×, Data Access 1.0×, Infrastructure 0.8×, Utilities 0.5×, Test Code 0.1×) with clear examples",
    "Coverage propagation explanation with concrete scenarios showing transitive coverage benefits for call graph analysis",
    "DataFlowGraph integration with unified scoring's dependency factor clearly explained",
    "Tiered prioritization section provides practical effort estimates and strategic guidance for each tier",
    "Categorized debt analysis with cross-category dependencies (Architecture blocks Testing) is comprehensive and actionable",
    "Debt density metric explanation includes formulas, interpretation guidelines, and CI/CD integration examples",
    "All four supported languages thoroughly documented: Rust (full support with syn parser, call graph, semantic classification), Python/JS/TS (partial support with specific limitations noted)",
    "God object detection with complexity weighting formula and examples showing why it reduces false positives",
    "Constructor detection documented with AST-based approach and name-based fallback strategy",
    "Extensibility guidance helps contributors understand architecture for adding new language analyzers",
    "Internal links to other chapters validated and working (output-formats.md, configuration.md)"
  ],
  "improvement_suggestions": [
    "Correct debt type count from 25 to 24 to match implementation (high priority)",
    "Replace all 'Medium' risk level references with 'Moderate' enum variant name (high priority)",
    "Clarify the two-tier system relationship (Tier vs RecommendationTier) with use case guidance (medium priority)",
    "Add concrete numeric thresholds for dependency factor (5+ callers = high, 2-4 = moderate, 0-1 = low)",
    "Include language-specific debt patterns subsection explaining which patterns apply to which languages",
    "Add est_branches field to JSON structure examples with formula explanation",
    "Clarify minimal_count field meaning and note it's specific to unified scoring, not a standard risk category",
    "Add cross-reference from scoring weights to configuration chapter",
    "Add guidance on when to skip coverage integration for faster iteration during development",
    "Make unified JSON format availability status more prominent (internal-only, not user-facing)",
    "Consider adding quick-start section at beginning for users wanting immediate hands-on experience",
    "Add flowchart showing decision tree for choosing analysis strategies (unified score vs. tiered vs. category-based)",
    "Include troubleshooting section addressing common interpretation mistakes",
    "Add more examples of analyzing multi-language projects (e.g., Rust backend + TypeScript frontend)",
    "Consider adding glossary of technical terms (entropy, cyclomatic, cognitive, dampening, transitive coverage)",
    "Include example CI/CD pipeline configurations for different CI systems (GitHub Actions, GitLab CI, Jenkins)",
    "Add section on interpreting results for different team roles (developers, tech leads, managers)",
    "Include references to academic papers or resources for readers wanting deeper understanding of metrics"
  ],
  "cross_references": [
    "output-formats.md",
    "configuration.md"
  ],
  "metadata": {
    "analyzed_at": "2025-11-13T07:20:00Z",
    "feature_inventory": ".prodigy/book-analysis/features.json",
    "topics_covered": [
      "Complexity metrics",
      "Debt patterns",
      "Risk scoring",
      "Interpreting results",
      "Unified scoring system (0-10 scale)",
      "Legacy risk scoring",
      "Tiered prioritization",
      "Entropy-based analysis",
      "Data flow analysis",
      "Coverage integration",
      "Language support (Rust, Python, JavaScript, TypeScript)",
      "God object detection",
      "Constructor detection",
      "Purity detection",
      "Call graph analysis"
    ],
    "validation_focus": "Ensure all analyzer types and metrics are explained",
    "validation_result": "PASS - All analyzer types thoroughly documented with capabilities and limitations clearly stated",
    "documentation_length": "2625 lines",
    "issue_count": 10,
    "high_severity_issues": 0,
    "medium_severity_issues": 2,
    "low_severity_issues": 8
  }
}
