<!DOCTYPE HTML>
<html lang="en" class="rust sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Debtmap Documentation</title>
        <meta name="robots" content="noindex">


        <!-- Custom HTML head -->

        <meta name="description" content="Comprehensive guide to Debtmap code complexity and technical debt analyzer">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="highlight-css" href="highlight.css">
        <link rel="stylesheet" id="tomorrow-night-css" href="tomorrow-night.css">
        <link rel="stylesheet" id="ayu-highlight-css" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->


        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "";
            const default_light_theme = "rust";
            const default_dark_theme = "navy";
            window.path_to_searchindex_js = "searchindex.js";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="toc.js"></script>
    </head>
    <body>
    <div id="mdbook-help-container">
        <div id="mdbook-help-popup">
            <h2 class="mdbook-help-title">Keyboard shortcuts</h2>
            <div>
                <p>Press <kbd>â†</kbd> or <kbd>â†’</kbd> to navigate between chapters</p>
                <p>Press <kbd>S</kbd> or <kbd>/</kbd> to search in the book</p>
                <p>Press <kbd>?</kbd> to show this help</p>
                <p>Press <kbd>Esc</kbd> to hide this help</p>
            </div>
        </div>
    </div>
    <div id="body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('rust')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            let sidebar = null;
            const sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
                sidebar_toggle.checked = false;
            }
            if (sidebar === 'visible') {
                sidebar_toggle.checked = true;
            } else {
                html.classList.remove('sidebar-visible');
            }
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="toc.html"></iframe>
            </noscript>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="default_theme">Auto</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search (`/`)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="/ s" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Debtmap Documentation</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://github.com/iepathos/debtmap" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <div class="search-wrapper">
                            <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                            <div class="spinner-wrapper">
                                <i class="fa fa-spinner fa-spin"></i>
                            </div>
                        </div>
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="introduction"><a class="header" href="#introduction">Introduction</a></h1>
<blockquote>
<p>ğŸš§ <strong>Early Prototype</strong> - This project is under active development and APIs may change</p>
</blockquote>
<p>Debtmap is a fast code complexity and technical debt analyzer written in Rust. Debtmap identifies which code to refactor for maximum cognitive debt reduction and which code to test for maximum risk reduction, providing data-driven prioritization for both.</p>
<h2 id="what-is-debtmap"><a class="header" href="#what-is-debtmap">What is Debtmap?</a></h2>
<p>Unlike traditional static analysis tools that simply flag complex code, Debtmap answers two critical questions:</p>
<ol>
<li><strong>â€œWhat should I refactor to reduce cognitive burden?â€</strong> - Identifies overly complex code that slows down development</li>
<li><strong>â€œWhat should I test first to reduce the most risk?â€</strong> - Pinpoints untested complex code that threatens stability</li>
</ol>
<p>Debtmap analyzes your codebase to identify complexity hotspots, technical debt patterns, and architectural risks. It supports Rust, Python, JavaScript, and TypeScript with full AST parsing and analysis capabilities. Rust includes additional advanced features like macro expansion and trait tracking.</p>
<p><strong>What Makes Debtmap Different:</strong></p>
<ul>
<li><strong>Entropy-Based Complexity Analysis</strong>: Uses information theory to distinguish genuinely complex code from pattern-based repetitive code, reducing false positives by up to 70%</li>
<li><strong>Coverage-Risk Correlation</strong>: The only tool that combines complexity metrics with test coverage to identify genuinely risky code (high complexity + low coverage = critical risk)</li>
<li><strong>Risk-Driven Prioritization</strong>: Prioritizes refactoring and testing efforts based on complexity, coverage, and dependency factors</li>
<li><strong>Actionable Guidance</strong>: Provides specific recommendations like â€œextract nested conditionsâ€ or â€œsplit this 80-line functionâ€ rather than just flagging issues</li>
<li><strong>Performance</strong>: 10-100x faster than Java/Python-based competitors (written in Rust with parallel processing)</li>
</ul>
<h2 id="why-use-debtmap"><a class="header" href="#why-use-debtmap">Why Use Debtmap?</a></h2>
<p>Debtmap helps you make data-driven decisions about where to focus your refactoring and testing efforts:</p>
<ul>
<li><strong>Identify Complexity</strong> - Find complex functions and modules that need refactoring, with concrete metrics showing which changes will have the most impact</li>
<li><strong>Detect Technical Debt</strong> - Discover 30+ debt patterns including code smells, security vulnerabilities, resource management issues, and architectural problems</li>
<li><strong>Assess Risk</strong> - Prioritize improvements based on sophisticated risk scoring that combines complexity, test coverage, and dependency impact</li>
<li><strong>Track Quality</strong> - Monitor code quality metrics over time with the <code>compare</code> command (which can use <code>--plan</code> to automatically extract target locations from implementation plans and track improvements) to verify that refactoring efforts achieved their goals</li>
<li><strong>Get Actionable Recommendations</strong> - Receive specific guidance like â€œrefactoring this will reduce complexity by 60%â€ or â€œtesting this will reduce risk by 5%â€</li>
<li><strong>Automated Debt Reduction</strong> - Integrates with Prodigy workflows for AI-driven automated refactoring with iterative validation and testing</li>
</ul>
<h2 id="key-features"><a class="header" href="#key-features">Key Features</a></h2>
<h3 id="analysis-capabilities"><a class="header" href="#analysis-capabilities">Analysis Capabilities</a></h3>
<ul>
<li><strong>Multi-language support</strong> - Full support for Rust, Python, JavaScript, and TypeScript with AST parsing, complexity analysis, and debt detection</li>
<li><strong>Entropy-based complexity analysis</strong> - Distinguishes between genuinely complex code and pattern-based repetitive code using information theory</li>
<li><strong>Token classification system</strong> - Advanced token categorization with weighted entropy for accurate complexity assessment</li>
<li><strong>Threshold presets</strong> - Quick setup with strict, balanced (default), or lenient presets matching different project types and quality standards</li>
<li><strong>Comprehensive debt detection</strong> - Identifies 30+ technical debt patterns across security (5 types), code organization (god objects, feature envy, magic values), resource management (5 types), testing quality (3 types), and error handling (4 types)</li>
<li><strong>Security vulnerability detection</strong> - Finds hardcoded secrets, weak crypto, SQL injection risks, and unsafe code patterns</li>
<li><strong>Resource management analysis</strong> - Identifies inefficient allocations, nested loops, and blocking I/O patterns</li>
<li><strong>Code organization analysis</strong> - Detects god objects, feature envy, primitive obsession, and magic values</li>
<li><strong>Testing quality assessment</strong> - Analyzes test complexity, flaky patterns, and assertion quality</li>
<li><strong>File-level aggregation</strong> - Multiple aggregation methods (sum, weighted, logarithmic) for identifying files needing organizational refactoring</li>
<li><strong>Context-aware analysis</strong> - Reduces false positives through intelligent context detection (enabled by default)</li>
</ul>
<h3 id="risk-analysis--prioritization"><a class="header" href="#risk-analysis--prioritization">Risk Analysis &amp; Prioritization</a></h3>
<ul>
<li><strong>Coverage-based risk analysis</strong> - Correlates complexity with test coverage to identify truly risky code</li>
<li><strong>Risk-driven testing recommendations</strong> - Prioritizes testing efforts based on complexity-coverage correlation and dependency impact</li>
<li><strong>Call graph analysis</strong> - Tracks upstream callers and downstream callees to understand dependency impact</li>
<li><strong>Tiered prioritization</strong> - Surfaces critical architectural issues above simple testing gaps</li>
<li><strong>Quantified impact</strong> - Shows concrete metrics like â€œrefactoring this will reduce complexity by 60%â€</li>
</ul>
<h3 id="performance--output"><a class="header" href="#performance--output">Performance &amp; Output</a></h3>
<ul>
<li><strong>Parallel processing</strong> - Built with Rust and Rayon for blazing-fast analysis of large codebases</li>
<li><strong>Multiple output formats</strong> - JSON (legacy and unified structures), Markdown, and human-readable terminal formats for different tool integration needs</li>
<li><strong>Configurable thresholds</strong> - Customize complexity and duplication thresholds to match your standards</li>
<li><strong>Incremental analysis</strong> - Smart caching system for analyzing only changed files</li>
<li><strong>Intelligent caching</strong> - Smart cache system with automatic pruning, configurable strategies (LRU, LFU, FIFO), location options (local/shared), and environment-based configuration for fast repeated analysis</li>
<li><strong>Verbosity controls</strong> - Multiple verbosity levels (-v, -vv, -vvv) for progressive detail</li>
</ul>
<h3 id="configuration--customization"><a class="header" href="#configuration--customization">Configuration &amp; Customization</a></h3>
<ul>
<li><strong>Flexible suppression</strong> - Inline comment-based suppression for specific code sections</li>
<li><strong>Configuration file</strong> - <code>.debtmap.toml</code>, <code>.debtmap.yml</code>, or <code>.debtmap.json</code> for project-specific settings</li>
<li><strong>Test-friendly</strong> - Easily exclude test fixtures and example code from debt analysis</li>
<li><strong>Macro expansion support</strong> - Handles Rust macro expansions with configurable warnings</li>
</ul>
<h3 id="commands"><a class="header" href="#commands">Commands</a></h3>
<ul>
<li><strong><code>analyze</code></strong> - Comprehensive debt analysis with unified prioritization</li>
<li><strong><code>validate</code></strong> - Enforce quality thresholds in CI/CD pipelines</li>
<li><strong><code>compare</code></strong> - Track improvements over time and verify refactoring goals</li>
<li><strong><code>init</code></strong> - Generate configuration file with sensible defaults (â€“force to overwrite)</li>
</ul>
<h2 id="target-audience"><a class="header" href="#target-audience">Target Audience</a></h2>
<p>Debtmap is designed for:</p>
<ul>
<li><strong>Development teams</strong> - Get concrete metrics for planning sprints. Know exactly which refactoring will reduce complexity by 60% or which function needs 6 unit tests for full coverage.</li>
<li><strong>Engineering managers</strong> - Track quality trends over time with the <code>compare</code> command. Monitor whether refactoring efforts are actually improving codebase health.</li>
<li><strong>Code reviewers</strong> - Focus reviews on high-risk areas identified by Debtmap. Prioritize reviewing untested complex code over simple utility functions.</li>
<li><strong>Developers refactoring legacy codebases</strong> - Receive actionable guidance like â€œextract nested conditionsâ€, â€œsplit this 80-line function into 3 smaller functionsâ€, or â€œadd error handling for this catch blockâ€.</li>
</ul>
<h2 id="getting-started"><a class="header" href="#getting-started">Getting Started</a></h2>
<p>Ready to analyze your codebase? Check out:</p>
<ul>
<li><a href="./installation.html">Installation</a> - Installing Debtmap on your system</li>
<li><a href="./getting-started.html">Getting Started</a> - Installation and first analysis</li>
<li><a href="./analysis-guide.html">Analysis Guide</a> - Understanding the metrics and output</li>
<li><a href="./output-formats.html">Output Formats</a> - JSON, Markdown, and terminal formats</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="why-debtmap"><a class="header" href="#why-debtmap">Why Debtmap?</a></h1>
<p>Technical debt analysis tools are everywhere. So why another one? Debtmap takes a fundamentally different approach to code quality analysisâ€”one that reduces false positives and gives you actionable insights instead of just flagging â€œcomplexâ€ code.</p>
<h2 id="the-problem-with-traditional-static-analysis"><a class="header" href="#the-problem-with-traditional-static-analysis">The Problem with Traditional Static Analysis</a></h2>
<p>Most static analysis tools flag code as â€œcomplexâ€ based purely on metrics like cyclomatic complexity or lines of code. The problem? Not all complexity is equal.</p>
<p>Consider this common pattern:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn validate_config(config: &amp;Config) -&gt; Result&lt;()&gt; {
    if config.output_dir.is_none() {
        return Err(anyhow!("output_dir required"))
    }
    if config.max_workers.is_none() {
        return Err(anyhow!("max_workers required"))
    }
    if config.timeout_secs.is_none() {
        return Err(anyhow!("timeout_secs required"))
    }
    if config.log_level.is_none() {
        return Err(anyhow!("log_level required"))
    }
    if config.cache_dir.is_none() {
        return Err(anyhow!("cache_dir required"))
    }
    // ... 15 more similar checks
    Ok(())
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Traditional tools say:</strong> â€œCyclomatic complexity: 20 - CRITICAL! Refactor immediately!â€</p>
<p><strong>Reality:</strong> This is a simple validation function with a repetitive pattern. Yes, it has 20 branches, but theyâ€™re all identical in structure. An experienced developer can read and understand this in seconds.</p>
<p>Now compare with this function:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn reconcile_state(current: &amp;State, desired: &amp;State) -&gt; Vec&lt;Action&gt; {
    let mut actions = vec![];

    match (current.mode, desired.mode) {
        (Mode::Active, Mode::Standby) =&gt; {
            if current.has_active_connections() {
                actions.push(Action::DrainConnections);
                actions.push(Action::WaitForDrain);
            }
            actions.push(Action::TransitionToStandby);
        }
        (Mode::Standby, Mode::Active) =&gt; {
            if desired.requires_warmup() {
                actions.push(Action::Warmup);
            }
            actions.push(Action::TransitionToActive);
        }
        (Mode::Active, Mode::Maintenance) =&gt; {
            // Complex state transitions based on multiple conditions
            if current.has_pending_operations() {
                if desired.force_maintenance {
                    actions.push(Action::AbortPending);
                } else {
                    actions.push(Action::FinishPending);
                }
            }
            actions.push(Action::TransitionToMaintenance);
        }
        // ... more complex state transitions
        _ =&gt; {}
    }

    actions
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Traditional tools say:</strong> â€œCyclomatic complexity: 8 - moderateâ€</p>
<p><strong>Reality:</strong> This function involves complex state machine logic with conditional transitions, side effects, and non-obvious control flow. Itâ€™s genuinely complex and error-prone.</p>
<p><strong>The key insight:</strong> Traditional metrics treat both functions equally, but theyâ€™re fundamentally different in terms of cognitive load and risk.</p>
<h2 id="debtmaps-unique-approach"><a class="header" href="#debtmaps-unique-approach">Debtmapâ€™s Unique Approach</a></h2>
<h3 id="1-entropy-based-complexity-analysis"><a class="header" href="#1-entropy-based-complexity-analysis">1. Entropy-Based Complexity Analysis</a></h3>
<p>Debtmap uses information theory to distinguish between genuinely complex code and repetitive pattern-based code.</p>
<p><strong>How it works:</strong></p>
<ul>
<li>Calculate the <strong>variety</strong> of code patterns in a function</li>
<li>High variety (many different patterns) = high entropy = genuinely complex</li>
<li>Low variety (repetitive patterns) = low entropy = simple despite high branch count</li>
</ul>
<p><strong>Applied to our examples:</strong></p>
<pre><code>validate_config():
- Cyclomatic complexity: 20
- Pattern entropy: 0.3 (low - all branches identical)
- Entropy-adjusted complexity: 5
- Assessment: Low risk despite high branch count

reconcile_state():
- Cyclomatic complexity: 8
- Pattern entropy: 0.85 (high - diverse conditional logic)
- Entropy-adjusted complexity: 9
- Assessment: High risk - genuinely complex logic
</code></pre>
<p>This approach <strong>significantly reduces false positives</strong> compared to traditional cyclomatic complexity metrics by recognizing that repetitive patterns are easier to understand than diverse, complex logic.</p>
<h3 id="2-coverage-risk-correlation"><a class="header" href="#2-coverage-risk-correlation">2. Coverage-Risk Correlation</a></h3>
<p>Debtmap is the only Rust analysis tool that natively combines code complexity with test coverage to compute risk scores.</p>
<p><strong>Why this matters:</strong></p>
<ul>
<li>Complex code with good tests = managed risk</li>
<li>Simple code without tests = unmanaged risk (but low priority)</li>
<li>Complex code without tests = CRITICAL gap</li>
</ul>
<p><strong>Example:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Function A: Complex but well-tested
fn parse_query(sql: &amp;str) -&gt; Result&lt;Query&gt; {
    // Complexity: 15, Coverage: 95%
    // Risk Score: 3.2 (moderate - complexity managed by tests)
}

// Function B: Moderate complexity, no tests
fn apply_migrations(db: &amp;mut Database) -&gt; Result&lt;()&gt; {
    // Complexity: 8, Coverage: 0%
    // Risk Score: 8.9 (critical - untested with moderate complexity)
}
<span class="boring">}</span></code></pre></pre>
<p>Debtmap integrates with LCOV coverage data to automatically prioritize Function B over Function A, even though A is more complex. This is because the risk is about <strong>untested complexity</strong>, not just complexity alone.</p>
<p><strong>What makes this unique:</strong></p>
<p>Debtmap is the only Rust-focused tool that natively combines complexity analysis with LCOV coverage data to compute risk scores. While other tools support coverage reporting, they donâ€™t correlate it with complexity metrics to prioritize technical debt and testing efforts.</p>
<h3 id="3-actionable-recommendations"><a class="header" href="#3-actionable-recommendations">3. Actionable Recommendations</a></h3>
<p>Most tools tell you <strong>what</strong> is wrong. Debtmap tells you <strong>what to do about it</strong> and <strong>what impact it will have</strong>.</p>
<p><strong>Compare:</strong></p>
<p><strong>SonarQube:</strong></p>
<pre><code>Function 'process_request' has complexity 15 (threshold: 10)
Severity: Major
</code></pre>
<p><strong>Debtmap:</strong></p>
<pre><code>#1 SCORE: 8.9 [CRITICAL]
â”œâ”€ TEST GAP: ./src/handlers.rs:127 process_request()
â”œâ”€ ACTION: Add 8 unit tests for full coverage
â”œâ”€ IMPACT: -5.2 risk reduction
â”œâ”€ WHY: Complex logic (cyclo=15) with 0% coverage
â””â”€ SUGGEST: Extract validation to separate functions, test each independently
</code></pre>
<p>Debtmap tells you:</p>
<ul>
<li><strong>Specific location</strong> (file:line)</li>
<li><strong>Quantified gap</strong> (8 missing tests)</li>
<li><strong>Expected impact</strong> (-5.2 risk reduction)</li>
<li><strong>Rationale</strong> (complexity + no coverage)</li>
<li><strong>Refactoring suggestions</strong> (extract functions)</li>
</ul>
<h3 id="4-context-aware-analysis"><a class="header" href="#4-context-aware-analysis">4. Context-Aware Analysis</a></h3>
<p>Debtmap understands that not all code needs the same level of scrutiny.</p>
<p><strong>Entry Points:</strong> Main functions, CLI handlers, and framework integration points are typically tested via integration tests, not unit tests. Debtmapâ€™s analysis accounts for this:</p>
<pre><pre class="playground"><code class="language-rust">// Entry point - flagged as low priority for unit test coverage
fn main() {
    // Debtmap: "Integration test coverage expected - low priority"
}

// Core business logic - flagged as high priority
fn calculate_risk_score(metrics: &amp;Metrics) -&gt; f64 {
    // Debtmap: "High complexity + low coverage = CRITICAL"
}</code></pre></pre>
<p><strong>Call Graph Analysis:</strong> Debtmap traces function dependencies to prioritize functions called by many untested paths:</p>
<pre><code>parse_input() [untested]
  â”œâ”€ called by: main() [integration tested]
  â””â”€ called by: process_batch() [untested]

Priority: HIGH (called from untested code path)
</code></pre>
<h3 id="5-performance"><a class="header" href="#5-performance">5. Performance</a></h3>
<p>Debtmap is written in Rust and uses parallel processing for analysis. Being a native Rust binary with no JVM overhead, itâ€™s designed for fast local development workflow integration.</p>
<p><strong>Typical analysis time:</strong></p>
<ul>
<li>Small project (~10k LOC): 1-2 seconds</li>
<li>Medium project (~50k LOC): 5-8 seconds</li>
<li>Large project (~200k LOC): 20-30 seconds</li>
</ul>
<p>This speed means you can run debtmap in your local development workflow without breaking flow, not just in CI.</p>
<h2 id="what-problem-does-debtmap-solve"><a class="header" href="#what-problem-does-debtmap-solve">What Problem Does Debtmap Solve?</a></h2>
<p>Debtmap addresses a gap that existing tools donâ€™t fill: <strong>quantified technical debt prioritization with actionable refactoring guidance</strong>.</p>
<h3 id="the-gap-in-existing-tools"><a class="header" href="#the-gap-in-existing-tools">The Gap in Existing Tools</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Tool Type</th><th>What It Does</th><th>What It Doesnâ€™t Do</th></tr></thead><tbody>
<tr><td><strong>Linters</strong> (clippy, ESLint)</td><td>Find code style issues and common mistakes</td><td>Donâ€™t quantify risk or prioritize by impact</td></tr>
<tr><td><strong>Complexity Analyzers</strong> (SonarQube, CodeClimate)</td><td>Flag complex code</td><td>Donâ€™t correlate with test coverage or provide refactoring impact estimates</td></tr>
<tr><td><strong>Coverage Tools</strong> (tarpaulin, codecov)</td><td>Show what code is tested</td><td>Donâ€™t identify which untested code is most risky</td></tr>
</tbody></table>
</div>
<p><strong>Note:</strong> Debtmap is not a security scanner. Use tools like <code>cargo-audit</code> and <code>cargo-geiger</code> for security vulnerability detection. Debtmap focuses on technical debt prioritization, though complex untested code can sometimes harbor security issues.</p>
<p><strong>What Debtmap uniquely provides:</strong></p>
<ol>
<li><strong>Quantified Debt Scoring</strong> - Not just â€œthis is complex,â€ but â€œthis scores 8.9/10 on riskâ€</li>
<li><strong>Coverage-Risk Correlation</strong> - Identifies untested complex code, not just complex code</li>
<li><strong>Impact Quantification</strong> - â€œAdding 6 tests will reduce risk by 3.7 pointsâ€</li>
<li><strong>Actionable Recommendations</strong> - Specific refactoring suggestions with effort estimates</li>
<li><strong>Dependency-Aware Prioritization</strong> - Prioritizes code that impacts many other functions</li>
</ol>
<h3 id="debtmap-vs-traditional-tools"><a class="header" href="#debtmap-vs-traditional-tools">Debtmap vs Traditional Tools</a></h3>
<p><strong>SonarQube / CodeClimate:</strong></p>
<ul>
<li><strong>They say:</strong> â€œFunction has complexity 15 (threshold exceeded)â€</li>
<li><strong>Debtmap says:</strong> â€œAdd 8 tests (-5.2 risk). Extract validation logic to reduce complexity by 60%â€</li>
</ul>
<p><strong>Coverage Tools (tarpaulin, codecov):</strong></p>
<ul>
<li><strong>They say:</strong> â€œ67% line coverage, 54% branch coverageâ€</li>
<li><strong>Debtmap says:</strong> â€œ3 critical gaps: untested complex functions that are called from 12+ code pathsâ€</li>
</ul>
<p><strong>Linters (clippy):</strong></p>
<ul>
<li><strong>They say:</strong> â€œConsider using Iterator::any() instead of a for loopâ€</li>
<li><strong>Debtmap says:</strong> â€œThis function has high cognitive complexity (12) and is called by 8 untested modules - prioritize adding tests before refactoringâ€</li>
</ul>
<h3 id="when-to-use-debtmap"><a class="header" href="#when-to-use-debtmap">When to Use Debtmap</a></h3>
<p><strong>Use Debtmap when you need to:</strong></p>
<ul>
<li>Decide which technical debt to tackle first (limited time/resources)</li>
<li>Identify critical testing gaps (high-complexity, zero-coverage code)</li>
<li>Quantify the impact of refactoring efforts</li>
<li>Reduce false positives from repetitive validation code</li>
<li>Prioritize refactoring based on risk, not just complexity</li>
<li>Get specific, actionable recommendations with effort estimates</li>
</ul>
<p><strong>Use other tools for different needs:</strong></p>
<ul>
<li><strong>clippy</strong> - Catch Rust idiom violations and common mistakes</li>
<li><strong>tarpaulin</strong> - Generate LCOV coverage data (Debtmap analyzes it)</li>
<li><strong>SonarQube</strong> - Multi-language analysis with centralized dashboards</li>
</ul>
<p><strong>Security is a separate concern:</strong></p>
<ul>
<li><strong>cargo-audit</strong> - Find known vulnerabilities in dependencies</li>
<li><strong>cargo-geiger</strong> - Detect unsafe code usage</li>
<li>Debtmap doesnâ€™t scan for security issues, though complex code may harbor security risks</li>
</ul>
<h3 id="recommended-workflow"><a class="header" href="#recommended-workflow">Recommended Workflow</a></h3>
<p>Debtmap works <strong>alongside</strong> existing tools, not instead of them:</p>
<pre><code class="language-bash"># 1. Local development loop (before commit)
cargo fmt                    # Format code
cargo clippy                 # Check idioms and common issues
cargo test                   # Run tests
debtmap analyze .            # Identify new technical debt

# 2. CI/CD pipeline (PR validation)
cargo test --all-features    # Full test suite
cargo clippy -- -D warnings  # Fail on warnings
debtmap validate .           # Enforce debt thresholds

# 3. Weekly planning (prioritize work)
cargo tarpaulin --out lcov   # Generate coverage
debtmap analyze . --lcov lcov.info --top 20
# Review top 20 debt items, plan sprint work

# 4. Monthly review (track trends)
debtmap analyze . --format json --output debt-$(date +%Y%m).json
debtmap compare --before debt-202410.json --after debt-202411.json
</code></pre>
<h3 id="the-bottom-line"><a class="header" href="#the-bottom-line">The Bottom Line</a></h3>
<p><strong>Debtmap isnâ€™t a replacement for linters or coverage tools.</strong> It solves a different problem: turning raw complexity and coverage data into <strong>prioritized, actionable technical debt recommendations</strong>.</p>
<p>If youâ€™re asking â€œWhere should I focus my refactoring efforts?â€ or â€œWhich code needs tests most urgently?â€, thatâ€™s what Debtmap is built for.</p>
<h2 id="key-differentiators"><a class="header" href="#key-differentiators">Key Differentiators</a></h2>
<ol>
<li><strong>Entropy analysis</strong> - Reduces false positives from repetitive code</li>
<li><strong>Native coverage integration</strong> - Built-in LCOV support for risk scoring</li>
<li><strong>Actionable recommendations</strong> - Specific steps with quantified impact</li>
<li><strong>Context-aware</strong> - Understands entry points, call graphs, and testing patterns</li>
<li><strong>Fast</strong> - Rust performance for local development workflow</li>
<li><strong>Tiered prioritization</strong> - Critical/High/Moderate/Low classification with clear rationale</li>
</ol>
<h2 id="next-steps"><a class="header" href="#next-steps">Next Steps</a></h2>
<p>Ready to try it? Head to <a href="getting-started.html">Getting Started</a> to install debtmap and run your first analysis.</p>
<p>Want to understand how it works under the hood? See <a href="architecture.html">Architecture</a> for the analysis pipeline.</p>
<p>Have questions? Check the <a href="faq.html">FAQ</a> for common questions and answers.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="getting-started-1"><a class="header" href="#getting-started-1">Getting Started</a></h1>
<p>This guide will help you install Debtmap and run your first analysis in just a few minutes.</p>
<h2 id="prerequisites"><a class="header" href="#prerequisites">Prerequisites</a></h2>
<p>Before installing Debtmap, youâ€™ll need:</p>
<ul>
<li><strong>For pre-built binaries</strong>: No prerequisites! The install script handles everything.</li>
<li><strong>For cargo install or building from source</strong>:
<ul>
<li>Rust toolchain (rustc and cargo)</li>
<li>Supported platforms: Linux, macOS, Windows</li>
<li>Rust edition 2021 or later</li>
</ul>
</li>
</ul>
<p><strong>Optional</strong> (for coverage-based risk analysis):</p>
<ul>
<li><strong>Rust projects</strong>: <code>cargo-tarpaulin</code> for coverage data</li>
<li><strong>JavaScript/TypeScript</strong>: Jest or other tools generating LCOV format</li>
<li><strong>Python</strong>: pytest with coverage plugin</li>
</ul>
<h2 id="installation"><a class="header" href="#installation">Installation</a></h2>
<h3 id="quick-install-recommended"><a class="header" href="#quick-install-recommended">Quick Install (Recommended)</a></h3>
<p>Install the latest release with a single command:</p>
<pre><code class="language-bash">curl -sSL https://raw.githubusercontent.com/iepathos/debtmap/master/install.sh | bash
</code></pre>
<p>Or with wget:</p>
<pre><code class="language-bash">wget -qO- https://raw.githubusercontent.com/iepathos/debtmap/master/install.sh | bash
</code></pre>
<p>This will:</p>
<ul>
<li>Automatically detect your OS and architecture</li>
<li>Download the appropriate pre-built binary from the latest GitHub release</li>
<li>Install debtmap to <code>~/.cargo/bin</code> if it exists, otherwise <code>~/.local/bin</code></li>
<li>Offer to automatically add the install directory to your PATH if needed</li>
</ul>
<h3 id="using-cargo"><a class="header" href="#using-cargo">Using Cargo</a></h3>
<p>If you have Rust installed:</p>
<pre><code class="language-bash">cargo install debtmap
</code></pre>
<h3 id="from-source"><a class="header" href="#from-source">From Source</a></h3>
<p>For the latest development version:</p>
<pre><code class="language-bash"># Clone the repository
git clone https://github.com/iepathos/debtmap.git
cd debtmap

# Build and install
cargo install --path .
</code></pre>
<h3 id="verify-installation"><a class="header" href="#verify-installation">Verify Installation</a></h3>
<p>After installation, verify Debtmap is working:</p>
<pre><code class="language-bash"># Check version
debtmap --version

# See available commands
debtmap --help
</code></pre>
<p><strong>Common installation issues:</strong></p>
<ul>
<li><strong>Binary not in PATH</strong>: Add <code>~/.cargo/bin</code> or <code>~/.local/bin</code> to your PATH
<pre><code class="language-bash">export PATH="$HOME/.cargo/bin:$PATH"  # Add to ~/.bashrc or ~/.zshrc
</code></pre>
</li>
<li><strong>Permission issues</strong>: Run the install script with your current user (donâ€™t use sudo)</li>
<li><strong>Cargo not found</strong>: Install Rust from https://rustup.rs</li>
</ul>
<h2 id="quick-start"><a class="header" href="#quick-start">Quick Start</a></h2>
<p>Here are the most common commands to get you started:</p>
<pre><code class="language-bash"># Analyze current directory (simplest command)
debtmap analyze .

# Analyze with coverage data for risk scoring (recommended)
# Note: --lcov is a shorthand alias for --coverage-file
debtmap analyze . --lcov target/coverage/lcov.info

# Generate coverage first (for Rust projects)
cargo tarpaulin --out lcov --output-dir target/coverage
debtmap analyze . --lcov target/coverage/lcov.info

# Analyze with custom thresholds
# Note: threshold-duplication specifies minimum lines of duplicated code to detect
debtmap analyze ./src --threshold-complexity 15 --threshold-duplication 50

# Output as JSON (for CI/CD integration)
debtmap analyze ./src --format json --output report.json

# Show only top 10 high-priority issues
debtmap analyze . --top 10

# Initialize configuration file for project-specific settings
debtmap init

# Validate against thresholds (CI/CD integration)
debtmap validate ./src --max-debt-density 5.0

# Compare before/after to track improvements
debtmap analyze . --format json --output before.json
# ... make improvements ...
debtmap analyze . --format json --output after.json
debtmap compare --before before.json --after after.json

# Advanced comparison: focus on specific function
debtmap compare --before before.json --after after.json --target-location src/main.rs:main:10

# Extract target from implementation plan
debtmap compare --before before.json --after after.json --plan IMPLEMENTATION_PLAN.md
</code></pre>
<h3 id="advanced-options"><a class="header" href="#advanced-options">Advanced Options</a></h3>
<p>Debtmap provides many powerful options to customize your analysis:</p>
<p><strong>Verbosity Levels:</strong></p>
<pre><code class="language-bash"># Show main factors contributing to scores
debtmap analyze . -v

# Show detailed calculations
debtmap analyze . -vv

# Show all debug information
debtmap analyze . -vvv
</code></pre>
<p><strong>Filtering and Prioritization:</strong></p>
<pre><code class="language-bash"># Only show high-priority items
debtmap analyze . --min-priority high

# Filter by specific categories
debtmap analyze . --filter Architecture,Testing

# Group results by debt category
debtmap analyze . --group-by-category
</code></pre>
<p><strong>Cache Management:</strong></p>
<pre><code class="language-bash"># Skip cache for fresh analysis
debtmap analyze . --no-cache

# Clear cache and rebuild
debtmap analyze . --clear-cache

# View cache statistics
debtmap analyze . --cache-stats

# Specify custom cache location
debtmap analyze . --cache-location /custom/path

# Migrate cache from local to shared location
debtmap analyze . --migrate-cache
</code></pre>
<p><strong>Performance Control:</strong></p>
<pre><code class="language-bash"># Limit parallel jobs
debtmap analyze . --jobs 4

# Disable parallel processing
debtmap analyze . --no-parallel
</code></pre>
<p><strong>Output Control:</strong></p>
<pre><code class="language-bash"># Plain output (no colors/emoji, for CI/CD)
debtmap analyze . --plain

# Compact summary output
debtmap analyze . --summary

# Control aggregation behavior
debtmap analyze . --aggregate-only          # Show only aggregated results
debtmap analyze . --no-aggregation          # Skip aggregation entirely
debtmap analyze . --aggregation-method sum  # Choose aggregation method

# Adjust detail level in output
debtmap analyze . --detail-level high       # More detailed output
</code></pre>
<p><strong>Expert Options:</strong></p>
<p>These advanced options are available for power users and specialized use cases:</p>
<pre><code class="language-bash"># Analysis behavior
--semantic-off              # Disable semantic analysis
--no-context-aware          # Disable context-aware analysis
--multi-pass                # Enable multi-pass analysis for deeper insights
--validate-loc              # Validate lines of code calculations

# Rust-specific options
--verbose-macro-warnings    # Show detailed macro expansion warnings
--show-macro-stats          # Display macro usage statistics

# Filtering and thresholds
--threshold-preset &lt;name&gt;   # Use predefined threshold preset
--min-problematic &lt;count&gt;   # Minimum problematic items to report
--max-files &lt;count&gt;         # Limit analysis to N files
--no-god-object             # Disable god object detection

# Advanced reporting
--attribution               # Include code attribution information
</code></pre>
<p>For detailed documentation of these options, run <code>debtmap analyze --help</code>.</p>
<h2 id="first-analysis"><a class="header" href="#first-analysis">First Analysis</a></h2>
<p>Letâ€™s run your first analysis! Navigate to a project directory and run:</p>
<pre><code class="language-bash">debtmap analyze .
</code></pre>
<p><strong>What happens during analysis:</strong></p>
<ol>
<li><strong>File Discovery</strong> - Debtmap scans your project for supported source files (Rust, Python, JavaScript, TypeScript)</li>
<li><strong>Parsing</strong> - Each file is parsed into an Abstract Syntax Tree (AST)</li>
<li><strong>Metrics Calculation</strong> - Complexity, debt patterns, and risk scores are computed</li>
<li><strong>Prioritization</strong> - Results are ranked by priority (CRITICAL, HIGH, MEDIUM, LOW)</li>
<li><strong>Output</strong> - Results are displayed in your chosen format</li>
</ol>
<p><strong>Expected timing</strong>: Analyzing a 10,000 LOC project typically takes 2-5 seconds. The first run may be slightly slower as Debtmap builds its cache.</p>
<p><strong>About Caching:</strong>
Debtmap caches parsed ASTs and computed metrics to speed up subsequent analyses:</p>
<ul>
<li><strong>Cache location</strong>: <code>XDG_CACHE_HOME/debtmap</code> on Linux, <code>~/Library/Caches/debtmap</code> on macOS, <code>%LOCALAPPDATA%/debtmap</code> on Windows</li>
<li><strong>Whatâ€™s cached</strong>: Parsed ASTs and computed metrics for each file</li>
<li><strong>Invalidation</strong>: Cache is automatically invalidated when files are modified</li>
</ul>
<h3 id="example-output"><a class="header" href="#example-output">Example Output</a></h3>
<p>When you run <code>debtmap analyze .</code>, youâ€™ll see output like this:</p>
<pre><code>â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    PRIORITY TECHNICAL DEBT FIXES
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“Š ANALYSIS SUMMARY
   Files analyzed: 47
   Total functions: 234
   Debt items found: 12
   Analysis time: 3.2s

ğŸ¯ TOP 5 RECOMMENDATIONS

#1 SCORE: 8.9 [CRITICAL]
â”œâ”€ TEST GAP: ./src/parser.rs:38 parse_input()
â”œâ”€ ACTION: Add 6 unit tests for full coverage
â”œâ”€ IMPACT: -3.7 risk reduction
â”œâ”€ WHY: Complex logic (cyclo=12) with 0% coverage
â””â”€ DETAILS: Function has 12 decision points with no test coverage.
            Untested error paths could cause production failures.

#2 SCORE: 7.2 [HIGH]
â”œâ”€ TEST GAP: ./src/analyzer/complexity.rs:145 calculate_entropy()
â”œâ”€ ACTION: Add 4 unit tests for edge cases
â”œâ”€ IMPACT: -2.8 risk reduction
â”œâ”€ WHY: Medium complexity (cyclo=8) with 35% coverage
â””â”€ DETAILS: Missing tests for error conditions and boundary cases.

#3 SCORE: 5.8 [HIGH]
â”œâ”€ COMPLEXITY: ./src/io/formatter.rs:89 format_output()
â”œâ”€ ACTION: Extract 3 functions from 16 branches
â”œâ”€ IMPACT: -2.1 risk reduction
â”œâ”€ WHY: High complexity (cyclo=16, entropy=0.75)
â””â”€ DETAILS: Function handles multiple output formats with nested conditionals.
            Consider extracting format-specific logic.

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
</code></pre>
<p>With coverage integration (<code>--lcov</code>), youâ€™ll see more detailed test gap analysis:</p>
<pre><code class="language-bash">debtmap analyze . --lcov target/coverage/lcov.info --top 3
</code></pre>
<pre><code>â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    PRIORITY TECHNICAL DEBT FIXES
    (Coverage-Integrated Analysis)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“Š ANALYSIS SUMMARY
   Files analyzed: 47
   Coverage: 68.5% lines, 54.2% branches
   Critical gaps: 3
   High-priority gaps: 7

ğŸ¯ TOP 3 CRITICAL GAPS

#1 SCORE: 9.1 [CRITICAL]
â”œâ”€ TEST GAP: ./src/core/state_machine.rs:67 transition_state()
â”œâ”€ COVERAGE: 0% (0/8 branches covered)
â”œâ”€ COMPLEXITY: 15 (high entropy: 0.82)
â”œâ”€ ACTION: Add 8 unit tests covering all state transitions
â”œâ”€ IMPACT: -4.5 risk reduction
â”œâ”€ WHY: Core business logic with complex branching, completely untested
â””â”€ CALL GRAPH: Called from 12 different code paths
                5 of those callers are also untested

#2 SCORE: 8.4 [CRITICAL]
â”œâ”€ TEST GAP: ./src/parser.rs:38 parse_input()
â”œâ”€ COVERAGE: 25% (2/8 branches covered)
â”œâ”€ COMPLEXITY: 12 (medium entropy: 0.61)
â”œâ”€ ACTION: Add 6 tests for untested error paths
â”œâ”€ IMPACT: -3.2 risk reduction
â””â”€ MISSING BRANCHES:
    â€¢ Line 45: Error path for invalid syntax
    â€¢ Line 52: Error path for unexpected EOF
    â€¢ Line 58: Error path for malformed input
    â€¢ Line 63: Edge case: empty input
    â€¢ Line 68: Edge case: very large input
    â€¢ Line 72: Edge case: special characters

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
</code></pre>
<p><strong>Reading the output:</strong></p>
<ul>
<li><strong>Score</strong>: Risk score from 0-10 (higher = more urgent)</li>
<li><strong>Tier</strong>: CRITICAL (8.0+), HIGH (5.0-7.9), MODERATE (2.0-4.9), LOW (&lt;2.0)</li>
<li><strong>Location</strong>: File path, line number, and function name</li>
<li><strong>Action</strong>: Specific steps to address the issue</li>
<li><strong>Impact</strong>: Estimated risk reduction from taking action</li>
<li><strong>Why</strong>: Rationale explaining why this is flagged</li>
<li><strong>Management</strong>: Use <code>--clear-cache</code> to clear, <code>--no-cache</code> to skip, or <code>--cache-stats</code> to view statistics</li>
</ul>
<p><strong>Language support</strong>:</p>
<ul>
<li><strong>Rust</strong>: Full support with advanced features (trait detection, purity analysis, call graphs)</li>
<li><strong>Python</strong>: Partial support (complexity metrics, basic debt detection)</li>
<li><strong>JavaScript/TypeScript</strong>: Partial support (complexity metrics, basic debt detection)</li>
</ul>
<h3 id="example-output-1"><a class="header" href="#example-output-1">Example Output</a></h3>
<p>When you run <code>debtmap analyze .</code>, youâ€™ll see output like this:</p>
<pre><code>â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    PRIORITY TECHNICAL DEBT FIXES
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ¯ TOP 3 RECOMMENDATIONS (by unified priority)

#1 SCORE: 8.9 [CRITICAL]
â”œâ”€ TEST GAP: ./src/analyzers/rust_call_graph.rs:38 add_function_to_graph()
â”œâ”€ ACTION: Add 6 unit tests for full coverage
â”œâ”€ IMPACT: Full test coverage, -3.7 risk
â”œâ”€ COMPLEXITY: cyclomatic=6, branches=6, cognitive=8, nesting=2, lines=32
â”œâ”€ DEPENDENCIES: 0 upstream, 11 downstream
â””â”€ WHY: Business logic with 0% coverage, manageable complexity (cyclo=6, cog=8)

#2 SCORE: 8.9 [CRITICAL]
â”œâ”€ TEST GAP: ./src/debt/smells.rs:196 detect_data_clumps()
â”œâ”€ ACTION: Add 5 unit tests for full coverage
â”œâ”€ IMPACT: Full test coverage, -3.7 risk
â”œâ”€ COMPLEXITY: cyclomatic=5, branches=5, cognitive=11, nesting=5, lines=31
â”œâ”€ DEPENDENCIES: 0 upstream, 4 downstream
â””â”€ WHY: Business logic with 0% coverage, manageable complexity (cyclo=5, cog=11)

#3 SCORE: 8.6 [CRITICAL]
â”œâ”€ TEST GAP: ./src/risk/context/dependency.rs:247 explain()
â”œâ”€ ACTION: Add 5 unit tests for full coverage
â”œâ”€ IMPACT: Full test coverage, -3.6 risk
â”œâ”€ COMPLEXITY: cyclomatic=5, branches=5, cognitive=9, nesting=1, lines=24
â”œâ”€ DEPENDENCIES: 0 upstream, 1 downstream
â””â”€ WHY: Business logic with 0% coverage, manageable complexity (cyclo=5, cog=9)


ğŸ“Š TOTAL DEBT SCORE: 4907
ğŸ“ˆ OVERALL COVERAGE: 67.12%
</code></pre>
<h2 id="understanding-the-output"><a class="header" href="#understanding-the-output">Understanding the Output</a></h2>
<p>Letâ€™s break down what this output means:</p>
<h3 id="priority-levels"><a class="header" href="#priority-levels">Priority Levels</a></h3>
<ul>
<li><strong>CRITICAL</strong> (9.0-10.0): Immediate action required - high complexity with no test coverage</li>
<li><strong>HIGH</strong> (7.0-8.9): Should be addressed soon - moderate-high complexity with poor coverage</li>
<li><strong>MEDIUM</strong> (5.0-6.9): Plan for next sprint - moderate complexity or partial coverage gaps</li>
<li><strong>LOW</strong> (3.0-4.9): Nice to have - well-tested or simple functions</li>
</ul>
<p><strong>Note:</strong> These are default priority thresholds. You can customize them in <code>.debtmap.toml</code> under the <code>[tiers]</code> section to match your teamâ€™s standards.</p>
<h3 id="key-metrics"><a class="header" href="#key-metrics">Key Metrics</a></h3>
<ul>
<li>
<p><strong>Unified Score</strong> (0-10 scale): Overall priority combining complexity, coverage, and dependencies</p>
<ul>
<li>Higher score = higher priority</li>
<li>Takes into account multiple risk factors</li>
</ul>
</li>
<li>
<p><strong>Debt Type</strong>: Category of the issue</p>
<ul>
<li><code>TestGap</code>: Missing test coverage</li>
<li><code>Complexity</code>: Exceeds complexity thresholds</li>
<li><code>Duplication</code>: Repeated code blocks</li>
<li><code>CodeSmell</code>: Anti-patterns and bad practices</li>
</ul>
</li>
<li>
<p><strong>Complexity Metrics</strong>:</p>
<ul>
<li><strong>Cyclomatic</strong>: Number of decision points (branches, loops)</li>
<li><strong>Cognitive</strong>: How difficult the code is to understand</li>
<li><strong>Nesting</strong>: Maximum indentation depth</li>
<li><strong>Lines</strong>: Function length</li>
</ul>
</li>
<li>
<p><strong>Dependencies</strong>:</p>
<ul>
<li><strong>Upstream callers</strong>: Functions that call this function</li>
<li><strong>Downstream callees</strong>: Functions this function calls</li>
<li>More dependencies = higher impact when this code breaks</li>
</ul>
</li>
</ul>
<h3 id="recommendation-structure"><a class="header" href="#recommendation-structure">Recommendation Structure</a></h3>
<p>Each recommendation shows:</p>
<ul>
<li><strong>ACTION</strong>: What you should do (e.g., â€œAdd 6 unit testsâ€)</li>
<li><strong>IMPACT</strong>: Expected improvement (e.g., â€œFull test coverage, -3.7 riskâ€)</li>
<li><strong>WHY</strong>: The reasoning behind this recommendation</li>
</ul>
<h3 id="organizing-results"><a class="header" href="#organizing-results">Organizing Results</a></h3>
<p>When analyzing large codebases, you can organize and filter results to focus on specific areas:</p>
<p><strong>Group by Debt Category:</strong></p>
<pre><code class="language-bash">debtmap analyze . --group-by-category
</code></pre>
<p>This organizes results by type: Architecture, Testing, Performance, CodeQuality</p>
<p><strong>Filter by Priority:</strong></p>
<pre><code class="language-bash"># Show only high and critical priority items
debtmap analyze . --min-priority high

# Combine with --top to limit results
debtmap analyze . --min-priority high --top 10
</code></pre>
<p><strong>Filter by Category:</strong></p>
<pre><code class="language-bash"># Focus on specific debt types
debtmap analyze . --filter Architecture,Testing

# Available categories: Architecture, Testing, Performance, CodeQuality
</code></pre>
<p>These filtering options help you focus on specific types of technical debt, making it easier to plan targeted improvements.</p>
<h3 id="summary-statistics"><a class="header" href="#summary-statistics">Summary Statistics</a></h3>
<ul>
<li>
<p><strong>Total Debt Score</strong>: Sum of all debt scores across your codebase</p>
<ul>
<li>Lower is better</li>
<li>Track over time to measure improvement</li>
</ul>
</li>
<li>
<p><strong>Overall Coverage</strong>: Percentage of code covered by tests</p>
<ul>
<li>Only shown when coverage data is provided</li>
</ul>
</li>
</ul>
<h3 id="output-formats"><a class="header" href="#output-formats">Output Formats</a></h3>
<p>Debtmap supports multiple output formats:</p>
<ul>
<li><strong>Terminal</strong> (default): Human-readable colored output with tables</li>
<li><strong>JSON</strong>: Machine-readable format for CI/CD integration</li>
<li><strong>Markdown</strong>: Documentation-friendly format for reports</li>
</ul>
<p>Example JSON output:</p>
<pre><code class="language-bash"># By default, JSON uses legacy format
debtmap analyze . --format json --output report.json

# For the new unified format (with consistent structure and type field):
debtmap analyze . --format json --output-format unified --output report.json
</code></pre>
<p><strong>JSON Format Options:</strong></p>
<ul>
<li><strong>legacy</strong> (default): Uses <code>{File: {...}}</code> and <code>{Function: {...}}</code> wrappers for backward compatibility with existing tools</li>
<li><strong>unified</strong>: New format (spec 108) with consistent structure and <code>type</code> field for all items</li>
</ul>
<p>Recommendation: Use <code>unified</code> for new integrations, <code>legacy</code> only for compatibility with existing tooling.</p>
<p>Example Markdown output:</p>
<pre><code class="language-bash">debtmap analyze . --format markdown --output report.md
</code></pre>
<h2 id="whats-next"><a class="header" href="#whats-next">Whatâ€™s Next?</a></h2>
<p>Now that youâ€™ve run your first analysis, explore these topics:</p>
<ul>
<li><strong><a href="./analysis-guide.html">Analysis Guide</a></strong> - Deep dive into complexity metrics, debt patterns, and risk scoring</li>
<li><strong><a href="./output-formats.html">Output Formats</a></strong> - Detailed guide to JSON schema and integration options</li>
<li><strong>Configuration</strong> - Customize thresholds and filters with <code>.debtmap.toml</code></li>
<li><strong>CI/CD Integration</strong> - Use the <code>validate</code> command to enforce quality gates</li>
</ul>
<h3 id="generate-a-configuration-file"><a class="header" href="#generate-a-configuration-file">Generate a Configuration File</a></h3>
<p>Create a project-specific configuration:</p>
<pre><code class="language-bash">debtmap init
</code></pre>
<p>This creates a <code>.debtmap.toml</code> file with sensible defaults that you can customize for your project.</p>
<p><strong>Key Configuration Options:</strong></p>
<p>The configuration file allows you to customize:</p>
<ul>
<li><strong>Threshold customization</strong> - Adjust complexity, duplication, and file size thresholds</li>
<li><strong>Scoring weights</strong> - Fine-tune how coverage, complexity, and dependencies are weighted</li>
<li><strong>Language selection</strong> - Enable/disable specific language analyzers</li>
<li><strong>Ignore patterns</strong> - Exclude test files or generated code from analysis</li>
<li><strong>God object thresholds</strong> - Configure what constitutes a â€œgod objectâ€ anti-pattern</li>
<li><strong>Entropy analysis</strong> - Control entropy-based complexity detection</li>
<li><strong>Priority tiers</strong> - Customize CRITICAL/HIGH/MEDIUM/LOW threshold ranges</li>
</ul>
<p>See the Configuration chapter for complete documentation of all available options.</p>
<h3 id="try-analysis-with-coverage"><a class="header" href="#try-analysis-with-coverage">Try Analysis with Coverage</a></h3>
<p>For more accurate risk assessment, run analysis with coverage data:</p>
<pre><code class="language-bash"># For Rust projects
cargo tarpaulin --out lcov --output-dir target/coverage
debtmap analyze . --lcov target/coverage/lcov.info

# For Python projects
pytest --cov --cov-report=lcov
debtmap analyze . --lcov coverage.lcov

# For JavaScript/TypeScript projects
jest --coverage --coverageReporters=lcov
debtmap analyze . --lcov coverage/lcov.info
</code></pre>
<p>Coverage data helps Debtmap identify <strong>truly risky code</strong> - functions that are both complex AND untested.</p>
<hr />
<p><strong>Need help?</strong> Report issues at https://github.com/iepathos/debtmap/issues</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="cli-reference"><a class="header" href="#cli-reference">CLI Reference</a></h1>
<p>Complete reference for Debtmap command-line interface.</p>
<h2 id="quick-start-1"><a class="header" href="#quick-start-1">Quick Start</a></h2>
<pre><code class="language-bash"># Basic analysis
debtmap analyze src/

# With coverage integration
debtmap analyze src/ --coverage-file coverage.lcov

# Generate JSON report
debtmap analyze . --format json --output report.json

# Show top 10 priority items only
debtmap analyze . --top 10 --min-priority high

# Initialize configuration and validate
debtmap init
debtmap validate . --config debtmap.toml
</code></pre>
<h2 id="commands-1"><a class="header" href="#commands-1">Commands</a></h2>
<p>Debtmap provides four main commands:</p>
<h3 id="analyze"><a class="header" href="#analyze"><code>analyze</code></a></h3>
<p>Analyze code for complexity and technical debt.</p>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash">debtmap analyze &lt;PATH&gt; [OPTIONS]
</code></pre>
<p><strong>Arguments:</strong></p>
<ul>
<li><code>&lt;PATH&gt;</code> - Path to analyze (file or directory)</li>
</ul>
<p><strong>Description:</strong>
Primary command for code analysis. Supports multiple output formats (json, markdown, terminal), coverage file integration, caching, parallel processing, context-aware risk analysis, and comprehensive filtering options.</p>
<p>See <a href="cli-reference.html#options">Options</a> section below for all available flags.</p>
<h3 id="init"><a class="header" href="#init"><code>init</code></a></h3>
<p>Initialize a Debtmap configuration file.</p>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash">debtmap init [OPTIONS]
</code></pre>
<p><strong>Options:</strong></p>
<ul>
<li><code>-f, --force</code> - Force overwrite existing config</li>
</ul>
<p><strong>Description:</strong>
Creates a <code>debtmap.toml</code> configuration file in the current directory with default settings. Use <code>--force</code> to overwrite an existing configuration file.</p>
<h3 id="validate"><a class="header" href="#validate"><code>validate</code></a></h3>
<p>Validate code against thresholds defined in configuration file.</p>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash">debtmap validate &lt;PATH&gt; [OPTIONS]
</code></pre>
<p><strong>Arguments:</strong></p>
<ul>
<li><code>&lt;PATH&gt;</code> - Path to analyze</li>
</ul>
<p><strong>Options:</strong></p>
<p><em>Configuration &amp; Output:</em></p>
<ul>
<li><code>-c, --config &lt;CONFIG&gt;</code> - Configuration file path</li>
<li><code>-f, --format &lt;FORMAT&gt;</code> - Output format: json, markdown, terminal</li>
<li><code>-o, --output &lt;OUTPUT&gt;</code> - Output file path (defaults to stdout)</li>
</ul>
<p><em>Coverage &amp; Context:</em></p>
<ul>
<li><code>--coverage-file &lt;PATH&gt;</code> / <code>--lcov &lt;PATH&gt;</code> - LCOV coverage file for risk analysis</li>
<li><code>--context</code> / <code>--enable-context</code> - Enable context-aware risk analysis</li>
<li><code>--context-providers &lt;PROVIDERS&gt;</code> - Context providers to use (comma-separated)</li>
<li><code>--disable-context &lt;PROVIDERS&gt;</code> - Disable specific context providers</li>
</ul>
<p><em>Thresholds &amp; Validation:</em></p>
<ul>
<li><code>--max-debt-density &lt;N&gt;</code> - Maximum debt density allowed (per 1000 LOC)</li>
</ul>
<p><em>Display Filtering:</em></p>
<ul>
<li><code>--top &lt;N&gt;</code> / <code>--head &lt;N&gt;</code> - Show only top N priority items</li>
<li><code>--tail &lt;N&gt;</code> - Show only bottom N priority items</li>
<li><code>-s, --summary</code> - Use summary format with tiered priority display</li>
</ul>
<p><em>Analysis Control:</em></p>
<ul>
<li><code>--semantic-off</code> - Disable semantic analysis (fallback mode)</li>
</ul>
<p><em>Debugging &amp; Verbosity:</em></p>
<ul>
<li><code>-v, --verbose</code> - Increase verbosity level (can be repeated: -v, -vv, -vvv)</li>
</ul>
<p><strong>Description:</strong>
Similar to <code>analyze</code> but enforces thresholds defined in configuration file. Returns non-zero exit code if thresholds are exceeded, making it suitable for CI/CD integration.</p>
<p>The <code>validate</code> command supports a focused subset of <code>analyze</code> options, primarily for output control, coverage integration, context-aware analysis, and display filtering.</p>
<p><strong>Note:</strong> The following <code>analyze</code> options are NOT available in the <code>validate</code> command:</p>
<ul>
<li><code>--threshold-complexity</code>, <code>--threshold-duplication</code>, <code>--threshold-preset</code> (configure these in <code>.debtmap.toml</code> instead)</li>
<li><code>--jobs</code>, <code>--no-parallel</code> (performance tuning)</li>
<li><code>--cache-*</code> options (caching control)</li>
<li><code>--languages</code> (language filtering)</li>
</ul>
<p>Configure analysis thresholds in your <code>.debtmap.toml</code> configuration file for use with the <code>validate</code> command.</p>
<p><strong>Exit Codes:</strong></p>
<ul>
<li><code>0</code> - Success (no errors, all thresholds passed)</li>
<li>Non-zero - Failure (errors occurred or thresholds exceeded)</li>
</ul>
<h3 id="compare"><a class="header" href="#compare"><code>compare</code></a></h3>
<p>Compare two analysis results and generate a diff report.</p>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash">debtmap compare --before &lt;FILE&gt; --after &lt;FILE&gt; [OPTIONS]
</code></pre>
<p><strong>Required Options:</strong></p>
<ul>
<li><code>--before &lt;FILE&gt;</code> - Path to â€œbeforeâ€ analysis JSON</li>
<li><code>--after &lt;FILE&gt;</code> - Path to â€œafterâ€ analysis JSON</li>
</ul>
<p><strong>Optional Target Location:</strong></p>
<ul>
<li><code>--plan &lt;FILE&gt;</code> - Path to implementation plan (to extract target location)</li>
<li><code>--target-location &lt;LOCATION&gt;</code> - Target location in format <code>file:function:line</code></li>
</ul>
<p><strong>Note:</strong> <code>--plan</code> and <code>--target-location</code> are mutually exclusive options. Using both together will cause a CLI error. Use one or the other to specify the target location.</p>
<p><strong>Output Options:</strong></p>
<ul>
<li><code>-f, --format &lt;FORMAT&gt;</code> - Output format: json, markdown, terminal (default: json)</li>
<li><code>-o, --output &lt;OUTPUT&gt;</code> - Output file (defaults to stdout)</li>
</ul>
<p><strong>Description:</strong>
Compares two analysis results and generates a diff showing improvements or regressions in code quality metrics.</p>
<h2 id="options"><a class="header" href="#options">Options</a></h2>
<p>Options are organized by category for clarity. Most options apply to the <code>analyze</code> command, with a subset available for <code>validate</code>.</p>
<h3 id="output-control"><a class="header" href="#output-control">Output Control</a></h3>
<p>Control how analysis results are formatted and displayed.</p>
<p><strong>Format Options:</strong></p>
<ul>
<li><code>-f, --format &lt;FORMAT&gt;</code> - Output format: json, markdown, terminal (default: terminal for analyze)</li>
<li><code>--output-format &lt;JSON_FORMAT&gt;</code> - JSON structure format: legacy or unified (default: legacy)
<ul>
<li><code>legacy</code> - Current format with <code>{File: {...}}</code> and <code>{Function: {...}}</code> wrappers</li>
<li><code>unified</code> - New format with consistent structure and â€˜typeâ€™ field</li>
</ul>
</li>
<li><code>-o, --output &lt;OUTPUT&gt;</code> - Output file path (defaults to stdout)</li>
<li><code>--plain</code> - Plain output mode: ASCII-only, no colors, no emoji, machine-parseable</li>
</ul>
<p><strong>Display Filtering:</strong></p>
<ul>
<li><code>--top &lt;N&gt;</code> / <code>--head &lt;N&gt;</code> - Show only top N priority items</li>
<li><code>--tail &lt;N&gt;</code> - Show only bottom N priority items (lowest priority)</li>
<li><code>-s, --summary</code> - Use summary format with tiered priority display (compact output)</li>
<li><code>--min-priority &lt;PRIORITY&gt;</code> - Minimum priority to display: low, medium, high, critical</li>
<li><code>--filter &lt;CATEGORIES&gt;</code> - Filter by debt categories (comma-separated)</li>
<li><code>--aggregate-only</code> - Show only aggregated file-level scores</li>
<li><code>--group-by-category</code> - Group output by debt category</li>
</ul>
<h3 id="analysis-control"><a class="header" href="#analysis-control">Analysis Control</a></h3>
<p>Configure analysis behavior, thresholds, and language selection.</p>
<p><strong>Thresholds:</strong></p>
<ul>
<li><code>--threshold-complexity &lt;N&gt;</code> - Complexity threshold (default: 10) [analyze command]</li>
<li><code>--threshold-duplication &lt;N&gt;</code> - Duplication threshold in lines (default: 50) [analyze command]</li>
<li><code>--threshold-preset &lt;PRESET&gt;</code> - Complexity threshold preset: strict, balanced, lenient [analyze command]
<ul>
<li><code>strict</code> - Strict thresholds for high code quality standards</li>
<li><code>balanced</code> - Balanced thresholds for typical projects (default)</li>
<li><code>lenient</code> - Lenient thresholds for legacy or complex domains</li>
</ul>
</li>
<li><code>--max-debt-density &lt;N&gt;</code> - Maximum debt density allowed per 1000 LOC [validate command]</li>
</ul>
<p><strong>Note:</strong> Threshold options (<code>--threshold-complexity</code>, <code>--threshold-duplication</code>, <code>--threshold-preset</code>) are command-line options for the <code>analyze</code> command. For the <code>validate</code> command, these thresholds are configured via the <code>--config</code> file (<code>debtmap.toml</code>) rather than as command-line flags.</p>
<p><strong>Language Selection:</strong></p>
<ul>
<li><code>--languages &lt;LANGS&gt;</code> - Comma-separated list of languages to analyze
<ul>
<li>Example: <code>--languages rust,python,javascript</code></li>
<li>Supported: rust, python, javascript, typescript</li>
</ul>
</li>
</ul>
<p><strong>Analysis Modes:</strong></p>
<ul>
<li><code>--semantic-off</code> - Disable semantic analysis (fallback mode)</li>
<li><code>--no-context-aware</code> - Disable context-aware false positive reduction (enabled by default)</li>
<li><code>--multi-pass</code> - Enable multi-pass analysis with attribution</li>
<li><code>--attribution</code> - Show complexity attribution details</li>
</ul>
<h3 id="context--coverage"><a class="header" href="#context--coverage">Context &amp; Coverage</a></h3>
<p>Enable context-aware risk analysis and integrate test coverage data.</p>
<p><strong>Context-Aware Risk Analysis:</strong></p>
<ul>
<li><code>--context</code> / <code>--enable-context</code> - Enable context-aware risk analysis</li>
<li><code>--context-providers &lt;PROVIDERS&gt;</code> - Context providers to use (comma-separated)
<ul>
<li>Available: <code>critical_path</code>, <code>dependency</code>, <code>git_history</code></li>
<li>Example: <code>--context-providers critical_path,git_history</code></li>
</ul>
</li>
<li><code>--disable-context &lt;PROVIDERS&gt;</code> - Disable specific context providers (comma-separated)</li>
</ul>
<p><strong>Coverage Integration:</strong></p>
<ul>
<li><code>--coverage-file &lt;PATH&gt;</code> / <code>--lcov &lt;PATH&gt;</code> - LCOV coverage file for risk analysis
<ul>
<li>Coverage data dampens debt scores for well-tested code (multiplier = 1.0 - coverage)</li>
<li>Surfaces untested complex functions as higher priority</li>
<li>Total debt score with coverage â‰¤ score without coverage</li>
</ul>
</li>
<li><code>--validate-loc</code> - Validate LOC consistency across analysis modes (with/without coverage)</li>
</ul>
<h3 id="performance--caching"><a class="header" href="#performance--caching">Performance &amp; Caching</a></h3>
<p>Optimize analysis performance through parallelization and caching.</p>
<p><strong>Parallel Processing:</strong></p>
<ul>
<li><code>--no-parallel</code> - Disable parallel call graph construction (enabled by default)</li>
<li><code>-j, --jobs &lt;N&gt;</code> - Number of threads for parallel processing
<ul>
<li><code>0</code> = use all available CPU cores (default)</li>
<li>Specify number to limit thread count</li>
</ul>
</li>
</ul>
<p><strong>Caching:</strong></p>
<ul>
<li><code>--no-cache</code> - Disable caching for this run (caching is enabled by default)</li>
<li><code>--clear-cache</code> - Clear cache before running analysis</li>
<li><code>--force-cache-rebuild</code> - Force cache rebuild (same as â€“clear-cache)</li>
<li><code>--cache-stats</code> - Show cache statistics and location</li>
<li><code>--migrate-cache</code> - Migrate cache from local to shared location</li>
<li><code>--cache-location &lt;LOCATION&gt;</code> - Cache location strategy: local, shared, or path
<ul>
<li>Can also be set via <code>DEBTMAP_CACHE_DIR</code> environment variable</li>
<li>Affects where analysis results are cached for faster subsequent runs</li>
</ul>
</li>
</ul>
<p><strong>Other Performance:</strong></p>
<ul>
<li><code>--max-files &lt;N&gt;</code> - Maximum number of files to analyze (0 = no limit)</li>
</ul>
<h3 id="debugging--verbosity"><a class="header" href="#debugging--verbosity">Debugging &amp; Verbosity</a></h3>
<p>Control diagnostic output and debugging information.</p>
<p><strong>Verbosity Levels:</strong></p>
<ul>
<li><code>-v, --verbose</code> - Increase verbosity level (can be repeated: -v, -vv, -vvv)
<ul>
<li><code>-v</code> - Show main score factors</li>
<li><code>-vv</code> - Show detailed calculations</li>
<li><code>-vvv</code> - Show all debug information</li>
</ul>
</li>
</ul>
<p><strong>Specialized Debugging:</strong></p>
<ul>
<li><code>--verbose-macro-warnings</code> - Show verbose macro parsing warnings (Rust analysis)</li>
<li><code>--show-macro-stats</code> - Show macro expansion statistics at end of analysis</li>
<li><code>--detail-level &lt;LEVEL&gt;</code> - Detail level for diagnostic reports
<ul>
<li>Options: summary, standard, comprehensive, debug (default: standard)</li>
</ul>
</li>
</ul>
<h3 id="aggregation"><a class="header" href="#aggregation">Aggregation</a></h3>
<p>Control file-level aggregation and god object detection.</p>
<p><strong>File Aggregation:</strong></p>
<ul>
<li><code>--aggregate-only</code> - Show only aggregated file-level scores</li>
<li><code>--no-aggregation</code> - Disable file-level aggregation</li>
<li><code>--aggregation-method &lt;METHOD&gt;</code> - File aggregation method (default: weighted_sum)
<ul>
<li>Options: sum, weighted_sum, logarithmic_sum, max_plus_average</li>
</ul>
</li>
<li><code>--min-problematic &lt;N&gt;</code> - Minimum number of problematic functions for file aggregation</li>
<li><code>--no-god-object</code> - Disable god object detection</li>
</ul>
<h3 id="option-aliases"><a class="header" href="#option-aliases">Option Aliases</a></h3>
<p>Common option shortcuts and aliases for convenience:</p>
<ul>
<li><code>--lcov</code> is alias for <code>--coverage-file</code></li>
<li><code>--enable-context</code> is alias for <code>--context</code></li>
<li><code>--head</code> is alias for <code>--top</code></li>
<li><code>-s</code> is short form for <code>--summary</code></li>
<li><code>-v</code> is short form for <code>--verbose</code></li>
<li><code>-f</code> is short form for <code>--format</code></li>
<li><code>-o</code> is short form for <code>--output</code></li>
<li><code>-c</code> is short form for <code>--config</code></li>
<li><code>-j</code> is short form for <code>--jobs</code></li>
</ul>
<h3 id="deprecated-options"><a class="header" href="#deprecated-options">Deprecated Options</a></h3>
<p>The following options are deprecated and should be migrated:</p>
<ul>
<li><code>--cache</code> (hidden) - <strong>Deprecated:</strong> caching is now enabled by default
<ul>
<li><strong>Migration:</strong> Remove this flag, use <code>--no-cache</code> to disable if needed</li>
</ul>
</li>
<li><code>--explain-score</code> (hidden) - <strong>Deprecated:</strong> use <code>-v</code> instead
<ul>
<li><strong>Migration:</strong> Use <code>-v</code>, <code>-vv</code>, or <code>-vvv</code> for increasing verbosity levels</li>
</ul>
</li>
</ul>
<h2 id="configuration"><a class="header" href="#configuration">Configuration</a></h2>
<h3 id="configuration-file"><a class="header" href="#configuration-file">Configuration File</a></h3>
<p>Created via <code>debtmap init</code> command. The configuration file (<code>debtmap.toml</code>) is used by the <code>validate</code> command for threshold enforcement and default settings.</p>
<p><strong>Creating Configuration:</strong></p>
<pre><code class="language-bash"># Create new config
debtmap init

# Overwrite existing config
debtmap init --force
</code></pre>
<h3 id="environment-variables"><a class="header" href="#environment-variables">Environment Variables</a></h3>
<ul>
<li><code>DEBTMAP_CACHE_DIR</code> - Override default cache directory location
<ul>
<li>Can also be set via <code>--cache-location</code> flag</li>
<li>Affects where analysis results are cached for faster subsequent runs</li>
</ul>
</li>
</ul>
<h3 id="getting-help"><a class="header" href="#getting-help">Getting Help</a></h3>
<p>Get help for any command:</p>
<pre><code class="language-bash"># General help
debtmap --help

# Command-specific help
debtmap analyze --help
debtmap validate --help
debtmap compare --help
debtmap init --help
</code></pre>
<h2 id="common-workflows"><a class="header" href="#common-workflows">Common Workflows</a></h2>
<h3 id="basic-analysis"><a class="header" href="#basic-analysis">Basic Analysis</a></h3>
<p>Analyze a project and view results in terminal:</p>
<pre><code class="language-bash">debtmap analyze src/
</code></pre>
<p>Generate JSON report for further processing:</p>
<pre><code class="language-bash">debtmap analyze . --format json --output report.json
</code></pre>
<p>Generate Markdown report:</p>
<pre><code class="language-bash">debtmap analyze . --format markdown --output report.md
</code></pre>
<h3 id="coverage-integrated-analysis"><a class="header" href="#coverage-integrated-analysis">Coverage-Integrated Analysis</a></h3>
<p>Analyze with test coverage to surface untested complex code:</p>
<pre><code class="language-bash"># Generate coverage file first (example for Rust)
cargo tarpaulin --out lcov

# Run analysis with coverage
debtmap analyze src/ --coverage-file lcov.info
</code></pre>
<p>Coverage dampens debt scores for well-tested code, making untested complex functions more visible.</p>
<h3 id="context-aware-analysis"><a class="header" href="#context-aware-analysis">Context-Aware Analysis</a></h3>
<p>Enable context providers for risk-aware prioritization:</p>
<pre><code class="language-bash"># Use all context providers
debtmap analyze . --context

# Use specific context providers
debtmap analyze . --context --context-providers critical_path,git_history
</code></pre>
<p>Context-aware analysis reduces false positives and prioritizes code based on:</p>
<ul>
<li>Critical execution paths</li>
<li>Dependency relationships</li>
<li>Git history (change frequency)</li>
</ul>
<h3 id="filtered--focused-analysis"><a class="header" href="#filtered--focused-analysis">Filtered &amp; Focused Analysis</a></h3>
<p>Show only top priority items:</p>
<pre><code class="language-bash">debtmap analyze . --top 10 --min-priority high
</code></pre>
<p>Filter by specific debt categories:</p>
<pre><code class="language-bash">debtmap analyze . --filter complexity,duplication
</code></pre>
<p>Use summary mode for compact output:</p>
<pre><code class="language-bash">debtmap analyze . --summary
</code></pre>
<p>Show only file-level aggregations:</p>
<pre><code class="language-bash">debtmap analyze . --aggregate-only
</code></pre>
<h3 id="performance-tuning"><a class="header" href="#performance-tuning">Performance Tuning</a></h3>
<p>Control parallelization:</p>
<pre><code class="language-bash"># Use 8 threads
debtmap analyze . --jobs 8

# Disable parallel processing
debtmap analyze . --no-parallel
</code></pre>
<p>Manage caching:</p>
<pre><code class="language-bash"># Use shared cache location
debtmap analyze . --cache-location shared

# Clear cache and rebuild
debtmap analyze . --clear-cache

# Show cache statistics
debtmap analyze . --cache-stats
</code></pre>
<p>Limit analysis scope:</p>
<pre><code class="language-bash"># Analyze maximum 100 files
debtmap analyze . --max-files 100

# Analyze specific languages only
debtmap analyze . --languages rust,python
</code></pre>
<h3 id="cicd-integration"><a class="header" href="#cicd-integration">CI/CD Integration</a></h3>
<p>Use the <code>validate</code> command in CI/CD pipelines:</p>
<pre><code class="language-bash"># Initialize configuration (one time)
debtmap init

# Edit debtmap.toml to set thresholds
# ...

# In CI pipeline: validate against thresholds
debtmap validate . --config debtmap.toml --max-debt-density 50
</code></pre>
<p>The <code>validate</code> command returns non-zero exit code if thresholds are exceeded, failing the build.</p>
<h3 id="comparison--tracking"><a class="header" href="#comparison--tracking">Comparison &amp; Tracking</a></h3>
<p>Compare analysis results before and after changes:</p>
<pre><code class="language-bash"># Before changes
debtmap analyze . --format json --output before.json

# Make code changes...

# After changes
debtmap analyze . --format json --output after.json

# Generate comparison report
debtmap compare --before before.json --after after.json --format markdown
</code></pre>
<p>With implementation plan:</p>
<pre><code class="language-bash">debtmap compare --before before.json --after after.json --plan IMPLEMENTATION_PLAN.md
</code></pre>
<h3 id="debugging-analysis"><a class="header" href="#debugging-analysis">Debugging Analysis</a></h3>
<p>Increase verbosity to understand scoring:</p>
<pre><code class="language-bash"># Show main score factors
debtmap analyze src/ -v

# Show detailed calculations
debtmap analyze src/ -vv

# Show all debug information
debtmap analyze src/ -vvv
</code></pre>
<p>Show macro expansion statistics (Rust):</p>
<pre><code class="language-bash">debtmap analyze . --show-macro-stats --verbose-macro-warnings
</code></pre>
<p>Use detailed diagnostic reports:</p>
<pre><code class="language-bash">debtmap analyze . --detail-level comprehensive
</code></pre>
<h2 id="examples"><a class="header" href="#examples">Examples</a></h2>
<h3 id="basic-analysis-1"><a class="header" href="#basic-analysis-1">Basic Analysis</a></h3>
<pre><code class="language-bash"># Analyze current directory
debtmap analyze .

# Analyze specific directory
debtmap analyze src/

# Generate JSON output
debtmap analyze . --format json --output report.json
</code></pre>
<h3 id="with-coverage"><a class="header" href="#with-coverage">With Coverage</a></h3>
<pre><code class="language-bash"># Analyze with LCOV coverage file
debtmap analyze src/ --coverage-file coverage.lcov

# Alternative alias
debtmap analyze src/ --lcov coverage.lcov
</code></pre>
<h3 id="context-aware-analysis-1"><a class="header" href="#context-aware-analysis-1">Context-Aware Analysis</a></h3>
<pre><code class="language-bash"># Enable all context providers
debtmap analyze . --context

# Use specific context providers
debtmap analyze . --context --context-providers critical_path,git_history

# Disable specific providers
debtmap analyze . --context --disable-context dependency
</code></pre>
<h3 id="filtered-output"><a class="header" href="#filtered-output">Filtered Output</a></h3>
<pre><code class="language-bash"># Top 10 priority items only
debtmap analyze . --top 10

# High priority and above
debtmap analyze . --min-priority high

# Specific categories
debtmap analyze . --filter complexity,duplication

# Summary format
debtmap analyze . --summary

# Group by category
debtmap analyze . --group-by-category
</code></pre>
<h3 id="performance-tuning-1"><a class="header" href="#performance-tuning-1">Performance Tuning</a></h3>
<pre><code class="language-bash"># Use 8 threads
debtmap analyze . --jobs 8

# Disable parallelization
debtmap analyze . --no-parallel

# Limit file count
debtmap analyze . --max-files 100

# Shared cache
debtmap analyze . --cache-location shared

# Clear and rebuild cache
debtmap analyze . --clear-cache
</code></pre>
<h3 id="validation"><a class="header" href="#validation">Validation</a></h3>
<pre><code class="language-bash"># Initialize config
debtmap init --force

# Validate against config
debtmap validate . --config debtmap.toml

# With max debt density threshold
debtmap validate . --max-debt-density 50
</code></pre>
<h3 id="comparison"><a class="header" href="#comparison">Comparison</a></h3>
<pre><code class="language-bash"># Compare two analyses
debtmap compare --before before.json --after after.json

# With markdown output
debtmap compare --before before.json --after after.json --format markdown

# With implementation plan
debtmap compare --before before.json --after after.json --plan IMPLEMENTATION_PLAN.md

# With target location
debtmap compare --before before.json --after after.json --target-location "src/main.rs:process_file:42"
</code></pre>
<h3 id="language-selection"><a class="header" href="#language-selection">Language Selection</a></h3>
<pre><code class="language-bash"># Analyze only Rust files
debtmap analyze . --languages rust

# Multiple languages
debtmap analyze . --languages rust,python,javascript
</code></pre>
<h3 id="threshold-configuration"><a class="header" href="#threshold-configuration">Threshold Configuration</a></h3>
<pre><code class="language-bash"># Custom complexity threshold
debtmap analyze . --threshold-complexity 15

# Use preset
debtmap analyze . --threshold-preset strict

# Custom duplication threshold
debtmap analyze . --threshold-duplication 100
</code></pre>
<h3 id="plainmachine-readable-output"><a class="header" href="#plainmachine-readable-output">Plain/Machine-Readable Output</a></h3>
<pre><code class="language-bash"># Plain output (no colors, no emoji)
debtmap analyze . --plain

# Combine with JSON for CI
debtmap analyze . --format json --plain --output report.json
</code></pre>
<h2 id="command-compatibility-matrix"><a class="header" href="#command-compatibility-matrix">Command Compatibility Matrix</a></h2>
<div class="table-wrapper"><table><thead><tr><th>Option</th><th>analyze</th><th>validate</th><th>compare</th><th>init</th></tr></thead><tbody>
<tr><td><code>&lt;PATH&gt;</code> argument</td><td>âœ“</td><td>âœ“</td><td>âœ—</td><td>âœ—</td></tr>
<tr><td><code>--format</code></td><td>âœ“</td><td>âœ“</td><td>âœ“</td><td>âœ—</td></tr>
<tr><td><code>--output</code></td><td>âœ“</td><td>âœ“</td><td>âœ“</td><td>âœ—</td></tr>
<tr><td><code>--coverage-file</code></td><td>âœ“</td><td>âœ“</td><td>âœ—</td><td>âœ—</td></tr>
<tr><td><code>--context</code></td><td>âœ“</td><td>âœ“</td><td>âœ—</td><td>âœ—</td></tr>
<tr><td><code>--threshold-*</code></td><td>âœ“</td><td>âœ—</td><td>âœ—</td><td>âœ—</td></tr>
<tr><td><code>--top / --tail</code></td><td>âœ“</td><td>âœ“</td><td>âœ—</td><td>âœ—</td></tr>
<tr><td><code>--cache-*</code></td><td>âœ“</td><td>âœ—</td><td>âœ—</td><td>âœ—</td></tr>
<tr><td><code>--jobs</code></td><td>âœ“</td><td>âœ—</td><td>âœ—</td><td>âœ—</td></tr>
<tr><td><code>--verbose</code></td><td>âœ“</td><td>âœ“</td><td>âœ—</td><td>âœ—</td></tr>
<tr><td><code>--config</code></td><td>âœ—</td><td>âœ“</td><td>âœ—</td><td>âœ—</td></tr>
<tr><td><code>--before / --after</code></td><td>âœ—</td><td>âœ—</td><td>âœ“</td><td>âœ—</td></tr>
<tr><td><code>--force</code></td><td>âœ—</td><td>âœ—</td><td>âœ—</td><td>âœ“</td></tr>
</tbody></table>
</div>
<p><strong>Note:</strong> The <code>validate</code> command supports output control (<code>--format</code>, <code>--output</code>), coverage integration (<code>--coverage-file</code>), context-aware analysis (<code>--context</code>), display filtering (<code>--top</code>, <code>--tail</code>, <code>--summary</code>), and verbosity options (<code>--verbose</code>) from the <code>analyze</code> command. Analysis thresholds (<code>--threshold-complexity</code>, <code>--threshold-duplication</code>, <code>--threshold-preset</code>) are configured via the <code>--config</code> file rather than as command-line options. Performance options like <code>--cache-*</code> and <code>--jobs</code> are specific to the <code>analyze</code> command.</p>
<h2 id="troubleshooting"><a class="header" href="#troubleshooting">Troubleshooting</a></h2>
<h3 id="performance-issues"><a class="header" href="#performance-issues">Performance Issues</a></h3>
<p><strong>Problem:</strong> Analysis is slow on large codebases</p>
<p><strong>Solutions:</strong></p>
<pre><code class="language-bash"># Use more threads (if you have CPU cores available)
debtmap analyze . --jobs 16

# Enable caching (on by default, but ensure it's not disabled)
debtmap analyze . # caching is automatic

# Use shared cache for team
debtmap analyze . --cache-location shared

# Limit analysis scope
debtmap analyze . --max-files 500 --languages rust
</code></pre>
<h3 id="memory-issues"><a class="header" href="#memory-issues">Memory Issues</a></h3>
<p><strong>Problem:</strong> Analysis runs out of memory</p>
<p><strong>Solutions:</strong></p>
<pre><code class="language-bash"># Disable parallelization
debtmap analyze . --no-parallel

# Limit file count
debtmap analyze . --max-files 100

# Analyze in batches by language
debtmap analyze . --languages rust
debtmap analyze . --languages python
</code></pre>
<h3 id="output-issues"><a class="header" href="#output-issues">Output Issues</a></h3>
<p><strong>Problem:</strong> Terminal output has garbled characters</p>
<p><strong>Solution:</strong></p>
<pre><code class="language-bash"># Use plain mode
debtmap analyze . --plain
</code></pre>
<p><strong>Problem:</strong> Want machine-readable output</p>
<p><strong>Solution:</strong></p>
<pre><code class="language-bash"># Use JSON with plain mode
debtmap analyze . --format json --plain --output report.json
</code></pre>
<h3 id="cache-issues"><a class="header" href="#cache-issues">Cache Issues</a></h3>
<p><strong>Problem:</strong> Stale cached results</p>
<p><strong>Solutions:</strong></p>
<pre><code class="language-bash"># Clear cache
debtmap analyze . --clear-cache

# Check cache statistics
debtmap analyze . --cache-stats

# Disable cache temporarily
debtmap analyze . --no-cache
</code></pre>
<h3 id="threshold-issues"><a class="header" href="#threshold-issues">Threshold Issues</a></h3>
<p><strong>Problem:</strong> Too many items flagged</p>
<p><strong>Solutions:</strong></p>
<pre><code class="language-bash"># Use lenient preset
debtmap analyze . --threshold-preset lenient

# Increase threshold
debtmap analyze . --threshold-complexity 20

# Filter to high priority only
debtmap analyze . --min-priority high
</code></pre>
<p><strong>Problem:</strong> Not enough items flagged</p>
<p><strong>Solutions:</strong></p>
<pre><code class="language-bash"># Use strict preset
debtmap analyze . --threshold-preset strict

# Lower threshold
debtmap analyze . --threshold-complexity 5

# Show all items
debtmap analyze . --min-priority low
</code></pre>
<h2 id="best-practices"><a class="header" href="#best-practices">Best Practices</a></h2>
<h3 id="regular-analysis"><a class="header" href="#regular-analysis">Regular Analysis</a></h3>
<p>Run analysis regularly to track code quality trends:</p>
<pre><code class="language-bash"># Daily in CI
debtmap validate . --config debtmap.toml

# Weekly deep analysis with coverage
debtmap analyze . --coverage-file coverage.lcov --format json --output weekly-report.json
</code></pre>
<h3 id="team-workflows"><a class="header" href="#team-workflows">Team Workflows</a></h3>
<p>Use shared cache for consistent team experience:</p>
<pre><code class="language-bash"># Set environment variable for all team members
export DEBTMAP_CACHE_DIR=/shared/team/debtmap-cache

# Or use flag
debtmap analyze . --cache-location shared
</code></pre>
<h3 id="performance-optimization"><a class="header" href="#performance-optimization">Performance Optimization</a></h3>
<p>For large codebases:</p>
<pre><code class="language-bash"># Use maximum parallelization
debtmap analyze . --jobs 0  # 0 = all cores

# Cache aggressively
debtmap analyze . --cache-location shared

# Focus on changed files in CI
# (implement via custom scripts to analyze git diff)
</code></pre>
<h3 id="integration-with-coverage"><a class="header" href="#integration-with-coverage">Integration with Coverage</a></h3>
<p>Always analyze with coverage when available:</p>
<pre><code class="language-bash"># Rust example
cargo tarpaulin --out lcov
debtmap analyze src/ --coverage-file lcov.info

# Python example
pytest --cov --cov-report=lcov
debtmap analyze . --coverage-file coverage.lcov
</code></pre>
<p>Coverage integration helps prioritize untested complex code.</p>
<h2 id="additional-tools"><a class="header" href="#additional-tools">Additional Tools</a></h2>
<h3 id="prodigy-validate-debtmap-improvement"><a class="header" href="#prodigy-validate-debtmap-improvement">prodigy-validate-debtmap-improvement</a></h3>
<p>Specialized validation tool for Prodigy workflow integration.</p>
<p><strong>Description:</strong>
This binary is part of the Prodigy workflow system and provides specialized validation for Debtmap improvement workflows.</p>
<p><strong>Usage:</strong>
See Prodigy documentation for detailed usage instructions.</p>
<h2 id="see-also"><a class="header" href="#see-also">See Also</a></h2>
<ul>
<li><a href="./configuration.html">Configuration Format</a> - Detailed configuration file format</li>
<li><a href="./output-formats.html">Output Formats</a> - Understanding JSON, Markdown, and Terminal output</li>
<li><a href="./coverage.html">Coverage Integration</a> - Integrating test coverage data</li>
<li><a href="./context-providers.html">Context Providers</a> - Understanding context-aware analysis</li>
<li><a href="./examples.html">Examples</a> - More comprehensive usage examples</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="analysis-guide"><a class="header" href="#analysis-guide">Analysis Guide</a></h1>
<p>This guide explains Debtmapâ€™s analysis capabilities, metrics, and methodologies in depth. Use this to understand what Debtmap measures, how it scores technical debt, and how to interpret analysis results for maximum impact.</p>
<h2 id="overview"><a class="header" href="#overview">Overview</a></h2>
<p>Debtmap analyzes code through multiple lenses to provide a comprehensive view of technical health:</p>
<ul>
<li><strong>Complexity Metrics</strong> - Quantifies how difficult code is to understand and test</li>
<li><strong>Debt Patterns</strong> - Identifies 13 types of technical debt requiring attention</li>
<li><strong>Risk Scoring</strong> - Correlates complexity with test coverage to find truly risky code</li>
<li><strong>Prioritization</strong> - Ranks findings by impact to guide refactoring efforts</li>
</ul>
<p>The goal is to move beyond simple â€œhere are your problemsâ€ to â€œhereâ€™s what to fix first and why.â€</p>
<h2 id="complexity-metrics"><a class="header" href="#complexity-metrics">Complexity Metrics</a></h2>
<p>Debtmap measures complexity using multiple complementary approaches. Each metric captures a different aspect of code difficulty.</p>
<h3 id="cyclomatic-complexity"><a class="header" href="#cyclomatic-complexity">Cyclomatic Complexity</a></h3>
<p>Measures the number of linearly independent paths through code - essentially counting decision points.</p>
<p><strong>How it works:</strong></p>
<ul>
<li>Start with a base complexity of 1</li>
<li>Add 1 for each: <code>if</code>, <code>else if</code>, <code>match</code> arm, <code>while</code>, <code>for</code>, <code>&amp;&amp;</code>, <code>||</code>, <code>?</code> operator</li>
<li>Does NOT increase for <code>else</code> (itâ€™s the alternate path, not a new decision)</li>
</ul>
<p><strong>Thresholds:</strong></p>
<ul>
<li><strong>1-5</strong>: Simple, easy to test - typically needs 1-3 test cases</li>
<li><strong>6-10</strong>: Moderate complexity - needs 4-8 test cases</li>
<li><strong>11-20</strong>: Complex, consider refactoring - needs 9+ test cases</li>
<li><strong>20+</strong>: Very complex, high risk - difficult to test thoroughly</li>
</ul>
<p><strong>Example:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn validate_user(age: u32, has_license: bool, country: &amp;str) -&gt; bool {
    // Complexity: 4
    // Base (1) + if (1) + &amp;&amp; (1) + match (1) = 4
    if age &gt;= 18 &amp;&amp; has_license {
        match country {
            "US" | "CA" =&gt; true,
            _ =&gt; false,
        }
    } else {
        false
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="cognitive-complexity"><a class="header" href="#cognitive-complexity">Cognitive Complexity</a></h3>
<p>Measures how difficult code is to understand by considering nesting depth and control flow interruptions.</p>
<p><strong>How it differs from cyclomatic:</strong></p>
<ul>
<li>Nesting increases weight (deeply nested code is harder to understand)</li>
<li>Linear sequences donâ€™t increase complexity (easier to follow)</li>
<li>Breaks and continues add complexity (interrupt normal flow)</li>
</ul>
<p><strong>Calculation:</strong></p>
<ul>
<li>Each structure (if, loop, match) gets a base score</li>
<li>Nesting multiplies the weight (nested structures = harder to understand)</li>
<li>Break/continue/return in middle of function adds cognitive load</li>
</ul>
<p><strong>Example:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Cyclomatic: 5, Cognitive: 8
fn process_items(items: Vec&lt;Item&gt;) -&gt; Vec&lt;Result&gt; {
    let mut results = vec![];

    for item in items {                    // +1 cognitive
        if item.is_valid() {               // +2 (nested in loop)
            match item.type {              // +3 (nested 2 levels)
                Type::A =&gt; results.push(process_a(item)),
                Type::B =&gt; {
                    if item.priority &gt; 5 { // +4 (nested 3 levels)
                        results.push(process_b_priority(item));
                    }
                }
                _ =&gt; continue,             // +1 (control flow interruption)
            }
        }
    }

    results
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Thresholds:</strong></p>
<ul>
<li><strong>0-5</strong>: Trivial - anyone can understand</li>
<li><strong>6-10</strong>: Simple - straightforward logic</li>
<li><strong>11-20</strong>: Moderate - requires careful reading</li>
<li><strong>21-40</strong>: Complex - difficult to understand</li>
<li><strong>40+</strong>: Very complex - needs refactoring</li>
</ul>
<h3 id="entropy-based-complexity-analysis"><a class="header" href="#entropy-based-complexity-analysis">Entropy-Based Complexity Analysis</a></h3>
<p>Uses information theory to distinguish genuinely complex code from pattern-based repetitive code. This dramatically reduces false positives for validation functions, dispatchers, and configuration parsers.</p>
<p><strong>How it works:</strong></p>
<ol>
<li>
<p><strong>Token Entropy</strong> (0.0-1.0): Measures variety in code tokens</p>
<ul>
<li>High entropy (0.7+): Diverse logic, genuinely complex</li>
<li>Low entropy (0.0-0.4): Repetitive patterns, less complex than it appears</li>
</ul>
</li>
<li>
<p><strong>Pattern Repetition</strong> (0.0-1.0): Detects repetitive structures in AST</p>
<ul>
<li>High repetition (0.7+): Similar blocks repeated (validation checks, case handlers)</li>
<li>Low repetition: Unique logic throughout</li>
</ul>
</li>
<li>
<p><strong>Branch Similarity</strong> (0.0-1.0): Analyzes similarity between conditional branches</p>
<ul>
<li>High similarity (0.8+): Branches do similar things (consistent handling)</li>
<li>Low similarity: Each branch has unique logic</li>
</ul>
</li>
<li>
<p><strong>Token Classification</strong>: Categorizes tokens by type with weighted importance</p>
<ul>
<li>Variables, methods, literals weighted differently</li>
<li>Focuses on structural complexity over superficial differences</li>
</ul>
</li>
</ol>
<p><strong>Dampening logic:</strong> Dampening is applied when multiple factors indicate repetitive patterns:</p>
<ul>
<li>Low token entropy (&lt; 0.4) indicates simple, repetitive patterns</li>
<li>High pattern repetition (&gt; 0.6) shows similar code blocks</li>
<li>High branch similarity (&gt; 0.7) indicates consistent branching logic</li>
</ul>
<p>When these conditions are met:</p>
<pre><code>effective_complexity = entropy Ã— pattern_factor Ã— similarity_factor
</code></pre>
<p><strong>Dampening cap:</strong> The dampening factor has a minimum of 0.7, ensuring no more than 30% reduction in complexity scores. This prevents over-correction of pattern-based code and maintains a baseline complexity floor for functions that still require understanding and maintenance.</p>
<p><strong>Example:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Without entropy: Cyclomatic = 15 (appears very complex)
// With entropy: Effective = 5 (pattern-based, dampened 67%)
fn validate_config(config: &amp;Config) -&gt; Result&lt;(), ValidationError&gt; {
    if config.name.is_empty() { return Err(ValidationError::EmptyName); }
    if config.port == 0 { return Err(ValidationError::InvalidPort); }
    if config.host.is_empty() { return Err(ValidationError::EmptyHost); }
    if config.timeout == 0 { return Err(ValidationError::InvalidTimeout); }
    // ... 11 more similar checks
    Ok(())
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Enable in <code>.debtmap.toml</code>:</strong></p>
<pre><code class="language-toml">[entropy]
enabled = true                 # Enable entropy analysis (default: true)
weight = 0.5                  # Weight in adjustment (0.0-1.0)
use_classification = true     # Advanced token classification
pattern_threshold = 0.7       # Pattern detection threshold
entropy_threshold = 0.4       # Entropy below this triggers dampening
branch_threshold = 0.8        # Branch similarity threshold
max_combined_reduction = 0.3  # Maximum 30% reduction
</code></pre>
<p><strong>Output fields in EntropyScore:</strong></p>
<ul>
<li><code>unique_variables</code>: Count of distinct variables in the function (measures variable diversity)</li>
<li><code>max_nesting</code>: Maximum nesting depth detected (contributes to dampening calculation)</li>
<li><code>dampening_applied</code>: Actual dampening factor applied to the complexity score</li>
</ul>
<h3 id="nesting-depth"><a class="header" href="#nesting-depth">Nesting Depth</a></h3>
<p>Maximum level of indentation in a function. Deep nesting makes code hard to follow.</p>
<p><strong>Thresholds:</strong></p>
<ul>
<li><strong>1-2</strong>: Flat, easy to read</li>
<li><strong>3-4</strong>: Moderate nesting</li>
<li><strong>5+</strong>: Deep nesting, consider extracting functions</li>
</ul>
<p><strong>Example:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Nesting depth: 4 (difficult to follow)
fn process(data: Data) -&gt; Result&lt;Output&gt; {
    if data.is_valid() {                    // Level 1
        for item in data.items {            // Level 2
            if item.active {                // Level 3
                match item.type {           // Level 4
                    Type::A =&gt; { /* ... */ }
                    Type::B =&gt; { /* ... */ }
                }
            }
        }
    }
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Refactored:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Nesting depth: 2 (much clearer)
fn process(data: Data) -&gt; Result&lt;Output&gt; {
    if !data.is_valid() {
        return Err(Error::Invalid);
    }

    data.items
        .iter()
        .filter(|item| item.active)
        .map(|item| process_item(item))     // Extract to separate function
        .collect()
}
<span class="boring">}</span></code></pre></pre>
<h3 id="function-length"><a class="header" href="#function-length">Function Length</a></h3>
<p>Number of lines in a function. Long functions often violate single responsibility principle.</p>
<p><strong>Thresholds:</strong></p>
<ul>
<li><strong>1-20 lines</strong>: Good - focused, single purpose</li>
<li><strong>21-50 lines</strong>: Acceptable - may have multiple steps</li>
<li><strong>51-100 lines</strong>: Long - consider breaking up</li>
<li><strong>100+ lines</strong>: Very long - definitely needs refactoring</li>
</ul>
<p><strong>Why length matters:</strong></p>
<ul>
<li>Harder to understand and remember</li>
<li>Harder to test thoroughly</li>
<li>Often violates single responsibility</li>
<li>Difficult to reuse</li>
</ul>
<h2 id="debt-patterns"><a class="header" href="#debt-patterns">Debt Patterns</a></h2>
<p>Debtmap detects 24 types of technical debt, organized into 4 strategic categories. Each debt type is mapped to a category that guides prioritization and remediation strategies.</p>
<h3 id="debt-type-enum"><a class="header" href="#debt-type-enum">Debt Type Enum</a></h3>
<p>The <code>DebtType</code> enum defines all specific debt patterns that Debtmap can detect:</p>
<p><strong>Testing Debt:</strong></p>
<ul>
<li><code>TestingGap</code> - Functions with insufficient test coverage</li>
<li><code>TestTodo</code> - TODO comments in test code</li>
<li><code>TestComplexity</code> - Test functions exceeding complexity thresholds</li>
<li><code>TestDuplication</code> - Duplicated code in test files</li>
<li><code>TestComplexityHotspot</code> - Complex test logic thatâ€™s hard to maintain</li>
<li><code>AssertionComplexity</code> - Complex test assertions</li>
<li><code>FlakyTestPattern</code> - Non-deterministic test behavior</li>
</ul>
<p><strong>Architecture Debt:</strong></p>
<ul>
<li><code>ComplexityHotspot</code> - Functions exceeding complexity thresholds</li>
<li><code>DeadCode</code> - Unreachable or unused code</li>
<li><code>GodObject</code> - Classes with too many responsibilities</li>
<li><code>GodModule</code> - Modules with too many responsibilities</li>
<li><code>FeatureEnvy</code> - Using more data from other objects than own</li>
<li><code>PrimitiveObsession</code> - Overusing basic types instead of domain objects</li>
<li><code>MagicValues</code> - Unexplained literal values</li>
</ul>
<p><strong>Performance Debt:</strong></p>
<ul>
<li><code>AllocationInefficiency</code> - Inefficient memory allocations</li>
<li><code>StringConcatenation</code> - Inefficient string building in loops</li>
<li><code>NestedLoops</code> - Multiple nested iterations (O(nÂ²) or worse)</li>
<li><code>BlockingIO</code> - Blocking I/O in async contexts</li>
<li><code>SuboptimalDataStructure</code> - Wrong data structure for access pattern</li>
<li><code>AsyncMisuse</code> - Improper async/await usage</li>
<li><code>ResourceLeak</code> - Resources not properly released</li>
<li><code>CollectionInefficiency</code> - Inefficient collection operations</li>
</ul>
<p><strong>Code Quality Debt:</strong></p>
<ul>
<li><code>Risk</code> - High-risk code (complex + poorly tested)</li>
<li><code>Duplication</code> - Duplicated code blocks</li>
<li><code>ErrorSwallowing</code> - Errors caught but ignored</li>
</ul>
<h3 id="debt-categories"><a class="header" href="#debt-categories">Debt Categories</a></h3>
<p>The <code>DebtCategory</code> enum groups debt types into strategic categories:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub enum DebtCategory {
    Architecture,  // Structure, design, complexity
    Testing,       // Coverage, test quality
    Performance,   // Speed, memory, efficiency
    CodeQuality,   // Maintainability, readability
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Category Mapping:</strong></p>
<div class="table-wrapper"><table><thead><tr><th>Debt Type</th><th>Category</th><th>Strategic Focus</th></tr></thead><tbody>
<tr><td>ComplexityHotspot, DeadCode, GodObject, GodModule, FeatureEnvy, PrimitiveObsession, MagicValues</td><td>Architecture</td><td>Structural improvements, design patterns</td></tr>
<tr><td>TestingGap, TestTodo, TestComplexity, TestDuplication, TestComplexityHotspot, AssertionComplexity, FlakyTestPattern</td><td>Testing</td><td>Test coverage, test quality</td></tr>
<tr><td>AllocationInefficiency, StringConcatenation, NestedLoops, BlockingIO, SuboptimalDataStructure, AsyncMisuse, ResourceLeak, CollectionInefficiency</td><td>Performance</td><td>Runtime efficiency, resource usage</td></tr>
<tr><td>Risk, Duplication, ErrorSwallowing</td><td>CodeQuality</td><td>Maintainability, reliability</td></tr>
</tbody></table>
</div>
<p><strong>Language-Specific Debt Patterns:</strong></p>
<p>Some debt patterns only apply to languages with specific features:</p>
<ul>
<li><strong>BlockingIO, AsyncMisuse</strong>: Async-capable languages (Rust, JavaScript, TypeScript)</li>
<li><strong>AllocationInefficiency, ResourceLeak</strong>: Languages with manual memory management (Rust)</li>
<li><strong>Error handling patterns</strong>: Vary by language error model (Result in Rust, exceptions in Python/JS)</li>
</ul>
<p>Debtmap automatically applies only the relevant debt patterns for each language during analysis.</p>
<h3 id="examples-by-category"><a class="header" href="#examples-by-category">Examples by Category</a></h3>
<h4 id="architecture-debt"><a class="header" href="#architecture-debt">Architecture Debt</a></h4>
<p><strong>ComplexityHotspot</strong>: Functions exceeding complexity thresholds</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Cyclomatic: 22, Cognitive: 35
fn process_transaction(tx: Transaction, account: &amp;mut Account) -&gt; Result&lt;Receipt&gt; {
    if tx.amount &lt;= 0 {
        return Err(Error::InvalidAmount);
    }
    // ... deeply nested logic with many branches
    Ok(receipt)
}
<span class="boring">}</span></code></pre></pre>
<p><strong>When detected</strong>: Cyclomatic &gt; 10 OR Cognitive &gt; 15 (configurable)
<strong>Action</strong>: Break into smaller functions, extract validation, simplify control flow</p>
<p><strong>GodObject / GodModule</strong>: Too many responsibilities</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// God module: handles parsing, validation, storage, notifications
mod user_service {
    fn parse_user() { /* ... */ }
    fn validate_user() { /* ... */ }
    fn save_user() { /* ... */ }
    fn send_email() { /* ... */ }
    fn log_activity() { /* ... */ }
    // ... 20+ more functions
}
<span class="boring">}</span></code></pre></pre>
<p><strong>When detected</strong>: Complexity-weighted scoring system (see detailed explanation below)
<strong>Action</strong>: Split into focused modules (parser, validator, repository, notifier)</p>
<h4 id="complexity-weighted-god-object-detection"><a class="header" href="#complexity-weighted-god-object-detection">Complexity-Weighted God Object Detection</a></h4>
<p>Debtmap uses <strong>complexity-weighted scoring</strong> for god object detection to reduce false positives on well-refactored code. This ensures that a file with 100 simple helper functions doesnâ€™t rank higher than a file with 10 complex functions.</p>
<p><strong>The Problem:</strong></p>
<p>Traditional god object detection counts methods:</p>
<ul>
<li>File A: 100 methods (average complexity: 1.5) â†’ Flagged as god object</li>
<li>File B: 10 methods (average complexity: 17.0) â†’ Not flagged</li>
</ul>
<p>But File A might be a well-organized utility module with many small helpers, while File B is truly problematic with highly complex functions that need refactoring.</p>
<p><strong>The Solution:</strong></p>
<p>Debtmap weights each function by its cyclomatic complexity using this formula:</p>
<pre><code>weight = (max(1, complexity) / 3)^1.5
</code></pre>
<p><strong>Weight Examples:</strong></p>
<ul>
<li>Simple helper (complexity 1): weight â‰ˆ 0.19</li>
<li>Baseline function (complexity 3): weight = 1.0</li>
<li>Moderate function (complexity 9): weight â‰ˆ 5.2</li>
<li>Complex function (complexity 17): weight â‰ˆ 13.5</li>
<li>Critical function (complexity 33): weight â‰ˆ 36.5</li>
</ul>
<p><strong>God Object Score Calculation:</strong></p>
<pre><code>weighted_method_count = sum(weight for each function)
complexity_penalty = 0.7 if avg_complexity &lt; 3, 1.0 if 3-10, 1.5 if &gt; 10

god_object_score = (
    (weighted_method_count / threshold) * 40% +
    (field_count / threshold) * 20% +
    (responsibility_count / threshold) * 15% +
    (lines_of_code / 500) * 25%
) * complexity_penalty
</code></pre>
<p><strong>Threshold</strong>: God object detected if <code>score &gt;= 70.0</code></p>
<p><strong>Real-World Example:</strong></p>
<pre><code>shared_cache.rs:
  - 100 functions, average complexity: 1.5
  - Weighted score: ~19.0 (100 * 0.19)
  - God object score: 45.2
  - Result: Not a god object âœ“

legacy_parser.rs:
  - 10 functions, average complexity: 17.0
  - Weighted score: ~135.0 (10 * 13.5)
  - God object score: 87.3
  - Result: God object detected âœ“
</code></pre>
<p><strong>Benefits:</strong></p>
<ul>
<li><strong>Reduces false positives</strong> on utility modules with many simple functions</li>
<li><strong>Focuses attention</strong> on truly problematic complex modules</li>
<li><strong>Rewards good refactoring</strong> - breaking large functions into small helpers improves score</li>
<li><strong>Aligns with reality</strong> - complexity matters more than count for maintainability</li>
</ul>
<p><strong>How to View:</strong></p>
<p>When Debtmap detects a god object, the output includes:</p>
<ul>
<li>Raw method count</li>
<li>Weighted method count</li>
<li>Average complexity</li>
<li>God object score</li>
<li>Recommended module splits based on responsibility clustering</li>
</ul>
<p><strong>MagicValues</strong>: Unexplained literals</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Bad: Magic numbers
fn calculate_price(quantity: u32) -&gt; f64 {
    quantity as f64 * 19.99 + 5.0  // What are these numbers?
}

// Good: Named constants
const UNIT_PRICE: f64 = 19.99;
const SHIPPING_COST: f64 = 5.0;
fn calculate_price(quantity: u32) -&gt; f64 {
    quantity as f64 * UNIT_PRICE + SHIPPING_COST
}
<span class="boring">}</span></code></pre></pre>
<h4 id="testing-debt"><a class="header" href="#testing-debt">Testing Debt</a></h4>
<p><strong>TestingGap</strong>: Functions with insufficient test coverage</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// 0% coverage - critical business logic untested
fn calculate_tax(amount: f64, region: &amp;str) -&gt; f64 {
    // Complex tax calculation logic
    // No tests exist for this function!
}
<span class="boring">}</span></code></pre></pre>
<p><strong>When detected</strong>: Coverage data shows function has &lt; 80% line coverage
<strong>Action</strong>: Add unit tests to cover all branches and edge cases</p>
<p><strong>TestComplexity</strong>: Test functions too complex</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[test]
fn complex_test() {
    // Cyclomatic: 12 (too complex for a test)
    for input in test_cases {
        if input.is_special() {
            match input.type {
                /* complex test logic */
            }
        }
    }
}
<span class="boring">}</span></code></pre></pre>
<p><strong>When detected</strong>: Test functions with cyclomatic &gt; 10 or cognitive &gt; 15
<strong>Action</strong>: Split into multiple focused tests, use test fixtures</p>
<p><strong>FlakyTestPattern</strong>: Non-deterministic tests</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[test]
fn flaky_test() {
    let result = async_operation().await;  // Timing-dependent
    thread::sleep(Duration::from_millis(100));  // Race condition!
    assert_eq!(result.status, "complete");
}
<span class="boring">}</span></code></pre></pre>
<p><strong>When detected</strong>: Pattern analysis for timing dependencies, random values
<strong>Action</strong>: Use mocks, deterministic test data, proper async test utilities</p>
<h4 id="performance-debt"><a class="header" href="#performance-debt">Performance Debt</a></h4>
<p><strong>AllocationInefficiency</strong>: Excessive allocations</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Bad: Allocates on every iteration
fn process_items(items: &amp;[Item]) -&gt; Vec&lt;String&gt; {
    let mut results = Vec::new();
    for item in items {
        results.push(item.name.clone());  // Unnecessary clone
    }
    results
}

// Good: Pre-allocate, avoid clones
fn process_items(items: &amp;[Item]) -&gt; Vec&lt;&amp;str&gt; {
    items.iter().map(|item| item.name.as_str()).collect()
}
<span class="boring">}</span></code></pre></pre>
<p><strong>BlockingIO</strong>: Blocking operations in async contexts</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Bad: Blocks async runtime
async fn load_data() -&gt; Result&lt;Data&gt; {
    let file = std::fs::read_to_string("data.json")?;  // Blocking!
    parse_json(&amp;file)
}

// Good: Async I/O
async fn load_data() -&gt; Result&lt;Data&gt; {
    let file = tokio::fs::read_to_string("data.json").await?;
    parse_json(&amp;file)
}
<span class="boring">}</span></code></pre></pre>
<p><strong>NestedLoops</strong>: O(nÂ²) or worse complexity</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Bad: O(nÂ²) nested loops
fn find_duplicates(items: &amp;[Item]) -&gt; Vec&lt;(Item, Item)&gt; {
    let mut dupes = vec![];
    for i in 0..items.len() {
        for j in i+1..items.len() {
            if items[i] == items[j] {
                dupes.push((items[i].clone(), items[j].clone()));
            }
        }
    }
    dupes
}

// Good: O(n) with HashSet
fn find_duplicates(items: &amp;[Item]) -&gt; Vec&lt;Item&gt; {
    let mut seen = HashSet::new();
    items.iter().filter(|item| !seen.insert(item)).cloned().collect()
}
<span class="boring">}</span></code></pre></pre>
<h4 id="code-quality-debt"><a class="header" href="#code-quality-debt">Code Quality Debt</a></h4>
<p><strong>Duplication</strong>: Duplicated code blocks</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// File A:
fn process_user(user: User) -&gt; Result&lt;()&gt; {
    validate_email(&amp;user.email)?;
    validate_age(user.age)?;
    save_to_database(&amp;user)?;
    send_welcome_email(&amp;user.email)?;
    Ok(())
}

// File B: Duplicated validation
fn process_admin(admin: Admin) -&gt; Result&lt;()&gt; {
    validate_email(&amp;admin.email)?;  // Duplicated
    validate_age(admin.age)?;       // Duplicated
    save_to_database(&amp;admin)?;
    grant_admin_privileges(&amp;admin)?;
    Ok(())
}
<span class="boring">}</span></code></pre></pre>
<p><strong>When detected</strong>: Similar code blocks &gt; 50 lines (configurable)
<strong>Action</strong>: Extract shared code into reusable functions</p>
<p><strong>ErrorSwallowing</strong>: Errors caught but ignored</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Bad: Error swallowed, no context
match risky_operation() {
    Ok(result) =&gt; process(result),
    Err(_) =&gt; {}, // Silent failure!
}

// Good: Error handled with context
match risky_operation() {
    Ok(result) =&gt; process(result),
    Err(e) =&gt; {
        log::error!("Risky operation failed: {}", e);
        return Err(e.into());
    }
}
<span class="boring">}</span></code></pre></pre>
<p><strong>When detected</strong>: Empty catch blocks, ignored Results
<strong>Action</strong>: Add proper error logging and propagation</p>
<p><strong>Risk</strong>: High-risk code (complex + poorly tested)</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Cyclomatic: 18, Coverage: 20%, Risk Score: 47.6 (HIGH)
fn process_payment(tx: Transaction) -&gt; Result&lt;Receipt&gt; {
    // Complex payment logic with minimal testing
    // High risk of bugs in production
}
<span class="boring">}</span></code></pre></pre>
<p><strong>When detected</strong>: Combines complexity metrics with coverage data
<strong>Action</strong>: Either add comprehensive tests OR refactor to reduce complexity</p>
<h3 id="debt-scoring-formula"><a class="header" href="#debt-scoring-formula">Debt Scoring Formula</a></h3>
<p>Each debt item gets a score based on priority and type:</p>
<pre><code>debt_score = priority_weight Ã— type_weight
</code></pre>
<p><strong>Priority weights:</strong></p>
<ul>
<li>Low = 1</li>
<li>Medium = 3</li>
<li>High = 5</li>
<li>Critical = 10</li>
</ul>
<p><strong>Combined examples:</strong></p>
<ul>
<li>Low Todo = 1 Ã— 1 = 1</li>
<li>Medium Fixme = 3 Ã— 2 = 6</li>
<li>High Complexity = 5 Ã— 5 = 25</li>
<li>Critical Complexity = 10 Ã— 5 = 50</li>
</ul>
<p><strong>Total debt score</strong> = Sum of all debt item scores</p>
<p>Lower is better. Track over time to measure improvement.</p>
<h2 id="risk-scoring"><a class="header" href="#risk-scoring">Risk Scoring</a></h2>
<p>Debtmapâ€™s risk scoring identifies code that is both complex AND poorly tested - the true risk hotspots.</p>
<h3 id="unified-scoring-system"><a class="header" href="#unified-scoring-system">Unified Scoring System</a></h3>
<p>Debtmap uses a <strong>unified scoring system</strong> (0-10 scale) as the primary prioritization mechanism. This multi-factor approach balances complexity, test coverage, and dependency impact, adjusted by function role.</p>
<h4 id="score-scale-and-priority-classifications"><a class="header" href="#score-scale-and-priority-classifications">Score Scale and Priority Classifications</a></h4>
<p>Functions receive scores from 0 (minimal risk) to 10 (critical risk):</p>
<div class="table-wrapper"><table><thead><tr><th>Score Range</th><th>Priority</th><th>Description</th><th>Action</th></tr></thead><tbody>
<tr><td><strong>9.0-10.0</strong></td><td>Critical</td><td>Severe risk requiring immediate attention</td><td>Address immediately</td></tr>
<tr><td><strong>7.0-8.9</strong></td><td>High</td><td>Significant risk, should be addressed soon</td><td>Plan for this sprint</td></tr>
<tr><td><strong>5.0-6.9</strong></td><td>Medium</td><td>Moderate risk, plan for future work</td><td>Schedule for next sprint</td></tr>
<tr><td><strong>3.0-4.9</strong></td><td>Low</td><td>Minor risk, lower priority</td><td>Monitor and address as time permits</td></tr>
<tr><td><strong>0.0-2.9</strong></td><td>Minimal</td><td>Well-managed code</td><td>Continue monitoring</td></tr>
</tbody></table>
</div>
<h4 id="scoring-formula"><a class="header" href="#scoring-formula">Scoring Formula</a></h4>
<p>The unified score combines three weighted factors:</p>
<pre><code>Base Score = (Complexity Factor Ã— 0.40) + (Coverage Factor Ã— 0.40) + (Dependency Factor Ã— 0.20)

Final Score = Base Score Ã— Role Multiplier
</code></pre>
<p><strong>Factor Calculations:</strong></p>
<p><strong>Complexity Factor</strong> (0-10 scale):</p>
<pre><code>Complexity Factor = min(10, ((cyclomatic / 10) + (cognitive / 20)) Ã— 5)
</code></pre>
<p>Normalized to 0-10 range based on cyclomatic and cognitive complexity.</p>
<p><strong>Coverage Factor</strong> (0-10 scale):</p>
<pre><code>Coverage Factor = 10 Ã— (1 - coverage_percentage) Ã— complexity_weight
</code></pre>
<p>Uncovered complex code scores higher than uncovered simple code. Coverage dampens the score - well-tested code gets lower scores.</p>
<p><strong>Dependency Factor</strong> (0-10 scale):
Based on call graph analysis with specific thresholds:</p>
<ul>
<li><strong>High impact</strong> (score 8-10): 5+ upstream callers, or on critical path from entry point (adds 2-3 points)</li>
<li><strong>Moderate impact</strong> (score 4-6): 2-4 upstream callers</li>
<li><strong>Low impact</strong> (score 1-3): 0-1 upstream callers</li>
<li><strong>Critical path bonus</strong>: Being on a critical path from an entry point adds 2-3 points to the base dependency score</li>
</ul>
<h4 id="default-weights"><a class="header" href="#default-weights">Default Weights</a></h4>
<p>The scoring formula uses configurable weights (default values shown):</p>
<ul>
<li><strong>Complexity: 40%</strong> - How difficult the code is to understand and test</li>
<li><strong>Coverage: 40%</strong> - How well the code is tested</li>
<li><strong>Dependency: 20%</strong> - How many other functions depend on this code</li>
</ul>
<p>These weights can be adjusted in <code>.debtmap.toml</code> to match your teamâ€™s priorities.</p>
<h4 id="role-based-prioritization"><a class="header" href="#role-based-prioritization">Role-Based Prioritization</a></h4>
<p>The unified score is multiplied by a <strong>role multiplier</strong> based on the functionâ€™s semantic classification:</p>
<div class="table-wrapper"><table><thead><tr><th>Role</th><th>Multiplier</th><th>Description</th><th>Example</th></tr></thead><tbody>
<tr><td><strong>Entry Points</strong></td><td>1.5Ã—</td><td>main(), HTTP handlers, API endpoints</td><td>User-facing code where bugs have immediate impact</td></tr>
<tr><td><strong>Business Logic</strong></td><td>1.2Ã—</td><td>Core domain functions, algorithms</td><td>Critical functionality</td></tr>
<tr><td><strong>Data Access</strong></td><td>1.0Ã—</td><td>Database queries, file I/O</td><td>Baseline importance</td></tr>
<tr><td><strong>Infrastructure</strong></td><td>0.8Ã—</td><td>Logging, configuration, monitoring</td><td>Supporting code</td></tr>
<tr><td><strong>Utilities</strong></td><td>0.5Ã—</td><td>Helpers, formatters, converters</td><td>Lower impact</td></tr>
<tr><td><strong>Test Code</strong></td><td>0.1Ã—</td><td>Test functions, fixtures, mocks</td><td>Internal quality</td></tr>
</tbody></table>
</div>
<p><strong>How role classification works:</strong></p>
<p>Debtmap identifies function roles through pattern analysis:</p>
<ul>
<li><strong>Entry points</strong>: Functions named <code>main</code>, handlers with routing decorators, public API functions</li>
<li><strong>Business logic</strong>: Core domain operations, calculation functions, decision-making code</li>
<li><strong>Data access</strong>: Database queries, file operations, network calls</li>
<li><strong>Infrastructure</strong>: Logging, config parsing, monitoring, error handling</li>
<li><strong>Utilities</strong>: Helper functions, formatters, type converters, validators</li>
<li><strong>Test code</strong>: Functions in test modules, test functions, fixtures</li>
</ul>
<p><strong>Example: Same complexity, different priorities</strong></p>
<p>Consider a function with base score 8.0:</p>
<pre><code>If classified as Entry Point:
  Final Score = 8.0 Ã— 1.5 = 12.0 (capped at 10.0) â†’ CRITICAL priority

If classified as Business Logic:
  Final Score = 8.0 Ã— 1.2 = 9.6 â†’ CRITICAL priority

If classified as Data Access:
  Final Score = 8.0 Ã— 1.0 = 8.0 â†’ HIGH priority

If classified as Utility:
  Final Score = 8.0 Ã— 0.5 = 4.0 â†’ LOW priority
</code></pre>
<p>This ensures that complex code in critical paths gets higher priority than equally complex utility code.</p>
<h4 id="coverage-propagation"><a class="header" href="#coverage-propagation">Coverage Propagation</a></h4>
<p>Coverage impact flows through the call graph using <strong>transitive coverage</strong>:</p>
<pre><code>Transitive Coverage = Direct Coverage + Î£(Caller Coverage Ã— Weight)
</code></pre>
<p><strong>How it works:</strong></p>
<p>Functions called by well-tested code inherit some coverage benefit, reducing their urgency. This helps identify which untested functions are on critical paths versus safely isolated utilities.</p>
<p><strong>Example scenarios:</strong></p>
<p><strong>Scenario 1: Untested function with well-tested callers</strong></p>
<pre><code>Function A: 0% direct coverage
  Called by:
    - handle_request (95% coverage)
    - process_payment (90% coverage)
    - validate_order (88% coverage)

Transitive coverage: ~40% (inherits coverage benefit from callers)
Final priority: Lower than isolated 0% coverage function
</code></pre>
<p><strong>Scenario 2: Untested function on critical path</strong></p>
<pre><code>Function B: 0% direct coverage
  Called by:
    - main (0% coverage)
    - startup (10% coverage)

Transitive coverage: ~5% (minimal coverage benefit)
Final priority: Higher - on critical path with no safety net
</code></pre>
<p>Coverage propagation prevents false alarms about utility functions called only by well-tested code, while highlighting genuinely risky untested code on critical paths.</p>
<h4 id="unified-score-example"><a class="header" href="#unified-score-example">Unified Score Example</a></h4>
<pre><code>Function: process_payment
  Location: src/payments.rs:145

Metrics:
  - Cyclomatic complexity: 18
  - Cognitive complexity: 25
  - Test coverage: 20%
  - Upstream callers: 3 (high dependency)
  - Role: Business Logic

Calculation:
  Complexity Factor = min(10, ((18/10) + (25/20)) Ã— 5) = min(10, 8.75) = 8.75
  Coverage Factor = 10 Ã— (1 - 0.20) Ã— 1.0 = 8.0
  Dependency Factor = 7.5 (3 upstream callers, moderate impact)

  Base Score = (8.75 Ã— 0.40) + (8.0 Ã— 0.40) + (7.5 Ã— 0.20)
             = 3.5 + 3.2 + 1.5
             = 8.2

  Final Score = 8.2 Ã— 1.2 (Business Logic multiplier)
              = 9.84 â†’ CRITICAL priority
</code></pre>
<h3 id="legacy-risk-scoring-pre-02x"><a class="header" href="#legacy-risk-scoring-pre-02x">Legacy Risk Scoring (Pre-0.2.x)</a></h3>
<p>Prior to the unified scoring system, Debtmap used a simpler additive risk formula. This is still available for compatibility but unified scoring is now the default and provides better prioritization.</p>
<h3 id="risk-categories"><a class="header" href="#risk-categories">Risk Categories</a></h3>
<p><strong>Note:</strong> The <code>RiskLevel</code> enum (Low, Medium, High, Critical) is used for <strong>legacy risk scoring compatibility</strong>. When using <strong>unified scoring</strong> (0-10 scale), refer to the priority classifications shown in the Unified Scoring System section above.</p>
<h4 id="legacy-risklevel-enum"><a class="header" href="#legacy-risklevel-enum">Legacy RiskLevel Enum</a></h4>
<p>For legacy risk scoring, Debtmap classifies functions into four risk levels:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub enum RiskLevel {
    Low,       // Score &lt; 10
    Medium,    // Score 10-24
    High,      // Score 25-49
    Critical,  // Score â‰¥ 50
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Critical</strong> (legacy score â‰¥ 50)</p>
<ul>
<li>High complexity (cyclomatic &gt; 15) AND low coverage (&lt; 30%)</li>
<li>Untested code thatâ€™s likely to break and hard to fix</li>
<li><strong>Action</strong>: Immediate attention required - add tests or refactor</li>
</ul>
<p><strong>High</strong> (legacy score 25-49)</p>
<ul>
<li>High complexity (cyclomatic &gt; 10) AND moderate coverage (&lt; 60%)</li>
<li>Risky code with incomplete testing</li>
<li><strong>Action</strong>: Should be addressed soon</li>
</ul>
<p><strong>Medium</strong> (legacy score 10-24)</p>
<ul>
<li>Moderate complexity (cyclomatic &gt; 5) AND low coverage (&lt; 50%)</li>
<li>OR: High complexity with good coverage</li>
<li><strong>Action</strong>: Plan for next sprint</li>
</ul>
<p><strong>Low</strong> (legacy score &lt; 10)</p>
<ul>
<li>Low complexity OR high coverage</li>
<li>Well-managed code</li>
<li><strong>Action</strong>: Monitor, low priority</li>
</ul>
<h4 id="unified-scoring-priority-levels"><a class="header" href="#unified-scoring-priority-levels">Unified Scoring Priority Levels</a></h4>
<p>When using unified scoring (default), functions are classified using the 0-10 scale:</p>
<ul>
<li><strong>Critical</strong> (9.0-10.0): Immediate attention</li>
<li><strong>High</strong> (7.0-8.9): Address this sprint</li>
<li><strong>Medium</strong> (5.0-6.9): Plan for next sprint</li>
<li><strong>Low</strong> (3.0-4.9): Monitor and address as time permits</li>
<li><strong>Minimal</strong> (0.0-2.9): Well-managed code</li>
</ul>
<p><strong>Well-tested complex code</strong> is an <strong>outcome</strong> in both systems, not a separate category:</p>
<ul>
<li>Complex function (cyclomatic 18, cognitive 25) with 95% coverage</li>
<li>Unified score: ~2.5 (Minimal priority due to coverage dampening)</li>
<li>Legacy risk score: ~8 (Low risk)</li>
<li>Falls into low-priority categories because good testing mitigates complexity</li>
<li>This is the desired state for inherently complex business logic</li>
</ul>
<h3 id="legacy-risk-calculation"><a class="header" href="#legacy-risk-calculation">Legacy Risk Calculation</a></h3>
<p><strong>Note:</strong> The legacy risk calculation is still supported for compatibility but has been superseded by the unified scoring system (see above). Unified scoring provides better prioritization through its multi-factor, weighted approach with role-based adjustments.</p>
<p>The legacy risk score uses a simpler additive formula:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>risk_score = complexity_factor + coverage_factor + debt_factor

where:
  complexity_factor = (cyclomatic / 5) + (cognitive / 10)
  coverage_factor = (1 - coverage_percentage) Ã— 50
  debt_factor = debt_score / 10  // If debt data available
<span class="boring">}</span></code></pre></pre>
<p><strong>Example (legacy scoring):</strong></p>
<pre><code>Function: process_payment
  - Cyclomatic complexity: 18
  - Cognitive complexity: 25
  - Coverage: 20%
  - Debt score: 15

Calculation:
  complexity_factor = (18 / 5) + (25 / 10) = 3.6 + 2.5 = 6.1
  coverage_factor = (1 - 0.20) Ã— 50 = 40
  debt_factor = 15 / 10 = 1.5

  risk_score = 6.1 + 40 + 1.5 = 47.6 (HIGH RISK)
</code></pre>
<p><strong>When to use legacy scoring:</strong></p>
<ul>
<li>Comparing with historical data from older Debtmap versions</li>
<li>Teams with existing workflows built around the old scale</li>
<li>Gradual migration to unified scoring</li>
</ul>
<p><strong>Why unified scoring is better:</strong></p>
<ul>
<li>Normalized 0-10 scale is more intuitive</li>
<li>Weighted factors (40% complexity, 40% coverage, 20% dependency) provide better balance</li>
<li>Role multipliers adjust priority based on function importance</li>
<li>Coverage propagation reduces false positives for utility functions</li>
</ul>
<h3 id="test-effort-assessment"><a class="header" href="#test-effort-assessment">Test Effort Assessment</a></h3>
<p>Debtmap estimates testing difficulty based on cognitive complexity:</p>
<p><strong>Difficulty Levels:</strong></p>
<ul>
<li><strong>Trivial</strong> (cognitive &lt; 5): 1-2 test cases, &lt; 1 hour</li>
<li><strong>Simple</strong> (cognitive 5-10): 3-5 test cases, 1-2 hours</li>
<li><strong>Moderate</strong> (cognitive 10-20): 6-10 test cases, 2-4 hours</li>
<li><strong>Complex</strong> (cognitive 20-40): 11-20 test cases, 4-8 hours</li>
<li><strong>VeryComplex</strong> (cognitive &gt; 40): 20+ test cases, 8+ hours</li>
</ul>
<p><strong>Test Effort includes:</strong></p>
<ul>
<li><strong>Cognitive load</strong>: How hard to understand the function</li>
<li><strong>Branch count</strong>: Number of paths to test</li>
<li><strong>Recommended test cases</strong>: Suggested number of tests</li>
</ul>
<h3 id="risk-distribution"><a class="header" href="#risk-distribution">Risk Distribution</a></h3>
<p>Debtmap provides codebase-wide risk metrics:</p>
<pre><code class="language-json">{
  "risk_distribution": {
    "critical_count": 12,
    "high_count": 45,
    "medium_count": 123,
    "low_count": 456,
    "minimal_count": 234,
    "total_functions": 870
  },
  "codebase_risk_score": 1247.5
}
</code></pre>
<p><strong>Interpreting distribution:</strong></p>
<ul>
<li><strong>Healthy codebase</strong>: Most functions in Low/Minimal priority (unified scoring) or Low/WellTested (legacy)</li>
<li><strong>Needs attention</strong>: Many Critical/High priority functions</li>
<li><strong>Technical debt</strong>: High codebase risk score</li>
</ul>
<p><strong>Note on minimal_count and well-tested functions:</strong></p>
<p>In unified scoring, <code>minimal_count</code> represents all functions scoring 0-2.9, which naturally includes:</p>
<ul>
<li>Simple utility functions</li>
<li>Helper functions with low complexity</li>
<li><strong>Well-tested complex code</strong> that scores low due to coverage dampening</li>
</ul>
<p>This is not a special category but an <strong>outcome</strong> of the unified scoring system. Complex business logic with 95% test coverage appropriately receives a minimal score (0-2.9), reflecting that good testing mitigates complexity risk. These functions are correctly de-prioritized because theyâ€™re well-managed, not because they need special handling.</p>
<h3 id="testing-recommendations"><a class="header" href="#testing-recommendations">Testing Recommendations</a></h3>
<p>When coverage data is provided, Debtmap generates prioritized testing recommendations with ROI analysis:</p>
<pre><code class="language-json">{
  "function": "process_transaction",
  "file": "src/payments.rs",
  "line": 145,
  "current_risk": 47.6,
  "potential_risk_reduction": 35.2,
  "test_effort_estimate": {
    "estimated_difficulty": "Complex",
    "cognitive_load": 25,
    "branch_count": 18,
    "recommended_test_cases": 12
  },
  "roi": 4.4,
  "rationale": "High complexity with low coverage (20%) and 3 downstream dependencies. Testing will reduce risk by 74%.",
  "dependencies": {
    "upstream_callers": ["handle_payment_request"],
    "downstream_callees": ["validate_amount", "check_balance", "record_transaction"]
  }
}
</code></pre>
<p><strong>ROI calculation:</strong></p>
<pre><code>roi = potential_risk_reduction / estimated_effort_hours
</code></pre>
<p>Higher ROI = better return on testing investment</p>
<h2 id="interpreting-results"><a class="header" href="#interpreting-results">Interpreting Results</a></h2>
<h3 id="understanding-output-formats"><a class="header" href="#understanding-output-formats">Understanding Output Formats</a></h3>
<p>Debtmap provides three output formats:</p>
<p><strong>Terminal</strong> (default): Human-readable with colors and tables</p>
<pre><code class="language-bash">debtmap analyze .
</code></pre>
<p><strong>JSON</strong>: Machine-readable for CI/CD integration</p>
<pre><code class="language-bash">debtmap analyze . --format json --output report.json
</code></pre>
<p><strong>Markdown</strong>: Documentation-friendly</p>
<pre><code class="language-bash">debtmap analyze . --format markdown --output report.md
</code></pre>
<h3 id="json-structure"><a class="header" href="#json-structure">JSON Structure</a></h3>
<pre><code class="language-json">{
  "timestamp": "2025-10-09T12:00:00Z",
  "project_path": "/path/to/project",
  "complexity": {
    "metrics": [
      {
        "name": "process_data",
        "file": "src/main.rs",
        "line": 42,
        "cyclomatic": 15,
        "cognitive": 22,
        "nesting": 4,
        "length": 68,
        "is_test": false,
        "visibility": "Public",
        "is_trait_method": false,
        "in_test_module": false,
        "entropy_score": {
          "token_entropy": 0.65,
          "pattern_repetition": 0.25,
          "branch_similarity": 0.30,
          "effective_complexity": 0.85
        },
        "is_pure": false,
        "purity_confidence": 0.8,
        "detected_patterns": ["validation_pattern"],
        "upstream_callers": ["main", "process_request"],
        "downstream_callees": ["validate", "save", "notify"]
      }
    ],
    "summary": {
      "total_functions": 150,
      "average_complexity": 5.3,
      "max_complexity": 22,
      "high_complexity_count": 8
    }
  },
  "technical_debt": {
    "items": [
      {
        "id": "complexity_src_main_rs_42",
        "debt_type": "Complexity",
        "priority": "High",
        "file": "src/main.rs",
        "line": 42,
        "column": 1,
        "message": "Function exceeds complexity threshold",
        "context": "Cyclomatic: 15, Cognitive: 22"
      }
    ],
    "by_type": {
      "Complexity": [...],
      "Duplication": [...],
      "Todo": [...]
    }
  }
}
</code></pre>
<h4 id="json-output-format-variants"><a class="header" href="#json-output-format-variants">JSON Output Format Variants</a></h4>
<p>Debtmap supports two JSON output format variants for different integration needs:</p>
<p><strong>Legacy Format (default):</strong></p>
<ul>
<li>Uses wrapper objects: <code>{"File": {...}}</code> and <code>{"Function": {...}}</code></li>
<li>Compatible with existing tooling and scripts</li>
<li>Shown in the JSON structure example above</li>
</ul>
<p><strong>Unified Format (spec 108 - future enhancement):</strong></p>
<ul>
<li>Uses consistent structure with <code>"type"</code> field discriminator</li>
<li>Simpler parsing for new integrations</li>
<li>Example structure:</li>
</ul>
<pre><code class="language-json">{
  "type": "function",
  "name": "process_data",
  "file": "src/main.rs",
  "line": 42,
  "metrics": { /* ... */ }
}
</code></pre>
<p><strong>Note:</strong> The unified format is currently an internal representation. If you need this format exposed as a CLI option, please open a feature request on GitHub. The legacy format remains the stable default for all current integrations.</p>
<h3 id="reading-function-metrics"><a class="header" href="#reading-function-metrics">Reading Function Metrics</a></h3>
<p><strong>Key fields:</strong></p>
<ul>
<li><code>cyclomatic</code>: Decision points - guides test case count</li>
<li><code>cognitive</code>: Understanding difficulty - guides refactoring priority</li>
<li><code>nesting</code>: Indentation depth - signals need for extraction</li>
<li><code>length</code>: Lines of code - signals SRP violations</li>
<li><code>visibility</code>: Function visibility (<code>"Private"</code>, <code>"Crate"</code>, or <code>"Public"</code> from FunctionVisibility enum)</li>
<li><code>is_pure</code>: No side effects - easier to test (Option type, may be None)</li>
<li><code>purity_confidence</code>: How certain we are about purity 0.0-1.0 (Option type, may be None)</li>
<li><code>is_trait_method</code>: Whether this function implements a trait method</li>
<li><code>in_test_module</code>: Whether function is inside a <code>#[cfg(test)]</code> module</li>
<li><code>detected_patterns</code>: Complexity adjustment patterns identified (e.g., â€œvalidation_patternâ€)</li>
<li><code>entropy_score</code>: Pattern analysis for false positive reduction</li>
<li><code>upstream_callers</code>: Impact radius if this function breaks</li>
<li><code>downstream_callees</code>: Functions this depends on</li>
</ul>
<p><strong>Entropy interpretation:</strong></p>
<ul>
<li><code>token_entropy &lt; 0.4</code>: Repetitive code, likely pattern-based</li>
<li><code>pattern_repetition &gt; 0.7</code>: High similarity between blocks</li>
<li><code>branch_similarity &gt; 0.8</code>: Similar conditional branches</li>
<li><code>effective_complexity &lt; 1.0</code>: Dampening applied</li>
</ul>
<h3 id="prioritizing-work"><a class="header" href="#prioritizing-work">Prioritizing Work</a></h3>
<p>Debtmap provides multiple prioritization strategies, with <strong>unified scoring (0-10 scale)</strong> as the recommended default for most workflows:</p>
<p><strong>1. By Unified Score (default - recommended)</strong></p>
<pre><code class="language-bash">debtmap analyze . --top 10
</code></pre>
<p>Shows top 10 items by <strong>combined complexity, coverage, and dependency factors</strong>, weighted and adjusted by function role.</p>
<p><strong>Why use unified scoring:</strong></p>
<ul>
<li>Balances complexity (40%), coverage (40%), and dependency impact (20%)</li>
<li>Adjusts for function importance (entry points prioritized over utilities)</li>
<li>Normalized 0-10 scale is intuitive and consistent</li>
<li>Reduces false positives through coverage propagation</li>
<li>Best for <strong>sprint planning</strong> and <strong>function-level refactoring decisions</strong></li>
</ul>
<p><strong>Example:</strong></p>
<pre><code class="language-bash"># Show top 20 critical items
debtmap analyze . --min-priority 7.0 --top 20

# Focus on high-impact functions (score &gt;= 7.0)
debtmap analyze . --format json | jq '.functions[] | select(.unified_score &gt;= 7.0)'
</code></pre>
<p><strong>2. By Risk Category (legacy compatibility)</strong></p>
<pre><code class="language-bash">debtmap analyze . --min-priority high
</code></pre>
<p>Shows only HIGH and CRITICAL priority items using legacy risk scoring.</p>
<p><strong>Note:</strong> Legacy risk scoring uses additive formulas and unbounded scales. Prefer unified scoring for new workflows.</p>
<p><strong>3. By Debt Type</strong></p>
<pre><code class="language-bash">debtmap analyze . --filter Architecture,Testing
</code></pre>
<p>Focuses on specific categories:</p>
<ul>
<li><code>Architecture</code>: God objects, complexity, dead code</li>
<li><code>Testing</code>: Coverage gaps, test quality</li>
<li><code>Performance</code>: Resource leaks, inefficiencies</li>
<li><code>CodeQuality</code>: Code smells, maintainability</li>
</ul>
<p><strong>4. By ROI (with coverage)</strong></p>
<pre><code class="language-bash">debtmap analyze . --lcov coverage.lcov --top 20
</code></pre>
<p>Prioritizes by return on investment for testing/refactoring. Combines unified scoring with test effort estimates to identify high-value work.</p>
<p><strong>Choosing the right strategy:</strong></p>
<ul>
<li><strong>Sprint planning for developers</strong>: Use unified scoring (<code>--top N</code>)</li>
<li><strong>Architectural review</strong>: Use tiered prioritization (<code>--summary</code>)</li>
<li><strong>Category-focused work</strong>: Use debt type filtering (<code>--filter</code>)</li>
<li><strong>Testing priorities</strong>: Use ROI analysis with coverage data (<code>--lcov</code>)</li>
<li><strong>Historical comparisons</strong>: Use legacy risk scoring (for consistency with old reports)</li>
</ul>
<h3 id="tiered-prioritization"><a class="header" href="#tiered-prioritization">Tiered Prioritization</a></h3>
<p><strong>Note:</strong> Tiered prioritization uses <strong>traditional debt scoring</strong> (additive, higher = worse) and is complementary to the unified scoring system (0-10 scale). Both systems can be used together:</p>
<ul>
<li><strong>Unified scoring</strong> (0-10 scale): Best for <strong>function-level prioritization</strong> and sprint planning</li>
<li><strong>Tiered prioritization</strong> (debt tiers): Best for <strong>architectural focus</strong> and strategic debt planning</li>
</ul>
<p>Use <code>--summary</code> for tiered view focusing on architectural issues, or default output for function-level unified scores.</p>
<p>Debtmap uses a tier-based system to map debt scores to actionable priority levels. Each tier includes effort estimates and strategic guidance for efficient debt remediation.</p>
<h4 id="tier-levels"><a class="header" href="#tier-levels">Tier Levels</a></h4>
<p>The <code>Tier</code> enum defines four priority levels based on score thresholds:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub enum Tier {
    Critical,  // Score â‰¥ 90
    High,      // Score 70-89.9
    Moderate,  // Score 50-69.9
    Low,       // Score &lt; 50
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Score-to-Tier Mapping:</strong></p>
<ul>
<li><strong>Critical</strong> (â‰¥ 90): Immediate action required - blocks progress</li>
<li><strong>High</strong> (70-89.9): Should be addressed this sprint</li>
<li><strong>Moderate</strong> (50-69.9): Plan for next sprint</li>
<li><strong>Low</strong> (&lt; 50): Background maintenance work</li>
</ul>
<h4 id="effort-estimates-per-tier"><a class="header" href="#effort-estimates-per-tier">Effort Estimates Per Tier</a></h4>
<p>Each tier includes estimated effort based on typical remediation patterns:</p>
<div class="table-wrapper"><table><thead><tr><th>Tier</th><th>Estimated Effort</th><th>Typical Work</th></tr></thead><tbody>
<tr><td><strong>Critical</strong></td><td>1-2 days</td><td>Major refactoring, comprehensive testing, architectural changes</td></tr>
<tr><td><strong>High</strong></td><td>2-4 hours</td><td>Extract functions, add test coverage, fix resource leaks</td></tr>
<tr><td><strong>Moderate</strong></td><td>1-2 hours</td><td>Simplify logic, reduce duplication, improve error handling</td></tr>
<tr><td><strong>Low</strong></td><td>30 minutes</td><td>Address TODOs, minor cleanup, documentation</td></tr>
</tbody></table>
</div>
<p><strong>Effort calculation considers:</strong></p>
<ul>
<li>Complexity metrics (cyclomatic, cognitive)</li>
<li>Test coverage gaps</li>
<li>Number of dependencies (upstream/downstream)</li>
<li>Debt category (Architecture debt takes longer than CodeQuality)</li>
</ul>
<h4 id="tiered-display-grouping"><a class="header" href="#tiered-display-grouping">Tiered Display Grouping</a></h4>
<p><code>TieredDisplay</code> groups similar debt items for batch action recommendations:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct TieredDisplay {
    pub tier: Tier,
    pub items: Vec&lt;DebtItem&gt;,
    pub total_score: f64,
    pub estimated_total_effort_hours: f64,
    pub batch_recommendations: Vec&lt;String&gt;,
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Grouping strategy:</strong></p>
<ul>
<li>Groups items by tier and similarity pattern</li>
<li>Prevents grouping of god objects (always show individually)</li>
<li>Prevents grouping of Critical items (each needs individual attention)</li>
<li>Suggests batch actions for similar Low/Moderate items</li>
</ul>
<p><strong>Example batch recommendations:</strong></p>
<pre><code class="language-json">{
  "tier": "Moderate",
  "total_score": 245.8,
  "estimated_total_effort_hours": 12.5,
  "batch_recommendations": [
    "Extract 5 validation functions from similar patterns",
    "Add test coverage for 8 moderately complex functions (grouped by module)",
    "Refactor 3 functions with similar nested loop patterns"
  ]
}
</code></pre>
<h4 id="using-tiered-prioritization"><a class="header" href="#using-tiered-prioritization">Using Tiered Prioritization</a></h4>
<p><strong>1. Start with Critical tier:</strong></p>
<pre><code class="language-bash">debtmap analyze . --min-priority critical
</code></pre>
<p>Focus on items with score â‰¥ 90. These typically represent:</p>
<ul>
<li>Complex functions with 0% coverage</li>
<li>God objects blocking feature development</li>
<li>Critical resource leaks or security issues</li>
</ul>
<p><strong>2. Plan High tier work:</strong></p>
<pre><code class="language-bash">debtmap analyze . --min-priority high --format json &gt; sprint-plan.json
</code></pre>
<p>Schedule 2-4 hours per item for this sprint. Look for:</p>
<ul>
<li>Functions approaching complexity thresholds</li>
<li>Moderate coverage gaps on important code paths</li>
<li>Performance bottlenecks with clear solutions</li>
</ul>
<p><strong>3. Batch Moderate tier items:</strong></p>
<pre><code class="language-bash">debtmap analyze . --min-priority moderate
</code></pre>
<p>Review batch recommendations. Examples:</p>
<ul>
<li>â€œ10 validation functions detected - extract common patternâ€</li>
<li>â€œ5 similar test files with duplication - create shared fixturesâ€</li>
<li>â€œ8 functions with magic values - create constants moduleâ€</li>
</ul>
<p><strong>4. Schedule Low tier background work:</strong>
Address during slack time or as warm-up tasks for new contributors.</p>
<h4 id="strategic-guidance-by-tier"><a class="header" href="#strategic-guidance-by-tier">Strategic Guidance by Tier</a></h4>
<p><strong>Critical Tier Strategy:</strong></p>
<ul>
<li><strong>Block new features</strong> until addressed</li>
<li><strong>Pair programming</strong> recommended for complex items</li>
<li><strong>Architectural review</strong> before major refactoring</li>
<li><strong>Comprehensive testing</strong> after changes</li>
</ul>
<p><strong>High Tier Strategy:</strong></p>
<ul>
<li><strong>Sprint planning priority</strong></li>
<li><strong>Impact analysis</strong> before changes</li>
<li><strong>Code review</strong> from senior developers</li>
<li><strong>Integration testing</strong> after changes</li>
</ul>
<p><strong>Moderate Tier Strategy:</strong></p>
<ul>
<li><strong>Batch similar items</strong> for efficiency</li>
<li><strong>Extract patterns</strong> across multiple files</li>
<li><strong>Incremental improvement</strong> over multiple PRs</li>
<li><strong>Regression testing</strong> for affected areas</li>
</ul>
<p><strong>Low Tier Strategy:</strong></p>
<ul>
<li><strong>Good first issues</strong> for new contributors</li>
<li><strong>Documentation improvements</strong></li>
<li><strong>Code cleanup</strong> during refactoring nearby code</li>
<li><strong>Technical debt gardening</strong> sessions</li>
</ul>
<h3 id="categorized-debt-analysis"><a class="header" href="#categorized-debt-analysis">Categorized Debt Analysis</a></h3>
<p>Debtmap provides <code>CategorizedDebt</code> analysis that groups debt items by category and identifies cross-category dependencies. This helps teams understand strategic relationships between different types of technical debt.</p>
<h4 id="categorysummary"><a class="header" href="#categorysummary">CategorySummary</a></h4>
<p>Each category gets a summary with metrics for planning:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct CategorySummary {
    pub category: DebtCategory,
    pub total_score: f64,
    pub item_count: usize,
    pub estimated_effort_hours: f64,
    pub average_severity: f64,
    pub top_items: Vec&lt;DebtItem&gt;,  // Up to 5 highest priority
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Effort estimation formulas:</strong></p>
<ul>
<li><strong>Architecture debt</strong>: <code>complexity_score / 10 Ã— 2</code> hours (structural changes take longer)</li>
<li><strong>Testing debt</strong>: <code>complexity_score / 10 Ã— 1.5</code> hours (writing tests)</li>
<li><strong>Performance debt</strong>: <code>complexity_score / 10 Ã— 1.8</code> hours (profiling + optimization)</li>
<li><strong>CodeQuality debt</strong>: <code>complexity_score / 10 Ã— 1.2</code> hours (refactoring)</li>
</ul>
<p><strong>Example category summary:</strong></p>
<pre><code class="language-json">{
  "category": "Architecture",
  "total_score": 487.5,
  "item_count": 15,
  "estimated_effort_hours": 97.5,
  "average_severity": 32.5,
  "top_items": [
    {
      "debt_type": "GodObject",
      "file": "src/services/user_service.rs",
      "score": 95.0,
      "estimated_effort_hours": 16.0
    },
    {
      "debt_type": "ComplexityHotspot",
      "file": "src/payments/processor.rs",
      "score": 87.3,
      "estimated_effort_hours": 14.0
    }
  ]
}
</code></pre>
<h4 id="cross-category-dependencies"><a class="header" href="#cross-category-dependencies">Cross-Category Dependencies</a></h4>
<p><code>CrossCategoryDependency</code> identifies blocking relationships between different debt categories:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct CrossCategoryDependency {
    pub from_category: DebtCategory,
    pub to_category: DebtCategory,
    pub blocking_items: Vec&lt;(DebtItem, DebtItem)&gt;,
    pub impact_level: ImpactLevel,  // Critical, High, Medium, Low
    pub recommendation: String,
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Common dependency patterns:</strong></p>
<p><strong>1. Architecture blocks Testing:</strong></p>
<ul>
<li><strong>Pattern</strong>: God objects are too complex to test effectively</li>
<li><strong>Example</strong>: <code>UserService</code> has 50+ functions, making comprehensive testing impractical</li>
<li><strong>Impact</strong>: Critical - cannot improve test coverage without refactoring</li>
<li><strong>Recommendation</strong>: â€œSplit god object into 4-5 focused modules before adding testsâ€</li>
</ul>
<p><strong>2. Async issues require Architecture changes:</strong></p>
<ul>
<li><strong>Pattern</strong>: Blocking I/O in async contexts requires architectural redesign</li>
<li><strong>Example</strong>: Sync database calls in async handlers</li>
<li><strong>Impact</strong>: High - performance problems require design changes</li>
<li><strong>Recommendation</strong>: â€œIntroduce async database layer before optimizing handlersâ€</li>
</ul>
<p><strong>3. Complexity affects Testability:</strong></p>
<ul>
<li><strong>Pattern</strong>: High cyclomatic complexity makes thorough testing difficult</li>
<li><strong>Example</strong>: Function with 22 branches needs 22+ test cases</li>
<li><strong>Impact</strong>: High - testing effort grows exponentially with complexity</li>
<li><strong>Recommendation</strong>: â€œReduce complexity to &lt; 10 before writing comprehensive testsâ€</li>
</ul>
<p><strong>4. Performance requires Architecture:</strong></p>
<ul>
<li><strong>Pattern</strong>: O(nÂ²) nested loops need different data structures</li>
<li><strong>Example</strong>: Linear search in loops should use HashMap</li>
<li><strong>Impact</strong>: Medium - optimization requires structural changes</li>
<li><strong>Recommendation</strong>: â€œRefactor data structure before micro-optimizationsâ€</li>
</ul>
<p><strong>Example cross-category dependency:</strong></p>
<pre><code class="language-json">{
  "from_category": "Architecture",
  "to_category": "Testing",
  "impact_level": "Critical",
  "blocking_items": [
    {
      "blocker": {
        "debt_type": "GodObject",
        "file": "src/services/user_service.rs",
        "functions": 52,
        "score": 95.0
      },
      "blocked": {
        "debt_type": "TestingGap",
        "file": "src/services/user_service.rs",
        "coverage": 15,
        "score": 78.0
      }
    }
  ],
  "recommendation": "Split UserService into focused modules (auth, profile, settings, notifications) before attempting to improve test coverage. Current structure makes comprehensive testing impractical.",
  "estimated_unblock_effort_hours": 16.0
}
</code></pre>
<h4 id="using-categorized-debt-analysis"><a class="header" href="#using-categorized-debt-analysis">Using Categorized Debt Analysis</a></h4>
<p><strong>View all category summaries:</strong></p>
<pre><code class="language-bash">debtmap analyze . --format json | jq '.categorized_debt.summaries'
</code></pre>
<p><strong>Focus on specific category:</strong></p>
<pre><code class="language-bash">debtmap analyze . --filter Architecture --top 10
</code></pre>
<p><strong>Identify blocking relationships:</strong></p>
<pre><code class="language-bash">debtmap analyze . --format json | jq '.categorized_debt.cross_category_dependencies[] | select(.impact_level == "Critical")'
</code></pre>
<p><strong>Strategic planning workflow:</strong></p>
<ol>
<li>
<p><strong>Review category summaries:</strong></p>
<ul>
<li>Identify which category has highest total score</li>
<li>Check estimated effort hours per category</li>
<li>Note average severity to gauge urgency</li>
</ul>
</li>
<li>
<p><strong>Check cross-category dependencies:</strong></p>
<ul>
<li>Find Critical and High impact blockers</li>
<li>Prioritize blockers before blocked items</li>
<li>Plan architectural changes before optimization</li>
</ul>
</li>
<li>
<p><strong>Plan remediation order:</strong></p>
<pre><code>Example decision tree:
- Architecture score &gt; 400? â†’ Address god objects first
- Testing gap with low complexity? â†’ Quick wins, add tests
- Performance issues + architecture debt? â†’ Refactor structure first
- High code quality debt but good architecture? â†’ Incremental cleanup
</code></pre>
</li>
<li>
<p><strong>Use category-specific strategies:</strong></p>
<ul>
<li><strong>Architecture</strong>: Pair programming, design reviews, incremental refactoring</li>
<li><strong>Testing</strong>: TDD for new code, characterization tests for legacy</li>
<li><strong>Performance</strong>: Profiling first, optimize hot paths, avoid premature optimization</li>
<li><strong>CodeQuality</strong>: Code review focus, linting rules, consistent patterns</li>
</ul>
</li>
</ol>
<h4 id="categorizeddebt-output-structure"><a class="header" href="#categorizeddebt-output-structure">CategorizedDebt Output Structure</a></h4>
<pre><code class="language-json">{
  "categorized_debt": {
    "summaries": [
      {
        "category": "Architecture",
        "total_score": 487.5,
        "item_count": 15,
        "estimated_effort_hours": 97.5,
        "average_severity": 32.5,
        "top_items": [...]
      },
      {
        "category": "Testing",
        "total_score": 356.2,
        "item_count": 23,
        "estimated_effort_hours": 53.4,
        "average_severity": 15.5,
        "top_items": [...]
      },
      {
        "category": "Performance",
        "total_score": 234.8,
        "item_count": 12,
        "estimated_effort_hours": 42.3,
        "average_severity": 19.6,
        "top_items": [...]
      },
      {
        "category": "CodeQuality",
        "total_score": 189.3,
        "item_count": 31,
        "estimated_effort_hours": 22.7,
        "average_severity": 6.1,
        "top_items": [...]
      }
    ],
    "cross_category_dependencies": [
      {
        "from_category": "Architecture",
        "to_category": "Testing",
        "impact_level": "Critical",
        "blocking_items": [...],
        "recommendation": "..."
      }
    ]
  }
}
</code></pre>
<h3 id="debt-density-metric"><a class="header" href="#debt-density-metric">Debt Density Metric</a></h3>
<p>Debt density normalizes technical debt scores across projects of different sizes, providing a per-1000-lines-of-code metric for fair comparison.</p>
<h4 id="formula"><a class="header" href="#formula">Formula</a></h4>
<pre><code>debt_density = (total_debt_score / total_lines_of_code) Ã— 1000
</code></pre>
<p><strong>Example calculation:</strong></p>
<pre><code>Project A:
  - Total debt score: 1,250
  - Total lines of code: 25,000
  - Debt density: (1,250 / 25,000) Ã— 1000 = 50

Project B:
  - Total debt score: 2,500
  - Total lines of code: 50,000
  - Debt density: (2,500 / 50,000) Ã— 1000 = 50
</code></pre>
<p>Projects A and B have <strong>equal debt density</strong> (50) despite B having twice the absolute debt, because B is also twice as large. They have proportionally similar technical debt.</p>
<h4 id="interpretation-guidelines"><a class="header" href="#interpretation-guidelines">Interpretation Guidelines</a></h4>
<p>Use these thresholds to assess codebase health:</p>
<div class="table-wrapper"><table><thead><tr><th>Debt Density</th><th>Assessment</th><th>Description</th></tr></thead><tbody>
<tr><td><strong>0-50</strong></td><td>Clean</td><td>Well-maintained codebase, minimal debt</td></tr>
<tr><td><strong>51-100</strong></td><td>Moderate</td><td>Typical technical debt, manageable</td></tr>
<tr><td><strong>101-150</strong></td><td>High</td><td>Significant debt, prioritize remediation</td></tr>
<tr><td><strong>150+</strong></td><td>Critical</td><td>Severe debt burden, may impede development</td></tr>
</tbody></table>
</div>
<p><strong>Context matters:</strong></p>
<ul>
<li><strong>Early-stage projects</strong>: Often have higher density (rapid iteration)</li>
<li><strong>Mature projects</strong>: Should trend toward lower density over time</li>
<li><strong>Legacy systems</strong>: May have high density, track trend over time</li>
<li><strong>Greenfield rewrites</strong>: Aim for density &lt; 50</li>
</ul>
<h4 id="using-debt-density"><a class="header" href="#using-debt-density">Using Debt Density</a></h4>
<p><strong>1. Compare projects fairly:</strong></p>
<pre><code class="language-bash"># Small microservice (5,000 LOC, debt = 250)
# Debt density: 50

# Large monolith (100,000 LOC, debt = 5,000)
# Debt density: 50

# Equal health despite size difference
</code></pre>
<p><strong>2. Track improvement over time:</strong></p>
<pre><code>Sprint 1: 50,000 LOC, debt = 7,500, density = 150 (High)
Sprint 5: 52,000 LOC, debt = 6,500, density = 125 (Improving)
Sprint 10: 54,000 LOC, debt = 4,860, density = 90 (Moderate)
</code></pre>
<p><strong>3. Set team goals:</strong></p>
<pre><code>Current density: 120
Target density: &lt; 80 (by Q4)
Reduction needed: 40 points

Strategy:
- Fix 2-3 Critical items per sprint
- Prevent new debt (enforce thresholds)
- Refactor before adding features in high-debt modules
</code></pre>
<p><strong>4. Benchmark across teams/projects:</strong></p>
<pre><code class="language-json">{
  "team_metrics": [
    {
      "project": "auth-service",
      "debt_density": 45,
      "assessment": "Clean",
      "trend": "stable"
    },
    {
      "project": "billing-service",
      "debt_density": 95,
      "assessment": "Moderate",
      "trend": "improving"
    },
    {
      "project": "legacy-api",
      "debt_density": 165,
      "assessment": "Critical",
      "trend": "worsening"
    }
  ]
}
</code></pre>
<h4 id="limitations"><a class="header" href="#limitations">Limitations</a></h4>
<p><strong>Debt density doesnâ€™t account for:</strong></p>
<ul>
<li><strong>Code importance</strong>: 100 LOC in payment logic â‰  100 LOC in logging utils</li>
<li><strong>Complexity distribution</strong>: One 1000-line god object vs. 1000 simple functions</li>
<li><strong>Test coverage</strong>: 50% coverage on critical paths vs. low-priority features</li>
<li><strong>Team familiarity</strong>: New codebase vs. well-understood legacy system</li>
</ul>
<p><strong>Best practices:</strong></p>
<ul>
<li>Use density as <strong>one metric among many</strong></li>
<li>Combine with category analysis and tiered prioritization</li>
<li>Focus on <strong>trend</strong> (improving/stable/worsening) over absolute number</li>
<li>Consider <strong>debt per module</strong> for more granular insights</li>
</ul>
<h4 id="debt-density-in-cicd"><a class="header" href="#debt-density-in-cicd">Debt Density in CI/CD</a></h4>
<p><strong>Track density over time:</strong></p>
<pre><code class="language-bash"># Generate report with density
debtmap analyze . --format json --output debt-report.json

# Extract density for trending
DENSITY=$(jq '.debt_density' debt-report.json)

# Store in metrics database
echo "debtmap.density:${DENSITY}|g" | nc -u -w0 statsd 8125
</code></pre>
<p><strong>Set threshold gates:</strong></p>
<pre><code class="language-yaml"># .github/workflows/debt-check.yml
- name: Check debt density
  run: |
    DENSITY=$(debtmap analyze . --format json | jq '.debt_density')
    if (( $(echo "$DENSITY &gt; 150" | bc -l) )); then
      echo "âŒ Debt density too high: $DENSITY (limit: 150)"
      exit 1
    fi
    echo "âœ… Debt density acceptable: $DENSITY"
</code></pre>
<h3 id="actionable-insights"><a class="header" href="#actionable-insights">Actionable Insights</a></h3>
<p>Each recommendation includes:</p>
<p><strong>ACTION</strong>: What to do</p>
<ul>
<li>â€œAdd 6 unit tests for full coverageâ€</li>
<li>â€œRefactor into 3 smaller functionsâ€</li>
<li>â€œExtract validation to separate functionâ€</li>
</ul>
<p><strong>IMPACT</strong>: Expected improvement</p>
<ul>
<li>â€œFull test coverage, -3.7 riskâ€</li>
<li>â€œReduce complexity from 22 to 8â€</li>
<li>â€œEliminate 120 lines of duplicationâ€</li>
</ul>
<p><strong>WHY</strong>: Rationale</p>
<ul>
<li>â€œBusiness logic with 0% coverage, manageable complexityâ€</li>
<li>â€œHigh complexity with low coverage threatens stabilityâ€</li>
<li>â€œRepeated validation pattern across 5 filesâ€</li>
</ul>
<p><strong>Example workflow:</strong></p>
<ol>
<li>Run analysis with coverage: <code>debtmap analyze . --lcov coverage.lcov</code></li>
<li>Filter to CRITICAL items: <code>--min-priority critical</code></li>
<li>Review top 5 recommendations</li>
<li>Start with highest ROI items</li>
<li>Rerun analysis to track progress</li>
</ol>
<h3 id="common-patterns-to-recognize"><a class="header" href="#common-patterns-to-recognize">Common Patterns to Recognize</a></h3>
<p><strong>Pattern 1: High Complexity, Well Tested</strong></p>
<pre><code>Complexity: 25, Coverage: 95%, Risk: LOW
</code></pre>
<p>This is actually good! Complex but thoroughly tested code. Learn from this approach.</p>
<p><strong>Pattern 2: Moderate Complexity, No Tests</strong></p>
<pre><code>Complexity: 12, Coverage: 0%, Risk: CRITICAL
</code></pre>
<p>Highest priority - manageable complexity, should be easy to test.</p>
<p><strong>Pattern 3: Low Complexity, No Tests</strong></p>
<pre><code>Complexity: 3, Coverage: 0%, Risk: LOW
</code></pre>
<p>Low priority - simple code, less risky without tests.</p>
<p><strong>Pattern 4: Repetitive High Complexity (Dampened)</strong></p>
<pre><code>Cyclomatic: 20, Effective: 7 (65% dampened), Risk: LOW
</code></pre>
<p>Validation or dispatch pattern - looks complex but is repetitive. Lower priority.</p>
<p><strong>Pattern 5: God Object</strong></p>
<pre><code>File: services.rs, Functions: 50+, Responsibilities: 15+
</code></pre>
<p>Architectural issue - split before adding features.</p>
<h2 id="analyzer-types"><a class="header" href="#analyzer-types">Analyzer Types</a></h2>
<p>Debtmap supports multiple programming languages with varying levels of analysis capability.</p>
<h3 id="supported-languages"><a class="header" href="#supported-languages">Supported Languages</a></h3>
<p><strong>Rust</strong> (Full Support)</p>
<ul>
<li><strong>Parser</strong>: syn (native Rust AST)</li>
<li><strong>Capabilities</strong>:
<ul>
<li>Full complexity metrics (cyclomatic, cognitive, entropy)</li>
<li>Trait implementation tracking</li>
<li>Purity detection with confidence scoring</li>
<li>Call graph analysis (upstream callers, downstream callees)</li>
<li>Semantic function classification (entry points, business logic, data access, infrastructure, utilities, test code)</li>
<li>Enhanced call graph with transitive relationships</li>
<li>Macro expansion support for accurate complexity analysis</li>
<li>Pattern-based adjustments for macros and code generation</li>
<li>Visibility tracking (pub, pub(crate), private)</li>
<li>Test module detection (#[cfg(test)])</li>
</ul>
</li>
</ul>
<p><strong>Semantic Classification:</strong></p>
<p>Debtmap automatically identifies function roles in Rust code to apply appropriate role multipliers in unified scoring:</p>
<ul>
<li><strong>Entry Points</strong>: Functions named <code>main</code>, <code>start</code>, or public functions in <code>bin/</code> modules</li>
<li><strong>Business Logic</strong>: Core domain functions with complex logic, algorithms, business rules</li>
<li><strong>Data Access</strong>: Functions performing database queries, file I/O, network operations</li>
<li><strong>Infrastructure</strong>: Logging, configuration, monitoring, error handling utilities</li>
<li><strong>Utilities</strong>: Helper functions, formatters, type converters, validation functions</li>
<li><strong>Test Code</strong>: Functions in <code>#[cfg(test)]</code> modules, functions with <code>#[test]</code> attribute</li>
</ul>
<p>This classification feeds directly into the unified scoring systemâ€™s role multiplier (see Risk Scoring section).</p>
<p><strong>Python</strong> (Partial Support)</p>
<ul>
<li><strong>Parser</strong>: rustpython-parser</li>
<li><strong>Capabilities</strong>:
<ul>
<li>Complexity metrics (cyclomatic, cognitive)</li>
<li>Python-specific error handling patterns</li>
<li>Purity detection for pure functions</li>
<li>Basic debt pattern detection</li>
<li>Limited call graph support</li>
</ul>
</li>
</ul>
<p><strong>JavaScript</strong> (Partial Support)</p>
<ul>
<li><strong>Parser</strong>: tree-sitter (JavaScript grammar)</li>
<li><strong>File extensions</strong>: .js, .jsx, .mjs, .cjs</li>
<li><strong>Capabilities</strong>:
<ul>
<li>ECMAScript complexity patterns</li>
<li>Basic complexity metrics</li>
<li>Function extraction</li>
<li>Limited pattern detection</li>
</ul>
</li>
</ul>
<p><strong>TypeScript</strong> (Partial Support)</p>
<ul>
<li><strong>Parser</strong>: tree-sitter (TypeScript grammar)</li>
<li><strong>File extensions</strong>: .ts, .tsx, .mts, .cts</li>
<li><strong>Capabilities</strong>:
<ul>
<li>Similar to JavaScript support</li>
<li>Type information currently not utilized</li>
<li>Basic complexity metrics</li>
<li>Limited pattern detection</li>
</ul>
</li>
</ul>
<p><strong>Unsupported Languages:</strong></p>
<p>Debtmapâ€™s <code>Language</code> enum contains only the four supported languages: Rust, Python, JavaScript, and TypeScript. Files with unsupported extensions are filtered out during the file discovery phase and never reach the analysis stage.</p>
<p>Files with extensions like <code>.cpp</code> (C++), <code>.java</code>, <code>.go</code>, <code>.rb</code> (Ruby), <code>.php</code>, <code>.cs</code> (C#), <code>.swift</code>, <code>.kt</code> (Kotlin), <code>.scala</code>, and others are silently filtered during discovery.</p>
<p><strong>File filtering behavior:</strong></p>
<ul>
<li>Discovery scans project for files matching supported extensions</li>
<li>Unsupported files are skipped silently (no warnings or errors)</li>
<li>No analysis, metrics, or debt patterns are generated for filtered files</li>
<li>Use <code>--languages</code> flag to explicitly control which languages to analyze</li>
</ul>
<p><strong>Example:</strong></p>
<pre><code class="language-bash"># Only analyze Rust files (skip Python/JS/TS)
debtmap analyze . --languages rust

# Analyze Rust and Python only
debtmap analyze . --languages rust,python
</code></pre>
<h3 id="language-detection"><a class="header" href="#language-detection">Language Detection</a></h3>
<p>Automatic detection by file extension:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let language = Language::from_path(&amp;path);
<span class="boring">}</span></code></pre></pre>
<p>Explicit language selection:</p>
<pre><code class="language-bash">debtmap analyze . --languages rust,python
</code></pre>
<h3 id="extensibility"><a class="header" href="#extensibility">Extensibility</a></h3>
<p>Debtmapâ€™s architecture allows adding new languages:</p>
<ol>
<li><strong>Implement Analyzer trait:</strong></li>
</ol>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub trait Analyzer: Send + Sync {
    fn parse(&amp;self, content: &amp;str, path: PathBuf) -&gt; Result&lt;Ast&gt;;
    fn analyze(&amp;self, ast: &amp;Ast) -&gt; FileMetrics;
    fn language(&amp;self) -&gt; Language;
}
<span class="boring">}</span></code></pre></pre>
<ol start="2">
<li><strong>Register in get_analyzer():</strong></li>
</ol>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub fn get_analyzer(language: Language) -&gt; Box&lt;dyn Analyzer&gt; {
    match language {
        Language::Rust =&gt; Box::new(RustAnalyzer::new()),
        Language::YourLanguage =&gt; Box::new(YourAnalyzer::new()),
        // ...
    }
}
<span class="boring">}</span></code></pre></pre>
<p>See <code>src/analyzers/rust.rs</code> for a complete implementation example.</p>
<h2 id="advanced-features"><a class="header" href="#advanced-features">Advanced Features</a></h2>
<h3 id="purity-detection"><a class="header" href="#purity-detection">Purity Detection</a></h3>
<p>Debtmap detects pure functions - those without side effects that always return the same output for the same input.</p>
<p><strong>What makes a function pure:</strong></p>
<ul>
<li>No I/O operations (file, network, database)</li>
<li>No mutable global state</li>
<li>No random number generation</li>
<li>No system calls</li>
<li>Deterministic output</li>
</ul>
<p><strong>Purity detection is optional:</strong></p>
<ul>
<li>Both <code>is_pure</code> and <code>purity_confidence</code> are <code>Option</code> types</li>
<li>May be <code>None</code> for some functions or languages where detection is not available</li>
<li>Rust has the most comprehensive purity detection support</li>
</ul>
<p><strong>Confidence scoring (when available):</strong></p>
<ul>
<li><strong>0.9-1.0</strong>: Very confident (no side effects detected)</li>
<li><strong>0.7-0.8</strong>: Likely pure (minimal suspicious patterns)</li>
<li><strong>0.5-0.6</strong>: Uncertain (some suspicious patterns)</li>
<li><strong>0.0-0.4</strong>: Likely impure (side effects detected)</li>
</ul>
<p><strong>Example:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Pure: confidence = 0.95
fn calculate_total(items: &amp;[Item]) -&gt; f64 {
    items.iter().map(|i| i.price).sum()
}

// Impure: confidence = 0.1 (I/O detected)
fn save_total(items: &amp;[Item]) -&gt; Result&lt;()&gt; {
    let total = items.iter().map(|i| i.price).sum();
    write_to_file(total)  // Side effect!
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Benefits:</strong></p>
<ul>
<li>Pure functions are easier to test</li>
<li>Can be safely cached or memoized</li>
<li>Safe to parallelize</li>
<li>Easier to reason about</li>
</ul>
<h3 id="data-flow-analysis"><a class="header" href="#data-flow-analysis">Data Flow Analysis</a></h3>
<p>Debtmap builds a comprehensive <code>DataFlowGraph</code> that extends basic call graph analysis with variable dependencies, data transformations, I/O operations, and purity tracking.</p>
<h4 id="call-graph-foundation"><a class="header" href="#call-graph-foundation">Call Graph Foundation</a></h4>
<p><strong>Upstream callers</strong> - Who calls this function</p>
<ul>
<li>Indicates impact radius</li>
<li>More callers = higher impact if it breaks</li>
</ul>
<p><strong>Downstream callees</strong> - What this function calls</p>
<ul>
<li>Indicates dependencies</li>
<li>More callees = more integration testing needed</li>
</ul>
<p><strong>Example:</strong></p>
<pre><code class="language-json">{
  "name": "process_payment",
  "upstream_callers": [
    "handle_checkout",
    "process_subscription",
    "handle_refund"
  ],
  "downstream_callees": [
    "validate_payment_method",
    "calculate_fees",
    "record_transaction",
    "send_receipt"
  ]
}
</code></pre>
<h4 id="variable-dependency-tracking"><a class="header" href="#variable-dependency-tracking">Variable Dependency Tracking</a></h4>
<p><code>DataFlowGraph</code> tracks which variables each function depends on:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct DataFlowGraph {
    // Maps function_id -&gt; set of variable names used
    variable_dependencies: HashMap&lt;String, HashSet&lt;String&gt;&gt;,
    // ...
}
<span class="boring">}</span></code></pre></pre>
<p><strong>What it tracks:</strong></p>
<ul>
<li>Local variables accessed in function body</li>
<li>Function parameters</li>
<li>Captured variables (closures)</li>
<li>Mutable vs immutable references</li>
</ul>
<p><strong>Benefits:</strong></p>
<ul>
<li>Identify functions coupled through shared state</li>
<li>Detect potential side effect chains</li>
<li>Guide refactoring to reduce coupling</li>
</ul>
<p><strong>Example output:</strong></p>
<pre><code class="language-json">{
  "function": "calculate_total",
  "variable_dependencies": ["items", "tax_rate", "discount", "total"],
  "parameter_count": 3,
  "local_var_count": 1
}
</code></pre>
<h4 id="data-transformation-patterns"><a class="header" href="#data-transformation-patterns">Data Transformation Patterns</a></h4>
<p><code>DataFlowGraph</code> identifies common functional programming patterns:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub enum TransformationType {
    Map,        // Transform each element
    Filter,     // Select subset of elements
    Reduce,     // Aggregate to single value
    FlatMap,    // Transform and flatten
    Unknown,    // Other transformations
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Pattern detection:</strong></p>
<ul>
<li>Recognizes iterator chains (<code>.map()</code>, <code>.filter()</code>, <code>.fold()</code>)</li>
<li>Identifies functional vs imperative data flow</li>
<li>Tracks input/output variable relationships</li>
</ul>
<p><strong>Example:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Detected as: Filter â†’ Map â†’ Reduce pattern
fn total_active_users(users: &amp;[User]) -&gt; f64 {
    users.iter()
        .filter(|u| u.active)      // Filter transformation
        .map(|u| u.balance)        // Map transformation
        .sum()                      // Reduce transformation
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Transformation metadata:</strong></p>
<pre><code class="language-json">{
  "function": "total_active_users",
  "input_vars": ["users"],
  "output_vars": ["sum_result"],
  "transformation_type": "Reduce",
  "is_functional_style": true,
  "pipeline_length": 3
}
</code></pre>
<h4 id="io-operation-detection"><a class="header" href="#io-operation-detection">I/O Operation Detection</a></h4>
<p>Tracks functions performing I/O operations for purity and performance analysis:</p>
<p><strong>I/O categories tracked:</strong></p>
<ul>
<li><strong>File I/O</strong>: <code>std::fs</code>, <code>File::open</code>, <code>read_to_string</code></li>
<li><strong>Network I/O</strong>: HTTP requests, socket operations</li>
<li><strong>Database I/O</strong>: SQL queries, ORM operations</li>
<li><strong>System calls</strong>: Process spawning, environment access</li>
<li><strong>Blocking operations</strong>: <code>thread::sleep</code>, synchronous I/O in async</li>
</ul>
<p><strong>Example detection:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Detected I/O operations: FileRead, FileWrite
fn save_config(config: &amp;Config, path: &amp;Path) -&gt; Result&lt;()&gt; {
    let json = serde_json::to_string(config)?;  // No I/O
    std::fs::write(path, json)?;                 // FileWrite detected
    Ok(())
}
<span class="boring">}</span></code></pre></pre>
<p><strong>I/O metadata:</strong></p>
<pre><code class="language-json">{
  "function": "save_config",
  "io_operations": ["FileWrite"],
  "is_blocking": true,
  "affects_purity": true,
  "async_safe": false
}
</code></pre>
<h4 id="purity-analysis-integration"><a class="header" href="#purity-analysis-integration">Purity Analysis Integration</a></h4>
<p><code>DataFlowGraph</code> integrates with purity detection to provide comprehensive side effect analysis:</p>
<p><strong>Side effect tracking:</strong></p>
<ul>
<li>I/O operations (file, network, console)</li>
<li>Global state mutations</li>
<li>Random number generation</li>
<li>System time access</li>
<li>Non-deterministic behavior</li>
</ul>
<p><strong>Purity confidence factors:</strong></p>
<ul>
<li><strong>1.0</strong>: Pure mathematical function, no side effects</li>
<li><strong>0.8</strong>: Pure with deterministic data transformations</li>
<li><strong>0.5</strong>: Mixed - some suspicious patterns</li>
<li><strong>0.2</strong>: Likely impure - I/O detected</li>
<li><strong>0.0</strong>: Definitely impure - multiple side effects</li>
</ul>
<p><strong>Example analysis:</strong></p>
<pre><code class="language-json">{
  "function": "calculate_discount",
  "is_pure": true,
  "purity_confidence": 0.95,
  "side_effects": [],
  "deterministic": true,
  "safe_to_parallelize": true,
  "safe_to_cache": true
}
</code></pre>
<h4 id="modification-impact-analysis"><a class="header" href="#modification-impact-analysis">Modification Impact Analysis</a></h4>
<p><code>DataFlowGraph</code> calculates the impact of modifying a function:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct ModificationImpact {
    pub function_name: String,
    pub affected_functions: Vec&lt;String&gt;,  // Upstream callers
    pub dependency_count: usize,          // Downstream callees
    pub has_side_effects: bool,
    pub risk_level: RiskLevel,
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Risk level calculation:</strong></p>
<ul>
<li><strong>Critical</strong>: Many upstream callers + side effects + low test coverage</li>
<li><strong>High</strong>: Many callers OR side effects with moderate coverage</li>
<li><strong>Medium</strong>: Few callers with side effects OR many callers with good coverage</li>
<li><strong>Low</strong>: Few callers, no side effects, or well-tested</li>
</ul>
<p><strong>Example impact analysis:</strong></p>
<pre><code class="language-json">{
  "function": "validate_payment_method",
  "modification_impact": {
    "affected_functions": [
      "process_payment",
      "refund_payment",
      "update_payment_method",
      "validate_subscription"
    ],
    "affected_count": 4,
    "dependency_count": 8,
    "has_side_effects": true,
    "io_operations": ["DatabaseRead", "NetworkCall"],
    "risk_level": "High",
    "recommendation": "Comprehensive testing required - 4 functions depend on this, performs I/O"
  }
}
</code></pre>
<p><strong>Using modification impact:</strong></p>
<pre><code class="language-bash"># Analyze impact before refactoring
debtmap analyze . --format json | jq '.functions[] | select(.name == "validate_payment_method") | .modification_impact'
</code></pre>
<p><strong>Impact analysis uses:</strong></p>
<ul>
<li><strong>Refactoring planning</strong>: Understand blast radius before changes</li>
<li><strong>Test prioritization</strong>: Focus tests on high-impact functions</li>
<li><strong>Code review</strong>: Flag high-risk changes for extra scrutiny</li>
<li><strong>Dependency management</strong>: Identify tightly coupled components</li>
</ul>
<h4 id="dataflowgraph-methods"><a class="header" href="#dataflowgraph-methods">DataFlowGraph Methods</a></h4>
<p>Key methods for data flow analysis:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Add function with its dependencies
pub fn add_function(&amp;mut self, function_id: String, callees: Vec&lt;String&gt;)

// Track variable dependencies
pub fn add_variable_dependency(&amp;mut self, function_id: String, var_name: String)

// Record I/O operations
pub fn add_io_operation(&amp;mut self, function_id: String, io_type: IoType)

// Calculate modification impact
pub fn calculate_modification_impact(&amp;self, function_id: &amp;str) -&gt; ModificationImpact

// Get all functions affected by a change
pub fn get_affected_functions(&amp;self, function_id: &amp;str) -&gt; Vec&lt;String&gt;

// Find functions with side effects
pub fn find_functions_with_side_effects(&amp;self) -&gt; Vec&lt;String&gt;
<span class="boring">}</span></code></pre></pre>
<p><strong>Integration in analysis pipeline:</strong></p>
<ol>
<li>Parser builds initial call graph</li>
<li>DataFlowGraph extends with variable/I/O tracking</li>
<li>Purity analyzer adds side effect information</li>
<li>Modification impact calculated for each function</li>
<li>Results used in prioritization and risk scoring</li>
</ol>
<p><strong>Connection to Unified Scoring:</strong></p>
<p>The dependency analysis from DataFlowGraph directly feeds into the <strong>unified scoring systemâ€™s dependency factor</strong> (20% weight):</p>
<ul>
<li><strong>Dependency Factor Calculation</strong>: Functions with high upstream caller count or on critical paths from entry points receive higher dependency scores (8-10)</li>
<li><strong>Isolated Utilities</strong>: Functions with few or no callers score lower (1-3) on dependency factor</li>
<li><strong>Impact Prioritization</strong>: This helps prioritize functions where bugs have wider impact across the codebase</li>
<li><strong>Modification Risk</strong>: The modification impact analysis uses dependency data to calculate blast radius when changes are made</li>
</ul>
<p><strong>Example:</strong></p>
<pre><code>Function: validate_payment_method
  Upstream callers: 4 (high impact)
  â†’ Dependency Factor: 8.0

Function: format_currency_string
  Upstream callers: 0 (utility)
  â†’ Dependency Factor: 1.5

Both have same complexity, but validate_payment_method gets higher unified score
due to its critical role in the call graph.
</code></pre>
<p>This integration ensures that the unified scoring system considers not just internal function complexity and test coverage, but also the functionâ€™s importance in the broader codebase architecture.</p>
<h3 id="entropy-based-complexity"><a class="header" href="#entropy-based-complexity">Entropy-Based Complexity</a></h3>
<p>Advanced pattern detection to reduce false positives.</p>
<p><strong>Token Classification:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>enum TokenType {
    Variable,     // Weight: 1.0
    Method,       // Weight: 1.5 (more important)
    Literal,      // Weight: 0.5 (less important)
    Keyword,      // Weight: 0.8
    Operator,     // Weight: 0.6
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Shannon Entropy Calculation:</strong></p>
<pre><code>H(X) = -Î£ p(x) Ã— logâ‚‚(p(x))
</code></pre>
<p>where p(x) is the probability of each token type.</p>
<p><strong>Dampening Decision:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>if entropy_score.token_entropy &lt; 0.4
   &amp;&amp; entropy_score.pattern_repetition &gt; 0.6
   &amp;&amp; entropy_score.branch_similarity &gt; 0.7
{
    // Apply dampening
    effective_complexity = base_complexity Ã— (1 - dampening_factor);
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Output explanation:</strong></p>
<pre><code>Function: validate_input
  Cyclomatic: 15 â†’ Effective: 5
  Reasoning:
    - High pattern repetition detected (85%)
    - Low token entropy indicates simple patterns (0.32)
    - Similar branch structures found (92% similarity)
    - Complexity reduced by 67% due to pattern-based code
</code></pre>
<h3 id="entropy-analysis-caching"><a class="header" href="#entropy-analysis-caching">Entropy Analysis Caching</a></h3>
<p><code>EntropyAnalyzer</code> includes an LRU-style cache for performance optimization when analyzing large codebases or performing repeated analysis.</p>
<h4 id="cache-structure"><a class="header" href="#cache-structure">Cache Structure</a></h4>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>struct CacheEntry {
    score: EntropyScore,
    timestamp: Instant,
    hit_count: usize,
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Cache configuration:</strong></p>
<ul>
<li><strong>Default size</strong>: 1000 entries</li>
<li><strong>Eviction policy</strong>: LRU (Least Recently Used)</li>
<li><strong>Memory per entry</strong>: ~128 bytes</li>
<li><strong>Total memory overhead</strong>: ~128 KB for default size</li>
</ul>
<h4 id="cache-statistics"><a class="header" href="#cache-statistics">Cache Statistics</a></h4>
<p>The analyzer tracks cache performance:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct CacheStats {
    pub hits: usize,
    pub misses: usize,
    pub evictions: usize,
    pub hit_rate: f64,
    pub memory_bytes: usize,
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Example stats output:</strong></p>
<pre><code class="language-json">{
  "entropy_cache_stats": {
    "hits": 3427,
    "misses": 1573,
    "evictions": 573,
    "hit_rate": 0.685,
    "memory_bytes": 128000
  }
}
</code></pre>
<p><strong>Hit rate interpretation:</strong></p>
<ul>
<li><strong>&gt; 0.7</strong>: Excellent - many repeated analyses, cache is effective</li>
<li><strong>0.4-0.7</strong>: Good - moderate reuse, typical for incremental analysis</li>
<li><strong>&lt; 0.4</strong>: Low - mostly unique functions, cache less helpful</li>
</ul>
<h4 id="performance-benefits"><a class="header" href="#performance-benefits">Performance Benefits</a></h4>
<p><strong>Typical performance gains:</strong></p>
<ul>
<li><strong>Cold analysis</strong>: 100ms baseline (no cache benefit)</li>
<li><strong>Incremental analysis</strong>: 30-40ms (~60-70% faster) for unchanged functions</li>
<li><strong>Re-analysis</strong>: 15-20ms (~80-85% faster) for recently analyzed functions</li>
</ul>
<p><strong>Best for:</strong></p>
<ul>
<li><strong>Watch mode</strong>: Analyzing on file save (repeated analysis of same files)</li>
<li><strong>CI/CD</strong>: Comparing feature branch to main (overlap in functions)</li>
<li><strong>Large codebases</strong>: Many similar functions benefit from pattern caching</li>
</ul>
<p><strong>Memory estimation:</strong></p>
<pre><code>Total cache memory = entry_count Ã— 128 bytes

Examples:
- 1,000 entries: ~128 KB (default)
- 5,000 entries: ~640 KB (large projects)
- 10,000 entries: ~1.25 MB (very large)
</code></pre>
<h4 id="cache-management"><a class="header" href="#cache-management">Cache Management</a></h4>
<p><strong>Automatic eviction:</strong></p>
<ul>
<li>When cache reaches size limit, oldest entries evicted</li>
<li>Hit count influences retention (frequently accessed stay longer)</li>
<li>Timestamp used for LRU ordering</li>
</ul>
<p><strong>Cache invalidation:</strong></p>
<ul>
<li>Function source changes invalidate entry</li>
<li>Cache cleared between major analysis runs</li>
<li>No manual invalidation needed</li>
</ul>
<p><strong>Configuration (if exposed in future):</strong></p>
<pre><code class="language-toml">[entropy.cache]
enabled = true
size = 1000           # Number of entries
ttl_seconds = 3600    # Optional: expire after 1 hour
</code></pre>
<h3 id="context-aware-analysis-2"><a class="header" href="#context-aware-analysis-2">Context-Aware Analysis</a></h3>
<p>Debtmap adjusts analysis based on code context:</p>
<p><strong>Pattern Recognition:</strong></p>
<ul>
<li>Validation patterns (repetitive checks)</li>
<li>Dispatcher patterns (routing logic)</li>
<li>Builder patterns (fluent APIs)</li>
<li>Configuration parsers (key-value processing)</li>
</ul>
<p><strong>Adjustment Strategies:</strong></p>
<ul>
<li>Reduce false positives for recognized patterns</li>
<li>Apply appropriate thresholds by pattern type</li>
<li>Consider pattern confidence in scoring</li>
</ul>
<p><strong>Example:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Recognized as "validation_pattern"
// Complexity dampening applied
fn validate_user_input(input: &amp;UserInput) -&gt; Result&lt;()&gt; {
    if input.name.is_empty() { return Err(Error::EmptyName); }
    if input.email.is_empty() { return Err(Error::EmptyEmail); }
    if input.age &lt; 13 { return Err(Error::TooYoung); }
    // ... more similar validations
    Ok(())
}
<span class="boring">}</span></code></pre></pre>
<h3 id="coverage-integration"><a class="header" href="#coverage-integration">Coverage Integration</a></h3>
<p>Debtmap parses LCOV coverage data for risk analysis:</p>
<p><strong>LCOV Support:</strong></p>
<ul>
<li>Standard format from most coverage tools</li>
<li>Line-level coverage tracking</li>
<li>Function-level aggregation</li>
</ul>
<p><strong>Coverage Index:</strong></p>
<ul>
<li>O(1) exact name lookups (~0.5Î¼s)</li>
<li>O(log n) line-based fallback (~5-8Î¼s)</li>
<li>~200 bytes per function</li>
<li>Thread-safe (Arc<CoverageIndex>)</li>
</ul>
<h4 id="performance-characteristics"><a class="header" href="#performance-characteristics">Performance Characteristics</a></h4>
<p><strong>Index Build Performance:</strong></p>
<ul>
<li>Index construction: O(n), approximately 20-30ms for 5,000 functions</li>
<li>Memory usage: ~200 bytes per record (~2MB for 5,000 functions)</li>
<li>Scales linearly with function count</li>
</ul>
<p><strong>Lookup Performance:</strong></p>
<ul>
<li>Exact match (function name): O(1) average, ~0.5Î¼s per lookup</li>
<li>Line-based fallback: O(log n), ~5-8Î¼s per lookup</li>
<li>Cache-friendly data structure for hot paths</li>
</ul>
<p><strong>Analysis Overhead:</strong></p>
<ul>
<li>Coverage integration overhead: ~2.5x baseline analysis time</li>
<li>Target overhead: â‰¤3x (maintained through optimizations)</li>
<li>Example timing: 53ms baseline â†’ 130ms with coverage (2.45x overhead)</li>
<li>Overhead includes index build + lookups + coverage propagation</li>
</ul>
<p><strong>When to use coverage integration:</strong></p>
<ul>
<li><strong>Skip coverage</strong> (faster iteration): For rapid development iteration or quick local checks, omit <code>--lcov</code> to get baseline results 2.5x faster</li>
<li><strong>Include coverage</strong> (comprehensive analysis): Use coverage integration for final validation, sprint planning, and CI/CD gates where comprehensive risk analysis is needed</li>
</ul>
<p><strong>Thread Safety:</strong></p>
<ul>
<li>Coverage index wrapped in <code>Arc&lt;CoverageIndex&gt;</code> for lock-free parallel access</li>
<li>Multiple analyzer threads can query coverage simultaneously</li>
<li>No contention on reads, suitable for parallel analysis pipelines</li>
</ul>
<p><strong>Memory Footprint:</strong></p>
<pre><code>Total memory = (function_count Ã— 200 bytes) + index overhead

Examples:
- 1,000 functions: ~200 KB
- 5,000 functions: ~2 MB
- 10,000 functions: ~4 MB
</code></pre>
<p><strong>Scalability:</strong></p>
<ul>
<li>Tested with codebases up to 10,000 functions</li>
<li>Performance remains predictable and acceptable</li>
<li>Memory usage stays bounded and reasonable</li>
</ul>
<p><strong>Generating coverage:</strong></p>
<pre><code class="language-bash"># Rust
cargo tarpaulin --out lcov --output-dir target/coverage

# Python
pytest --cov --cov-report=lcov

# JavaScript/TypeScript
jest --coverage --coverageReporters=lcov

# Go
go test -coverprofile=coverage.out
gocover-cobertura &lt; coverage.out &gt; coverage.lcov
</code></pre>
<p><strong>Using with Debtmap:</strong></p>
<pre><code class="language-bash">debtmap analyze . --lcov target/coverage/lcov.info
</code></pre>
<p><strong>Coverage dampening:</strong>
When coverage data is provided, debt scores are dampened for well-tested code:</p>
<pre><code>final_score = base_score Ã— (1 - coverage_percentage)
</code></pre>
<p>This ensures well-tested complex code gets lower priority than untested simple code.</p>
<h2 id="example-outputs"><a class="header" href="#example-outputs">Example Outputs</a></h2>
<h3 id="high-complexity-function-needs-refactoring"><a class="header" href="#high-complexity-function-needs-refactoring">High Complexity Function (Needs Refactoring)</a></h3>
<p><strong>Terminal Output:</strong></p>
<pre><code>#1 SCORE: 9.2 [CRITICAL]
â”œâ”€ COMPLEXITY: ./src/payments/processor.rs:145 process_transaction()
â”œâ”€ ACTION: Refactor into 4 smaller functions
â”œâ”€ IMPACT: Reduce complexity from 25 to 8, improve testability
â”œâ”€ COMPLEXITY: cyclomatic=25, branches=25, cognitive=38, nesting=5, lines=120
â”œâ”€ DEPENDENCIES: 3 upstream, 8 downstream
â””â”€ WHY: Exceeds all complexity thresholds, difficult to test and maintain
</code></pre>
<p><strong>JSON Output:</strong></p>
<pre><code class="language-json">{
  "id": "complexity_src_payments_processor_rs_145",
  "debt_type": "Complexity",
  "priority": "Critical",
  "file": "src/payments/processor.rs",
  "line": 145,
  "message": "Function exceeds complexity threshold",
  "context": "Cyclomatic: 25, Cognitive: 38, Nesting: 5",
  "function_metrics": {
    "name": "process_transaction",
    "cyclomatic": 25,
    "cognitive": 38,
    "nesting": 5,
    "length": 120,
    "is_pure": false,
    "purity_confidence": 0.15,
    "upstream_callers": ["handle_payment", "handle_subscription", "handle_refund"],
    "downstream_callees": ["validate", "calculate_fees", "record_transaction", "send_receipt", "update_balance", "log_transaction", "check_fraud", "notify_user"]
  }
}
</code></pre>
<h3 id="well-tested-complex-function-good-example"><a class="header" href="#well-tested-complex-function-good-example">Well-Tested Complex Function (Good Example)</a></h3>
<p><strong>Terminal Output:</strong></p>
<pre><code>Function: calculate_tax (WELL TESTED - Good Example!)
  File: src/tax/calculator.rs:78
  Complexity: Cyclomatic=18, Cognitive=22
  Coverage: 98%
  Risk: LOW

  Why this is good:
  - High complexity is necessary (tax rules are complex)
  - Thoroughly tested with 45 test cases
  - Clear documentation of edge cases
  - Good example to follow for other complex logic
</code></pre>
<h3 id="test-gap-needs-testing"><a class="header" href="#test-gap-needs-testing">Test Gap (Needs Testing)</a></h3>
<p><strong>Terminal Output:</strong></p>
<pre><code>#2 SCORE: 8.9 [CRITICAL]
â”œâ”€ TEST GAP: ./src/analyzers/rust_call_graph.rs:38 add_function_to_graph()
â”œâ”€ ACTION: Add 6 unit tests for full coverage
â”œâ”€ IMPACT: Full test coverage, -3.7 risk reduction
â”œâ”€ COMPLEXITY: cyclomatic=6, branches=6, cognitive=8, nesting=2, lines=32
â”œâ”€ DEPENDENCIES: 0 upstream, 11 downstream
â”œâ”€ TEST EFFORT: Simple (2-3 hours)
â””â”€ WHY: Business logic with 0% coverage, manageable complexity (cyclo=6, cog=8)
    High impact - 11 functions depend on this
</code></pre>
<p><strong>JSON Output:</strong></p>
<pre><code class="language-json">{
  "function": "add_function_to_graph",
  "file": "src/analyzers/rust_call_graph.rs",
  "line": 38,
  "current_risk": 8.9,
  "potential_risk_reduction": 3.7,
  "recommendation": {
    "action": "Add unit tests",
    "details": "Add 6 unit tests for full coverage",
    "effort_estimate": "2-3 hours"
  },
  "test_effort": {
    "estimated_difficulty": "Simple",
    "cognitive_load": 8,
    "branch_count": 6,
    "recommended_test_cases": 6
  },
  "complexity": {
    "cyclomatic": 6,
    "cognitive": 8,
    "nesting": 2,
    "length": 32
  },
  "dependencies": {
    "upstream_callers": [],
    "downstream_callees": [
      "get_function_name", "extract_parameters", "parse_return_type",
      "add_to_registry", "update_call_sites", "resolve_types",
      "track_visibility", "record_location", "increment_counter",
      "validate_signature", "log_registration"
    ]
  },
  "roi": 4.5
}
</code></pre>
<h3 id="entropy-dampened-validation-function"><a class="header" href="#entropy-dampened-validation-function">Entropy-Dampened Validation Function</a></h3>
<p><strong>Terminal Output:</strong></p>
<pre><code>Function: validate_config
  File: src/config/validator.rs:23
  Cyclomatic: 20 â†’ Effective: 7 (65% dampened)
  Risk: LOW

  Entropy Analysis:
    â”œâ”€ Token Entropy: 0.28 (low variety - repetitive patterns)
    â”œâ”€ Pattern Repetition: 0.88 (high similarity between checks)
    â”œâ”€ Branch Similarity: 0.91 (consistent validation structure)
    â””â”€ Reasoning: Complexity reduced by 65% due to pattern-based code

  This appears complex but is actually a repetitive validation pattern.
  Lower priority for refactoring.
</code></pre>
<h3 id="beforeafter-refactoring-comparison"><a class="header" href="#beforeafter-refactoring-comparison">Before/After Refactoring Comparison</a></h3>
<p><strong>Before:</strong></p>
<pre><code>Function: process_order
  Cyclomatic: 22
  Cognitive: 35
  Coverage: 15%
  Risk Score: 52.3 (CRITICAL)
  Debt Score: 50 (Critical Complexity)
</code></pre>
<p><strong>After:</strong></p>
<pre><code>Function: process_order (refactored)
  Cyclomatic: 5
  Cognitive: 6
  Coverage: 92%
  Risk Score: 2.1 (LOW)
  Debt Score: 0 (no debt)

Extracted functions:
  - validate_order (Cyclomatic: 4, Coverage: 100%)
  - calculate_totals (Cyclomatic: 3, Coverage: 95%)
  - apply_discounts (Cyclomatic: 6, Coverage: 88%)
  - finalize_order (Cyclomatic: 4, Coverage: 90%)

Impact:
  âœ“ Complexity reduced by 77%
  âœ“ Coverage improved by 513%
  âœ“ Risk reduced by 96%
  âœ“ Created 4 focused, testable functions
</code></pre>
<h2 id="next-steps-1"><a class="header" href="#next-steps-1">Next Steps</a></h2>
<ul>
<li><strong><a href="./output-formats.html">Output Formats</a></strong> - Detailed JSON schema and integration patterns</li>
<li><strong><a href="./configuration.html">Configuration</a></strong> - Customize thresholds and analysis behavior</li>
</ul>
<p>For questions or issues, visit <a href="https://github.com/iepathos/debtmap/issues">GitHub Issues</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="configuration-1"><a class="header" href="#configuration-1">Configuration</a></h1>
<p>Debtmap is highly configurable through a <code>.debtmap.toml</code> file. This chapter explains how to customize Debtmapâ€™s behavior for your projectâ€™s specific needs.</p>
<h2 id="config-files"><a class="header" href="#config-files">Config Files</a></h2>
<p>Debtmap uses <strong>TOML format</strong> for configuration files (<code>.debtmap.toml</code>). TOML provides a clear, readable syntax well-suited for configuration.</p>
<h3 id="creating-a-configuration-file"><a class="header" href="#creating-a-configuration-file">Creating a Configuration File</a></h3>
<p>Debtmap looks for a <code>.debtmap.toml</code> file in the current directory and up to 10 parent directories. To create an initial configuration:</p>
<pre><code class="language-bash">debtmap init
</code></pre>
<p>This command creates a <code>.debtmap.toml</code> file with sensible defaults.</p>
<h3 id="configuration-file-discovery"><a class="header" href="#configuration-file-discovery">Configuration File Discovery</a></h3>
<p>When you run <code>debtmap</code>, it searches for <code>.debtmap.toml</code> starting in your current directory and traversing up to 10 parent directories. The first configuration file found is used.</p>
<p>If no configuration file is found, Debtmap uses built-in defaults that work well for most projects.</p>
<h3 id="basic-example"><a class="header" href="#basic-example">Basic Example</a></h3>
<p>Hereâ€™s a minimal <code>.debtmap.toml</code> configuration:</p>
<pre><code class="language-toml">[scoring]
coverage = 0.50      # 50% weight for test coverage gaps
complexity = 0.35    # 35% weight for code complexity
dependency = 0.15    # 15% weight for dependency criticality

[thresholds]
complexity = 10
max_file_length = 500
max_function_length = 50

[languages]
enabled = ["rust", "python", "javascript", "typescript"]
</code></pre>
<h2 id="scoring-configuration"><a class="header" href="#scoring-configuration">Scoring Configuration</a></h2>
<h3 id="scoring-weights"><a class="header" href="#scoring-weights">Scoring Weights</a></h3>
<p>The <code>[scoring]</code> section controls how different factors contribute to the overall debt score. Debtmap uses a <strong>weighted sum model</strong> where weights must sum to 1.0.</p>
<pre><code class="language-toml">[scoring]
coverage = 0.50      # Weight for test coverage gaps (default: 0.50)
complexity = 0.35    # Weight for code complexity (default: 0.35)
dependency = 0.15    # Weight for dependency criticality (default: 0.15)
</code></pre>
<p><strong>Active weights</strong> (used in scoring):</p>
<ul>
<li><code>coverage</code> - Prioritizes untested code (default: 0.50)</li>
<li><code>complexity</code> - Identifies complex areas (default: 0.35)</li>
<li><code>dependency</code> - Considers impact radius (default: 0.15)</li>
</ul>
<p><strong>Unused weights</strong> (reserved for future features):</p>
<ul>
<li><code>semantic</code> - Not currently used (default: 0.00)</li>
<li><code>security</code> - Not currently used (default: 0.00)</li>
<li><code>organization</code> - Not currently used (default: 0.00)</li>
</ul>
<p><strong>Validation rules:</strong></p>
<ul>
<li>All weights must be between 0.0 and 1.0</li>
<li>Active weights (coverage + complexity + dependency) must sum to 1.0 (Â±0.001 tolerance)</li>
<li>If weights donâ€™t sum to 1.0, they will be automatically normalized</li>
</ul>
<p><strong>Example - Prioritize complexity over coverage:</strong></p>
<pre><code class="language-toml">[scoring]
coverage = 0.30
complexity = 0.55
dependency = 0.15
</code></pre>
<h3 id="role-multipliers"><a class="header" href="#role-multipliers">Role Multipliers</a></h3>
<p>Role multipliers adjust complexity scores based on a functionâ€™s semantic role:</p>
<pre><code class="language-toml">[role_multipliers]
pure_logic = 1.2        # Prioritize pure computation (default: 1.2)
orchestrator = 0.8      # Reduce for delegation functions (default: 0.8)
io_wrapper = 0.7        # Reduce for I/O wrappers (default: 0.7)
entry_point = 0.9       # Slight reduction for main/CLI (default: 0.9)
pattern_match = 0.6     # Reduce for pattern matching (default: 0.6)
unknown = 1.0           # No adjustment (default: 1.0)
</code></pre>
<p>These multipliers help reduce false positives by recognizing that different function types have naturally different complexity levels.</p>
<h3 id="role-coverage-weights"><a class="header" href="#role-coverage-weights">Role Coverage Weights</a></h3>
<p>Adjust how coverage gaps are weighted based on function role:</p>
<pre><code class="language-toml">[role_coverage_weights]
entry_point = 0.6       # Reduce coverage penalty (often integration tested)
orchestrator = 0.8      # Reduce coverage penalty (tested via higher-level tests)
pure_logic = 1.0        # Full penalty (should have unit tests)
io_wrapper = 1.0        # Full penalty (should have unit tests)
pattern_match = 1.0     # Full penalty (should have unit tests)
unknown = 1.0           # Full penalty (default behavior)
</code></pre>
<p>Entry points and orchestrators get reduced coverage penalties since theyâ€™re often tested through integration tests rather than unit tests.</p>
<h2 id="thresholds-configuration"><a class="header" href="#thresholds-configuration">Thresholds Configuration</a></h2>
<h3 id="basic-thresholds"><a class="header" href="#basic-thresholds">Basic Thresholds</a></h3>
<p>Control when code is flagged as technical debt:</p>
<pre><code class="language-toml">[thresholds]
complexity = 10                      # Cyclomatic complexity threshold
duplication = 50                     # Duplication threshold
max_file_length = 500                # Maximum lines per file
max_function_length = 50             # Maximum lines per function
</code></pre>
<h3 id="minimum-thresholds"><a class="header" href="#minimum-thresholds">Minimum Thresholds</a></h3>
<p>Filter out trivial functions that arenâ€™t really technical debt:</p>
<pre><code class="language-toml">[thresholds]
minimum_debt_score = 2.0              # Only show items with debt score â‰¥ 2.0
minimum_cyclomatic_complexity = 3     # Ignore functions with cyclomatic &lt; 3
minimum_cognitive_complexity = 5      # Ignore functions with cognitive &lt; 5
minimum_risk_score = 2.0              # Only show Risk items with score â‰¥ 2.0
</code></pre>
<p>These minimum thresholds help focus on significant issues by filtering out simple functions with minor complexity.</p>
<h3 id="validation-thresholds"><a class="header" href="#validation-thresholds">Validation Thresholds</a></h3>
<p>The <code>[thresholds.validation]</code> subsection configures limits for the <code>debtmap validate</code> command:</p>
<pre><code class="language-toml">[thresholds.validation]
max_average_complexity = 10.0         # Maximum allowed average complexity (default: 10.0)
max_high_complexity_count = 100       # Maximum high complexity functions (default: 100)
max_debt_items = 2000                 # Maximum technical debt items (default: 2000)
max_total_debt_score = 1000           # Maximum total debt score (default: 1000)
max_codebase_risk_score = 7.0         # Maximum codebase risk score (default: 7.0)
max_high_risk_functions = 50          # Maximum high-risk functions (default: 50)
min_coverage_percentage = 0.0         # Minimum required coverage % (default: 0.0)
max_debt_density = 50.0               # Maximum debt per 1000 LOC (default: 50.0)
</code></pre>
<p>Use <code>debtmap validate</code> in CI to enforce code quality standards:</p>
<pre><code class="language-bash"># Fail build if validation thresholds are exceeded
debtmap validate
</code></pre>
<h2 id="language-configuration"><a class="header" href="#language-configuration">Language Configuration</a></h2>
<h3 id="enabling-languages"><a class="header" href="#enabling-languages">Enabling Languages</a></h3>
<p>Specify which languages to analyze:</p>
<pre><code class="language-toml">[languages]
enabled = ["rust", "python", "javascript", "typescript"]
</code></pre>
<h3 id="language-specific-features"><a class="header" href="#language-specific-features">Language-Specific Features</a></h3>
<p>Configure features for individual languages:</p>
<pre><code class="language-toml">[languages.rust]
detect_dead_code = false        # Rust: disabled by default (compiler handles it)
detect_complexity = true
detect_duplication = true

[languages.python]
detect_dead_code = true
detect_complexity = true
detect_duplication = true

[languages.javascript]
detect_dead_code = true
detect_complexity = true
detect_duplication = true

[languages.typescript]
detect_dead_code = true
detect_complexity = true
detect_duplication = true
</code></pre>
<p><strong>Note:</strong> Rustâ€™s dead code detection is disabled by default since the Rust compiler already provides excellent unused code warnings.</p>
<h2 id="exclusion-patterns"><a class="header" href="#exclusion-patterns">Exclusion Patterns</a></h2>
<h3 id="file-and-directory-exclusion"><a class="header" href="#file-and-directory-exclusion">File and Directory Exclusion</a></h3>
<p>Use glob patterns to exclude files and directories from analysis:</p>
<pre><code class="language-toml">[ignore]
patterns = [
    "target/**",              # Rust build output
    "venv/**",                # Python virtual environment
    "node_modules/**",        # JavaScript dependencies
    "*.min.js",               # Minified files
    "benches/**",             # Benchmark code
    "tests/**/*",             # Test files
    "**/test_*.rs",           # Test files (prefix)
    "**/*_test.rs",           # Test files (suffix)
    "**/fixtures/**",         # Test fixtures
    "**/mocks/**",            # Mock implementations
    "**/stubs/**",            # Stub implementations
    "**/examples/**",         # Example code
    "**/demo/**",             # Demo code
]
</code></pre>
<p><strong>Glob pattern syntax:</strong></p>
<ul>
<li><code>*</code> - Matches any characters except <code>/</code></li>
<li><code>**</code> - Matches any characters including <code>/</code> (recursive)</li>
<li><code>?</code> - Matches a single character</li>
<li><code>[abc]</code> - Matches any character in the set</li>
</ul>
<p><strong>Note:</strong> Function-level filtering (e.g., ignoring specific function name patterns) is handled by role detection and context-aware analysis rather than explicit ignore patterns. See the Context-Aware Detection section for function-level filtering options.</p>
<h2 id="display-configuration"><a class="header" href="#display-configuration">Display Configuration</a></h2>
<p>Control how results are displayed:</p>
<pre><code class="language-toml">[display]
tiered = true           # Use tiered priority display (default: true)
items_per_tier = 5      # Show 5 items per tier (default: 5)
</code></pre>
<p>When <code>tiered = true</code>, Debtmap groups results into priority tiers (Critical, High, Medium, Low) and shows the top items from each tier.</p>
<h2 id="output-configuration"><a class="header" href="#output-configuration">Output Configuration</a></h2>
<p>Set the default output format:</p>
<pre><code class="language-toml">[output]
default_format = "terminal"    # Options: "terminal", "json", "markdown"
</code></pre>
<p><strong>Supported formats:</strong></p>
<ul>
<li><code>"terminal"</code> - Human-readable colored output for the terminal (default)</li>
<li><code>"json"</code> - Machine-readable JSON for integration with other tools</li>
<li><code>"markdown"</code> - Markdown format for documentation and reports</li>
</ul>
<p>This can be overridden with the <code>--format</code> CLI flag:</p>
<pre><code class="language-bash">debtmap analyze --format json      # JSON output
debtmap analyze --format markdown  # Markdown output
</code></pre>
<h2 id="normalization-configuration"><a class="header" href="#normalization-configuration">Normalization Configuration</a></h2>
<p>Control how raw scores are normalized to a 0-10 scale:</p>
<pre><code class="language-toml">[normalization]
linear_threshold = 10.0         # Use linear scaling below this value
logarithmic_threshold = 100.0   # Use logarithmic scaling above this value
sqrt_multiplier = 3.33          # Multiplier for square root scaling
log_multiplier = 10.0           # Multiplier for logarithmic scaling
show_raw_scores = true          # Show both raw and normalized scores
</code></pre>
<p>Normalization ensures scores are comparable across different codebases and prevents extreme outliers from dominating the results.</p>
<h2 id="advanced-configuration"><a class="header" href="#advanced-configuration">Advanced Configuration</a></h2>
<h3 id="entropy-based-complexity-scoring"><a class="header" href="#entropy-based-complexity-scoring">Entropy-Based Complexity Scoring</a></h3>
<p>Entropy analysis helps identify repetitive code patterns (like large match statements) that inflate complexity metrics:</p>
<pre><code class="language-toml">[entropy]
enabled = true                      # Enable entropy analysis (default: true)
weight = 1.0                        # Weight in complexity adjustment (default: 1.0)
min_tokens = 20                     # Minimum tokens for analysis (default: 20)
pattern_threshold = 0.7             # Pattern similarity threshold (default: 0.7)
entropy_threshold = 0.4             # Low entropy threshold (default: 0.4)
branch_threshold = 0.8              # Branch similarity threshold (default: 0.8)
use_classification = false          # Use smarter token classification (default: false)

# Maximum reductions to prevent over-correction
max_repetition_reduction = 0.20     # Max 20% reduction for repetition (default: 0.20)
max_entropy_reduction = 0.15        # Max 15% reduction for low entropy (default: 0.15)
max_branch_reduction = 0.25         # Max 25% reduction for similar branches (default: 0.25)
max_combined_reduction = 0.30       # Max 30% total reduction (default: 0.30)
</code></pre>
<p>Entropy scoring reduces false positives from functions like parsers and state machines that have high cyclomatic complexity but are actually simple and maintainable.</p>
<h3 id="god-object-detection"><a class="header" href="#god-object-detection">God Object Detection</a></h3>
<p>Configure detection of classes/structs with too many responsibilities:</p>
<pre><code class="language-toml">[god_object_detection]
enabled = true

# Rust-specific thresholds
[god_object_detection.rust]
max_methods = 20        # Maximum methods before flagging (default: 20)
max_fields = 15         # Maximum fields before flagging (default: 15)
max_traits = 5          # Maximum implemented traits
max_lines = 1000        # Maximum lines of code
max_complexity = 200    # Maximum total complexity

# Python-specific thresholds
[god_object_detection.python]
max_methods = 15
max_fields = 10
max_traits = 3
max_lines = 500
max_complexity = 150

# JavaScript-specific thresholds
[god_object_detection.javascript]
max_methods = 15
max_fields = 20         # JavaScript classes often have more properties
max_traits = 3
max_lines = 500
max_complexity = 150
</code></pre>
<p><strong>Note:</strong> Different languages have different defaults. Rust allows more methods since trait implementations add methods, while JavaScript classes should be smaller.</p>
<h3 id="context-aware-detection"><a class="header" href="#context-aware-detection">Context-Aware Detection</a></h3>
<p>Enable context-aware pattern detection to reduce false positives:</p>
<pre><code class="language-toml">[context]
enabled = false         # Opt-in (default: false)

# Custom context rules
[[context.rules]]
name = "allow_blocking_in_main"
pattern = "blocking_io"
action = "allow"
priority = 100
reason = "Main function can use blocking I/O"

[context.rules.context]
role = "main"

# Function pattern configuration
[context.function_patterns]
test_patterns = ["test_*", "bench_*"]
config_patterns = ["load_*_config", "parse_*_config"]
handler_patterns = ["handle_*", "*_handler"]
init_patterns = ["initialize_*", "setup_*"]
</code></pre>
<p>Context-aware detection adjusts severity based on where code appears (main functions, test code, configuration loaders, etc.).</p>
<h3 id="error-handling-detection"><a class="header" href="#error-handling-detection">Error Handling Detection</a></h3>
<p>Configure detection of error handling anti-patterns:</p>
<pre><code class="language-toml">[error_handling]
detect_async_errors = true          # Detect async error issues (default: true)
detect_context_loss = true          # Detect error context loss (default: true)
detect_propagation = true           # Analyze error propagation (default: true)
detect_panic_patterns = true        # Detect panic/unwrap usage (default: true)
detect_swallowing = true            # Detect swallowed errors (default: true)

# Custom error patterns
[[error_handling.custom_patterns]]
name = "custom_panic"
pattern = "my_panic_macro"
pattern_type = "macro_name"
severity = "high"
description = "Custom panic macro usage"
remediation = "Replace with Result-based error handling"

# Severity overrides
[[error_handling.severity_overrides]]
pattern = "unwrap"
context = "test"
severity = "low"        # Unwrap is acceptable in test code
</code></pre>
<h3 id="external-api-configuration"><a class="header" href="#external-api-configuration">External API Configuration</a></h3>
<p>Mark functions as public API for enhanced testing recommendations:</p>
<pre><code class="language-toml">[external_api]
detect_external_api = false         # Auto-detect public APIs (default: false)
api_functions = []                  # Explicitly mark API functions
api_files = []                      # Explicitly mark API files
</code></pre>
<p>When enabled, public API functions receive higher priority for test coverage.</p>
<h3 id="additional-advanced-options"><a class="header" href="#additional-advanced-options">Additional Advanced Options</a></h3>
<p>Debtmap supports additional advanced configuration options:</p>
<ul>
<li>
<p><strong><code>[loc]</code></strong> - Lines of code counting configuration. Controls whether to include tests (<code>include_tests</code>), generated files (<code>include_generated</code>), comments (<code>count_comments</code>), and blank lines (<code>count_blank_lines</code>) in LOC counts. All default to false.</p>
</li>
<li>
<p><strong><code>[tiers]</code></strong> - Tier threshold configuration for prioritization. Allows customization of complexity and dependency thresholds for different priority tiers (T2, T3, T4). Used internally for tiered reporting.</p>
</li>
<li>
<p><strong><code>[complexity_thresholds]</code></strong> - Enhanced complexity detection thresholds. Configures minimum total, cyclomatic, and cognitive complexity thresholds for flagging functions. Supplements the basic <code>[thresholds]</code> section with more granular control.</p>
</li>
</ul>
<p>These options are advanced features with sensible defaults. Most users wonâ€™t need to configure them explicitly.</p>
<h2 id="cli-integration"><a class="header" href="#cli-integration">CLI Integration</a></h2>
<p>CLI flags can override configuration file settings:</p>
<pre><code class="language-bash"># Override complexity threshold
debtmap analyze --threshold-complexity 15

# Provide coverage file
debtmap analyze --coverage-file coverage.json

# Enable context-aware detection
debtmap analyze --context

# Override output format
debtmap analyze --format json
</code></pre>
<h3 id="configuration-precedence"><a class="header" href="#configuration-precedence">Configuration Precedence</a></h3>
<p>Debtmap resolves configuration values in the following order (highest to lowest priority):</p>
<ol>
<li><strong>CLI flags</strong> - Command-line arguments (e.g., <code>--threshold-complexity 15</code>)</li>
<li><strong>Configuration file</strong> - Settings from <code>.debtmap.toml</code></li>
<li><strong>Built-in defaults</strong> - Debtmapâ€™s sensible default values</li>
</ol>
<p>This allows you to set project-wide defaults in <code>.debtmap.toml</code> while customizing specific runs with CLI flags.</p>
<h2 id="configuration-validation"><a class="header" href="#configuration-validation">Configuration Validation</a></h2>
<h3 id="automatic-validation"><a class="header" href="#automatic-validation">Automatic Validation</a></h3>
<p>Debtmap automatically validates your configuration when loading:</p>
<ul>
<li><strong>Scoring weights</strong> must sum to 1.0 (Â±0.001 tolerance)</li>
<li><strong>Individual weights</strong> must be between 0.0 and 1.0</li>
<li><strong>Invalid configurations</strong> fall back to defaults with a warning</li>
</ul>
<h3 id="normalization"><a class="header" href="#normalization">Normalization</a></h3>
<p>If scoring weights donâ€™t sum exactly to 1.0, Debtmap automatically normalizes them:</p>
<pre><code class="language-toml"># Input (sums to 0.80)
[scoring]
coverage = 0.40
complexity = 0.30
dependency = 0.10

# Automatically normalized to:
# coverage = 0.50
# complexity = 0.375
# dependency = 0.125
</code></pre>
<h3 id="debug-validation"><a class="header" href="#debug-validation">Debug Validation</a></h3>
<p>To verify which configuration file is being loaded, check debug logs:</p>
<pre><code class="language-bash">RUST_LOG=debug debtmap analyze
</code></pre>
<p>Look for log messages like:</p>
<pre><code>DEBUG debtmap::config: Loaded config from /path/to/.debtmap.toml
</code></pre>
<h2 id="complete-configuration-example"><a class="header" href="#complete-configuration-example">Complete Configuration Example</a></h2>
<p>Hereâ€™s a comprehensive configuration showing all major sections:</p>
<pre><code class="language-toml"># Scoring configuration
[scoring]
coverage = 0.50
complexity = 0.35
dependency = 0.15

# Basic thresholds
[thresholds]
complexity = 10
duplication = 50
max_file_length = 500
max_function_length = 50
minimum_debt_score = 2.0
minimum_cyclomatic_complexity = 3
minimum_cognitive_complexity = 5
minimum_risk_score = 2.0

# Validation thresholds for CI
[thresholds.validation]
max_average_complexity = 10.0
max_high_complexity_count = 100
max_debt_items = 2000
max_total_debt_score = 1000
max_codebase_risk_score = 7.0
max_high_risk_functions = 50
min_coverage_percentage = 0.0
max_debt_density = 50.0

# Language configuration
[languages]
enabled = ["rust", "python", "javascript", "typescript"]

[languages.rust]
detect_dead_code = false
detect_complexity = true
detect_duplication = true

# Exclusion patterns
[ignore]
patterns = [
    "target/**",
    "node_modules/**",
    "tests/**/*",
    "**/*_test.rs",
]

# Display configuration
[display]
tiered = true
items_per_tier = 5

# Output configuration
[output]
default_format = "terminal"

# Entropy configuration
[entropy]
enabled = true
weight = 1.0
min_tokens = 20

# God object detection
[god_object_detection]
enabled = true

[god_object_detection.rust]
max_methods = 20
max_fields = 15
</code></pre>
<h2 id="configuration-best-practices"><a class="header" href="#configuration-best-practices">Configuration Best Practices</a></h2>
<h3 id="for-strict-quality-standards"><a class="header" href="#for-strict-quality-standards">For Strict Quality Standards</a></h3>
<pre><code class="language-toml">[scoring]
coverage = 0.60         # Emphasize test coverage
complexity = 0.30
dependency = 0.10

[thresholds]
minimum_debt_score = 3.0        # Higher bar for flagging issues
max_function_length = 30        # Enforce smaller functions

[thresholds.validation]
max_average_complexity = 8.0    # Stricter complexity limits
max_debt_items = 500            # Stricter debt limits
min_coverage_percentage = 80.0  # Require 80% coverage
</code></pre>
<h3 id="for-legacy-codebases"><a class="header" href="#for-legacy-codebases">For Legacy Codebases</a></h3>
<pre><code class="language-toml">[scoring]
coverage = 0.30         # Reduce coverage weight (legacy code often lacks tests)
complexity = 0.50       # Focus on complexity
dependency = 0.20

[thresholds]
minimum_debt_score = 5.0        # Only show highest priority items
minimum_cyclomatic_complexity = 10   # Filter out moderate complexity

[thresholds.validation]
max_debt_items = 10000          # Accommodate large debt
max_total_debt_score = 5000     # Higher limits for legacy code
</code></pre>
<h3 id="for-open-source-libraries"><a class="header" href="#for-open-source-libraries">For Open Source Libraries</a></h3>
<pre><code class="language-toml">[scoring]
coverage = 0.55         # Prioritize test coverage (public API)
complexity = 0.30
dependency = 0.15

[external_api]
detect_external_api = true      # Flag untested public APIs

[thresholds.validation]
min_coverage_percentage = 90.0  # High coverage for public API
max_high_complexity_count = 20  # Keep complexity low
</code></pre>
<h2 id="troubleshooting-1"><a class="header" href="#troubleshooting-1">Troubleshooting</a></h2>
<h3 id="configuration-not-loading"><a class="header" href="#configuration-not-loading">Configuration Not Loading</a></h3>
<p><strong>Check file location:</strong></p>
<pre><code class="language-bash"># Ensure file is named .debtmap.toml (note the dot prefix)
ls -la .debtmap.toml

# Debtmap searches current directory + 10 parent directories
pwd
</code></pre>
<p><strong>Check file syntax:</strong></p>
<pre><code class="language-bash"># Verify TOML syntax is valid
debtmap analyze 2&gt;&amp;1 | grep -i "failed to parse"
</code></pre>
<h3 id="weights-dont-sum-to-10"><a class="header" href="#weights-dont-sum-to-10">Weights Donâ€™t Sum to 1.0</a></h3>
<p><strong>Error message:</strong></p>
<pre><code>Warning: Invalid scoring weights: Active scoring weights must sum to 1.0, but sum to 0.800. Using defaults.
</code></pre>
<p><strong>Fix:</strong> Ensure coverage + complexity + dependency = 1.0</p>
<pre><code class="language-toml">[scoring]
coverage = 0.50
complexity = 0.35
dependency = 0.15    # Sum = 1.0 âœ“
</code></pre>
<h3 id="no-results-shown"><a class="header" href="#no-results-shown">No Results Shown</a></h3>
<p><strong>Possible causes:</strong></p>
<ol>
<li>Minimum thresholds too high</li>
<li>All code excluded by ignore patterns</li>
<li>No supported languages in project</li>
</ol>
<p><strong>Solutions:</strong></p>
<pre><code class="language-toml"># Lower minimum thresholds
[thresholds]
minimum_debt_score = 1.0
minimum_cyclomatic_complexity = 1

# Check language configuration
[languages]
enabled = ["rust", "python", "javascript", "typescript"]

# Review ignore patterns
[ignore]
patterns = [
    # Make sure you're not excluding too much
]
</code></pre>
<h2 id="related-chapters"><a class="header" href="#related-chapters">Related Chapters</a></h2>
<ul>
<li><a href="./getting-started.html">Getting Started</a> - Initial setup and basic usage</li>
<li><a href="./analysis-guide.html">Analysis Guide</a> - Understanding scoring and prioritization</li>
<li><a href="./output-formats.html">Output Formats</a> - Formatting and exporting results</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="suppression-patterns"><a class="header" href="#suppression-patterns">Suppression Patterns</a></h1>
<p>Debtmap provides flexible suppression mechanisms to help you focus on the technical debt that matters most. You can suppress specific debt items inline with comments, or exclude entire files and functions through configuration.</p>
<h2 id="why-use-suppressions"><a class="header" href="#why-use-suppressions">Why Use Suppressions?</a></h2>
<p>Not all detected technical debt requires immediate action. Suppressions allow you to:</p>
<ul>
<li><strong>Focus on priorities</strong>: Hide known, accepted debt to see new issues clearly</li>
<li><strong>Handle false positives</strong>: Suppress patterns that donâ€™t apply to your context</li>
<li><strong>Document decisions</strong>: Explain why certain debt is acceptable using reason annotations</li>
<li><strong>Exclude test code</strong>: Ignore complexity in test fixtures and setup functions</li>
</ul>
<h2 id="inline-comment-suppression"><a class="header" href="#inline-comment-suppression">Inline Comment Suppression</a></h2>
<p>Debtmap supports four inline comment formats that work with your languageâ€™s comment syntax:</p>
<h3 id="single-line-suppression"><a class="header" href="#single-line-suppression">Single-Line Suppression</a></h3>
<p>Suppress debt on the same line as the comment:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// debtmap:ignore
// TODO: Implement caching later - performance is acceptable for now
<span class="boring">}</span></code></pre></pre>
<pre><code class="language-python"># debtmap:ignore
# FIXME: Refactor this after the Q2 release
</code></pre>
<p>The suppression applies to debt detected on the same line as the comment.</p>
<h3 id="next-line-suppression"><a class="header" href="#next-line-suppression">Next-Line Suppression</a></h3>
<p>Suppress debt on the line immediately following the comment:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// debtmap:ignore-next-line
fn complex_algorithm() {
    // ...20 lines of complex code...
}
<span class="boring">}</span></code></pre></pre>
<pre><code class="language-typescript">// debtmap:ignore-next-line
function calculateMetrics(data: DataPoint[]): Metrics {
    // ...complex implementation...
}
</code></pre>
<p>This format is useful when you want the suppression comment to appear before the code it affects.</p>
<h3 id="block-suppression"><a class="header" href="#block-suppression">Block Suppression</a></h3>
<p>Suppress multiple lines of code between start and end markers:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// debtmap:ignore-start
fn setup_test_environment() {
    // TODO: Add more test cases
    // FIXME: Handle edge cases
    // Complex test setup code...
}
// debtmap:ignore-end
<span class="boring">}</span></code></pre></pre>
<pre><code class="language-python"># debtmap:ignore-start
def mock_api_responses():
    # TODO: Add more mock scenarios
    # Multiple lines of mock setup
    pass
# debtmap:ignore-end
</code></pre>
<p><strong>Important</strong>: Every <code>ignore-start</code> must have a matching <code>ignore-end</code>. Debtmap tracks unclosed blocks and can warn you about them.</p>
<h2 id="type-specific-suppression"><a class="header" href="#type-specific-suppression">Type-Specific Suppression</a></h2>
<p>You can suppress specific types of debt using bracket notation instead of suppressing everything:</p>
<h3 id="suppress-specific-types"><a class="header" href="#suppress-specific-types">Suppress Specific Types</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// debtmap:ignore[todo]
// TODO: This TODO is ignored, but FIXMEs and complexity are still reported
<span class="boring">}</span></code></pre></pre>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// debtmap:ignore[todo,fixme]
// TODO: Both TODOs and FIXMEs are ignored here
// FIXME: But complexity issues would still be detected
<span class="boring">}</span></code></pre></pre>
<h3 id="supported-debt-types"><a class="header" href="#supported-debt-types">Supported Debt Types</a></h3>
<ul>
<li><code>todo</code> - TODO comments</li>
<li><code>fixme</code> - FIXME comments</li>
<li><code>hack</code> - HACK markers</li>
<li><code>smell</code> or <code>codesmell</code> - Code smell patterns</li>
<li><code>complexity</code> - High cognitive complexity</li>
<li><code>duplication</code> - Code duplication</li>
<li><code>god_object</code> - God object warnings</li>
<li><code>testing</code> - Testing gap warnings</li>
<li><code>dependency</code> - Dependency issues</li>
<li><code>*</code> - All types (wildcard)</li>
</ul>
<h3 id="wildcard-suppression"><a class="header" href="#wildcard-suppression">Wildcard Suppression</a></h3>
<p>Use <code>[*]</code> to explicitly suppress all types (equivalent to no bracket notation):</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// debtmap:ignore[*]
// Suppresses all debt types
<span class="boring">}</span></code></pre></pre>
<h3 id="type-specific-blocks"><a class="header" href="#type-specific-blocks">Type-Specific Blocks</a></h3>
<p>Block suppressions also support type filtering:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// debtmap:ignore-start[complexity]
fn intentionally_complex_for_performance() {
    // Complex nested logic is intentional here
    // Complexity warnings suppressed, but TODOs still detected
}
// debtmap:ignore-end
<span class="boring">}</span></code></pre></pre>
<h2 id="suppression-reasons"><a class="header" href="#suppression-reasons">Suppression Reasons</a></h2>
<p>Document why youâ€™re suppressing debt using the <code>--</code> separator:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// debtmap:ignore -- Intentional for backward compatibility
// TODO: Remove this after all clients upgrade to v2.0
<span class="boring">}</span></code></pre></pre>
<pre><code class="language-python"># debtmap:ignore[complexity] -- Performance-critical hot path
def optimize_query(params):
    # Complex but necessary for performance
    pass
</code></pre>
<pre><code class="language-typescript">// debtmap:ignore-next-line -- Waiting on upstream library fix
function workaroundBug() {
    // FIXME: Remove when library v3.0 is released
}
</code></pre>
<p><strong>Best Practice</strong>: Always include reasons for suppressions. This helps future maintainers understand the context and know when suppressions can be removed.</p>
<h2 id="config-file-exclusions"><a class="header" href="#config-file-exclusions">Config File Exclusions</a></h2>
<p>For broader exclusions, use the <code>[ignore]</code> section in <code>.debtmap.toml</code>:</p>
<h3 id="file-pattern-exclusions"><a class="header" href="#file-pattern-exclusions">File Pattern Exclusions</a></h3>
<pre><code class="language-toml">[ignore]
patterns = [
    "target/**",              # Build artifacts
    "node_modules/**",        # Dependencies
    "**/*_test.rs",           # Test files with _test suffix
    "tests/**",               # All test directories
    "**/fixtures/**",         # Test fixtures
    "**/mocks/**",            # Mock implementations
    "**/*.min.js",            # Minified files
    "**/demo/**",             # Demo code
    "**/*.generated.rs",      # Generated files
    "vendor/**",              # Vendor code
    "third_party/**",         # Third-party code
]
</code></pre>
<h3 id="function-name-exclusions"><a class="header" href="#function-name-exclusions">Function Name Exclusions</a></h3>
<p>Exclude entire function families by name pattern:</p>
<pre><code class="language-toml">[ignore.functions]
patterns = [
    # Test setup functions
    "setup_test_*",
    "teardown_test_*",
    "create_test_*",
    "mock_*",

    # Generated code
    "derive_*",
    "__*",                    # Python dunder methods

    # CLI parsing (naturally complex)
    "parse_args",
    "parse_cli",
    "build_cli",

    # Serialization (naturally complex pattern matching)
    "serialize_*",
    "deserialize_*",
    "to_json",
    "from_json",
]
</code></pre>
<p>Function patterns use wildcard matching where <code>*</code> matches any characters.</p>
<h2 id="glob-pattern-syntax"><a class="header" href="#glob-pattern-syntax">Glob Pattern Syntax</a></h2>
<p>File patterns use standard glob syntax:</p>
<div class="table-wrapper"><table><thead><tr><th>Pattern</th><th>Matches</th><th>Example</th></tr></thead><tbody>
<tr><td><code>*</code></td><td>Any characters within a path component</td><td><code>*.rs</code> matches <code>main.rs</code></td></tr>
<tr><td><code>**</code></td><td>Any directories (recursive)</td><td><code>tests/**</code> matches <code>tests/unit/foo.rs</code></td></tr>
<tr><td><code>?</code></td><td>Single character</td><td><code>test?.rs</code> matches <code>test1.rs</code></td></tr>
<tr><td><code>[abc]</code></td><td>Character class</td><td><code>test[123].rs</code> matches <code>test1.rs</code></td></tr>
<tr><td><code>[!abc]</code></td><td>Negated class</td><td><code>test[!0].rs</code> matches <code>test1.rs</code> but not <code>test0.rs</code></td></tr>
</tbody></table>
</div>
<h3 id="glob-pattern-examples"><a class="header" href="#glob-pattern-examples">Glob Pattern Examples</a></h3>
<pre><code class="language-toml">[ignore]
patterns = [
    "src/**/*_generated.rs",  # Generated files in any subdirectory
    "**/test_*.py",           # Python test files anywhere
    "legacy/**/[!i]*.js",     # Legacy JS files not starting with 'i'
    "**/*.{min.js,min.css}",  # Minified assets
]
</code></pre>
<h2 id="language-specific-comment-syntax"><a class="header" href="#language-specific-comment-syntax">Language-Specific Comment Syntax</a></h2>
<p>Debtmap automatically uses the correct comment syntax for each language:</p>
<div class="table-wrapper"><table><thead><tr><th>Language</th><th>Comment Prefix</th><th>Example</th></tr></thead><tbody>
<tr><td>Rust</td><td><code>//</code></td><td><code>// debtmap:ignore</code></td></tr>
<tr><td>JavaScript</td><td><code>//</code></td><td><code>// debtmap:ignore</code></td></tr>
<tr><td>TypeScript</td><td><code>//</code></td><td><code>// debtmap:ignore</code></td></tr>
<tr><td>Python</td><td><code>#</code></td><td><code># debtmap:ignore</code></td></tr>
</tbody></table>
</div>
<p>You donâ€™t need to configure thisâ€”Debtmap detects the language and uses the appropriate syntax.</p>
<h2 id="explicitly-specified-files"><a class="header" href="#explicitly-specified-files">Explicitly Specified Files</a></h2>
<p><strong>Important behavior</strong>: When you analyze a specific file directly, ignore patterns are bypassed:</p>
<pre><code class="language-bash"># Respects [ignore] patterns in .debtmap.toml
debtmap analyze .
debtmap analyze src/

# Bypasses ignore patterns - analyzes the file even if patterns would exclude it
debtmap analyze src/test_helper.rs
</code></pre>
<p>This ensures you can always analyze specific files when needed, even if they match an ignore pattern.</p>
<h2 id="suppression-statistics"><a class="header" href="#suppression-statistics">Suppression Statistics</a></h2>
<p>Debtmap tracks suppression usage and can detect issues:</p>
<ul>
<li><strong>Total suppressions</strong>: Count of active suppressions</li>
<li><strong>Suppressions by type</strong>: How many of each debt type are suppressed</li>
<li><strong>Unclosed blocks</strong>: Detection of <code>ignore-start</code> without matching <code>ignore-end</code></li>
</ul>
<p>Future versions may include a command to report suppression statistics for your codebase.</p>
<h2 id="best-practices-1"><a class="header" href="#best-practices-1">Best Practices</a></h2>
<h3 id="use-suppressions-sparingly"><a class="header" href="#use-suppressions-sparingly">Use Suppressions Sparingly</a></h3>
<p>Suppressions hide information, so use them intentionally:</p>
<p>âœ… <strong>Good use cases:</strong></p>
<ul>
<li>Test fixtures and mock data</li>
<li>Known technical debt with an accepted timeline</li>
<li>Intentional complexity for performance</li>
<li>False positives specific to your domain</li>
</ul>
<p>âŒ <strong>Poor use cases:</strong></p>
<ul>
<li>Hiding all debt to make reports look clean</li>
<li>Suppressing instead of fixing simple issues</li>
<li>Using wildcards when specific types would work</li>
</ul>
<h3 id="always-include-reasons"><a class="header" href="#always-include-reasons">Always Include Reasons</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// âœ… Good: Clear reason and timeline
// debtmap:ignore[complexity] -- Hot path optimization, profiled and necessary
fn fast_algorithm() { }

// âŒ Bad: No context for future maintainers
// debtmap:ignore
fn fast_algorithm() { }
<span class="boring">}</span></code></pre></pre>
<h3 id="prefer-specific-over-broad"><a class="header" href="#prefer-specific-over-broad">Prefer Specific Over Broad</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// âœ… Good: Only suppress the specific debt type
// debtmap:ignore[todo] -- Remove after v2.0 migration
// TODO: Migrate to new API

// âŒ Bad: Suppresses everything, including real issues
// debtmap:ignore
// TODO: Migrate to new API
<span class="boring">}</span></code></pre></pre>
<h3 id="use-config-for-systematic-exclusions"><a class="header" href="#use-config-for-systematic-exclusions">Use Config for Systematic Exclusions</a></h3>
<p>For patterns that apply project-wide, use <code>.debtmap.toml</code> instead of inline comments:</p>
<pre><code class="language-toml"># âœ… Good: One config applies to all test files
[ignore]
patterns = ["tests/**"]

# âŒ Bad: Repetitive inline suppressions in every test file
</code></pre>
<h3 id="review-suppressions-periodically"><a class="header" href="#review-suppressions-periodically">Review Suppressions Periodically</a></h3>
<p>Suppressions can become outdated:</p>
<ul>
<li>Remove suppressions when the reason no longer applies</li>
<li>Check if suppressed debt can now be fixed</li>
<li>Verify reasons are still accurate after refactoring</li>
</ul>
<p><strong>Solution:</strong> Periodically search for suppressions:</p>
<pre><code class="language-bash">rg "debtmap:ignore" --type rust
</code></pre>
<h3 id="ensure-blocks-are-closed"><a class="header" href="#ensure-blocks-are-closed">Ensure Blocks Are Closed</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// âœ… Good: Properly closed block
// debtmap:ignore-start
fn test_setup() { }
// debtmap:ignore-end

// âŒ Bad: Unclosed block affects all subsequent code
// debtmap:ignore-start
fn test_setup() { }
// (missing ignore-end)
<span class="boring">}</span></code></pre></pre>
<p>Debtmap detects unclosed blocks and can warn you about them.</p>
<h2 id="common-patterns"><a class="header" href="#common-patterns">Common Patterns</a></h2>
<h3 id="suppressing-test-code"><a class="header" href="#suppressing-test-code">Suppressing Test Code</a></h3>
<pre><code class="language-toml"># In .debtmap.toml
[ignore]
patterns = [
    "tests/**/*",
    "**/*_test.rs",
    "**/test_*.py",
    "**/fixtures/**",
]

[ignore.functions]
patterns = [
    "test_*",
    "setup_*",
    "teardown_*",
    "mock_*",
]
</code></pre>
<h3 id="suppressing-generated-code"><a class="header" href="#suppressing-generated-code">Suppressing Generated Code</a></h3>
<pre><code class="language-toml">[ignore]
patterns = [
    "**/*_generated.*",
    "**/proto/**",
    "**/bindings/**",
]

[ignore.functions]
patterns = [
    "derive_*",
    "__*",
]
</code></pre>
<h3 id="temporary-suppressions-with-timeline"><a class="header" href="#temporary-suppressions-with-timeline">Temporary Suppressions with Timeline</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// debtmap:ignore[complexity] -- TODO: Refactor during Q2 2025 sprint
fn legacy_payment_processor() {
    // Complex legacy code scheduled for refactoring
}
<span class="boring">}</span></code></pre></pre>
<h3 id="suppressing-false-positives"><a class="header" href="#suppressing-false-positives">Suppressing False Positives</a></h3>
<pre><code class="language-python"># debtmap:ignore[duplication] -- Similar but semantically different
def calculate_tax_us():
    # US tax calculation
    pass

# debtmap:ignore[duplication] -- Similar but semantically different
def calculate_tax_eu():
    # EU tax calculation with different rules
    pass
</code></pre>
<h3 id="conditional-suppression"><a class="header" href="#conditional-suppression">Conditional Suppression</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[cfg(test)]
// debtmap:ignore[complexity]
fn test_helper() {
    // Complex test setup is acceptable
}
<span class="boring">}</span></code></pre></pre>
<h3 id="suppression-with-detailed-justification"><a class="header" href="#suppression-with-detailed-justification">Suppression with Detailed Justification</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// debtmap:ignore[complexity] -- Required by specification XYZ-123
// This function implements the state machine defined in spec XYZ-123.
// Complexity is inherent to the specification and cannot be reduced
// without violating requirements.
fn state_machine() { ... }
<span class="boring">}</span></code></pre></pre>
<h2 id="troubleshooting-2"><a class="header" href="#troubleshooting-2">Troubleshooting</a></h2>
<h3 id="suppression-not-working"><a class="header" href="#suppression-not-working">Suppression Not Working</a></h3>
<ol>
<li><strong>Check comment syntax</strong>: Ensure youâ€™re using the correct comment prefix for your language (<code>//</code> for Rust/JS/TS, <code>#</code> for Python)</li>
<li><strong>Verify spelling</strong>: Itâ€™s <code>debtmap:ignore</code>, not <code>debtmap-ignore</code> or <code>debtmap_ignore</code></li>
<li><strong>Check line matching</strong>: For same-line suppressions, ensure the debt is on the same line as the comment</li>
<li><strong>Verify type names</strong>: Use <code>todo</code>, <code>fixme</code>, <code>complexity</code>, etc. (lowercase)</li>
</ol>
<p><strong>Common syntax errors:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Wrong: debtmap: ignore (space after colon)
// Right: debtmap:ignore

// Wrong: debtmap:ignore[Complexity] (capital C)
// Right: debtmap:ignore[complexity]
<span class="boring">}</span></code></pre></pre>
<p><strong>Check placement:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Wrong: comment after code
fn function() { } // debtmap:ignore

// Right: comment before code
// debtmap:ignore
fn function() { }
<span class="boring">}</span></code></pre></pre>
<h3 id="unclosed-block-warning"><a class="header" href="#unclosed-block-warning">Unclosed Block Warning</a></h3>
<p>If you see warnings about unclosed blocks:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Problem: Missing ignore-end
// debtmap:ignore-start
fn test_helper() { }
// (Should have debtmap:ignore-end here)

// Solution: Add the closing marker
// debtmap:ignore-start
fn test_helper() { }
// debtmap:ignore-end
<span class="boring">}</span></code></pre></pre>
<h3 id="file-still-being-analyzed"><a class="header" href="#file-still-being-analyzed">File Still Being Analyzed</a></h3>
<p>If a file in your ignore patterns is still being analyzed:</p>
<ol>
<li>Check if youâ€™re analyzing the specific file directly (bypasses ignore patterns)</li>
<li>Verify the glob pattern matches the file path</li>
<li>Check for typos in the pattern</li>
<li>Test the pattern in isolation</li>
</ol>
<p><strong>Test pattern with find:</strong></p>
<pre><code class="language-bash">find . -path "tests/**/*" -type f
</code></pre>
<p><strong>Use double asterisk for subdirectories:</strong></p>
<pre><code class="language-toml"># Wrong: "tests/*" (only direct children)
# Right: "tests/**/*" (all descendants)
</code></pre>
<p><strong>Check relative paths:</strong></p>
<pre><code class="language-toml"># Patterns are relative to project root
patterns = [
    "src/legacy/**",  # âœ“ Correct
    "/src/legacy/**", # âœ— Wrong (absolute path)
]
</code></pre>
<h3 id="function-suppression-not-working"><a class="header" href="#function-suppression-not-working">Function Suppression Not Working</a></h3>
<p>If function name patterns arenâ€™t working:</p>
<ol>
<li>Verify the pattern is under <code>[ignore.functions]</code>, not <code>[ignore]</code></li>
<li>Check the function name exactly matches (case-sensitive)</li>
<li>Remember <code>*</code> is a wildcard: <code>test_*</code> matches <code>test_foo</code> but not <code>my_test</code></li>
</ol>
<h2 id="related-topics"><a class="header" href="#related-topics">Related Topics</a></h2>
<ul>
<li><a href="configuration.html">Configuration</a> - Full <code>.debtmap.toml</code> reference</li>
<li><a href="cli-reference.html">CLI Reference</a> - Command-line analysis options</li>
<li><a href="analysis-guide.html">Analysis Guide</a> - Understanding debt detection</li>
<li><a href="output-formats.html">Output Formats</a> - Viewing suppressed items in reports</li>
</ul>
<h2 id="summary"><a class="header" href="#summary">Summary</a></h2>
<p>Suppressions help you focus on actionable technical debt:</p>
<ul>
<li><strong>Inline comments</strong>: <code>debtmap:ignore</code>, <code>ignore-next-line</code>, <code>ignore-start/end</code></li>
<li><strong>Type-specific</strong>: Use <code>[type1,type2]</code> to suppress selectively</li>
<li><strong>Reasons</strong>: Always use <code>-- reason</code> to document why</li>
<li><strong>Config patterns</strong>: Use <code>.debtmap.toml</code> for systematic exclusions</li>
<li><strong>Function patterns</strong>: Use <code>[ignore.functions]</code> for function name matching</li>
<li><strong>Best practices</strong>: Use sparingly, prefer specific over broad, review periodically</li>
</ul>
<p>With proper use of suppressions, your Debtmap reports show only the debt that matters to your team.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="output-formats-1"><a class="header" href="#output-formats-1">Output Formats</a></h1>
<p>Debtmap provides multiple output formats to suit different workflows, from interactive terminal reports to machine-readable JSON for CI/CD integration. This chapter covers all available formats and how to use them effectively.</p>
<h2 id="format-selection"><a class="header" href="#format-selection">Format Selection</a></h2>
<p>Select the output format using the <code>-f</code> or <code>--format</code> flag:</p>
<pre><code class="language-bash"># Terminal output (default) - human-readable with colors
debtmap analyze .

# JSON output - machine-readable for tooling
debtmap analyze . --format json

# Markdown output - documentation and reports
debtmap analyze . --format markdown
</code></pre>
<p>Available formats:</p>
<ul>
<li><strong>terminal</strong> (default): Interactive output with colors, emoji, and formatting</li>
<li><strong>json</strong>: Structured data for programmatic processing</li>
<li><strong>markdown</strong>: Reports suitable for documentation and PR comments</li>
</ul>
<h3 id="writing-to-files"><a class="header" href="#writing-to-files">Writing to Files</a></h3>
<p>By default, output goes to stdout. Use <code>-o</code> or <code>--output</code> to write to a file:</p>
<pre><code class="language-bash"># Write JSON to file
debtmap analyze . --format json -o report.json

# Write markdown report
debtmap analyze . --format markdown -o DEBT_REPORT.md

# Terminal output to file (preserves colors)
debtmap analyze . -o analysis.txt
</code></pre>
<h2 id="terminal-output"><a class="header" href="#terminal-output">Terminal Output</a></h2>
<p>The terminal format provides an interactive, color-coded report designed for developer workflows. Itâ€™s the default format and optimized for readability.</p>
<h3 id="output-structure"><a class="header" href="#output-structure">Output Structure</a></h3>
<p>Terminal output is organized into five main sections:</p>
<ol>
<li><strong>Header</strong> - Analysis report title</li>
<li><strong>Codebase Summary</strong> - High-level metrics and debt score</li>
<li><strong>Complexity Hotspots</strong> - Top 5 most complex functions with refactoring guidance</li>
<li><strong>Technical Debt</strong> - High-priority debt items requiring attention</li>
<li><strong>Pass/Fail Status</strong> - Overall quality assessment</li>
</ol>
<h3 id="example-terminal-output"><a class="header" href="#example-terminal-output">Example Terminal Output</a></h3>
<pre><code>â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
           DEBTMAP ANALYSIS REPORT
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“Š CODEBASE Summary
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Files analyzed:      42
  Total functions:     287
  Average complexity:  6.3
  Debt items:          15
  Total debt score:    156 (threshold: 100)

âš ï¸  COMPLEXITY HOTSPOTS (Top 5)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  1. src/analyzers/rust.rs:245 parse_function() - Cyclomatic: 18, Cognitive: 24
     ACTION: Extract 3-5 pure functions using decompose-then-transform strategy
     PATTERNS: Decompose into logical units, then apply functional patterns
     BENEFIT: Pure functions are easily testable and composable

  2. src/debt/smells.rs:196 detect_data_clumps() - Cyclomatic: 15, Cognitive: 20
     â†“ Entropy: 0.32, Repetition: 85%, Effective: 0.6x
       High pattern repetition detected (85%)

ğŸ”§ TECHNICAL DEBT (15 items)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  High Priority (5):
    - src/risk/scoring.rs:142 - TODO: Implement caching for score calculations
    - src/core/metrics.rs:89 - High complexity: cyclomatic=16
    - src/debt/patterns.rs:201 - Code duplication: 65 lines duplicated

âœ“ Pass/Fail: PASS
</code></pre>
<h3 id="color-coding-and-symbols"><a class="header" href="#color-coding-and-symbols">Color Coding and Symbols</a></h3>
<p>The terminal output uses colors and symbols for quick visual scanning:</p>
<p><strong>Status Indicators:</strong></p>
<ul>
<li>âœ“ Green: Passing, good, well-tested</li>
<li>âš ï¸  Yellow: Warning, moderate complexity</li>
<li>âœ— Red: Failing, critical, high complexity</li>
<li>ğŸ“Š Blue: Information, metrics</li>
<li>ğŸ”§ Orange: Technical debt items</li>
<li>ğŸ¯ Cyan: Recommendations</li>
</ul>
<p><strong>Complexity Classification:</strong></p>
<ul>
<li><strong>LOW</strong> (0-5): Green - Simple, easy to maintain</li>
<li><strong>MODERATE</strong> (6-10): Yellow - Consider refactoring</li>
<li><strong>HIGH</strong> (11-15): Orange - Should refactor</li>
<li><strong>SEVERE</strong> (&gt;15): Red - Urgent refactoring needed</li>
</ul>
<blockquote>
<p><strong>Note:</strong> These levels match the <code>ComplexityLevel</code> enum in the implementation.</p>
</blockquote>
<p><strong>Debt Score Thresholds:</strong></p>
<p>The default debt threshold is <strong>100</strong>. Scores are colored based on this threshold:</p>
<ul>
<li><strong>Green (â‰¤50)</strong>: Healthy - Below half threshold (score â‰¤ threshold/2)</li>
<li><strong>Yellow (51-100)</strong>: Attention needed - Between half and full threshold (threshold/2 &lt; score â‰¤ threshold)</li>
<li><strong>Red (&gt;100)</strong>: Action required - Exceeds threshold (score &gt; threshold)</li>
</ul>
<blockquote>
<p><strong>Note:</strong> Boundary values use strict inequalities: 50 is Green, 100 is Yellow (not Red), 101+ is Red.</p>
</blockquote>
<h3 id="refactoring-guidance"><a class="header" href="#refactoring-guidance">Refactoring Guidance</a></h3>
<p>For complex functions (cyclomatic complexity &gt; 5), the terminal output provides actionable refactoring recommendations:</p>
<pre><code>ACTION: Extract 3-5 pure functions using decompose-then-transform strategy
PATTERNS: Decompose into logical units, then apply functional patterns
BENEFIT: Pure functions are easily testable and composable
</code></pre>
<p>Guidance levels:</p>
<ul>
<li><strong>Moderate</strong> (6-10): Extract 2-3 pure functions using direct functional transformation</li>
<li><strong>High</strong> (11-15): Extract 3-5 pure functions using decompose-then-transform strategy</li>
<li><strong>Severe</strong> (&gt;15): Extract 5+ pure functions into modules with functional core/imperative shell</li>
</ul>
<p>See the <a href="./analysis-guide.html">Analysis Guide</a> for metric explanations.</p>
<h3 id="plain-terminal-mode"><a class="header" href="#plain-terminal-mode">Plain Terminal Mode</a></h3>
<p>For environments without color support or when piping to tools, use <code>--plain</code>:</p>
<pre><code class="language-bash"># ASCII-only output, no colors, no emoji
debtmap analyze . --plain
</code></pre>
<p>Plain mode:</p>
<ul>
<li>Removes ANSI color codes</li>
<li>Replaces emoji with text labels</li>
<li>Uses ASCII box-drawing characters</li>
<li>Machine-parseable structure</li>
</ul>
<blockquote>
<p><strong>Note:</strong> Terminal output formatting can be customized via <code>FormattingConfig</code>, which controls color mode and emoji mode. The <code>--plain</code> flag uses this configuration to disable both colors and emoji. Additionally, you can control formatting through environment variables:</p>
<ul>
<li><code>NO_COLOR=1</code> - Disables colors (per <a href="https://no-color.org">no-color.org</a> standard)</li>
<li><code>CLICOLOR=0</code> - Disables colors</li>
<li><code>CLICOLOR_FORCE=1</code> - Forces colors even when output is not a terminal</li>
</ul>
</blockquote>
<h3 id="verbosity-levels"><a class="header" href="#verbosity-levels">Verbosity Levels</a></h3>
<p>Control detail level with <code>-v</code> flags (can be repeated):</p>
<pre><code class="language-bash"># Standard output
debtmap analyze .

# Level 1: Show main score factors
debtmap analyze . -v

# Level 2: Show detailed calculations
debtmap analyze . -vv

# Level 3: Show all debug information
debtmap analyze . -vvv
</code></pre>
<p><strong>Verbosity features:</strong></p>
<ul>
<li><code>-v</code>: Show main score factors (complexity, coverage, dependency breakdown)</li>
<li><code>-vv</code>: Show detailed calculations with formulas and intermediate values</li>
<li><code>-vvv</code>: Show all debug information including entropy metrics, role detection, and cache hits</li>
</ul>
<blockquote>
<p><strong>Note:</strong> Verbosity flags affect terminal output only. JSON and markdown formats include all data regardless of verbosity level.</p>
</blockquote>
<p>Each level includes all information from the previous levels, progressively adding more detail to help understand how scores are calculated.</p>
<p><strong>Example Output Differences:</strong></p>
<p>Standard output shows basic metrics:</p>
<pre><code>Total debt score: 156 (threshold: 100)
</code></pre>
<p>Level 1 (<code>-v</code>) adds score breakdowns:</p>
<pre><code>Total debt score: 156 (threshold: 100)
  Complexity contribution: 85 (54%)
  Coverage gaps: 45 (29%)
  Dependency issues: 26 (17%)
</code></pre>
<p>Level 2 (<code>-vv</code>) adds detailed calculations:</p>
<pre><code>Total debt score: 156 (threshold: 100)
  Complexity contribution: 85 (54%)
    Formula: sum(cyclomatic_weight * severity_multiplier)
    High complexity functions: 5 Ã— 12 = 60
    Medium complexity: 8 Ã— 3 = 24
    Base penalty: 1
  Coverage gaps: 45 (29%)
    Uncovered complex functions: 3 Ã— 15 = 45
</code></pre>
<p>Level 3 (<code>-vvv</code>) adds all internal details:</p>
<pre><code>Total debt score: 156 (threshold: 100)
  ... (all level 2 output) ...
  Debug info:
    Entropy metrics cached: 42/50 functions
    Function role detection: BusinessLogic=12, Utility=8, TestHelper=5
    Cache hit rate: 84%
</code></pre>
<h3 id="risk-analysis-output"><a class="header" href="#risk-analysis-output">Risk Analysis Output</a></h3>
<p>When coverage data is provided via <code>--lcov</code>, terminal output includes a dedicated risk analysis section:</p>
<pre><code>â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
           RISK ANALYSIS REPORT
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“ˆ RISK Summary
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Codebase Risk Score: 45.5 (MEDIUM)
Complexity-Coverage Correlation: -0.65

Risk Distribution:
  Critical: 2 functions
  High: 5 functions
  Medium: 10 functions
  Low: 15 functions
  Well Tested: 20 functions

ğŸ¯ CRITICAL RISKS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. src/core/parser.rs:142 parse_complex_ast()
   Risk: 85.0 | Complexity: 15 | Coverage: 0%
   Recommendation: Add 5 unit tests (est: 2-3 hours)
   Impact: -40 risk reduction

ğŸ’¡ RECOMMENDATIONS (by ROI)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. test_me() - ROI: 5.0x
   Current Risk: 75 | Reduction: 40 | Effort: Moderate
   Rationale: High risk function with low coverage
</code></pre>
<p><strong>Risk Level Classification:</strong></p>
<ul>
<li><strong>LOW</strong> (&lt;30): Green - score &lt; 30.0</li>
<li><strong>MEDIUM</strong> (30-59): Yellow - 30.0 â‰¤ score &lt; 60.0</li>
<li><strong>HIGH</strong> (â‰¥60): Red - score â‰¥ 60.0</li>
</ul>
<blockquote>
<p><strong>Note:</strong> 60 is the start of HIGH risk level.</p>
</blockquote>
<h2 id="json-output"><a class="header" href="#json-output">JSON Output</a></h2>
<p>JSON output provides complete analysis results in a machine-readable format, ideal for CI/CD pipelines, custom tooling, and programmatic analysis.</p>
<h3 id="basic-usage"><a class="header" href="#basic-usage">Basic Usage</a></h3>
<pre><code class="language-bash"># Generate JSON output
debtmap analyze . --format json

# Save to file
debtmap analyze . --format json -o report.json

# Pretty-printed by default for readability
debtmap analyze . --format json | jq .
</code></pre>
<blockquote>
<p><strong>Note:</strong> JSON output is automatically pretty-printed for readability.</p>
</blockquote>
<h3 id="json-schema-structure"><a class="header" href="#json-schema-structure">JSON Schema Structure</a></h3>
<p>Debtmap outputs a structured JSON document with the following top-level fields:</p>
<pre><code class="language-json">{
  "project_path": "/path/to/project",
  "timestamp": "2025-01-09T12:00:00Z",
  "complexity": { ... },
  "technical_debt": { ... },
  "dependencies": { ... },
  "duplications": [ ... ]
}
</code></pre>
<h3 id="full-schema-example"><a class="header" href="#full-schema-example">Full Schema Example</a></h3>
<p>Hereâ€™s a complete annotated JSON output example:</p>
<pre><code class="language-json">{
  // Project metadata
  "project_path": "/Users/dev/myproject",
  "timestamp": "2025-01-09T15:30:00Z",

  // Complexity analysis results
  "complexity": {
    "metrics": [
      {
        "name": "calculate_risk_score",
        "file": "src/risk/scoring.rs",
        "line": 142,
        "cyclomatic": 12,
        "cognitive": 18,
        "nesting": 4,
        "length": 85,
        "is_test": false,
        "visibility": "pub",
        "is_trait_method": false,
        "in_test_module": false,
        "entropy_score": {
          "token_entropy": 0.65,
          "pattern_repetition": 0.30,
          "branch_similarity": 0.45,
          "effective_complexity": 0.85
        },
        "is_pure": false,
        "purity_confidence": 0.75,
        "detected_patterns": ["nested_loops", "complex_conditionals"],
        "upstream_callers": ["analyze_codebase", "generate_report"],
        "downstream_callees": ["get_metrics", "apply_weights"]
      }
    ],
    "summary": {
      "total_functions": 287,
      "average_complexity": 6.3,
      "max_complexity": 24,
      "high_complexity_count": 12
    }
  },

  // Technical debt items
  "technical_debt": {
    "items": [
      {
        "id": "debt_001",
        "debt_type": "Complexity",
        "priority": "High",
        "file": "src/analyzers/rust.rs",
        "line": 245,
        "column": 5,
        "message": "High cyclomatic complexity: 18",
        "context": "Function parse_function has excessive branching"
      },
      {
        "id": "debt_002",
        "debt_type": "Todo",
        "priority": "Medium",
        "file": "src/core/cache.rs",
        "line": 89,
        "column": null,
        "message": "TODO: Implement LRU eviction policy",
        "context": null
      }
    ],
    "by_type": {
      "Complexity": [ /* same structure as items */ ],
      "Todo": [ /* ... */ ],
      "Duplication": [ /* ... */ ]
    },
    "priorities": ["Low", "Medium", "High", "Critical"]
  },

  // Dependency analysis
  "dependencies": {
    "modules": [
      {
        "module": "risk::scoring",
        "dependencies": ["core::metrics", "debt::patterns"],
        "dependents": ["commands::analyze", "io::output"]
      }
    ],
    "circular": [
      {
        "cycle": ["module_a", "module_b", "module_c", "module_a"]
      }
    ]
  },

  // Code duplication blocks
  "duplications": [
    {
      "hash": "abc123def456",
      "lines": 15,
      "locations": [
        {
          "file": "src/parser/rust.rs",
          "start_line": 42,
          "end_line": 57
        },
        {
          "file": "src/parser/python.rs",
          "start_line": 89,
          "end_line": 104
        }
      ]
    }
  ]
}
</code></pre>
<h3 id="field-descriptions"><a class="header" href="#field-descriptions">Field Descriptions</a></h3>
<p><strong>FunctionMetrics Fields:</strong></p>
<ul>
<li>
<p><code>name</code>: Function name</p>
</li>
<li>
<p><code>file</code>: Path to source file</p>
</li>
<li>
<p><code>line</code>: Line number where function is defined</p>
</li>
<li>
<p><code>cyclomatic</code>: Cyclomatic complexity score</p>
</li>
<li>
<p><code>cognitive</code>: Cognitive complexity score</p>
</li>
<li>
<p><code>nesting</code>: Maximum nesting depth</p>
</li>
<li>
<p><code>length</code>: Lines of code in function</p>
</li>
<li>
<p><code>is_test</code>: Whether this is a test function</p>
</li>
<li>
<p><code>visibility</code>: Rust visibility modifier (pub, pub(crate), or null)</p>
</li>
<li>
<p><code>is_trait_method</code>: Whether this implements a trait</p>
</li>
<li>
<p><code>in_test_module</code>: Whether inside #[cfg(test)]</p>
</li>
<li>
<p><code>entropy_score</code>: Optional entropy analysis with structure:</p>
<pre><code class="language-json">{
  "token_entropy": 0.65,        // Token distribution entropy (0-1): measures variety of tokens
  "pattern_repetition": 0.30,   // Pattern repetition score (0-1): detects repeated code patterns
  "branch_similarity": 0.45,    // Branch similarity metric (0-1): compares similarity between branches
  "effective_complexity": 0.85  // Adjusted complexity multiplier: complexity adjusted for entropy
}
</code></pre>
<p><strong>EntropyScore Fields:</strong></p>
<ul>
<li><code>token_entropy</code>: Measures the variety and distribution of tokens in the function (0-1, higher = more variety)</li>
<li><code>pattern_repetition</code>: Detects repeated code patterns within the function (0-1, higher = more repetition)</li>
<li><code>branch_similarity</code>: Measures similarity between different code branches (0-1, higher = more similar)</li>
<li><code>effective_complexity</code>: The overall complexity multiplier adjusted for entropy effects</li>
</ul>
</li>
<li>
<p><code>is_pure</code>: Whether function is pure (no side effects)</p>
</li>
<li>
<p><code>purity_confidence</code>: Confidence level (0.0-1.0)</p>
</li>
<li>
<p><code>detected_patterns</code>: List of detected code patterns</p>
</li>
<li>
<p><code>upstream_callers</code>: Functions that call this one</p>
</li>
<li>
<p><code>downstream_callees</code>: Functions this one calls</p>
</li>
</ul>
<p><strong>DebtItem Fields:</strong></p>
<ul>
<li><code>id</code>: Unique identifier</li>
<li><code>debt_type</code>: Type of debt (see DebtType enum below)</li>
<li><code>priority</code>: Priority level (Low, Medium, High, Critical)</li>
<li><code>file</code>: Path to file containing debt</li>
<li><code>line</code>: Line number</li>
<li><code>column</code>: Optional column number</li>
<li><code>message</code>: Human-readable description</li>
<li><code>context</code>: Optional additional context</li>
</ul>
<p><strong>DebtType Enum:</strong></p>
<ul>
<li><code>Todo</code>: TODO markers</li>
<li><code>Fixme</code>: FIXME markers</li>
<li><code>CodeSmell</code>: Code smell patterns</li>
<li><code>Duplication</code>: Duplicated code</li>
<li><code>Complexity</code>: Excessive complexity</li>
<li><code>Dependency</code>: Dependency issues</li>
<li><code>ErrorSwallowing</code>: Suppressed errors</li>
<li><code>ResourceManagement</code>: Resource management issues</li>
<li><code>CodeOrganization</code>: Organizational problems</li>
<li><code>TestComplexity</code>: Complex test code</li>
<li><code>TestTodo</code>: TODOs in tests</li>
<li><code>TestDuplication</code>: Duplicated test code</li>
<li><code>TestQuality</code>: Test quality issues</li>
</ul>
<h3 id="json-format-variants"><a class="header" href="#json-format-variants">JSON Format Variants</a></h3>
<p>Debtmap supports two JSON output formats:</p>
<pre><code class="language-bash"># Legacy format (default) - backward compatible
debtmap analyze . --format json --output-format legacy

# Unified format - new consistent structure
debtmap analyze . --format json --output-format unified
</code></pre>
<blockquote>
<p><strong>Note:</strong> The <code>--output-format</code> flag only applies when using <code>--format json</code>. It has no effect with markdown or terminal formats.</p>
</blockquote>
<p><strong>Legacy format:</strong> Uses <code>{File: {...}}</code> and <code>{Function: {...}}</code> wrappers for backward compatibility with existing tooling.</p>
<p><strong>Unified format:</strong> Consistent structure with a <code>type</code> field, making parsing simpler and more predictable. Recommended for new integrations.</p>
<h3 id="risk-insights-json"><a class="header" href="#risk-insights-json">Risk Insights JSON</a></h3>
<p>When using <code>--lcov</code>, debtmap also outputs risk analysis in JSON:</p>
<pre><code class="language-json">{
  "items": [
    {
      "location": {
        "file": "src/risk/scoring.rs",
        "function": "calculate_priority",
        "line": 66
      },
      "debt_type": "TestGap",
      "unified_score": {
        "complexity_factor": 3.2,
        "coverage_factor": 10.0,
        "dependency_factor": 2.5,
        "role_multiplier": 1.2,
        "final_score": 9.4
      },
      "function_role": "BusinessLogic",
      "recommendation": {
        "action": "Add unit tests",
        "details": "Add 6 unit tests for full coverage",
        "effort_estimate": "2-3 hours"
      },
      "expected_impact": {
        "risk_reduction": 3.9,
        "complexity_reduction": 0,
        "coverage_improvement": 100
      },
      "upstream_dependencies": 0,
      "downstream_dependencies": 3,
      "nesting_depth": 1,
      "function_length": 13
    }
  ],
  "call_graph": {
    "total_functions": 1523,
    "entry_points": 12,
    "test_functions": 456,
    "max_depth": 8
  },
  "overall_coverage": 82.3,
  "total_impact": {
    "risk_reduction": 45.2,
    "complexity_reduction": 12.3,
    "coverage_improvement": 18.5
  }
}
</code></pre>
<h2 id="markdown-output"><a class="header" href="#markdown-output">Markdown Output</a></h2>
<p>Markdown format generates documentation-friendly reports suitable for README files, PR comments, and technical documentation.</p>
<h3 id="basic-usage-1"><a class="header" href="#basic-usage-1">Basic Usage</a></h3>
<pre><code class="language-bash"># Generate markdown report
debtmap analyze . --format markdown

# Save to documentation
debtmap analyze . --format markdown -o docs/DEBT_REPORT.md
</code></pre>
<h3 id="markdown-structure"><a class="header" href="#markdown-structure">Markdown Structure</a></h3>
<p>Markdown output includes:</p>
<ol>
<li><strong>Executive Summary</strong> - High-level metrics and health dashboard</li>
<li><strong>Complexity Analysis</strong> - Detailed complexity breakdown by file</li>
<li><strong>Technical Debt</strong> - Categorized debt items with priorities</li>
<li><strong>Dependencies</strong> - Module dependencies and circular references</li>
<li><strong>Recommendations</strong> - Prioritized action items</li>
</ol>
<h3 id="example-markdown-output"><a class="header" href="#example-markdown-output">Example Markdown Output</a></h3>
<pre><code class="language-markdown"># Debtmap Analysis Report

**Generated:** 2025-01-09 15:30:00 UTC
**Project:** /Users/dev/myproject

## Executive Summary

- **Files Analyzed:** 42
- **Total Functions:** 287
- **Average Complexity:** 6.3
- **Total Debt Items:** 15
- **Debt Score:** 156/100 âš ï¸

### Health Dashboard

| Metric | Value | Status |
|--------|-------|--------|
| Complexity | 6.3 avg | âœ… Good |
| Debt Score | 156 | âš ï¸ Attention |
| High Priority Items | 5 | âš ï¸ Action Needed |

## Complexity Analysis

### Top 5 Complex Functions

| Function | File | Cyclomatic | Cognitive | Priority |
|----------|------|-----------|-----------|----------|
| parse_function | src/analyzers/rust.rs:245 | 18 | 24 | High |
| detect_data_clumps | src/debt/smells.rs:196 | 15 | 20 | Medium |
| analyze_dependencies | src/core/deps.rs:89 | 14 | 18 | Medium |

### Refactoring Recommendations

**src/analyzers/rust.rs:245** - `parse_function()`
- **Complexity:** Cyclomatic: 18, Cognitive: 24
- **Action:** Extract 3-5 pure functions using decompose-then-transform strategy
- **Patterns:** Decompose into logical units, then apply functional patterns
- **Benefit:** Improved testability and maintainability

## Technical Debt

### High Priority (5 items)

- **src/risk/scoring.rs:142** - TODO: Implement caching for score calculations
- **src/core/metrics.rs:89** - High complexity: cyclomatic=16
- **src/debt/patterns.rs:201** - Code duplication: 65 lines duplicated

### Medium Priority (8 items)

...

## Dependencies

### Circular Dependencies

- `risk::scoring` â†’ `core::metrics` â†’ `risk::scoring`

## Recommendations

1. **Refactor parse_function** (High Priority)
   - Reduce complexity from 18 to &lt;10
   - Extract helper functions
   - Estimated effort: 4-6 hours

2. **Add tests for scoring module** (High Priority)
   - Current coverage: 35%
   - Target coverage: 80%
   - Estimated effort: 2-3 hours
</code></pre>
<h3 id="enhanced-markdown-features"><a class="header" href="#enhanced-markdown-features">Enhanced Markdown Features</a></h3>
<p>Debtmap can generate enhanced markdown reports with additional visualizations and insights:</p>
<ul>
<li><strong>Complexity distribution charts</strong> - Visual representation of complexity across codebase</li>
<li><strong>Risk heat maps</strong> - Color-coded risk matrices</li>
<li><strong>Dependency graphs</strong> - Module relationship diagrams</li>
<li><strong>Quick wins section</strong> - Low-effort, high-impact improvements</li>
<li><strong>Strategic priorities</strong> - Long-term architectural improvements</li>
<li><strong>Team guidance</strong> - Role-specific recommendations</li>
</ul>
<blockquote>
<p><strong>Note:</strong> Enhanced markdown features are implemented in the codebase (<code>src/io/writers/enhanced_markdown/</code> module) but the specific flags or configuration options to enable them are not currently documented. Refer to the source code or use <code>--help</code> to discover available options.</p>
</blockquote>
<h3 id="rendering-to-htmlpdf"><a class="header" href="#rendering-to-htmlpdf">Rendering to HTML/PDF</a></h3>
<p>Markdown reports can be converted to other formats:</p>
<pre><code class="language-bash"># Generate markdown
debtmap analyze . --format markdown -o report.md

# Convert to HTML with pandoc
pandoc report.md -o report.html --standalone --css style.css

# Convert to PDF
pandoc report.md -o report.pdf --pdf-engine=xelatex
</code></pre>
<h2 id="tool-integration"><a class="header" href="#tool-integration">Tool Integration</a></h2>
<h3 id="cicd-pipelines"><a class="header" href="#cicd-pipelines">CI/CD Pipelines</a></h3>
<p>Debtmap JSON output integrates seamlessly with CI/CD systems.</p>
<h4 id="github-actions"><a class="header" href="#github-actions">GitHub Actions</a></h4>
<pre><code class="language-yaml">name: Code Quality

on: [pull_request]

jobs:
  analyze:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Install debtmap
        run: cargo install debtmap

      - name: Run analysis
        run: |
          debtmap analyze . \
            --format json \
            --output analysis.json \
            --lcov coverage/lcov.info

      - name: Check thresholds
        run: |
          DEBT_SCORE=$(jq '.technical_debt.items | length' analysis.json)
          if [ "$DEBT_SCORE" -gt 100 ]; then
            echo "âŒ Debt score too high: $DEBT_SCORE"
            exit 1
          fi

      - name: Comment on PR
        uses: actions/github-script@v6
        with:
          script: |
            const fs = require('fs');
            const analysis = JSON.parse(fs.readFileSync('analysis.json'));
            const summary = `## Debtmap Analysis

            - **Debt Items:** ${analysis.technical_debt.items.length}
            - **Average Complexity:** ${analysis.complexity.summary.average_complexity}
            - **High Complexity Functions:** ${analysis.complexity.summary.high_complexity_count}
            `;
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: summary
            });
</code></pre>
<h4 id="gitlab-ci"><a class="header" href="#gitlab-ci">GitLab CI</a></h4>
<pre><code class="language-yaml">code_quality:
  stage: test
  script:
    - cargo install debtmap
    - debtmap analyze . --format json --output gl-code-quality.json
    - |
      DEBT=$(jq '.technical_debt.items | length' gl-code-quality.json)
      if [ "$DEBT" -gt 50 ]; then
        echo "Debt threshold exceeded"
        exit 1
      fi
  artifacts:
    reports:
      codequality: gl-code-quality.json
</code></pre>
<h4 id="jenkins-pipeline"><a class="header" href="#jenkins-pipeline">Jenkins Pipeline</a></h4>
<pre><code class="language-groovy">pipeline {
    agent any

    stages {
        stage('Analyze') {
            steps {
                sh 'debtmap analyze . --format json -o report.json'

                script {
                    def json = readJSON file: 'report.json'
                    def debtScore = json.technical_debt.items.size()

                    if (debtScore &gt; 100) {
                        error("Debt score ${debtScore} exceeds threshold")
                    }
                }
            }
        }
    }

    post {
        always {
            archiveArtifacts artifacts: 'report.json'
        }
    }
}
</code></pre>
<h3 id="querying-json-with-jq"><a class="header" href="#querying-json-with-jq">Querying JSON with jq</a></h3>
<p>Common jq queries for analyzing debtmap output:</p>
<pre><code class="language-bash"># Get total debt items
jq '.technical_debt.items | length' report.json

# Get high-priority items only
jq '.technical_debt.items[] | select(.priority == "High")' report.json

# Get functions with complexity &gt; 10
jq '.complexity.metrics[] | select(.cyclomatic &gt; 10)' report.json

# Calculate average complexity
jq '.complexity.summary.average_complexity' report.json

# Get all TODO items
jq '.technical_debt.items[] | select(.debt_type == "Todo")' report.json

# Get top 5 complex functions
jq '.complexity.metrics | sort_by(-.cyclomatic) | .[0:5] | .[] | {name, file, cyclomatic}' report.json

# Get files with circular dependencies
jq '.dependencies.circular[] | .cycle' report.json

# Count debt items by type
jq '.technical_debt.items | group_by(.debt_type) | map({type: .[0].debt_type, count: length})' report.json

# Get functions with 0% coverage (when using --lcov)
jq '.complexity.metrics[] | select(.coverage == 0)' report.json

# Extract file paths with high debt
jq '.technical_debt.items[] | select(.priority == "High" or .priority == "Critical") | .file' report.json | sort -u
</code></pre>
<h3 id="filtering-and-transformation-examples"><a class="header" href="#filtering-and-transformation-examples">Filtering and Transformation Examples</a></h3>
<h4 id="python-script-to-parse-json"><a class="header" href="#python-script-to-parse-json">Python Script to Parse JSON</a></h4>
<pre><code class="language-python">#!/usr/bin/env python3
import json
import sys

def analyze_debtmap_output(json_file):
    with open(json_file) as f:
        data = json.load(f)

    # Get high-priority items
    high_priority = [
        item for item in data['technical_debt']['items']
        if item['priority'] in ['High', 'Critical']
    ]

    # Group by file
    by_file = {}
    for item in high_priority:
        file = item['file']
        if file not in by_file:
            by_file[file] = []
        by_file[file].append(item)

    # Print summary
    print(f"High-priority debt items: {len(high_priority)}")
    print(f"Files affected: {len(by_file)}")
    print("\nBy file:")
    for file, items in sorted(by_file.items(), key=lambda x: -len(x[1])):
        print(f"  {file}: {len(items)} items")

    return by_file

if __name__ == '__main__':
    analyze_debtmap_output(sys.argv[1])
</code></pre>
<h4 id="shell-script-for-threshold-checking"><a class="header" href="#shell-script-for-threshold-checking">Shell Script for Threshold Checking</a></h4>
<pre><code class="language-bash">#!/bin/bash
set -e

REPORT="$1"
DEBT_THRESHOLD=100
COMPLEXITY_THRESHOLD=10

# Check debt score
DEBT_SCORE=$(jq '.technical_debt.items | length' "$REPORT")
if [ "$DEBT_SCORE" -gt "$DEBT_THRESHOLD" ]; then
    echo "âŒ Debt score $DEBT_SCORE exceeds threshold $DEBT_THRESHOLD"
    exit 1
fi

# Check average complexity
AVG_COMPLEXITY=$(jq '.complexity.summary.average_complexity' "$REPORT")
if (( $(echo "$AVG_COMPLEXITY &gt; $COMPLEXITY_THRESHOLD" | bc -l) )); then
    echo "âŒ Average complexity $AVG_COMPLEXITY exceeds threshold $COMPLEXITY_THRESHOLD"
    exit 1
fi

echo "âœ… All quality checks passed"
echo "   Debt score: $DEBT_SCORE/$DEBT_THRESHOLD"
echo "   Avg complexity: $AVG_COMPLEXITY"
</code></pre>
<h3 id="editor-integration"><a class="header" href="#editor-integration">Editor Integration</a></h3>
<h4 id="vs-code-tasks"><a class="header" href="#vs-code-tasks">VS Code Tasks</a></h4>
<p>Create <code>.vscode/tasks.json</code>:</p>
<pre><code class="language-json">{
  "version": "2.0.0",
  "tasks": [
    {
      "label": "Debtmap: Analyze",
      "type": "shell",
      "command": "debtmap",
      "args": [
        "analyze",
        ".",
        "--format",
        "terminal"
      ],
      "problemMatcher": [],
      "presentation": {
        "reveal": "always",
        "panel": "new"
      }
    },
    {
      "label": "Debtmap: Generate Report",
      "type": "shell",
      "command": "debtmap",
      "args": [
        "analyze",
        ".",
        "--format",
        "markdown",
        "-o",
        "DEBT_REPORT.md"
      ],
      "problemMatcher": []
    }
  ]
}
</code></pre>
<h4 id="problem-matcher-for-vs-code"><a class="header" href="#problem-matcher-for-vs-code">Problem Matcher for VS Code</a></h4>
<p>Parse debtmap output in VS Codeâ€™s Problems panel:</p>
<pre><code class="language-json">{
  "problemMatcher": {
    "owner": "debtmap",
    "fileLocation": "absolute",
    "pattern": {
      "regexp": "^(.+?):(\\d+):(\\d+)?\\s*-\\s*(.+)$",
      "file": 1,
      "line": 2,
      "column": 3,
      "message": 4
    }
  }
}
</code></pre>
<h3 id="webhook-integration"><a class="header" href="#webhook-integration">Webhook Integration</a></h3>
<p>Send debtmap results to webhooks for notifications:</p>
<pre><code class="language-bash">#!/bin/bash

# Run analysis
debtmap analyze . --format json -o report.json

# Send to Slack
DEBT_SCORE=$(jq '.technical_debt.items | length' report.json)
curl -X POST "$SLACK_WEBHOOK_URL" \
  -H 'Content-Type: application/json' \
  -d "{\"text\": \"Debtmap Analysis Complete\nâ€¢ Debt Score: $DEBT_SCORE\nâ€¢ High Priority: $(jq '[.technical_debt.items[] | select(.priority == "High")] | length' report.json)\"}"

# Send to custom webhook
curl -X POST "$CUSTOM_WEBHOOK_URL" \
  -H 'Content-Type: application/json' \
  -d @report.json
</code></pre>
<h2 id="output-filtering"><a class="header" href="#output-filtering">Output Filtering</a></h2>
<p>Debtmap provides several flags to filter and limit output:</p>
<blockquote>
<p><strong>Note:</strong> Filtering options (<code>--top</code>, <code>--tail</code>, <code>--summary</code>, <code>--filter</code>) apply to all output formats (terminal, JSON, and markdown). The filtered data is applied at the analysis level before formatting, ensuring consistent results across all output types.</p>
</blockquote>
<h3 id="limiting-results"><a class="header" href="#limiting-results">Limiting Results</a></h3>
<pre><code class="language-bash"># Show only top 10 priority items
debtmap analyze . --top 10

# Show bottom 5 lowest priority items
debtmap analyze . --tail 5
</code></pre>
<h3 id="priority-filtering"><a class="header" href="#priority-filtering">Priority Filtering</a></h3>
<pre><code class="language-bash"># Show only high and critical priority items
debtmap analyze . --min-priority high

# Filter by specific debt categories
debtmap analyze . --filter Architecture,Testing
</code></pre>
<p>Available categories:</p>
<ul>
<li><code>Architecture</code>: God objects, complexity hotspots, dead code</li>
<li><code>Testing</code>: Testing gaps, coverage issues</li>
<li><code>Performance</code>: Resource leaks, inefficient patterns</li>
<li><code>CodeQuality</code>: Code smells, maintainability</li>
</ul>
<h3 id="grouping-output"><a class="header" href="#grouping-output">Grouping Output</a></h3>
<pre><code class="language-bash"># Group results by debt category
debtmap analyze . --group-by-category

# Combine filters for focused analysis
debtmap analyze . --filter Architecture --min-priority high --top 5
</code></pre>
<h3 id="summary-mode"><a class="header" href="#summary-mode">Summary Mode</a></h3>
<pre><code class="language-bash"># Compact tiered priority display
debtmap analyze . --summary

# Combines well with filtering
debtmap analyze . --summary --min-priority medium
</code></pre>
<h2 id="best-practices-2"><a class="header" href="#best-practices-2">Best Practices</a></h2>
<h3 id="when-to-use-each-format"><a class="header" href="#when-to-use-each-format">When to Use Each Format</a></h3>
<p><strong>Use Terminal Format When:</strong></p>
<ul>
<li>Developing locally and reviewing code</li>
<li>Getting quick feedback on changes</li>
<li>Presenting results to team members</li>
<li>Exploring complexity hotspots interactively</li>
</ul>
<p><strong>Use JSON Format When:</strong></p>
<ul>
<li>Integrating with CI/CD pipelines</li>
<li>Building custom analysis tools</li>
<li>Tracking metrics over time</li>
<li>Programmatically processing results</li>
<li>Feeding into dashboards or monitoring systems</li>
</ul>
<p><strong>Use Markdown Format When:</strong></p>
<ul>
<li>Generating documentation</li>
<li>Creating PR comments</li>
<li>Sharing reports with stakeholders</li>
<li>Archiving analysis results</li>
<li>Producing executive summaries</li>
</ul>
<h3 id="quick-reference-table"><a class="header" href="#quick-reference-table">Quick Reference Table</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Format</th><th>Best For</th><th>Machine Readable</th><th>Human Readable</th><th>File Extension</th></tr></thead><tbody>
<tr><td>Terminal</td><td>Development</td><td>No</td><td>Yes</td><td>.txt</td></tr>
<tr><td>JSON</td><td>Automation</td><td>Yes</td><td>No</td><td>.json</td></tr>
<tr><td>Markdown</td><td>Documentation</td><td>Partially</td><td>Yes</td><td>.md</td></tr>
</tbody></table>
</div>
<h3 id="combining-formats"><a class="header" href="#combining-formats">Combining Formats</a></h3>
<p>Use multiple formats for comprehensive workflows:</p>
<pre><code class="language-bash"># Generate terminal output for review
debtmap analyze .

# Generate JSON for automation
debtmap analyze . --format json -o ci-report.json

# Generate markdown for documentation
debtmap analyze . --format markdown -o docs/DEBT.md
</code></pre>
<h3 id="performance-considerations"><a class="header" href="#performance-considerations">Performance Considerations</a></h3>
<ul>
<li><strong>Terminal format</strong>: Fastest, minimal overhead</li>
<li><strong>JSON format</strong>: Fast serialization, efficient for large codebases</li>
<li><strong>Markdown format</strong>: Slightly slower due to formatting, but still performant</li>
</ul>
<p>For very large codebases (&gt;10,000 files), use <code>--top</code> or <code>--filter</code> to limit output size.</p>
<h2 id="troubleshooting-3"><a class="header" href="#troubleshooting-3">Troubleshooting</a></h2>
<h3 id="common-issues"><a class="header" href="#common-issues">Common Issues</a></h3>
<p><strong>Colors not showing in terminal:</strong></p>
<ul>
<li>Check if terminal supports ANSI colors</li>
<li>Use <code>--plain</code> flag for ASCII-only output</li>
<li>Some CI systems may not support color codes</li>
</ul>
<p><strong>JSON parsing errors:</strong></p>
<ul>
<li>Ensure output is complete (check for errors during analysis)</li>
<li>Validate JSON with <code>jq</code> or online validators</li>
<li>Check for special characters in file paths</li>
</ul>
<p><strong>Markdown rendering issues:</strong></p>
<ul>
<li>Some markdown renderers donâ€™t support all features</li>
<li>Use standard markdown for maximum compatibility</li>
<li>Test with pandoc or GitHub/GitLab preview</li>
</ul>
<p><strong>File encoding problems:</strong></p>
<ul>
<li>Ensure UTF-8 encoding for all output files</li>
<li>Use <code>--plain</code> for pure ASCII output</li>
<li>Check locale settings (LC_ALL, LANG environment variables)</li>
</ul>
<h3 id="exit-codes"><a class="header" href="#exit-codes">Exit Codes</a></h3>
<blockquote>
<p><strong>IMPORTANT:</strong> Exit codes 1 and 2 are NOT YET IMPLEMENTED. Current behavior: Always returns <code>0</code> on successful analysis, regardless of threshold violations.</p>
<p>Planned behavior includes:</p>
<ul>
<li><code>0</code>: Success, all checks passed</li>
<li><code>1</code>: Analysis completed, but validation thresholds exceeded</li>
<li><code>2</code>: Error during analysis (invalid path, parsing error, etc.)</li>
</ul>
</blockquote>
<p>For now, use the <code>validate</code> command with threshold checks to enforce quality gates:</p>
<pre><code class="language-bash"># Use validate command for threshold enforcement
debtmap validate . --config debtmap.toml

# Or parse JSON output for threshold checking
debtmap analyze . --format json -o report.json
DEBT_SCORE=$(jq '.technical_debt.items | length' report.json)
if [ "$DEBT_SCORE" -gt 100 ]; then
    echo "Debt threshold exceeded"
    exit 1
fi
</code></pre>
<h2 id="see-also-1"><a class="header" href="#see-also-1">See Also</a></h2>
<ul>
<li><a href="./getting-started.html">Getting Started</a> - Basic usage and examples</li>
<li><a href="./analysis-guide.html">Analysis Guide</a> - Understanding metrics and scores</li>
<li><a href="./configuration.html">Configuration</a> - Customizing analysis behavior</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="architecture"><a class="header" href="#architecture">Architecture</a></h1>
<p>This chapter explains how debtmapâ€™s analysis pipeline works, from discovering files to producing prioritized technical debt recommendations.</p>
<h2 id="analysis-pipeline-overview"><a class="header" href="#analysis-pipeline-overview">Analysis Pipeline Overview</a></h2>
<p>Debtmapâ€™s analysis follows a multi-stage pipeline that transforms source code into actionable recommendations:</p>
<pre><code class="language-mermaid">graph TD
    A[File Discovery] --&gt; B[Language Detection]
    B --&gt; C{Parser}
    C --&gt;|Rust| D[syn AST]
    C --&gt;|Python| E[rustpython AST]
    C --&gt;|JS/TS| F[tree-sitter AST]

    D --&gt; G[Metric Extraction]
    E --&gt; G
    F --&gt; G

    G --&gt; H[Complexity Calculation]
    G --&gt; I[Call Graph Construction]
    G --&gt; J[Pattern Detection]

    H --&gt; K[Entropy Analysis]
    K --&gt; L[Effective Complexity]

    I --&gt; M[Dependency Analysis]
    J --&gt; N[Debt Classification]

    O[LCOV Coverage] --&gt; P[Coverage Mapping]
    P --&gt; Q[Risk Scoring]

    L --&gt; Q
    M --&gt; Q
    N --&gt; Q

    Q --&gt; R[Tiered Prioritization]
    R --&gt; S[Output Formatting]
    S --&gt; T[Terminal/JSON/Markdown]
</code></pre>
<h2 id="key-components"><a class="header" href="#key-components">Key Components</a></h2>
<h3 id="1-file-discovery-and-language-detection"><a class="header" href="#1-file-discovery-and-language-detection">1. File Discovery and Language Detection</a></h3>
<p><strong>Purpose:</strong> Identify source files to analyze and determine their language.</p>
<p><strong>How it works:</strong></p>
<ul>
<li>Walks the project directory tree (respecting <code>.gitignore</code> and <code>.debtmapignore</code>)</li>
<li>Detects language based on file extension (<code>.rs</code>, <code>.py</code>, <code>.js</code>, <code>.ts</code>)</li>
<li>Filters out test files, build artifacts, and vendored dependencies</li>
<li>Groups files by language for parallel processing</li>
</ul>
<p><strong>Configuration:</strong></p>
<pre><code class="language-toml">[analysis]
exclude_patterns = ["**/tests/**", "**/target/**", "**/node_modules/**"]
include_patterns = ["src/**/*.rs", "lib/**/*.py"]
</code></pre>
<h3 id="2-parser-layer"><a class="header" href="#2-parser-layer">2. Parser Layer</a></h3>
<p><strong>Purpose:</strong> Convert source code into Abstract Syntax Trees (ASTs) for analysis.</p>
<p><strong>Language-Specific Parsers:</strong></p>
<p><strong>Rust (syn):</strong></p>
<ul>
<li>Uses the <code>syn</code> crate for full Rust syntax support</li>
<li>Extracts: functions, structs, impls, traits, macros</li>
<li>Handles: async/await, generic types, lifetime annotations</li>
<li>Performance: ~10-20ms per file</li>
</ul>
<p><strong>Python (rustpython):</strong></p>
<ul>
<li>Uses rustpythonâ€™s parser for Python 3.x syntax</li>
<li>Extracts: functions, classes, methods, decorators</li>
<li>Handles: comprehensions, async/await, type hints</li>
<li>Performance: ~5-15ms per file</li>
</ul>
<p><strong>JavaScript/TypeScript (tree-sitter):</strong></p>
<ul>
<li>Uses tree-sitter for JS/TS parsing</li>
<li>Extracts: functions, classes, arrow functions, hooks</li>
<li>Handles: JSX/TSX, decorators, generics</li>
<li>Performance: ~8-18ms per file</li>
</ul>
<p><strong>Error Handling:</strong></p>
<ul>
<li>Syntax errors logged but donâ€™t stop analysis</li>
<li>Partial ASTs used when possible</li>
<li>Files with parse errors excluded from final report</li>
</ul>
<h3 id="3-metric-extraction"><a class="header" href="#3-metric-extraction">3. Metric Extraction</a></h3>
<p><strong>Purpose:</strong> Extract raw metrics from ASTs.</p>
<p><strong>Metrics Computed:</strong></p>
<p><strong>Function-Level:</strong></p>
<ul>
<li>Lines of code (LOC)</li>
<li>Cyclomatic complexity (branch count)</li>
<li>Nesting depth (max indentation level)</li>
<li>Parameter count</li>
<li>Return path count</li>
<li>Comment ratio</li>
</ul>
<p><strong>File-Level:</strong></p>
<ul>
<li>Total LOC</li>
<li>Number of functions/classes</li>
<li>Dependency count (imports)</li>
<li>Documentation coverage</li>
</ul>
<p><strong>Implementation:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct FunctionMetrics {
    pub name: String,
    pub location: Location,
    pub loc: u32,
    pub cyclomatic_complexity: u32,
    pub nesting_depth: u32,
    pub parameter_count: u32,
    pub return_paths: u32,
}
<span class="boring">}</span></code></pre></pre>
<h3 id="4-complexity-calculation-and-entropy-analysis"><a class="header" href="#4-complexity-calculation-and-entropy-analysis">4. Complexity Calculation and Entropy Analysis</a></h3>
<p><strong>Purpose:</strong> Compute effective complexity using entropy-adjusted metrics.</p>
<p><strong>Traditional Cyclomatic Complexity:</strong></p>
<ul>
<li>Count decision points (if, match, loop, etc.)</li>
<li>Each branch adds +1 to complexity</li>
<li>Does not distinguish between repetitive and varied logic</li>
</ul>
<p><strong>Entropy-Based Adjustment:</strong></p>
<p>Debtmap calculates pattern entropy to adjust cyclomatic complexity:</p>
<ol>
<li><strong>Extract patterns</strong> - Identify branch structures (e.g., all if/return patterns)</li>
<li><strong>Calculate variety</strong> - Measure information entropy of patterns</li>
<li><strong>Adjust complexity</strong> - Reduce score for low-entropy (repetitive) code</li>
</ol>
<p><strong>Formula:</strong></p>
<pre><code>Entropy = -Î£(p_i * log2(p_i))

where p_i = frequency of pattern i

Effective Complexity = Cyclomatic * (1 - (1 - Entropy/Max_Entropy) * 0.75)
</code></pre>
<p><strong>Example:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// 20 similar if/return statements
// Cyclomatic: 20, Entropy: 0.3
// Effective: 20 * (1 - (1 - 0.3/4.32) * 0.75) â‰ˆ 5.5
<span class="boring">}</span></code></pre></pre>
<p>This approach reduces false positives from validation/configuration code while still flagging genuinely complex logic.</p>
<h3 id="5-call-graph-construction"><a class="header" href="#5-call-graph-construction">5. Call Graph Construction</a></h3>
<p><strong>Purpose:</strong> Understand function dependencies and identify critical paths.</p>
<p><strong>Whatâ€™s Tracked:</strong></p>
<ul>
<li>Function calls within the same file</li>
<li>Cross-file calls (when possible to resolve)</li>
<li>Method calls on structs/classes</li>
<li>Trait/interface implementations</li>
</ul>
<p><strong>Analysis:</strong></p>
<ul>
<li><strong>Fan-in:</strong> How many functions call this function</li>
<li><strong>Fan-out:</strong> How many functions this function calls</li>
<li><strong>Depth:</strong> Distance from entry points (main, handlers)</li>
<li><strong>Cycles:</strong> Detect recursive calls</li>
</ul>
<p><strong>Usage:</strong></p>
<ul>
<li>Prioritize functions called from many untested paths</li>
<li>Identify central functions (high fan-in/fan-out)</li>
<li>Detect test coverage gaps in critical paths</li>
</ul>
<p><strong>Limitations:</strong></p>
<ul>
<li>Dynamic dispatch not fully resolved</li>
<li>Cross-crate calls require additional analysis</li>
<li>Closures and function pointers approximated</li>
</ul>
<h3 id="6-pattern-detection-and-debt-classification"><a class="header" href="#6-pattern-detection-and-debt-classification">6. Pattern Detection and Debt Classification</a></h3>
<p><strong>Purpose:</strong> Identify specific technical debt patterns.</p>
<p><strong>Debt Categories:</strong></p>
<p><strong>Test Gaps:</strong></p>
<ul>
<li>Functions with 0% coverage and high complexity</li>
<li>Untested error paths</li>
<li>Missing edge case tests</li>
</ul>
<p><strong>Complexity Issues:</strong></p>
<ul>
<li>Functions exceeding thresholds (default: 10)</li>
<li>Deep nesting (3+ levels)</li>
<li>Long functions (200+ LOC)</li>
</ul>
<p><strong>Design Smells:</strong></p>
<ul>
<li>God functions (high fan-out)</li>
<li>Unused code (fan-in = 0)</li>
<li>Circular dependencies</li>
</ul>
<p><strong>Implementation:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub enum DebtType {
    TestGap { missing_tests: u32 },
    HighComplexity { score: u32 },
    DeepNesting { depth: u32 },
    LongFunction { loc: u32 },
    TooManyParams { count: u32 },
}
<span class="boring">}</span></code></pre></pre>
<h3 id="7-coverage-integration"><a class="header" href="#7-coverage-integration">7. Coverage Integration</a></h3>
<p><strong>Purpose:</strong> Map test coverage data to complexity metrics for risk scoring.</p>
<p><strong>Coverage Data Flow:</strong></p>
<ol>
<li><strong>Read LCOV file</strong> - Parse coverage report from test runners</li>
<li><strong>Map to source</strong> - Match coverage lines to functions/branches</li>
<li><strong>Calculate coverage %</strong> - For each function, compute:
<ul>
<li>Line coverage: % of lines executed</li>
<li>Branch coverage: % of branches taken</li>
</ul>
</li>
<li><strong>Identify gaps</strong> - Find untested branches in complex functions</li>
</ol>
<p><strong>Coverage Scoring:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct CoverageMetrics {
    pub lines_covered: u32,
    pub lines_total: u32,
    pub branches_covered: u32,
    pub branches_total: u32,
    pub coverage_percent: f64,
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Special Cases:</strong></p>
<ul>
<li>Entry points (main, handlers) expect integration test coverage</li>
<li>Generated code excluded from coverage requirements</li>
<li>Test files themselves not analyzed for coverage</li>
</ul>
<h3 id="8-risk-scoring"><a class="header" href="#8-risk-scoring">8. Risk Scoring</a></h3>
<p><strong>Purpose:</strong> Combine complexity and coverage into a unified risk score.</p>
<p><strong>Risk Formula:</strong></p>
<pre><code>Risk Score = (Effective Complexity * Coverage Gap Weight) + (Call Graph Depth * Path Weight)

where:
- Effective Complexity: Entropy-adjusted cyclomatic complexity
- Coverage Gap Weight: 1.0 for 0% coverage, decreasing to 0.1 for 95%+
- Call Graph Depth: Distance from entry points
- Path Weight: Number of untested paths leading to this function
</code></pre>
<p><strong>Example Calculation:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn calculate_risk_score():
  Effective Complexity: 8.5
  Coverage: 30%
  Coverage Gap Weight: 0.7
  Call Graph Depth: 3
  Untested Paths: 2

  Risk = (8.5 * 0.7) + (3 * 2 * 0.3) = 5.95 + 1.8 = 7.75
<span class="boring">}</span></code></pre></pre>
<p><strong>Risk Tiers:</strong></p>
<ul>
<li><strong>Critical (8.0+):</strong> Immediate attention required</li>
<li><strong>High (5.0-7.9):</strong> Priority for next sprint</li>
<li><strong>Moderate (2.0-4.9):</strong> Address when refactoring nearby code</li>
<li><strong>Low (&lt;2.0):</strong> Monitor but no immediate action</li>
</ul>
<h3 id="9-tiered-prioritization"><a class="header" href="#9-tiered-prioritization">9. Tiered Prioritization</a></h3>
<p><strong>Purpose:</strong> Classify and rank technical debt items by urgency and impact.</p>
<p><strong>Prioritization Algorithm:</strong></p>
<ol>
<li><strong>Calculate base risk score</strong> (from Risk Scoring step)</li>
<li><strong>Apply context adjustments:</strong>
<ul>
<li>Entry points: -2.0 score (lower priority for unit tests)</li>
<li>Core business logic: +1.5 score (higher priority)</li>
<li>Frequently changed files: +1.0 score (git history analysis)</li>
<li>Critical paths: +0.5 score per untested caller</li>
</ul>
</li>
<li><strong>Classify into tiers:</strong>
<ul>
<li>Critical: score &gt;= 8.0</li>
<li>High: score &gt;= 5.0</li>
<li>Moderate: score &gt;= 2.0</li>
<li>Low: score &lt; 2.0</li>
</ul>
</li>
<li><strong>Sort within tiers by:</strong>
<ul>
<li>Impact (estimated risk reduction)</li>
<li>Effort (test count or refactoring size)</li>
<li>ROI (impact / effort)</li>
</ul>
</li>
</ol>
<p><strong>Output:</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct PrioritizedDebtItem {
    pub rank: u32,
    pub score: f64,
    pub tier: Tier,
    pub location: Location,
    pub debt_type: DebtType,
    pub action: String,
    pub impact: f64,
    pub effort: Effort,
}
<span class="boring">}</span></code></pre></pre>
<p>See <a href="tiered-prioritization.html">Tiered Prioritization</a> for detailed explanation of the ranking algorithm.</p>
<h3 id="10-output-formatting"><a class="header" href="#10-output-formatting">10. Output Formatting</a></h3>
<p><strong>Purpose:</strong> Present analysis results in user-friendly formats.</p>
<p><strong>Output Formats:</strong></p>
<p><strong>Terminal (default):</strong></p>
<ul>
<li>Color-coded by tier (red=critical, yellow=high, etc.)</li>
<li>Hierarchical tree view with unicode box characters</li>
<li>Collapsible sections for detailed recommendations</li>
<li>Summary statistics at top</li>
</ul>
<p><strong>JSON:</strong></p>
<ul>
<li>Machine-readable for CI/CD integration</li>
<li>Full metadata for each debt item</li>
<li>Structured for programmatic consumption</li>
<li>Schema-versioned for compatibility</li>
</ul>
<p><strong>Markdown:</strong></p>
<ul>
<li>Rendered in GitHub/GitLab for PR comments</li>
<li>Embedded code blocks with syntax highlighting</li>
<li>Collapsible details sections</li>
<li>Linked to source code locations</li>
</ul>
<p><strong>GitHub PR Comments:</strong></p>
<ul>
<li>Automated comments on pull requests</li>
<li>Inline annotations at specific lines</li>
<li>Comparison with base branch (new vs existing debt)</li>
<li>Summary card with key metrics</li>
</ul>
<p>See <a href="output-formats.html">Output Formats</a> for examples and configuration options.</p>
<h2 id="data-flow-example"><a class="header" href="#data-flow-example">Data Flow Example</a></h2>
<p>Letâ€™s trace a single function through the entire pipeline:</p>
<p><strong>Input: Source File</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// src/handlers.rs
pub fn process_request(req: Request) -&gt; Result&lt;Response&gt; {
    validate_auth(&amp;req)?;
    let data = parse_payload(&amp;req.body)?;
    let result = apply_business_logic(data)?;
    format_response(result)
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Stage 1: Parsing</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>FunctionAst {
    name: "process_request",
    location: Location { file: "src/handlers.rs", line: 2 },
    calls: ["validate_auth", "parse_payload", "apply_business_logic", "format_response"],
    ...
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Stage 2: Metric Extraction</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>FunctionMetrics {
    name: "process_request",
    cyclomatic_complexity: 4,  // 3 ?-operators + base
    nesting_depth: 1,
    loc: 5,
    ...
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Stage 3: Entropy Analysis</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Pattern: repetitive ?-operator error handling
Entropy: 0.4 (low variety)
Effective Complexity: 4 * 0.85 = 3.4
<span class="boring">}</span></code></pre></pre>
<p><strong>Stage 4: Call Graph</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>CallGraphNode {
    function: "process_request",
    fan_in: 3,  // called from 3 handlers
    fan_out: 4,  // calls 4 functions
    depth: 1,  // direct handler (entry point)
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Stage 5: Coverage (from LCOV)</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>CoverageMetrics {
    lines_covered: 5,
    lines_total: 5,
    branches_covered: 3,
    branches_total: 4,  // Missing one error path
    coverage_percent: 75%,
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Stage 6: Risk Scoring</strong></p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>Risk = (3.4 * 0.25) + (1 * 1 * 0.2) = 0.85 + 0.2 = 1.05
Tier: LOW (entry point with decent coverage)
<span class="boring">}</span></code></pre></pre>
<p><strong>Stage 7: Recommendation</strong></p>
<pre><code>#23 SCORE: 1.1 [LOW]
â”œâ”€ MINOR GAP: ./src/handlers.rs:2 process_request()
â”œâ”€ ACTION: Add 1 test for error path at line 3
â”œâ”€ IMPACT: -0.3 risk reduction
â””â”€ WHY: Entry point with 75% branch coverage, missing error case
</code></pre>
<h2 id="performance-characteristics-1"><a class="header" href="#performance-characteristics-1">Performance Characteristics</a></h2>
<p><strong>Analysis Speed:</strong></p>
<ul>
<li>Small project (&lt; 10k LOC): 1-3 seconds</li>
<li>Medium project (10-50k LOC): 5-15 seconds</li>
<li>Large project (50-200k LOC): 20-60 seconds</li>
<li>Very large project (200k+ LOC): 1-5 minutes</li>
</ul>
<p><strong>Parallelization:</strong></p>
<ul>
<li>File parsing: Parallel across all available cores</li>
<li>Metric extraction: Parallel per-file</li>
<li>Call graph construction: Sequential (requires cross-file state)</li>
<li>Risk scoring: Parallel per-function</li>
<li>Output formatting: Sequential</li>
</ul>
<p><strong>Memory Usage:</strong></p>
<ul>
<li>Approx 100-200 KB per file analyzed</li>
<li>Peak memory for large projects: 500 MB - 1 GB</li>
<li>Streaming mode available for very large codebases</li>
</ul>
<p><strong>Optimization Strategies:</strong></p>
<ul>
<li>Incremental analysis (cache previous results)</li>
<li>Skip unchanged files (git diff integration)</li>
<li>Parallel processing with rayon</li>
<li>Efficient AST traversal (visitor pattern)</li>
</ul>
<h2 id="extension-points"><a class="header" href="#extension-points">Extension Points</a></h2>
<p><strong>Custom Analyzers:</strong>
Implement the <code>Analyzer</code> trait to add language support:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub trait Analyzer {
    fn parse(&amp;self, content: &amp;str) -&gt; Result&lt;Ast&gt;;
    fn extract_metrics(&amp;self, ast: &amp;Ast) -&gt; Vec&lt;FunctionMetrics&gt;;
    fn detect_patterns(&amp;self, ast: &amp;Ast) -&gt; Vec&lt;DebtPattern&gt;;
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Custom Scoring:</strong>
Implement the <code>RiskScorer</code> trait to adjust scoring logic:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub trait RiskScorer {
    fn calculate_risk(&amp;self, metrics: &amp;FunctionMetrics, coverage: &amp;CoverageMetrics) -&gt; f64;
    fn classify_tier(&amp;self, score: f64) -&gt; Tier;
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Custom Output:</strong>
Implement the <code>OutputFormatter</code> trait for new formats:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub trait OutputFormatter {
    fn format(&amp;self, items: &amp;[PrioritizedDebtItem]) -&gt; Result&lt;String&gt;;
}
<span class="boring">}</span></code></pre></pre>
<h2 id="next-steps-2"><a class="header" href="#next-steps-2">Next Steps</a></h2>
<ul>
<li><strong>Understand prioritization:</strong> See <a href="tiered-prioritization.html">Tiered Prioritization</a></li>
<li><strong>Learn scoring strategies:</strong> See <a href="scoring-strategies.html">Scoring Strategies</a></li>
<li><strong>Configure analysis:</strong> See <a href="configuration.html">Configuration</a></li>
<li><strong>View examples:</strong> See <a href="examples.html">Examples</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="cache-management-1"><a class="header" href="#cache-management-1">Cache Management</a></h1>
<p>Debtmap includes a comprehensive caching system designed to significantly speed up repeated analyses, particularly beneficial for large codebases and CI/CD pipelines. The cache stores parsed ASTs, call graphs, analysis results, and file metrics to avoid redundant computation.</p>
<h2 id="cache-location-and-configuration"><a class="header" href="#cache-location-and-configuration">Cache Location and Configuration</a></h2>
<p>Debtmap uses a platform-specific, XDG-compliant cache directory structure by default. The cache location is determined by the following priority order:</p>
<ol>
<li><strong><code>DEBTMAP_CACHE_DIR</code></strong> environment variable (if set)</li>
<li><strong><code>XDG_CACHE_HOME/debtmap</code></strong> (if XDG_CACHE_HOME is set)</li>
<li><strong>Platform-specific defaults:</strong>
<ul>
<li>macOS: <code>~/Library/Caches/debtmap</code></li>
<li>Linux: <code>~/.cache/debtmap</code></li>
<li>Windows: <code>%LOCALAPPDATA%\debtmap</code></li>
</ul>
</li>
<li><strong>Fallback:</strong> System temporary directory</li>
</ol>
<h3 id="cache-strategy"><a class="header" href="#cache-strategy">Cache Strategy</a></h3>
<p>Debtmap supports two cache storage strategies:</p>
<ul>
<li><strong>Shared (default)</strong>: Stores cache in XDG-compliant shared directory</li>
<li><strong>Custom</strong>: Stores cache in user-specified location via <code>DEBTMAP_CACHE_DIR</code></li>
</ul>
<h3 id="project-identification"><a class="header" href="#project-identification">Project Identification</a></h3>
<p>To ensure cache isolation between different projects, debtmap generates a unique project ID using:</p>
<ol>
<li><strong>Git remote URL hash</strong> (preferred): SHA256 hash (first 16 characters) of the git remote origin URL</li>
<li><strong>Absolute path hash</strong> (fallback): SHA256 hash (first 16 characters) of the projectâ€™s absolute path</li>
</ol>
<p>This ensures that different projects never share cached data, even when analyzed from the same machine.</p>
<h2 id="cache-directory-structure"><a class="header" href="#cache-directory-structure">Cache Directory Structure</a></h2>
<p>The cache directory contains several subdirectories, each serving a specific purpose:</p>
<pre><code>debtmap/
â””â”€â”€ projects/
    â””â”€â”€ &lt;project-id&gt;/
        â”œâ”€â”€ call_graphs/     # Cached call graph data
        â”œâ”€â”€ analysis/        # Cached analysis results
        â”œâ”€â”€ metadata/        # Cache metadata and indices
        â”œâ”€â”€ temp/            # Temporary files during analysis
        â””â”€â”€ file_metrics/    # Cached file-level metrics
</code></pre>
<h2 id="environment-variables-1"><a class="header" href="#environment-variables-1">Environment Variables</a></h2>
<p>Debtmap provides extensive cache configuration through environment variables:</p>
<div class="table-wrapper"><table><thead><tr><th>Variable</th><th>Description</th><th>Default</th><th>Example</th></tr></thead><tbody>
<tr><td><code>DEBTMAP_CACHE_DIR</code></td><td>Custom cache directory path</td><td>Platform-specific</td><td><code>/tmp/my-cache</code></td></tr>
<tr><td><code>DEBTMAP_CACHE_AUTO_PRUNE</code></td><td>Enable automatic cache pruning</td><td><code>true</code></td><td><code>true</code> or <code>false</code></td></tr>
<tr><td><code>DEBTMAP_CACHE_MAX_SIZE</code></td><td>Maximum cache size in bytes</td><td><code>1073741824</code> (1GB)</td><td><code>524288000</code> (500MB)</td></tr>
<tr><td><code>DEBTMAP_CACHE_MAX_AGE_DAYS</code></td><td>Maximum age for cache entries</td><td><code>30</code></td><td><code>7</code></td></tr>
<tr><td><code>DEBTMAP_CACHE_MAX_ENTRIES</code></td><td>Maximum number of cache entries</td><td><code>10000</code></td><td><code>5000</code></td></tr>
<tr><td><code>DEBTMAP_CACHE_PRUNE_PERCENTAGE</code></td><td>Percentage to remove when pruning (0.0-1.0)</td><td><code>0.25</code> (25%)</td><td><code>0.3</code> (30%)</td></tr>
<tr><td><code>DEBTMAP_CACHE_STRATEGY</code></td><td>Pruning strategy (lru, lfu, fifo, age)</td><td><code>lru</code></td><td><code>lfu</code></td></tr>
<tr><td><code>DEBTMAP_CACHE_SYNC_PRUNE</code></td><td>Use synchronous pruning (blocks)</td><td><code>false</code></td><td><code>true</code></td></tr>
<tr><td><code>DEBTMAP_CACHE_SCOPE</code></td><td>Branch-specific cache scope</td><td>None</td><td><code>feature-branch</code></td></tr>
</tbody></table>
</div>
<h3 id="configuration-examples"><a class="header" href="#configuration-examples">Configuration Examples</a></h3>
<pre><code class="language-bash"># Use a custom cache location
export DEBTMAP_CACHE_DIR=/mnt/fast-ssd/debtmap-cache

# Configure cache limits for CI environment
export DEBTMAP_CACHE_MAX_SIZE=524288000  # 500MB
export DEBTMAP_CACHE_MAX_AGE_DAYS=7      # 1 week
export DEBTMAP_CACHE_STRATEGY=lru

# Disable auto-pruning (manual control)
export DEBTMAP_CACHE_AUTO_PRUNE=false

# Branch-specific caching
export DEBTMAP_CACHE_SCOPE="$(git branch --show-current)"
</code></pre>
<h2 id="command-line-options"><a class="header" href="#command-line-options">Command Line Options</a></h2>
<p>Debtmap provides several CLI flags for cache management:</p>
<div class="table-wrapper"><table><thead><tr><th>Option</th><th>Description</th></tr></thead><tbody>
<tr><td><code>--no-cache</code></td><td>Disable caching for this run (caching is enabled by default)</td></tr>
<tr><td><code>--clear-cache</code></td><td>Clear cache before running analysis</td></tr>
<tr><td><code>--force-cache-rebuild</code></td><td>Force cache rebuild (same as <code>--clear-cache</code>)</td></tr>
<tr><td><code>--cache-stats</code></td><td>Show cache statistics and location</td></tr>
<tr><td><code>--migrate-cache</code></td><td>Migrate cache from local to shared location</td></tr>
<tr><td><code>--cache-location &lt;path&gt;</code></td><td>Specify custom cache location for this run</td></tr>
</tbody></table>
</div>
<h3 id="cli-examples"><a class="header" href="#cli-examples">CLI Examples</a></h3>
<pre><code class="language-bash"># Run analysis without using cache
debtmap --no-cache

# Clear cache and rebuild from scratch
debtmap --clear-cache

# View cache statistics
debtmap --cache-stats

# Use custom cache location for this run
debtmap --cache-location /tmp/temp-cache

# Migrate existing cache to shared location
debtmap --migrate-cache
</code></pre>
<h2 id="automatic-pruning-strategies"><a class="header" href="#automatic-pruning-strategies">Automatic Pruning Strategies</a></h2>
<p>Debtmap automatically prunes the cache when configured limits are exceeded. Four pruning strategies are available:</p>
<h3 id="lru-least-recently-used---default"><a class="header" href="#lru-least-recently-used---default">LRU (Least Recently Used) - Default</a></h3>
<p>Removes entries that havenâ€™t been accessed recently. Best for general-purpose usage where recently analyzed code is more likely to be analyzed again.</p>
<p><strong>When to use:</strong> Default choice for most development workflows and CI pipelines.</p>
<pre><code class="language-bash">export DEBTMAP_CACHE_STRATEGY=lru
</code></pre>
<h3 id="lfu-least-frequently-used"><a class="header" href="#lfu-least-frequently-used">LFU (Least Frequently Used)</a></h3>
<p>Removes entries with the lowest access count. Best when certain files are analyzed repeatedly while others are analyzed infrequently.</p>
<p><strong>When to use:</strong> Projects with stable core modules that are analyzed frequently and peripheral code that changes rarely.</p>
<pre><code class="language-bash">export DEBTMAP_CACHE_STRATEGY=lfu
</code></pre>
<h3 id="fifo-first-in-first-out"><a class="header" href="#fifo-first-in-first-out">FIFO (First In First Out)</a></h3>
<p>Removes the oldest entries by creation time. Simpler strategy that doesnâ€™t consider access patterns.</p>
<p><strong>When to use:</strong> When you want predictable cache behavior or are testing cache performance.</p>
<pre><code class="language-bash">export DEBTMAP_CACHE_STRATEGY=fifo
</code></pre>
<h3 id="age-based-only"><a class="header" href="#age-based-only">Age-Based Only</a></h3>
<p>Only removes entries older than <code>DEBTMAP_CACHE_MAX_AGE_DAYS</code>. Does not prune based on size or entry count limits.</p>
<p><strong>When to use:</strong> When disk space is not a concern but you want to ensure cache freshness.</p>
<pre><code class="language-bash">export DEBTMAP_CACHE_STRATEGY=age
</code></pre>
<h2 id="default-configuration"><a class="header" href="#default-configuration">Default Configuration</a></h2>
<p>When no environment variables are set, debtmap uses the following defaults:</p>
<ul>
<li><strong>Max size:</strong> 1GB (1,073,741,824 bytes)</li>
<li><strong>Max age:</strong> 30 days</li>
<li><strong>Max entries:</strong> 10,000 entries</li>
<li><strong>Prune percentage:</strong> 25% (removes 25% of entries when limit is hit)</li>
<li><strong>Strategy:</strong> LRU (Least Recently Used)</li>
<li><strong>Auto-prune:</strong> Enabled</li>
</ul>
<h3 id="pruning-triggers"><a class="header" href="#pruning-triggers">Pruning Triggers</a></h3>
<p>Automatic pruning is triggered when:</p>
<ol>
<li><strong>Cache size exceeds max_size_bytes</strong> - Immediate pruning</li>
<li><strong>Entry count exceeds max_entries</strong> - Immediate pruning</li>
<li><strong>Entries older than max_age_days exist</strong> - Periodic pruning (checked daily)</li>
</ol>
<p>When pruning is triggered, debtmap removes enough entries to bring the cache below the configured limits, plus an additional buffer (based on prune_percentage) to avoid frequent pruning.</p>
<h2 id="performance-optimization-1"><a class="header" href="#performance-optimization-1">Performance Optimization</a></h2>
<h3 id="cache-benefits"><a class="header" href="#cache-benefits">Cache Benefits</a></h3>
<p>The cache system provides significant performance improvements:</p>
<ul>
<li><strong>Parsed ASTs:</strong> Avoid re-parsing source files that havenâ€™t changed</li>
<li><strong>Call graphs:</strong> Reuse expensive call graph computation</li>
<li><strong>Analysis results:</strong> Skip redundant metric calculations</li>
<li><strong>File metrics:</strong> Cache file-level complexity scores</li>
</ul>
<p><strong>Performance impact:</strong> Cache hits can reduce analysis time by 50-90% for large codebases, depending on the number of changed files.</p>
<h3 id="best-practices-for-ci-environments"><a class="header" href="#best-practices-for-ci-environments">Best Practices for CI Environments</a></h3>
<pre><code class="language-bash"># Example CI configuration for fast builds
export DEBTMAP_CACHE_DIR=/ci-cache/debtmap
export DEBTMAP_CACHE_MAX_SIZE=2147483648  # 2GB for CI
export DEBTMAP_CACHE_MAX_AGE_DAYS=14      # 2 weeks
export DEBTMAP_CACHE_STRATEGY=lru

# Run analysis with cache
debtmap --cache-stats  # Show cache hit rate
</code></pre>
<h3 id="background-vs-synchronous-pruning"><a class="header" href="#background-vs-synchronous-pruning">Background vs Synchronous Pruning</a></h3>
<p>By default, cache pruning runs in a background thread, allowing analysis to continue without waiting for cleanup. This is optimal for development and CI environments.</p>
<p>For testing or when you need deterministic behavior, use synchronous pruning:</p>
<pre><code class="language-bash">export DEBTMAP_CACHE_SYNC_PRUNE=true
</code></pre>
<p><strong>Synchronous pruning:</strong> Blocks during cleanup, ensuring cache is pruned before analysis continues. Used automatically in test environments.</p>
<p><strong>Background pruning (default):</strong> Spawns a separate thread for non-blocking cleanup. Analysis proceeds immediately while cleanup happens in parallel.</p>
<h2 id="troubleshooting-cache-issues"><a class="header" href="#troubleshooting-cache-issues">Troubleshooting Cache Issues</a></h2>
<h3 id="cache-taking-too-much-disk-space"><a class="header" href="#cache-taking-too-much-disk-space">Cache Taking Too Much Disk Space</a></h3>
<p><strong>Problem:</strong> Cache directory is consuming excessive disk space.</p>
<p><strong>Solutions:</strong></p>
<pre><code class="language-bash"># Option 1: Reduce max cache size
export DEBTMAP_CACHE_MAX_SIZE=524288000  # 500MB

# Option 2: Clear cache manually
debtmap --clear-cache

# Option 3: Reduce max age
export DEBTMAP_CACHE_MAX_AGE_DAYS=7

# Option 4: Inspect current cache usage
debtmap --cache-stats
</code></pre>
<h3 id="stale-cache-causing-incorrect-results"><a class="header" href="#stale-cache-causing-incorrect-results">Stale Cache Causing Incorrect Results</a></h3>
<p><strong>Problem:</strong> Cache contains outdated data, causing analysis to report incorrect results.</p>
<p><strong>Solutions:</strong></p>
<pre><code class="language-bash"># Force cache rebuild
debtmap --force-cache-rebuild

# Or disable cache for this run
debtmap --no-cache
</code></pre>
<h3 id="permission-errors"><a class="header" href="#permission-errors">Permission Errors</a></h3>
<p><strong>Problem:</strong> Cannot write to cache directory.</p>
<p><strong>Solutions:</strong></p>
<pre><code class="language-bash"># Use a custom cache location with proper permissions
export DEBTMAP_CACHE_DIR=$HOME/.local/cache/debtmap

# Or check permissions on the default cache directory
ls -la $(debtmap --cache-stats | grep "Cache location")
</code></pre>
<h3 id="inspecting-cache-statistics"><a class="header" href="#inspecting-cache-statistics">Inspecting Cache Statistics</a></h3>
<p>Use <code>--cache-stats</code> to inspect cache health:</p>
<pre><code class="language-bash">debtmap --cache-stats
</code></pre>
<p>This displays:</p>
<ul>
<li>Cache location path</li>
<li>Total cache size</li>
<li>Number of cached entries</li>
<li>Cache hit rate (if available)</li>
<li>Last pruning timestamp</li>
</ul>
<h3 id="debug-cache-issues"><a class="header" href="#debug-cache-issues">Debug Cache Issues</a></h3>
<p>To debug cache-related issues, check:</p>
<ol>
<li>
<p><strong>Cache directory exists and is writable:</strong></p>
<pre><code class="language-bash">ls -la ~/.cache/debtmap  # Linux
ls -la ~/Library/Caches/debtmap  # macOS
</code></pre>
</li>
<li>
<p><strong>Environment variables are set correctly:</strong></p>
<pre><code class="language-bash">env | grep DEBTMAP_CACHE
</code></pre>
</li>
<li>
<p><strong>Project ID generation:</strong>
Cache keys are based on project ID. Verify your project has a stable git remote or path.</p>
</li>
<li>
<p><strong>File timestamps:</strong>
Cache invalidation relies on file modification times. Ensure your build system doesnâ€™t modify timestamps unexpectedly.</p>
</li>
</ol>
<h2 id="cache-migration"><a class="header" href="#cache-migration">Cache Migration</a></h2>
<p>If you previously used a local cache strategy and want to migrate to the shared XDG-compliant location:</p>
<pre><code class="language-bash">debtmap --migrate-cache
</code></pre>
<p>This command:</p>
<ol>
<li>Identifies the old cache location</li>
<li>Creates the new shared cache directory</li>
<li>Copies cache data preserving metadata</li>
<li>Verifies the migration was successful</li>
</ol>
<p>After migration, you can safely delete the old cache directory.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="context-providers"><a class="header" href="#context-providers">Context Providers</a></h1>
<p>Context providers enhance debtmapâ€™s risk analysis by incorporating additional factors beyond complexity and test coverage. They analyze critical execution paths, dependency relationships, and version control history to provide a more comprehensive understanding of technical risk.</p>
<h2 id="overview-1"><a class="header" href="#overview-1">Overview</a></h2>
<p>Context providers implement the <code>ContextProvider</code> trait, which gathers risk-relevant information about functions and modules. Each provider analyzes a specific dimension of risk:</p>
<ul>
<li><strong>Critical Path Provider</strong>: Identifies functions on critical execution paths</li>
<li><strong>Dependency Provider</strong>: Analyzes call graph relationships and blast radius</li>
<li><strong>Git History Provider</strong>: Integrates version control history for change patterns</li>
</ul>
<p>Context providers help debtmap understand:</p>
<ul>
<li>Which code paths are most critical</li>
<li>How functions depend on each other</li>
<li>Which code changes most frequently</li>
<li>Where bugs are likely to occur</li>
</ul>
<p>This context-aware analysis improves prioritization accuracy and reduces false positives.</p>
<p>The <code>ContextAggregator</code> combines context from multiple enabled providers and adjusts risk scores using the formula:</p>
<pre><code>contextual_risk = base_risk Ã— (1.0 + context_contribution)
</code></pre>
<p>Where <code>context_contribution</code> is the weighted sum of all provider contributions:</p>
<pre><code>context_contribution = Î£(provider.contribution Ã— provider.weight)
</code></pre>
<h2 id="critical-path-provider"><a class="header" href="#critical-path-provider">Critical Path Provider</a></h2>
<p>The Critical Path provider identifies functions that lie on critical execution paths through your application. Functions on these paths have elevated risk because failures directly impact user-facing functionality.</p>
<h3 id="entry-point-detection"><a class="header" href="#entry-point-detection">Entry Point Detection</a></h3>
<p>The provider automatically detects entry points based on function names and file paths:</p>
<div class="table-wrapper"><table><thead><tr><th>Entry Type</th><th>Weight</th><th>Detection Pattern</th><th>User-Facing</th></tr></thead><tbody>
<tr><td>Main</td><td>10.0</td><td>Function named <code>main</code></td><td>Yes</td></tr>
<tr><td>API Endpoint</td><td>8.0</td><td><code>handle_*</code>, <code>*_handler</code>, <code>get_*</code>, <code>post_*</code> in <code>api/</code>, <code>handler/</code>, <code>route/</code> paths</td><td>Yes</td></tr>
<tr><td>CLI Command</td><td>7.0</td><td><code>cmd_*</code>, <code>command_*</code>, <code>*_command</code> in <code>cli/</code>, <code>command/</code> paths</td><td>Yes</td></tr>
<tr><td>Web Handler</td><td>7.0</td><td>Functions with <code>route</code>, <code>handler</code> in <code>web/</code>, <code>http/</code> paths</td><td>Yes</td></tr>
<tr><td>Event Handler</td><td>5.0</td><td><code>on_*</code>, <code>*_listener</code>, contains <code>event</code></td><td>No</td></tr>
<tr><td>Test Entry</td><td>2.0</td><td><code>test_*</code>, in <code>test/</code> paths</td><td>No</td></tr>
</tbody></table>
</div>
<p><strong>What it detects:</strong></p>
<ul>
<li>Entry points (main functions, CLI handlers, API endpoints)</li>
<li>Error handling paths</li>
<li>Data processing pipelines</li>
<li>Resource initialization</li>
</ul>
<h3 id="path-weighting"><a class="header" href="#path-weighting">Path Weighting</a></h3>
<p>Functions on critical paths receive contribution scores based on:</p>
<ul>
<li><strong>Path weight</strong>: The maximum entry point weight leading to the function</li>
<li><strong>User-facing flag</strong>: Doubles contribution for user-facing paths</li>
</ul>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Example: Function on main entry path (weight 10.0, user-facing)
contribution = (10.0 / 10.0) Ã— 2.0 = 2.0

// Example: Function on event handler path (weight 5.0, non-user-facing)
contribution = (5.0 / 10.0) Ã— 1.0 = 0.5
<span class="boring">}</span></code></pre></pre>
<p><strong>Impact on scoring:</strong></p>
<ul>
<li>Functions on critical paths get higher priority</li>
<li>Entry point multiplier: 1.5x</li>
<li>Business logic multiplier: 1.2x</li>
</ul>
<h3 id="use-cases"><a class="header" href="#use-cases">Use Cases</a></h3>
<ul>
<li><strong>API prioritization</strong>: Identify critical endpoints that need careful review</li>
<li><strong>Refactoring safety</strong>: Avoid breaking user-facing execution paths</li>
<li><strong>Test coverage</strong>: Ensure critical paths have adequate test coverage</li>
</ul>
<h3 id="enable"><a class="header" href="#enable">Enable</a></h3>
<pre><code class="language-bash">debtmap analyze . --context-providers critical_path
</code></pre>
<p><strong>Configuration:</strong></p>
<pre><code class="language-toml">[analysis]
context_providers = ["critical_path"]

[context.critical_path]
# Multiplier for entry points (default: 1.5)
entry_point_multiplier = 1.5

# Multiplier for business logic (default: 1.2)
business_logic_multiplier = 1.2
</code></pre>
<h2 id="dependency-provider"><a class="header" href="#dependency-provider">Dependency Provider</a></h2>
<p>The Dependency provider analyzes call graph relationships to identify functions with high architectural impact. It calculates how changes propagate through the dependency graph and determines the blast radius of modifications.</p>
<h3 id="dependency-chain-analysis"><a class="header" href="#dependency-chain-analysis">Dependency Chain Analysis</a></h3>
<p>The provider builds a dependency graph where:</p>
<ul>
<li><strong>Modules</strong> contain functions and have intrinsic risk scores</li>
<li><strong>Edges</strong> represent dependencies with coupling strength (0.0-1.0)</li>
<li><strong>Risk propagation</strong> flows through dependencies using iterative refinement</li>
</ul>
<p><strong>What it detects:</strong></p>
<ul>
<li>Upstream dependencies (functions this function calls)</li>
<li>Downstream dependencies (functions that call this function)</li>
<li>Transitive dependencies through the call graph</li>
<li>Dependency criticality</li>
</ul>
<h3 id="blast-radius-calculation"><a class="header" href="#blast-radius-calculation">Blast Radius Calculation</a></h3>
<p>The blast radius represents how many modules would be affected by changes to a function:</p>
<div class="table-wrapper"><table><thead><tr><th>Blast Radius</th><th>Contribution</th><th>Impact Level</th></tr></thead><tbody>
<tr><td>&gt; 10 modules</td><td>1.5</td><td>Critical dependency affecting many modules</td></tr>
<tr><td>6-10 modules</td><td>1.0</td><td>Important dependency with moderate impact</td></tr>
<tr><td>3-5 modules</td><td>0.5</td><td>Limited dependency impact</td></tr>
<tr><td>â‰¤ 2 modules</td><td>0.2</td><td>Minimal or isolated component</td></tr>
</tbody></table>
</div>
<h3 id="risk-propagation-formula"><a class="header" href="#risk-propagation-formula">Risk Propagation Formula</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>propagated_risk = base_risk Ã— criticality_factor + dependency_risk

where:
  criticality_factor = 1.0 + min(0.5, dependents.len() Ã— 0.1)
  dependency_risk = Î£(dependency.risk Ã— coupling_strength Ã— 0.3)
<span class="boring">}</span></code></pre></pre>
<p><strong>Impact on scoring:</strong></p>
<pre><code>dependency_factor = normalized_to_0_10(upstream + downstream)

Ranges:
- Entry points: 8-10 (critical path)
- Business logic: 6-8 (core functionality)
- Data access: 5-7 (important but stable)
- Utilities: 3-5 (lower priority)
- Test helpers: 1-3 (lowest priority)
</code></pre>
<h3 id="use-cases-1"><a class="header" href="#use-cases-1">Use Cases</a></h3>
<ul>
<li><strong>Architectural refactoring</strong>: Identify high-impact modules to refactor carefully</li>
<li><strong>Change impact analysis</strong>: Understand downstream effects of modifications</li>
<li><strong>Module decoupling</strong>: Find tightly coupled modules with high blast radius</li>
</ul>
<h3 id="enable-1"><a class="header" href="#enable-1">Enable</a></h3>
<pre><code class="language-bash">debtmap analyze . --context-providers dependency
</code></pre>
<p><strong>Configuration:</strong></p>
<pre><code class="language-toml">[analysis]
context_providers = ["dependency"]

[context.dependency]
# Include transitive dependencies (default: true)
include_transitive = true

# Maximum depth for transitive analysis (default: 5)
max_depth = 5
</code></pre>
<h2 id="git-history-provider"><a class="header" href="#git-history-provider">Git History Provider</a></h2>
<p>The Git History provider integrates version control data to detect change-prone code and bug patterns. Files with frequent changes and bug fixes indicate higher maintenance risk.</p>
<h3 id="metrics-collected"><a class="header" href="#metrics-collected">Metrics Collected</a></h3>
<p>The provider analyzes Git history to calculate:</p>
<ul>
<li><strong>Change frequency</strong>: Commits per month (recent activity indicator)</li>
<li><strong>Bug density</strong>: Ratio of bug fix commits to total commits</li>
<li><strong>Age</strong>: Days since first commit (maturity indicator)</li>
<li><strong>Author count</strong>: Number of unique contributors (complexity indicator)</li>
</ul>
<p><strong>What it analyzes:</strong></p>
<ul>
<li>Commit frequency per file/function</li>
<li>Bug fix patterns (commits with â€œfixâ€ in message)</li>
<li>Code churn (lines added/removed)</li>
<li>Recent activity</li>
</ul>
<h3 id="risk-classification"><a class="header" href="#risk-classification">Risk Classification</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Category</th><th>Conditions</th><th>Contribution</th><th>Explanation</th></tr></thead><tbody>
<tr><td>Very unstable</td><td>freq &gt; 5.0 AND bug_density &gt; 0.3</td><td>2.0</td><td>High churn with many bug fixes</td></tr>
<tr><td>Moderately unstable</td><td>freq &gt; 2.0 OR bug_density &gt; 0.2</td><td>1.0</td><td>Frequent changes or bug-prone</td></tr>
<tr><td>Slightly unstable</td><td>freq &gt; 1.0 OR bug_density &gt; 0.1</td><td>0.5</td><td>Some instability</td></tr>
<tr><td>Stable</td><td>freq â‰¤ 1.0 AND bug_density â‰¤ 0.1</td><td>0.1</td><td>Low change rate, few bugs</td></tr>
</tbody></table>
</div>
<h3 id="bug-fix-detection"><a class="header" href="#bug-fix-detection">Bug Fix Detection</a></h3>
<p>The provider identifies bug fixes by searching commit messages for patterns:</p>
<pre><code class="language-bash">git log --grep=fix --grep=bug --grep=Fix --grep=Bug -- &lt;file&gt;
</code></pre>
<h3 id="stability-score"><a class="header" href="#stability-score">Stability Score</a></h3>
<p>Stability is calculated using weighted factors:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>stability = (churn_factor Ã— 0.4) + (bug_factor Ã— 0.4) + (age_factor Ã— 0.2)

where:
  churn_factor = 1.0 / (1.0 + monthly_churn)
  bug_factor = 1.0 - (bug_fixes / total_commits)
  age_factor = min(1.0, age_days / 365.0)
<span class="boring">}</span></code></pre></pre>
<p><strong>Impact on scoring:</strong></p>
<ul>
<li>High-churn functions get higher priority</li>
<li>Recently fixed bugs indicate risk areas</li>
<li>Stable code (no recent changes) gets lower priority</li>
</ul>
<h3 id="use-cases-2"><a class="header" href="#use-cases-2">Use Cases</a></h3>
<ul>
<li><strong>Find change-prone code</strong>: Identify files that change frequently and need attention</li>
<li><strong>Detect bug hotspots</strong>: Locate areas with high bug fix rates</li>
<li><strong>Prioritize refactoring</strong>: Target unstable code for improvement</li>
<li><strong>Team collaboration patterns</strong>: Files touched by many authors may need better documentation</li>
</ul>
<h3 id="enable-2"><a class="header" href="#enable-2">Enable</a></h3>
<pre><code class="language-bash">debtmap analyze . --context-providers git_history
</code></pre>
<p><strong>Configuration:</strong></p>
<pre><code class="language-toml">[analysis]
context_providers = ["git_history"]

[context.git_history]
# Commits to analyze (default: 100)
max_commits = 100

# Time range in days (default: 90)
time_range_days = 90

# Minimum commits to consider "high churn" (default: 10)
high_churn_threshold = 10
</code></pre>
<h3 id="troubleshooting-4"><a class="header" href="#troubleshooting-4">Troubleshooting</a></h3>
<p><strong>Git repository not found</strong>: The provider requires a Git repository. If analysis fails:</p>
<pre><code class="language-bash"># Verify you're in a git repository
git rev-parse --git-dir

# If not a git repo, initialize one or disable git_history provider
debtmap analyze --disable-context git_history
</code></pre>
<p><strong>Performance issues</strong>: Git history analysis can be slow for large repositories:</p>
<pre><code class="language-bash"># Use only lightweight providers
debtmap analyze --context-providers critical_path,dependency
</code></pre>
<h2 id="enabling-context-providers"><a class="header" href="#enabling-context-providers">Enabling Context Providers</a></h2>
<p>Context-aware analysis is disabled by default. Enable it using CLI flags:</p>
<h3 id="enable-all-providers"><a class="header" href="#enable-all-providers">Enable All Providers</a></h3>
<pre><code class="language-bash"># Enable all available context providers
debtmap analyze --context
# or
debtmap analyze --enable-context
</code></pre>
<h3 id="enable-specific-providers"><a class="header" href="#enable-specific-providers">Enable Specific Providers</a></h3>
<pre><code class="language-bash"># Enable only critical_path and dependency
debtmap analyze --context-providers critical_path,dependency

# Enable only git_history
debtmap analyze --context-providers git_history

# Enable all three explicitly
debtmap analyze --context-providers critical_path,dependency,git_history
</code></pre>
<h3 id="disable-specific-providers"><a class="header" href="#disable-specific-providers">Disable Specific Providers</a></h3>
<pre><code class="language-bash"># Enable context but disable git_history (useful for non-git repos)
debtmap analyze --context --disable-context git_history

# Enable context but disable dependency analysis
debtmap analyze --context --disable-context dependency
</code></pre>
<h3 id="enabling-multiple-providers"><a class="header" href="#enabling-multiple-providers">Enabling Multiple Providers</a></h3>
<p>Combine providers for comprehensive analysis:</p>
<pre><code class="language-bash">debtmap analyze . --context-providers critical_path,dependency,git_history
</code></pre>
<p>Or via config:</p>
<pre><code class="language-toml">[analysis]
context_providers = ["critical_path", "dependency", "git_history"]
</code></pre>
<h2 id="provider-weights"><a class="header" href="#provider-weights">Provider Weights</a></h2>
<p>Each provider has a weight that determines its influence on the final risk score:</p>
<div class="table-wrapper"><table><thead><tr><th>Provider</th><th>Weight</th><th>Rationale</th></tr></thead><tbody>
<tr><td>critical_path</td><td>1.5</td><td>Critical paths have high impact on users</td></tr>
<tr><td>dependency_risk</td><td>1.2</td><td>Architectural dependencies affect many modules</td></tr>
<tr><td>git_history</td><td>1.0</td><td>Historical patterns indicate future risk</td></tr>
</tbody></table>
</div>
<p>The total context contribution is calculated as:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>total_contribution = Î£(contribution_i Ã— weight_i)

Example with all providers:
  critical_path: 2.0 Ã— 1.5 = 3.0
  dependency:    1.0 Ã— 1.2 = 1.2
  git_history:   0.5 Ã— 1.0 = 0.5
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  total_contribution = 4.7

contextual_risk = base_risk Ã— (1.0 + 4.7) = base_risk Ã— 5.7
<span class="boring">}</span></code></pre></pre>
<h2 id="how-context-affects-scoring"><a class="header" href="#how-context-affects-scoring">How Context Affects Scoring</a></h2>
<h3 id="base-scoring-no-context"><a class="header" href="#base-scoring-no-context">Base Scoring (No Context)</a></h3>
<pre><code>Base Score = (Complexity Ã— 0.40) + (Coverage Ã— 0.40) + (Dependency Ã— 0.20)
</code></pre>
<h3 id="with-context-providers"><a class="header" href="#with-context-providers">With Context Providers</a></h3>
<pre><code>Context-Adjusted Score = Base Score Ã— Role Multiplier Ã— Churn Multiplier

Role Multiplier (from critical path &amp; dependency analysis):
- Entry points: 1.5x
- Business logic: 1.2x
- Data access: 1.0x
- Infrastructure: 0.8x
- Utilities: 0.5x
- Test code: 0.1x

Churn Multiplier (from git history):
- High churn (10+ commits/month): 1.3x
- Medium churn (5-10 commits/month): 1.1x
- Low churn (1-5 commits/month): 1.0x
- Stable (0 commits/6 months): 0.8x
</code></pre>
<h2 id="context-details-structure"><a class="header" href="#context-details-structure">Context Details Structure</a></h2>
<p>When using <code>--format json</code>, context information is included in the output. The <code>ContextDetails</code> enum contains provider-specific data:</p>
<h3 id="criticalpath"><a class="header" href="#criticalpath">CriticalPath</a></h3>
<pre><code class="language-json">{
  "provider": "critical_path",
  "weight": 1.5,
  "contribution": 2.0,
  "details": {
    "CriticalPath": {
      "entry_points": ["main (Main)", "handle_request (ApiEndpoint)"],
      "path_weight": 10.0,
      "is_user_facing": true
    }
  }
}
</code></pre>
<h3 id="dependencychain"><a class="header" href="#dependencychain">DependencyChain</a></h3>
<pre><code class="language-json">{
  "provider": "dependency_risk",
  "weight": 1.2,
  "contribution": 1.5,
  "details": {
    "DependencyChain": {
      "depth": 3,
      "propagated_risk": 8.5,
      "dependents": ["module_a", "module_b", "module_c"],
      "blast_radius": 12
    }
  }
}
</code></pre>
<h3 id="historical"><a class="header" href="#historical">Historical</a></h3>
<pre><code class="language-json">{
  "provider": "git_history",
  "weight": 1.0,
  "contribution": 1.0,
  "details": {
    "Historical": {
      "change_frequency": 3.5,
      "bug_density": 0.25,
      "age_days": 180,
      "author_count": 5
    }
  }
}
</code></pre>
<h2 id="practical-examples"><a class="header" href="#practical-examples">Practical Examples</a></h2>
<h3 id="example-1-entry-point-vs-utility"><a class="header" href="#example-1-entry-point-vs-utility">Example 1: Entry Point vs Utility</a></h3>
<p><strong>Without context providers:</strong></p>
<pre><code>Function: main() - Entry point
Complexity: 8
Coverage: 50%
Score: 6.0 [MEDIUM]

Function: format_string() - Utility
Complexity: 8
Coverage: 50%
Score: 6.0 [MEDIUM]
</code></pre>
<p>Both functions have the same score.</p>
<p><strong>With context providers:</strong></p>
<pre><code>Function: main() - Entry point
Complexity: 8
Coverage: 50%
Base Score: 6.0
Role Multiplier: 1.5x (entry point)
Final Score: 9.0 [CRITICAL]

Function: format_string() - Utility
Complexity: 8
Coverage: 50%
Base Score: 6.0
Role Multiplier: 0.5x (utility)
Final Score: 3.0 [LOW]
</code></pre>
<p>Entry point is prioritized over utility.</p>
<h3 id="example-2-high-churn-function"><a class="header" href="#example-2-high-churn-function">Example 2: High-Churn Function</a></h3>
<p><strong>Without git history:</strong></p>
<pre><code>Function: process_payment()
Complexity: 12
Coverage: 60%
Score: 7.5 [HIGH]
</code></pre>
<p><strong>With git history:</strong></p>
<pre><code>Function: process_payment()
Complexity: 12
Coverage: 60%
Base Score: 7.5
Churn: 15 commits in last month (bug fixes)
Churn Multiplier: 1.3x
Final Score: 9.75 [CRITICAL]
</code></pre>
<p>High-churn function is elevated to critical.</p>
<h3 id="example-3-stable-well-tested-code"><a class="header" href="#example-3-stable-well-tested-code">Example 3: Stable Well-Tested Code</a></h3>
<p><strong>Without context:</strong></p>
<pre><code>Function: legacy_parser()
Complexity: 15
Coverage: 95%
Score: 3.5 [LOW]
</code></pre>
<p><strong>With context:</strong></p>
<pre><code>Function: legacy_parser()
Complexity: 15
Coverage: 95%
Base Score: 3.5
Churn: 0 commits in last 2 years
Churn Multiplier: 0.8x
Role: Data access (stable)
Role Multiplier: 1.0x
Final Score: 2.8 [LOW]
</code></pre>
<p>Stable, well-tested code gets even lower priority.</p>
<h3 id="example-4-api-endpoint-prioritization"><a class="header" href="#example-4-api-endpoint-prioritization">Example 4: API Endpoint Prioritization</a></h3>
<p>Analyze a web service to identify critical API endpoints:</p>
<pre><code class="language-bash">debtmap analyze --context-providers critical_path --format json
</code></pre>
<p>Functions on API endpoint paths will receive elevated risk scores. Use this to prioritize code review and testing for user-facing functionality.</p>
<h3 id="example-5-finding-change-prone-code"><a class="header" href="#example-5-finding-change-prone-code">Example 5: Finding Change-Prone Code</a></h3>
<p>Identify files with high change frequency and bug fixes:</p>
<pre><code class="language-bash">debtmap analyze --context-providers git_history --top 20
</code></pre>
<p>This highlights unstable areas of the codebase that may benefit from refactoring or increased test coverage.</p>
<h3 id="example-6-architectural-impact-analysis"><a class="header" href="#example-6-architectural-impact-analysis">Example 6: Architectural Impact Analysis</a></h3>
<p>Find high-impact modules with large blast radius:</p>
<pre><code class="language-bash">debtmap analyze --context-providers dependency --format json | \
  jq '.[] | select(.blast_radius &gt; 10)'
</code></pre>
<p>Use this to identify architectural choke points that require careful change management.</p>
<h3 id="example-7-comprehensive-risk-assessment"><a class="header" href="#example-7-comprehensive-risk-assessment">Example 7: Comprehensive Risk Assessment</a></h3>
<p>Combine all providers for holistic risk analysis:</p>
<pre><code class="language-bash">debtmap analyze --context -v
</code></pre>
<p>The verbose output shows how each provider contributes to the final risk score:</p>
<pre><code>function: process_payment
  base_risk: 5.0
  critical_path: +3.0 (on main path, user-facing)
  dependency: +1.2 (12 dependent modules)
  git_history: +1.0 (3.5 changes/month, 0.25 bug density)
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  contextual_risk: 26.0
</code></pre>
<h2 id="configuration-2"><a class="header" href="#configuration-2">Configuration</a></h2>
<p>Configure context providers in <code>.debtmap.toml</code>:</p>
<pre><code class="language-toml">[analysis]
# Enable context-aware analysis (default: true)
enable_context = true

# Specify which providers to use
context_providers = ["critical_path", "dependency", "git_history"]

# Disable specific providers
# disable_context = ["git_history"]

[context.git_history]
# Commits to analyze (default: 100)
max_commits = 100

# Time range in days (default: 90)
time_range_days = 90

# Minimum commits to consider "high churn" (default: 10)
high_churn_threshold = 10

[context.critical_path]
# Multiplier for entry points (default: 1.5)
entry_point_multiplier = 1.5

# Multiplier for business logic (default: 1.2)
business_logic_multiplier = 1.2

[context.dependency]
# Include transitive dependencies (default: true)
include_transitive = true

# Maximum depth for transitive analysis (default: 5)
max_depth = 5
</code></pre>
<h2 id="performance-considerations-1"><a class="header" href="#performance-considerations-1">Performance Considerations</a></h2>
<p>Context providers add computational overhead to analysis:</p>
<p><strong>Impact on analysis time:</strong></p>
<ul>
<li>Critical path: +10-15% (fast - call graph traversal)</li>
<li>Dependency: +20-30% (moderate - iterative risk propagation)</li>
<li>Git history: +30-50% (slow for large repos - multiple git commands per file)</li>
</ul>
<p><strong>Combined overhead:</strong> ~60-80% increase in analysis time</p>
<h3 id="optimization-tips"><a class="header" href="#optimization-tips">Optimization Tips</a></h3>
<ol>
<li><strong>Start minimal</strong>: Use <code>--context-providers critical_path,dependency</code> initially</li>
<li><strong>Add git_history selectively</strong>: Enable for critical modules only</li>
<li><strong>Use caching</strong>: The <code>ContextAggregator</code> caches results by <code>file:function</code> key</li>
<li><strong>Profile with verbose flags</strong>: Use <code>-vvv</code> to see provider execution times</li>
</ol>
<h3 id="for-large-projects"><a class="header" href="#for-large-projects">For Large Projects</a></h3>
<pre><code class="language-bash"># Disable git history for faster analysis
debtmap analyze . --disable-context git_history

# Or disable all context
debtmap analyze . --no-context-aware
</code></pre>
<h3 id="for-cicd"><a class="header" href="#for-cicd">For CI/CD</a></h3>
<pre><code class="language-bash"># Full analysis with context (run nightly)
debtmap analyze . --context-providers critical_path,dependency,git_history

# Fast analysis without context (run on every commit)
debtmap analyze . --no-context-aware
</code></pre>
<h3 id="when-to-use-each-provider"><a class="header" href="#when-to-use-each-provider">When to Use Each Provider</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Scenario</th><th>Recommended Providers</th></tr></thead><tbody>
<tr><td>API service refactoring</td><td><code>critical_path</code></td></tr>
<tr><td>Legacy codebase analysis</td><td><code>git_history</code></td></tr>
<tr><td>Microservice boundaries</td><td><code>dependency</code></td></tr>
<tr><td>Pre-release risk review</td><td>All providers (<code>--context</code>)</td></tr>
<tr><td>CI/CD integration</td><td><code>critical_path,dependency</code> (faster)</td></tr>
</tbody></table>
</div>
<h2 id="troubleshooting-5"><a class="header" href="#troubleshooting-5">Troubleshooting</a></h2>
<h3 id="git-history-analysis-slow"><a class="header" href="#git-history-analysis-slow">Git History Analysis Slow</a></h3>
<p><strong>Issue:</strong> Analysis takes much longer with git history enabled</p>
<p><strong>Solutions:</strong></p>
<p><strong>Reduce commit history:</strong></p>
<pre><code class="language-toml">[context.git_history]
max_commits = 50
time_range_days = 30
</code></pre>
<p><strong>Use shallow clone in CI:</strong></p>
<pre><code class="language-bash">git clone --depth 50 repo.git
debtmap analyze . --context-providers critical_path,dependency
</code></pre>
<h3 id="incorrect-role-classification"><a class="header" href="#incorrect-role-classification">Incorrect Role Classification</a></h3>
<p><strong>Issue:</strong> Function classified as wrong role (e.g., utility instead of business logic)</p>
<p><strong>Possible causes:</strong></p>
<ol>
<li>Function naming doesnâ€™t match patterns</li>
<li>Call graph analysis incomplete</li>
<li>Function is misplaced in codebase</li>
</ol>
<p><strong>Solutions:</strong></p>
<p><strong>Check with verbose output:</strong></p>
<pre><code class="language-bash">debtmap analyze . -vv | grep "Role classification"
</code></pre>
<p><strong>Manually verify call graph:</strong></p>
<pre><code class="language-bash">debtmap analyze . --show-call-graph
</code></pre>
<h3 id="context-providers-not-available"><a class="header" href="#context-providers-not-available">Context Providers Not Available</a></h3>
<p><strong>Issue:</strong> <code>--context-providers</code> flag not recognized</p>
<p><strong>Solution:</strong> Ensure youâ€™re using a recent version:</p>
<pre><code class="language-bash">debtmap --version
# Should be 0.2.0 or later
</code></pre>
<p>Update debtmap:</p>
<pre><code class="language-bash">cargo install debtmap --force
</code></pre>
<h3 id="common-issues-1"><a class="header" href="#common-issues-1">Common Issues</a></h3>
<p><strong>Issue</strong>: Context providers not affecting scores</p>
<p><strong>Solution</strong>: Ensure providers are enabled with <code>--context</code> or <code>--context-providers</code></p>
<pre><code class="language-bash"># Wrong: context flag missing
debtmap analyze

# Correct: context enabled
debtmap analyze --context
</code></pre>
<hr />
<p><strong>Issue</strong>: Git history provider fails with â€œNot a git repositoryâ€</p>
<p><strong>Solution</strong>: Disable git_history if not using version control</p>
<pre><code class="language-bash">debtmap analyze --context --disable-context git_history
</code></pre>
<hr />
<p><strong>Issue</strong>: Dependency analysis errors</p>
<p><strong>Solution</strong>: Check for circular dependencies or disable dependency provider</p>
<pre><code class="language-bash">debtmap analyze --context --disable-context dependency
</code></pre>
<hr />
<p><strong>Issue</strong>: Slow analysis with all providers</p>
<p><strong>Solution</strong>: Use selective providers or increase verbosity to identify bottlenecks</p>
<pre><code class="language-bash"># Faster: skip git_history
debtmap analyze --context-providers critical_path,dependency

# Debug: see provider execution times
debtmap analyze --context -vvv
</code></pre>
<hr />
<p>For more troubleshooting guidance, see the <a href="troubleshooting.html">Troubleshooting</a> chapter.</p>
<h2 id="advanced-usage"><a class="header" href="#advanced-usage">Advanced Usage</a></h2>
<h3 id="interpreting-context-contribution"><a class="header" href="#interpreting-context-contribution">Interpreting Context Contribution</a></h3>
<p>Enable verbose output to see detailed context contributions:</p>
<pre><code class="language-bash">debtmap analyze --context -v
</code></pre>
<p>Each function shows:</p>
<ul>
<li>Base risk score from complexity/coverage</li>
<li>Individual provider contributions</li>
<li>Total contextual risk score</li>
<li>Provider-specific explanations</li>
</ul>
<h3 id="architecture-exploration"><a class="header" href="#architecture-exploration">Architecture Exploration</a></h3>
<p>The <code>ContextAggregator</code> caches context by <code>file:function</code> key. This enables efficient re-analysis:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let mut aggregator = ContextAggregator::new()
    .with_provider(Box::new(CriticalPathProvider::new(analyzer)))
    .with_provider(Box::new(DependencyRiskProvider::new(graph)))
    .with_provider(Box::new(GitHistoryProvider::new(repo_root)?));

let context = aggregator.analyze(&amp;target);
let contribution = context.total_contribution();
<span class="boring">}</span></code></pre></pre>
<h3 id="custom-provider-implementation"><a class="header" href="#custom-provider-implementation">Custom Provider Implementation</a></h3>
<p>Advanced users can implement custom context providers by implementing the <code>ContextProvider</code> trait:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub trait ContextProvider: Send + Sync {
    fn name(&amp;self) -&gt; &amp;str;
    fn gather(&amp;self, target: &amp;AnalysisTarget) -&gt; Result&lt;Context&gt;;
    fn weight(&amp;self) -&gt; f64;
    fn explain(&amp;self, context: &amp;Context) -&gt; String;
}
<span class="boring">}</span></code></pre></pre>
<p>See <a href="https://github.com/your-repo/debtmap/blob/main/src/risk/context/mod.rs">src/risk/context/mod.rs</a> for implementation examples.</p>
<h2 id="future-enhancements"><a class="header" href="#future-enhancements">Future Enhancements</a></h2>
<h3 id="business-context-provider-planned"><a class="header" href="#business-context-provider-planned">Business Context Provider (Planned)</a></h3>
<p>A Business context provider is defined but not yet implemented. It will support:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>Business {
    priority: Priority,      // Critical, High, Medium, Low
    impact: Impact,          // Revenue, UserExperience, Security, Compliance
    annotations: Vec&lt;String&gt; // Custom business metadata
}
<span class="boring">}</span></code></pre></pre>
<p>This will allow manual prioritization based on business requirements through code annotations or configuration files.</p>
<h2 id="best-practices-3"><a class="header" href="#best-practices-3">Best Practices</a></h2>
<ol>
<li><strong>Use all providers for comprehensive analysis</strong> - Especially for production code</li>
<li><strong>Disable git history in CI</strong> - Use shallow clones or disable for speed</li>
<li><strong>Verify role classifications</strong> - Use <code>-vv</code> to see how functions are classified</li>
<li><strong>Tune multipliers for your project</strong> - Adjust in config based on architecture</li>
<li><strong>Combine with coverage data</strong> - Context providers enhance coverage-based risk analysis</li>
</ol>
<h2 id="summary-1"><a class="header" href="#summary-1">Summary</a></h2>
<p>Context providers transform debtmap from a static complexity analyzer into a comprehensive risk assessment tool. By combining:</p>
<ul>
<li><strong>Critical path analysis</strong> for user impact</li>
<li><strong>Dependency analysis</strong> for architectural risk</li>
<li><strong>Git history analysis</strong> for maintenance patterns</li>
</ul>
<p>You gain actionable insights for prioritizing technical debt and refactoring efforts. Start with <code>--context</code> to enable all providers, then refine based on your projectâ€™s needs.</p>
<h2 id="see-also-2"><a class="header" href="#see-also-2">See Also</a></h2>
<ul>
<li><a href="analysis-guide.html">Analysis Guide</a> - Core analysis concepts</li>
<li><a href="analysis-guide.html#risk-assessment">Risk Assessment</a> - Risk scoring methodology</li>
<li><a href="configuration.html">Configuration</a> - Complete configuration reference</li>
<li><a href="parallel-processing.html">Parallel Processing</a> - Performance optimization</li>
<li><a href="troubleshooting.html">Troubleshooting</a> - Common issues and solutions</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="coverage-integration-1"><a class="header" href="#coverage-integration-1">Coverage Integration</a></h1>
<p>Coverage integration is one of Debtmapâ€™s most powerful capabilities, enabling <strong>risk-based prioritization</strong> by correlating complexity metrics with test coverage. This helps you identify truly risky codeâ€”functions that are both complex and untestedâ€”rather than just highlighting complex but well-tested functions.</p>
<h2 id="why-coverage-matters"><a class="header" href="#why-coverage-matters">Why Coverage Matters</a></h2>
<p>Without coverage data, complexity analysis shows you <em>whatâ€™s complex</em>, but not <em>whatâ€™s risky</em>. A complex function with 100% test coverage poses far less risk than a simple function with 0% coverage on a critical path.</p>
<p>Coverage integration transforms Debtmap from a complexity analyzer into a <strong>risk assessment tool</strong>:</p>
<ul>
<li><strong>Prioritize testing efforts</strong>: Focus on high-complexity functions with low coverage</li>
<li><strong>Validate refactoring safety</strong>: See which complex code is already protected by tests</li>
<li><strong>Risk-based sprint planning</strong>: Surface truly risky code ahead of well-tested complexity</li>
<li><strong>Quantify risk reduction</strong>: Measure how coverage improvements reduce project risk</li>
</ul>
<h2 id="lcov-format-the-universal-standard"><a class="header" href="#lcov-format-the-universal-standard">LCOV Format: The Universal Standard</a></h2>
<p>Debtmap uses the <strong>LCOV format</strong> for coverage data. LCOV is a language-agnostic standard supported by virtually all coverage tools across all major languages.</p>
<h3 id="why-lcov"><a class="header" href="#why-lcov">Why LCOV?</a></h3>
<ul>
<li><strong>Universal compatibility</strong>: Works with Rust, Python, JavaScript, TypeScript, Go, and more</li>
<li><strong>Tool independence</strong>: Not tied to any specific test framework</li>
<li><strong>Simple text format</strong>: Easy to inspect and debug</li>
<li><strong>Widely supported</strong>: Generated by most modern coverage tools</li>
</ul>
<h3 id="lcov-file-structure"><a class="header" href="#lcov-file-structure">LCOV File Structure</a></h3>
<p>An LCOV file contains line-by-line coverage information:</p>
<pre><code class="language-lcov">SF:src/analyzer.rs
FN:42,calculate_complexity
FNDA:15,calculate_complexity
DA:42,15
DA:43,15
DA:44,12
DA:45,0
LH:3
LF:4
end_of_record
</code></pre>
<ul>
<li><code>SF:</code> - Source file path</li>
<li><code>FN:</code> - Function name and starting line</li>
<li><code>FNDA:</code> - Function execution count</li>
<li><code>DA:</code> - Line execution data (line number, hit count)</li>
<li><code>LH:</code> - Lines hit</li>
<li><code>LF:</code> - Lines found (total)</li>
</ul>
<h2 id="generating-coverage-data"><a class="header" href="#generating-coverage-data">Generating Coverage Data</a></h2>
<h3 id="rust-cargo-tarpaulin"><a class="header" href="#rust-cargo-tarpaulin">Rust: cargo-tarpaulin</a></h3>
<p><strong>Installation:</strong></p>
<pre><code class="language-bash">cargo install cargo-tarpaulin
</code></pre>
<p><strong>Generate LCOV:</strong></p>
<pre><code class="language-bash">cargo tarpaulin --out lcov --output-dir target/coverage
</code></pre>
<p><strong>Analyze with Debtmap:</strong></p>
<pre><code class="language-bash">debtmap analyze . --lcov target/coverage/lcov.info
</code></pre>
<p><strong>Common Issues:</strong></p>
<ul>
<li>Ensure tests compile before running tarpaulin</li>
<li>Use <code>--ignore-tests</code> if tests themselves show up in coverage</li>
<li>Check paths match your project structure (relative to project root)</li>
</ul>
<h3 id="javascripttypescript-jest"><a class="header" href="#javascripttypescript-jest">JavaScript/TypeScript: Jest</a></h3>
<p><strong>Configuration (package.json or jest.config.js):</strong></p>
<pre><code class="language-json">{
  "jest": {
    "coverageReporters": ["lcov", "text"]
  }
}
</code></pre>
<p><strong>Generate Coverage:</strong></p>
<pre><code class="language-bash">npm test -- --coverage
</code></pre>
<p><strong>Analyze with Debtmap:</strong></p>
<pre><code class="language-bash">debtmap analyze . --lcov coverage/lcov.info
</code></pre>
<h3 id="python-pytest-cov"><a class="header" href="#python-pytest-cov">Python: pytest-cov</a></h3>
<p><strong>Installation:</strong></p>
<pre><code class="language-bash">pip install pytest-cov
</code></pre>
<p><strong>Generate LCOV:</strong></p>
<pre><code class="language-bash">pytest --cov=src --cov-report=lcov
</code></pre>
<p><strong>Analyze with Debtmap:</strong></p>
<pre><code class="language-bash">debtmap analyze . --lcov coverage.lcov
</code></pre>
<h3 id="go-go-test-with-gocover-cobertura"><a class="header" href="#go-go-test-with-gocover-cobertura">Go: go test with gocover-cobertura</a></h3>
<p><strong>Generate Coverage:</strong></p>
<pre><code class="language-bash">go test -coverprofile=coverage.out ./...
gocover-cobertura &lt; coverage.out &gt; coverage.xml
# Convert to LCOV using lcov tools
</code></pre>
<p><strong>Note</strong>: Goâ€™s native coverage format requires conversion. Most CI systems support LCOV conversion plugins.</p>
<h2 id="how-coverage-affects-scoring"><a class="header" href="#how-coverage-affects-scoring">How Coverage Affects Scoring</a></h2>
<p>Coverage data impacts Debtmapâ€™s unified scoring system in two ways: the <strong>coverage factor</strong> and <strong>coverage dampening</strong>.</p>
<h3 id="coverage-factor-40-weight"><a class="header" href="#coverage-factor-40-weight">Coverage Factor (40% Weight)</a></h3>
<p>The coverage factor contributes <strong>40%</strong> to the unified debt score:</p>
<pre><code>Coverage Factor = 10 Ã— (1 - coverage_percentage) Ã— complexity_weight
</code></pre>
<p><strong>Examples:</strong></p>
<ul>
<li>0% coverage â†’ Factor = 10.0 (maximum penalty)</li>
<li>50% coverage â†’ Factor = 5.0 (moderate penalty)</li>
<li>100% coverage â†’ Factor = 0.0 (no penalty)</li>
</ul>
<p><strong>Special Cases:</strong></p>
<ul>
<li><strong>Test functions</strong>: Coverage factor = 0 (tests donâ€™t need their own coverage)</li>
<li><strong>No coverage data</strong>: Assumes worst case (factor = 10.0)</li>
<li><strong>Entry points</strong>: Weighted at 0.6x (entry points naturally have low coverage)</li>
</ul>
<h3 id="coverage-dampening-score-multiplier"><a class="header" href="#coverage-dampening-score-multiplier">Coverage Dampening (Score Multiplier)</a></h3>
<p>After base scores are calculated, coverage <em>dampens</em> the final debt score:</p>
<pre><code>Final Score = Base Score Ã— (1.0 - coverage_percentage)
</code></pre>
<p><strong>Examples:</strong></p>
<div class="table-wrapper"><table><thead><tr><th>Base Score</th><th>Coverage</th><th>Multiplier</th><th>Final Score</th><th>Priority</th></tr></thead><tbody>
<tr><td>8.5</td><td>100%</td><td>0.0</td><td>0.0</td><td>Minimal (well-tested)</td></tr>
<tr><td>8.5</td><td>50%</td><td>0.5</td><td>4.25</td><td>Medium</td></tr>
<tr><td>8.5</td><td>0%</td><td>1.0</td><td>8.5</td><td>High (untested)</td></tr>
</tbody></table>
</div>
<p><strong>Key Insight</strong>: Complex but well-tested code automatically drops in priority, while untested complex code rises to the top.</p>
<p><strong>Invariant</strong>: Total debt score with coverage â‰¤ total debt score without coverage.</p>
<h2 id="transitive-coverage-propagation"><a class="header" href="#transitive-coverage-propagation">Transitive Coverage Propagation</a></h2>
<p>Debtmap doesnâ€™t just look at <em>direct</em> coverageâ€”it propagates coverage through the <strong>call graph</strong> using transitive analysis.</p>
<h3 id="how-it-works"><a class="header" href="#how-it-works">How It Works</a></h3>
<p>A functionâ€™s effective coverage considers:</p>
<ol>
<li><strong>Direct coverage</strong>: Lines executed by tests</li>
<li><strong>Caller coverage</strong>: Coverage of functions that call this function</li>
</ol>
<pre><code>Transitive Coverage = Direct Coverage + Î£(Caller Coverage Ã— Weight)
</code></pre>
<h3 id="why-it-matters"><a class="header" href="#why-it-matters">Why It Matters</a></h3>
<p>A function with 0% direct coverage might have high transitive coverage if itâ€™s only called by well-tested functions:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// direct_coverage = 0%
// But called only by `process_request` (100% coverage)
// â†’ transitive_coverage = 85%
fn validate_input(data: &amp;str) -&gt; bool {
    data.len() &gt; 0
}

// direct_coverage = 100%
fn process_request(input: String) -&gt; Result&lt;()&gt; {
    if !validate_input(&amp;input) {
        return Err("Invalid");
    }
    // ...
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Effect</strong>: <code>validate_input</code> has reduced urgency because itâ€™s only reachable through well-tested code paths.</p>
<h2 id="performance-characteristics-2"><a class="header" href="#performance-characteristics-2">Performance Characteristics</a></h2>
<p>Coverage integration is highly optimized for large codebases:</p>
<ul>
<li><strong>Index Build</strong>: O(n), ~20-30ms for 5,000 functions</li>
<li><strong>Exact Lookup</strong>: O(1), ~0.5Î¼s per lookup</li>
<li><strong>Fallback Lookup</strong>: O(log n), ~5-8Î¼s when exact match fails</li>
<li><strong>Memory Usage</strong>: ~200 bytes per record (~2MB for 5,000 functions)</li>
<li><strong>Thread Safety</strong>: Lock-free parallel access via <code>Arc&lt;CoverageIndex&gt;</code></li>
<li><strong>Analysis Overhead</strong>: ~2.5x baseline (target: â‰¤3x)</li>
</ul>
<p><strong>Result</strong>: Coverage integration adds minimal overhead even on projects with thousands of functions.</p>
<h2 id="cli-options-reference"><a class="header" href="#cli-options-reference">CLI Options Reference</a></h2>
<h3 id="primary-coverage-options"><a class="header" href="#primary-coverage-options">Primary Coverage Options</a></h3>
<pre><code class="language-bash"># Provide LCOV coverage file
debtmap analyze . --coverage-file path/to/lcov.info

# Shorthand alias
debtmap analyze . --lcov path/to/lcov.info
</code></pre>
<h3 id="context-providers-1"><a class="header" href="#context-providers-1">Context Providers</a></h3>
<p>Coverage can be combined with other context providers for nuanced risk assessment:</p>
<pre><code class="language-bash"># Enable all context providers (includes coverage propagation)
debtmap analyze . --lcov coverage.info --enable-context

# Specify specific providers
debtmap analyze . --lcov coverage.info \
  --context-providers critical_path,dependency,git_history

# Disable specific providers
debtmap analyze . --lcov coverage.info \
  --disable-context git_history
</code></pre>
<p><strong>Available Context Providers</strong>:</p>
<ul>
<li><code>critical_path</code>: Identifies functions on critical execution paths</li>
<li><code>dependency</code>: Analyzes dependency relationships and impact</li>
<li><code>git_history</code>: Uses change frequency from version control</li>
</ul>
<p>See <a href="scoring-strategies.html">Scoring Strategies</a> for details on how these combine.</p>
<h3 id="validate-command-support"><a class="header" href="#validate-command-support">Validate Command Support</a></h3>
<p>The <code>validate</code> command also supports coverage integration for risk-based quality gates:</p>
<pre><code class="language-bash"># Fail CI builds if untested complex code exceeds thresholds
debtmap validate . --lcov coverage.info --max-debt-density 50
</code></pre>
<p>See <a href="cli-reference.html">CLI Reference</a> for complete validation options.</p>
<h2 id="troubleshooting-coverage-integration"><a class="header" href="#troubleshooting-coverage-integration">Troubleshooting Coverage Integration</a></h2>
<h3 id="coverage-not-correlating-with-functions"><a class="header" href="#coverage-not-correlating-with-functions">Coverage Not Correlating with Functions</a></h3>
<p><strong>Symptoms</strong>:</p>
<ul>
<li>Debtmap shows 0% coverage for all functions</li>
<li>Warning: â€œNo coverage data correlated with analyzed functionsâ€</li>
</ul>
<p><strong>Solutions</strong>:</p>
<ol>
<li><strong>Verify LCOV Format</strong>:</li>
</ol>
<pre><code class="language-bash">head coverage.info
# Should show: SF:, FN:, DA: lines
</code></pre>
<ol start="2">
<li><strong>Check Path Matching</strong>:
Coverage file paths must be relative to project root:</li>
</ol>
<pre><code class="language-bash"># Good: SF:src/analyzer.rs
# Bad:  SF:/home/user/project/src/analyzer.rs
</code></pre>
<ol start="3">
<li><strong>Enable Verbose Logging</strong>:</li>
</ol>
<pre><code class="language-bash">debtmap analyze . --lcov coverage.info -vv
</code></pre>
<p>This shows coverage lookup details for each function.</p>
<ol start="4">
<li><strong>Verify Coverage Tool Output</strong>:</li>
</ol>
<pre><code class="language-bash"># Ensure your coverage tool generated line data (DA: records)
grep "^DA:" coverage.info | head
</code></pre>
<h3 id="functions-still-show-up-despite-100-coverage"><a class="header" href="#functions-still-show-up-despite-100-coverage">Functions Still Show Up Despite 100% Coverage</a></h3>
<p><strong>This is expected behavior</strong> when:</p>
<ul>
<li>Function has high complexity (cyclomatic &gt; 10)</li>
<li>Function has other debt issues (duplication, nesting, etc.)</li>
<li>Youâ€™re viewing function-level output (coverage dampens but doesnâ€™t eliminate)</li>
</ul>
<p><strong>Coverage reduces priority but doesnâ€™t hide issues</strong>. Use filters to focus:</p>
<pre><code class="language-bash"># Show only critical and high priority items
debtmap analyze . --lcov coverage.info --min-priority high

# Show top 10 most urgent items
debtmap analyze . --lcov coverage.info --top 10
</code></pre>
<h3 id="coverage-file-path-issues"><a class="header" href="#coverage-file-path-issues">Coverage File Path Issues</a></h3>
<p><strong>Problem</strong>: Canâ€™t find coverage file</p>
<p><strong>Solutions</strong>:</p>
<pre><code class="language-bash"># Use absolute path
debtmap analyze . --lcov /absolute/path/to/coverage.info

# Or ensure relative path is from project root
debtmap analyze . --lcov ./target/coverage/lcov.info
</code></pre>
<h3 id="lcov-format-errors"><a class="header" href="#lcov-format-errors">LCOV Format Errors</a></h3>
<p><strong>Problem</strong>: â€œInvalid LCOV formatâ€ error</p>
<p><strong>Causes</strong>:</p>
<ul>
<li>Non-LCOV format (Cobertura XML, JaCoCo, etc.)</li>
<li>Corrupted file</li>
<li>Wrong file encoding</li>
</ul>
<p><strong>Solutions</strong>:</p>
<ul>
<li>Verify your coverage tool is configured for LCOV output</li>
<li>Check for binary/encoding issues: <code>file coverage.info</code></li>
<li>Regenerate coverage with explicit LCOV format flag</li>
</ul>
<p>See <a href="troubleshooting.html">Troubleshooting</a> for more debugging tips.</p>
<h2 id="best-practices-4"><a class="header" href="#best-practices-4">Best Practices</a></h2>
<h3 id="analysis-workflow"><a class="header" href="#analysis-workflow">Analysis Workflow</a></h3>
<ol>
<li>
<p><strong>Generate Coverage Before Analysis</strong>:</p>
<pre><code class="language-bash"># Rust example
cargo tarpaulin --out lcov --output-dir target/coverage
debtmap analyze . --lcov target/coverage/lcov.info
</code></pre>
</li>
<li>
<p><strong>Use Coverage for Sprint Planning</strong>:</p>
<pre><code class="language-bash"># Focus on untested complex code
debtmap analyze . --lcov coverage.info --top 20
</code></pre>
</li>
<li>
<p><strong>Combine with Tiered Prioritization</strong>:
Coverage automatically feeds into <a href="tiered-prioritization.html">Tiered Prioritization</a>:</p>
<ul>
<li><strong>Tier 1</strong>: Architectural issues (less affected by coverage)</li>
<li><strong>Tier 2</strong>: Complex untested code (coverage &lt; 50%, complexity &gt; 15)</li>
<li><strong>Tier 3</strong>: Testing gaps (coverage &lt; 80%, complexity 10-15)</li>
</ul>
</li>
<li>
<p><strong>Validate Refactoring Impact</strong>:</p>
<pre><code class="language-bash"># Before refactoring
debtmap analyze . --lcov coverage-before.info -o before.json

# After refactoring
debtmap analyze . --lcov coverage-after.info -o after.json

# Compare
debtmap compare --before before.json --after after.json
</code></pre>
</li>
</ol>
<h3 id="testing-strategy"><a class="header" href="#testing-strategy">Testing Strategy</a></h3>
<p><strong>Prioritize testing based on risk</strong>:</p>
<ol>
<li>
<p><strong>High Complexity + Low Coverage = Highest Priority</strong>:</p>
<pre><code class="language-bash">debtmap analyze . --lcov coverage.info \
  --filter Risk --min-priority high
</code></pre>
</li>
<li>
<p><strong>Focus on Business Logic</strong>:
Entry points and infrastructure code have natural coverage patterns. Focus unit tests on business logic functions.</p>
</li>
<li>
<p><strong>Use Dependency Analysis</strong>:</p>
<pre><code class="language-bash">debtmap analyze . --lcov coverage.info \
  --context-providers dependency -vv
</code></pre>
<p>Tests high-dependency functions firstâ€”they have the most impact.</p>
</li>
<li>
<p><strong>Donâ€™t Over-Test Entry Points</strong>:
Entry points (main, handlers) are better tested with integration tests, not unit tests. Debtmap weights their coverage factor at 0.6x to account for this.</p>
</li>
</ol>
<h3 id="configuration-3"><a class="header" href="#configuration-3">Configuration</a></h3>
<p>In <code>.debtmap.toml</code>:</p>
<pre><code class="language-toml">[scoring]
# Adjust coverage weight if your project prioritizes coverage differently
coverage = 0.40  # Default: 40%
complexity = 0.40
dependency = 0.20

[thresholds]
# Set minimum risk score to filter low-priority items
minimum_risk_score = 15.0

# Skip simple functions even if uncovered
minimum_cyclomatic_complexity = 5
</code></pre>
<p>See <a href="configuration.html">Configuration</a> for complete options.</p>
<h3 id="ci-integration"><a class="header" href="#ci-integration">CI Integration</a></h3>
<p><strong>Example GitHub Actions Workflow</strong>:</p>
<pre><code class="language-yaml">- name: Generate Coverage
  run: cargo tarpaulin --out lcov --output-dir target/coverage

- name: Analyze with Debtmap
  run: |
    debtmap analyze . \
      --lcov target/coverage/lcov.info \
      --format json \
      --output debtmap-report.json

- name: Validate Quality Gates
  run: |
    debtmap validate . \
      --lcov target/coverage/lcov.info \
      --max-debt-density 50
</code></pre>
<p><strong>Quality Gate Strategy</strong>:</p>
<ul>
<li>Fail builds on new critical debt (Tier 1 architectural issues)</li>
<li>Warn on new high-priority untested code (Tier 2)</li>
<li>Track coverage trends over time with <code>compare</code> command</li>
</ul>
<h2 id="complete-language-examples"><a class="header" href="#complete-language-examples">Complete Language Examples</a></h2>
<h3 id="rust-end-to-end"><a class="header" href="#rust-end-to-end">Rust End-to-End</a></h3>
<pre><code class="language-bash"># 1. Generate coverage
cargo tarpaulin --out lcov --output-dir target/coverage

# 2. Verify LCOV output
head target/coverage/lcov.info

# 3. Run Debtmap with coverage
debtmap analyze . --lcov target/coverage/lcov.info

# 4. Interpret results (look for [UNTESTED] markers on high-complexity functions)
</code></pre>
<h3 id="javascripttypescript-end-to-end"><a class="header" href="#javascripttypescript-end-to-end">JavaScript/TypeScript End-to-End</a></h3>
<pre><code class="language-bash"># 1. Configure Jest for LCOV (in package.json or jest.config.js)
# "coverageReporters": ["lcov", "text"]

# 2. Generate coverage
npm test -- --coverage

# 3. Verify LCOV output
head coverage/lcov.info

# 4. Run Debtmap
debtmap analyze . --lcov coverage/lcov.info --languages javascript,typescript
</code></pre>
<h3 id="python-end-to-end"><a class="header" href="#python-end-to-end">Python End-to-End</a></h3>
<pre><code class="language-bash"># 1. Install pytest-cov
pip install pytest-cov

# 2. Generate LCOV coverage
pytest --cov=src --cov-report=lcov

# 3. Verify output
head coverage.lcov

# 4. Run Debtmap
debtmap analyze . --lcov coverage.lcov --languages python
</code></pre>
<h3 id="go-end-to-end"><a class="header" href="#go-end-to-end">Go End-to-End</a></h3>
<pre><code class="language-bash"># 1. Generate native coverage
go test -coverprofile=coverage.out ./...

# 2. Convert to LCOV (requires gocover-cobertura or similar)
# Note: This step is tool-dependent

# 3. Run Debtmap
debtmap analyze . --lcov coverage.lcov --languages go
</code></pre>
<h2 id="faq"><a class="header" href="#faq">FAQ</a></h2>
<h3 id="why-does-my-100-covered-function-still-show-up"><a class="header" href="#why-does-my-100-covered-function-still-show-up">Why does my 100% covered function still show up?</a></h3>
<p>Coverage dampens debt scores but doesnâ€™t eliminate debt. A function with cyclomatic complexity 25 and 100% coverage still represents technical debtâ€”itâ€™s just lower priority than untested complex code.</p>
<p><strong>Use filters to focus on high-priority items</strong>:</p>
<pre><code class="language-bash">debtmap analyze . --lcov coverage.info --top 10
</code></pre>
<h3 id="whats-the-difference-between-direct-and-transitive-coverage"><a class="header" href="#whats-the-difference-between-direct-and-transitive-coverage">Whatâ€™s the difference between direct and transitive coverage?</a></h3>
<ul>
<li><strong>Direct coverage</strong>: Lines executed directly by tests</li>
<li><strong>Transitive coverage</strong>: Coverage considering call graph (functions called by well-tested code)</li>
</ul>
<p>Transitive coverage reduces urgency for functions only reachable through well-tested paths.</p>
<h3 id="should-i-test-everything-to-100-coverage"><a class="header" href="#should-i-test-everything-to-100-coverage">Should I test everything to 100% coverage?</a></h3>
<p><strong>No.</strong> Use Debtmapâ€™s risk scores to prioritize:</p>
<ol>
<li>Test high-complexity, low-coverage functions first</li>
<li>Entry points are better tested with integration tests</li>
<li>Simple utility functions (complexity &lt; 5) may not need dedicated unit tests</li>
</ol>
<p>Debtmap helps you achieve <strong>optimal coverage</strong>, not maximal coverage.</p>
<h3 id="how-do-i-debug-coverage-correlation-issues"><a class="header" href="#how-do-i-debug-coverage-correlation-issues">How do I debug coverage correlation issues?</a></h3>
<p>Use verbose logging:</p>
<pre><code class="language-bash">debtmap analyze . --lcov coverage.info -vv
</code></pre>
<p>This shows:</p>
<ul>
<li>Coverage file parsing details</li>
<li>Function-to-coverage correlation attempts</li>
<li>Path matching diagnostics</li>
</ul>
<h3 id="can-i-use-coverage-with-validate-command"><a class="header" href="#can-i-use-coverage-with-validate-command">Can I use coverage with validate command?</a></h3>
<p>Yes! The <code>validate</code> command supports <code>--lcov</code> for risk-based quality gates:</p>
<pre><code class="language-bash">debtmap validate . --lcov coverage.info --max-debt-density 50
</code></pre>
<p>See <a href="cli-reference.html#validate-command">CLI Reference</a> for details.</p>
<h2 id="further-reading"><a class="header" href="#further-reading">Further Reading</a></h2>
<ul>
<li><a href="scoring-strategies.html">Scoring Strategies</a> - Deep dive into how coverage affects unified scoring</li>
<li><a href="tiered-prioritization.html">Tiered Prioritization</a> - How coverage fits into tiered priority levels</li>
<li><a href="cli-reference.html">CLI Reference</a> - Complete coverage option documentation</li>
<li><a href="configuration.html">Configuration</a> - Customizing coverage scoring weights</li>
<li><a href="troubleshooting.html">Troubleshooting</a> - More debugging tips for coverage issues</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="entropy-analysis"><a class="header" href="#entropy-analysis">Entropy Analysis</a></h1>
<p>Entropy analysis is Debtmapâ€™s unique approach to distinguishing genuinely complex code from repetitive pattern-based code. This reduces false positives by 60-75% compared to traditional cyclomatic complexity metrics.</p>
<h2 id="overview-2"><a class="header" href="#overview-2">Overview</a></h2>
<p>Traditional static analysis tools flag code as â€œcomplexâ€ based purely on cyclomatic complexity or lines of code. However, not all complexity is equal:</p>
<ul>
<li><strong>Repetitive patterns</strong> (validation functions, dispatchers) have high cyclomatic complexity but low cognitive load</li>
<li><strong>Diverse logic</strong> (state machines, business rules) may have moderate cyclomatic complexity but high cognitive load</li>
</ul>
<p>Entropy analysis uses information theory to distinguish between these cases.</p>
<h2 id="how-it-works-1"><a class="header" href="#how-it-works-1">How It Works</a></h2>
<h3 id="shannon-entropy"><a class="header" href="#shannon-entropy">Shannon Entropy</a></h3>
<p>Shannon entropy measures the variety and unpredictability of code patterns:</p>
<pre><code>H(X) = -Î£ p(x) Ã— logâ‚‚(p(x))
</code></pre>
<p>Where:</p>
<ul>
<li><code>p(x)</code> = probability of each token type</li>
<li>High entropy (0.8-1.0) = many different patterns</li>
<li>Low entropy (0.0-0.3) = repetitive patterns</li>
</ul>
<h3 id="token-classification"><a class="header" href="#token-classification">Token Classification</a></h3>
<p>Debtmap classifies tokens by importance:</p>
<p><strong>High importance (weight: 1.0):</strong></p>
<ul>
<li>Control flow keywords (<code>if</code>, <code>match</code>, <code>for</code>, <code>while</code>)</li>
<li>Error handling (<code>try</code>, <code>catch</code>, <code>?</code>, <code>unwrap</code>)</li>
<li>Async keywords (<code>async</code>, <code>await</code>)</li>
</ul>
<p><strong>Medium importance (weight: 0.7):</strong></p>
<ul>
<li>Function calls</li>
<li>Method invocations</li>
<li>Operators</li>
</ul>
<p><strong>Low importance (weight: 0.3):</strong></p>
<ul>
<li>Identifiers (variable names)</li>
<li>Literals (strings, numbers)</li>
<li>Punctuation</li>
</ul>
<h3 id="pattern-repetition-detection"><a class="header" href="#pattern-repetition-detection">Pattern Repetition Detection</a></h3>
<p>Detects repetitive structures in the AST:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Low pattern repetition (0.2) - all branches identical
if a.is_none() { return Err(...) }
if b.is_none() { return Err(...) }
if c.is_none() { return Err(...) }

// High pattern repetition (0.9) - diverse branches
match state {
    Active =&gt; transition_to_standby(),
    Standby =&gt; transition_to_active(),
    Maintenance =&gt; schedule_restart(),
}
<span class="boring">}</span></code></pre></pre>
<h3 id="branch-similarity-analysis"><a class="header" href="#branch-similarity-analysis">Branch Similarity Analysis</a></h3>
<p>Analyzes similarity between conditional branches:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// High branch similarity (0.9) - branches are nearly identical
if condition_a {
    log("A happened");
    process_a();
}
if condition_b {
    log("B happened");
    process_b();
}

// Low branch similarity (0.2) - branches are very different
if needs_auth {
    authenticate_user()?;
    load_profile()?;
} else {
    show_guest_ui();
}
<span class="boring">}</span></code></pre></pre>
<h3 id="effective-complexity-adjustment"><a class="header" href="#effective-complexity-adjustment">Effective Complexity Adjustment</a></h3>
<p>Adjusts raw cyclomatic complexity based on entropy:</p>
<pre><code>Effective Complexity = Raw Complexity Ã— (1 - Entropy Adjustment)

Entropy Adjustment = min(
    max_reduction,
    (1 - shannon_entropy) Ã— weight +
    pattern_repetition Ã— weight +
    branch_similarity Ã— weight
)
</code></pre>
<p><strong>Example:</strong></p>
<pre><code>Raw Complexity: 20
Shannon Entropy: 0.3 (low variety)
Pattern Repetition: 0.8 (highly repetitive)
Branch Similarity: 0.9 (very similar branches)

Entropy Adjustment = min(0.7, (1 - 0.3) Ã— 0.4 + 0.8 Ã— 0.3 + 0.9 Ã— 0.3)
                    = min(0.7, 0.28 + 0.24 + 0.27)
                    = min(0.7, 0.79)
                    = 0.7

Effective Complexity = 20 Ã— (1 - 0.7) = 6
</code></pre>
<h2 id="real-world-examples"><a class="header" href="#real-world-examples">Real-World Examples</a></h2>
<h3 id="example-1-validation-function"><a class="header" href="#example-1-validation-function">Example 1: Validation Function</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn validate_config(config: &amp;Config) -&gt; Result&lt;()&gt; {
    if config.output_dir.is_none() {
        return Err(anyhow!("output_dir required"));
    }
    if config.max_workers.is_none() {
        return Err(anyhow!("max_workers required"));
    }
    if config.timeout_secs.is_none() {
        return Err(anyhow!("timeout_secs required"));
    }
    // ... 17 more similar checks
    Ok(())
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Traditional analysis:</strong></p>
<ul>
<li>Cyclomatic Complexity: 20</li>
<li>Assessment: CRITICAL</li>
</ul>
<p><strong>Entropy analysis:</strong></p>
<ul>
<li>Shannon Entropy: 0.3 (low variety)</li>
<li>Pattern Repetition: 0.9 (highly repetitive)</li>
<li>Branch Similarity: 0.95 (nearly identical)</li>
<li>Effective Complexity: 5</li>
<li>Assessment: LOW PRIORITY</li>
</ul>
<h3 id="example-2-state-machine-logic"><a class="header" href="#example-2-state-machine-logic">Example 2: State Machine Logic</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn reconcile_state(current: &amp;State, desired: &amp;State) -&gt; Vec&lt;Action&gt; {
    let mut actions = vec![];

    match (current.mode, desired.mode) {
        (Mode::Active, Mode::Standby) =&gt; {
            if current.has_active_connections() {
                actions.push(Action::DrainConnections);
                actions.push(Action::WaitForDrain);
            }
            actions.push(Action::TransitionToStandby);
        }
        (Mode::Standby, Mode::Active) =&gt; {
            if desired.requires_warmup() {
                actions.push(Action::Warmup);
            }
            actions.push(Action::TransitionToActive);
        }
        // ... more diverse state transitions
        _ =&gt; {}
    }

    actions
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Traditional analysis:</strong></p>
<ul>
<li>Cyclomatic Complexity: 8</li>
<li>Assessment: MODERATE</li>
</ul>
<p><strong>Entropy analysis:</strong></p>
<ul>
<li>Shannon Entropy: 0.85 (high variety)</li>
<li>Pattern Repetition: 0.2 (not repetitive)</li>
<li>Branch Similarity: 0.3 (diverse branches)</li>
<li>Effective Complexity: 9</li>
<li>Assessment: HIGH PRIORITY</li>
</ul>
<h2 id="configuration-4"><a class="header" href="#configuration-4">Configuration</a></h2>
<p>Configure entropy analysis in <code>.debtmap.toml</code>:</p>
<pre><code class="language-toml">[entropy]
# Enable entropy analysis (default: true)
enabled = true

# Weight of entropy in complexity adjustment (0.0-1.0, default: 0.5)
weight = 0.5

# Minimum tokens required for entropy calculation (default: 10)
min_tokens = 10

# Pattern similarity threshold for detection (0.0-1.0, default: 0.7)
pattern_threshold = 0.7

# Enable advanced token classification (default: true)
use_classification = true

# Entropy level below which dampening is applied (default: 0.5)
entropy_threshold = 0.5

# Branch similarity above which dampening is applied (default: 0.7)
branch_threshold = 0.7

# Maximum combined complexity reduction percentage (default: 0.7 = 70%)
max_combined_reduction = 0.7
</code></pre>
<h3 id="tuning-for-your-project"><a class="header" href="#tuning-for-your-project">Tuning for Your Project</a></h3>
<p><strong>Strict mode (fewer false positive reductions):</strong></p>
<pre><code class="language-toml">[entropy]
enabled = true
weight = 0.3
max_combined_reduction = 0.5
</code></pre>
<p><strong>Lenient mode (more aggressive false positive reduction):</strong></p>
<pre><code class="language-toml">[entropy]
enabled = true
weight = 0.7
max_combined_reduction = 0.9
</code></pre>
<p><strong>Disable entropy analysis:</strong></p>
<pre><code class="language-toml">[entropy]
enabled = false
</code></pre>
<p>Or via CLI:</p>
<pre><code class="language-bash">debtmap analyze . --semantic-off
</code></pre>
<h2 id="understanding-the-impact"><a class="header" href="#understanding-the-impact">Understanding the Impact</a></h2>
<h3 id="measuring-false-positive-reduction"><a class="header" href="#measuring-false-positive-reduction">Measuring False Positive Reduction</a></h3>
<p>Run analysis with and without entropy:</p>
<pre><code class="language-bash"># Without entropy
debtmap analyze . --semantic-off --top 20 &gt; without_entropy.txt

# With entropy (default)
debtmap analyze . --top 20 &gt; with_entropy.txt

# Compare
diff without_entropy.txt with_entropy.txt
</code></pre>
<p><strong>Expected results:</strong></p>
<ul>
<li>60-75% reduction in flagged validation functions</li>
<li>40-50% reduction in flagged dispatcher functions</li>
<li>20-30% reduction in flagged configuration parsers</li>
<li>No reduction in genuinely complex state machines or business logic</li>
</ul>
<h3 id="verifying-correctness"><a class="header" href="#verifying-correctness">Verifying Correctness</a></h3>
<p>Entropy analysis should:</p>
<ul>
<li><strong>Reduce</strong> flags on repetitive code (validators, dispatchers)</li>
<li><strong>Preserve</strong> flags on genuinely complex code (state machines, business logic)</li>
</ul>
<p>If entropy analysis incorrectly reduces flags on genuinely complex code, adjust configuration:</p>
<pre><code class="language-toml">[entropy]
weight = 0.3  # Reduce impact
max_combined_reduction = 0.5  # Limit maximum reduction
</code></pre>
<h2 id="best-practices-5"><a class="header" href="#best-practices-5">Best Practices</a></h2>
<ol>
<li><strong>Use default settings</strong> - They work well for most projects</li>
<li><strong>Verify results</strong> - Spot-check top-priority items to ensure correctness</li>
<li><strong>Tune conservatively</strong> - Start with default settings, adjust if needed</li>
<li><strong>Disable for debugging</strong> - Use <code>--semantic-off</code> if entropy seems incorrect</li>
<li><strong>Report issues</strong> - If entropy incorrectly flags code, report it</li>
</ol>
<h2 id="limitations-1"><a class="header" href="#limitations-1">Limitations</a></h2>
<p>Entropy analysis works best for:</p>
<ul>
<li>Functions with cyclomatic complexity 10-50</li>
<li>Code with clear repetitive patterns</li>
<li>Validation, dispatch, and configuration functions</li>
</ul>
<p>Entropy analysis is less effective for:</p>
<ul>
<li>Very simple functions (complexity &lt; 5)</li>
<li>Very complex functions (complexity &gt; 100)</li>
<li>Obfuscated or generated code</li>
</ul>
<h2 id="comparison-with-other-approaches"><a class="header" href="#comparison-with-other-approaches">Comparison with Other Approaches</a></h2>
<div class="table-wrapper"><table><thead><tr><th>Approach</th><th>False Positive Rate</th><th>Complexity</th><th>Speed</th></tr></thead><tbody>
<tr><td>Raw Cyclomatic Complexity</td><td>High (many false positives)</td><td>Low</td><td>Fast</td></tr>
<tr><td>Cognitive Complexity</td><td>Medium</td><td>Medium</td><td>Medium</td></tr>
<tr><td>Entropy Analysis (Debtmap)</td><td>Low</td><td>High</td><td>Fast</td></tr>
<tr><td>Manual Code Review</td><td>Very Low</td><td>Very High</td><td>Very Slow</td></tr>
</tbody></table>
</div>
<p>Debtmapâ€™s entropy analysis provides the best balance of accuracy and speed.</p>
<h2 id="see-also-3"><a class="header" href="#see-also-3">See Also</a></h2>
<ul>
<li><a href="why-debtmap.html">Why Debtmap?</a> - Real-world examples of entropy analysis</li>
<li><a href="analysis-guide.html">Analysis Guide</a> - General analysis concepts</li>
<li><a href="configuration.html">Configuration</a> - Complete configuration reference</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="god-object-detection-1"><a class="header" href="#god-object-detection-1">God Object Detection</a></h1>
<h2 id="overview-3"><a class="header" href="#overview-3">Overview</a></h2>
<p>Debtmap includes sophisticated god object detection that identifies files and types that have grown too large and taken on too many responsibilities. God objects (also called â€œgod classesâ€ or â€œgod modulesâ€) are a significant source of technical debt as they:</p>
<ul>
<li>Violate the Single Responsibility Principle</li>
<li>Become difficult to maintain and test</li>
<li>Create bottlenecks in development</li>
<li>Increase the risk of bugs due to high coupling</li>
<li>Have high coupling with many other modules</li>
<li>Are hard to test effectively</li>
</ul>
<p>This chapter explains how Debtmap identifies god objects, calculates their scores, and provides actionable refactoring recommendations.</p>
<h2 id="detection-criteria"><a class="header" href="#detection-criteria">Detection Criteria</a></h2>
<p>A file or type is classified as a god object based on five key metrics:</p>
<ol>
<li><strong>Method Count</strong> - Total number of methods/functions</li>
<li><strong>Field Count</strong> - Number of struct/class fields</li>
<li><strong>Responsibility Count</strong> - Distinct responsibilities inferred from method names (max_traits in config)</li>
<li><strong>Lines of Code</strong> - Total lines in the file</li>
<li><strong>Complexity Sum</strong> - Combined cyclomatic complexity of all functions</li>
</ol>
<h3 id="language-specific-thresholds"><a class="header" href="#language-specific-thresholds">Language-Specific Thresholds</a></h3>
<h4 id="rust"><a class="header" href="#rust">Rust</a></h4>
<ul>
<li><strong>Max Methods</strong>: 20 (includes both impl methods and standalone functions)</li>
<li><strong>Max Fields</strong>: 15</li>
<li><strong>Max Responsibilities</strong>: 5</li>
<li><strong>Max Lines</strong>: 1000</li>
<li><strong>Max Complexity</strong>: 200</li>
</ul>
<h4 id="python"><a class="header" href="#python">Python</a></h4>
<ul>
<li><strong>Max Methods</strong>: 15</li>
<li><strong>Max Fields</strong>: 10</li>
<li><strong>Max Responsibilities</strong>: 3</li>
<li><strong>Max Lines</strong>: 500</li>
<li><strong>Max Complexity</strong>: 150</li>
</ul>
<h4 id="javascripttypescript"><a class="header" href="#javascripttypescript">JavaScript/TypeScript</a></h4>
<ul>
<li><strong>Max Methods</strong>: 15</li>
<li><strong>Max Fields</strong>: 20</li>
<li><strong>Max Responsibilities</strong>: 3</li>
<li><strong>Max Lines</strong>: 500</li>
<li><strong>Max Complexity</strong>: 150</li>
</ul>
<p>These thresholds can be customized per-language in your <code>.debtmap.toml</code> configuration file.</p>
<h2 id="file-level-aggregation"><a class="header" href="#file-level-aggregation">File-Level Aggregation</a></h2>
<p>An important feature of Debtmapâ€™s god object detection is its <strong>file-level aggregation strategy</strong>. When analyzing a file, Debtmap:</p>
<ol>
<li>Finds the largest type (struct/class) by <code>method_count + field_count Ã— 2</code></li>
<li>Counts standalone functions in the file</li>
<li>Combines them: <code>total_methods = type_methods + standalone_functions</code></li>
</ol>
<p>This means files with many standalone functions (like <code>rust_call_graph.rs</code> with 270 functions) will be detected as god objects even without a large type. This is crucial for identifying:</p>
<ul>
<li>Pure functional modules with excessive functions</li>
<li>Utility files that have grown too large</li>
<li>Mixed paradigm files (structs + many helper functions)</li>
</ul>
<p><strong>Example:</strong> A file containing a struct with 15 methods plus 10 standalone functions will be analyzed as having 25 total methods, likely triggering god object detection.</p>
<p>See <code>src/organization/god_object_detector.rs:66-97</code> for implementation details.</p>
<h2 id="confidence-levels"><a class="header" href="#confidence-levels">Confidence Levels</a></h2>
<p>Debtmap assigns confidence levels based on how many thresholds are violated:</p>
<ul>
<li><strong>Definite</strong> (5 violations) - Clear god object requiring immediate refactoring</li>
<li><strong>Probable</strong> (3-4 violations) - Likely god object that should be refactored</li>
<li><strong>Possible</strong> (1-2 violations) - Potential god object worth reviewing</li>
<li><strong>NotGodObject</strong> (0 violations) - Within acceptable limits</li>
</ul>
<p>The final determination also requires <code>god_object_score &gt;= 70.0</code>. Both criteria must be met for a definite god object classification.</p>
<p>See <code>src/organization/god_object_analysis.rs:229-270</code> and <code>src/organization/god_object_detector.rs:152-163</code>.</p>
<h2 id="scoring-algorithms"><a class="header" href="#scoring-algorithms">Scoring Algorithms</a></h2>
<p>Debtmap provides three scoring algorithms to accommodate different analysis needs.</p>
<h3 id="simple-scoring"><a class="header" href="#simple-scoring">Simple Scoring</a></h3>
<p>The base scoring algorithm calculates god object score using four factors:</p>
<pre><code>method_factor = min(method_count / max_methods, 3.0)
field_factor = min(field_count / max_fields, 3.0)
responsibility_factor = min(responsibility_count / 3, 3.0)
size_factor = min(lines_of_code / max_lines, 3.0)

base_score = method_factor Ã— field_factor Ã— responsibility_factor Ã— size_factor
</code></pre>
<p><strong>Score Enforcement:</strong></p>
<ul>
<li>If <code>violation_count &gt; 0</code>: <code>final_score = max(base_score Ã— 50 Ã— violation_count, 100)</code></li>
<li>Else: <code>final_score = base_score Ã— 10</code></li>
</ul>
<p>The minimum score of 100 ensures that any god object receives sufficient priority in the technical debt analysis.</p>
<h3 id="complexity-weighted-scoring"><a class="header" href="#complexity-weighted-scoring">Complexity-Weighted Scoring</a></h3>
<p>Unlike raw method counting, this algorithm weights each method by its cyclomatic complexity. This ensures that 100 simple functions (complexity 1-3) score better than 10 highly complex functions (complexity 17+).</p>
<p>The formula is similar to simple scoring, but uses <code>weighted_method_count</code> (sum of complexity weights) instead of raw counts:</p>
<pre><code>method_factor = min(weighted_method_count / max_methods, 3.0)
</code></pre>
<p>Additionally, a <strong>complexity factor</strong> is applied:</p>
<ul>
<li>Average complexity &lt; 3.0: <code>0.7</code> (reward simple functions)</li>
<li>Average complexity &gt; 10.0: <code>1.5</code> (penalize complex functions)</li>
<li>Otherwise: <code>1.0</code></li>
</ul>
<p>The final score becomes:</p>
<pre><code>final_score = max(base_score Ã— 50 Ã— complexity_factor Ã— violation_count, 100)
</code></pre>
<p>This approach better reflects the true maintainability burden of a large module.</p>
<p>See <code>src/organization/god_object_analysis.rs:142-209</code>.</p>
<h3 id="purity-weighted-scoring-advanced"><a class="header" href="#purity-weighted-scoring-advanced">Purity-Weighted Scoring (Advanced)</a></h3>
<p><strong>Available for Rust only</strong> (requires <code>syn::ItemFn</code> analysis)</p>
<p>This advanced scoring variant reduces the impact of pure functions, preventing pure functional modules from being unfairly penalized. The algorithm:</p>
<ol>
<li>
<p>Analyzes each function for purity using three levels:</p>
<ul>
<li><strong>Pure</strong> (no side effects): weight multiplier <code>0.3</code></li>
<li><strong>Probably Pure</strong> (likely no side effects): weight multiplier <code>0.5</code></li>
<li><strong>Impure</strong> (has side effects): weight multiplier <code>1.0</code></li>
</ul>
</li>
<li>
<p>Combines complexity and purity weights:</p>
<pre><code>total_weight = complexity_weight Ã— purity_multiplier
</code></pre>
</li>
<li>
<p>Tracks the <code>PurityDistribution</code>:</p>
<ul>
<li><code>pure_count</code>, <code>probably_pure_count</code>, <code>impure_count</code></li>
<li><code>pure_weight_contribution</code>, <code>probably_pure_weight_contribution</code>, <code>impure_weight_contribution</code></li>
</ul>
</li>
</ol>
<p>This approach dramatically reduces scores for files with many pure helper functions while still flagging stateful god objects.</p>
<p>See <code>src/organization/god_object_detector.rs:196-258</code> and <code>src/organization/purity_analyzer.rs</code>.</p>
<h2 id="responsibility-detection"><a class="header" href="#responsibility-detection">Responsibility Detection</a></h2>
<p>Responsibilities are inferred from method names using common prefixes. Debtmap recognizes 28 standard prefixes grouped into 10 categories:</p>
<div class="table-wrapper"><table><thead><tr><th>Prefix(es)</th><th>Responsibility Category</th></tr></thead><tbody>
<tr><td><code>get</code>, <code>set</code></td><td>Data Access</td></tr>
<tr><td><code>calculate</code>, <code>compute</code></td><td>Computation</td></tr>
<tr><td><code>validate</code>, <code>check</code>, <code>verify</code>, <code>ensure</code></td><td>Validation</td></tr>
<tr><td><code>save</code>, <code>load</code>, <code>store</code>, <code>retrieve</code>, <code>fetch</code></td><td>Persistence</td></tr>
<tr><td><code>create</code>, <code>build</code>, <code>new</code>, <code>make</code>, <code>init</code></td><td>Construction</td></tr>
<tr><td><code>send</code>, <code>receive</code>, <code>handle</code>, <code>manage</code></td><td>Communication</td></tr>
<tr><td><code>update</code>, <code>modify</code>, <code>change</code>, <code>edit</code></td><td>Modification</td></tr>
<tr><td><code>delete</code>, <code>remove</code>, <code>clear</code>, <code>reset</code></td><td>Deletion</td></tr>
<tr><td><code>is</code>, <code>has</code>, <code>can</code>, <code>should</code>, <code>will</code></td><td>State Query</td></tr>
<tr><td><code>process</code>, <code>transform</code></td><td>Processing</td></tr>
</tbody></table>
</div>
<p><strong>Fallback:</strong> If a prefix doesnâ€™t match any category, Debtmap creates a default responsibility: <code>"{Prefix} Operations"</code> (with capitalized first letter).</p>
<p>Responsibility count directly affects:</p>
<ul>
<li>God object scoring (via <code>responsibility_factor</code>)</li>
<li>Refactoring recommendations (methods grouped by responsibility)</li>
</ul>
<p>See <code>src/organization/god_object_detector.rs:378-454</code>.</p>
<h2 id="examples-and-case-studies"><a class="header" href="#examples-and-case-studies">Examples and Case Studies</a></h2>
<h3 id="example-1-large-rust-module"><a class="header" href="#example-1-large-rust-module">Example 1: Large Rust Module</a></h3>
<p><strong>File:</strong> <code>rust_call_graph.rs</code> with 270 standalone functions</p>
<p><strong>Detection:</strong></p>
<ul>
<li><strong>Is God Object:</strong> Yes</li>
<li><strong>Method Count:</strong> 270</li>
<li><strong>Field Count:</strong> 0 (no struct)</li>
<li><strong>Responsibilities:</strong> 8</li>
<li><strong>Confidence:</strong> Definite</li>
<li><strong>Score:</strong> &gt;1000 (severe violation)</li>
</ul>
<p><strong>Recommendation:</strong> Break into multiple focused modules:</p>
<ul>
<li><code>CallGraphBuilder</code> (construction methods)</li>
<li><code>CallGraphAnalyzer</code> (analysis methods)</li>
<li><code>CallGraphFormatter</code> (output methods)</li>
</ul>
<h3 id="example-2-complex-python-class"><a class="header" href="#example-2-complex-python-class">Example 2: Complex Python Class</a></h3>
<p><strong>File:</strong> <code>data_manager.py</code> with class containing 25 methods and 12 fields</p>
<p><strong>Detection:</strong></p>
<ul>
<li><strong>Is God Object:</strong> Yes</li>
<li><strong>Method Count:</strong> 25</li>
<li><strong>Field Count:</strong> 12</li>
<li><strong>Responsibilities:</strong> 6 (Data Access, Validation, Persistence, etc.)</li>
<li><strong>Confidence:</strong> Probable</li>
<li><strong>Score:</strong> ~150-200</li>
</ul>
<p><strong>Recommendation:</strong> Split by responsibility:</p>
<ul>
<li><code>DataAccessLayer</code> (get/set methods)</li>
<li><code>DataValidator</code> (validate/check methods)</li>
<li><code>DataPersistence</code> (save/load methods)</li>
</ul>
<h3 id="example-3-mixed-paradigm-file"><a class="header" href="#example-3-mixed-paradigm-file">Example 3: Mixed Paradigm File</a></h3>
<p><strong>File:</strong> <code>utils.rs</code> with small struct (5 methods, 3 fields) + 20 standalone functions</p>
<p><strong>Detection:</strong></p>
<ul>
<li><strong>Is God Object:</strong> Yes</li>
<li><strong>Total Methods:</strong> 25 (5 + 20)</li>
<li><strong>Field Count:</strong> 3</li>
<li><strong>Confidence:</strong> Probable</li>
<li><strong>Score:</strong> ~120</li>
</ul>
<p><strong>Note:</strong> Without file-level aggregation, this would be missed. The struct alone is fine, but combined with standalone functions, it indicates an overgrown utility module.</p>
<h2 id="refactoring-recommendations"><a class="header" href="#refactoring-recommendations">Refactoring Recommendations</a></h2>
<p>When <code>is_god_object = true</code>, Debtmap generates <strong>recommended module splits</strong> using the <code>recommend_module_splits</code> function. This feature:</p>
<ol>
<li>
<p>Groups methods by their inferred responsibilities</p>
</li>
<li>
<p>Creates a <code>ModuleSplit</code> for each responsibility group containing:</p>
<ul>
<li><code>suggested_name</code> (e.g., â€œDataAccessManagerâ€, â€œValidationManagerâ€)</li>
<li><code>methods_to_move</code> (list of method names)</li>
<li><code>responsibility</code> (category name)</li>
<li><code>estimated_lines</code> (approximate LOC for the new module)</li>
</ul>
</li>
<li>
<p>Orders splits by cohesion (most focused responsibility groups first)</p>
</li>
</ol>
<p><strong>Example output:</strong></p>
<pre><code>Recommended Splits:
  1. DataAccessManager (12 methods, ~150 lines)
  2. ValidationManager (8 methods, ~100 lines)
  3. PersistenceManager (5 methods, ~75 lines)
</code></pre>
<p>This provides an actionable roadmap for breaking down god objects into focused, single-responsibility modules.</p>
<p>See <code>src/organization/god_object_detector.rs:165-177</code> and <code>src/organization/god_object_analysis.rs:40-45</code>.</p>
<h3 id="code-examples"><a class="header" href="#code-examples">Code Examples</a></h3>
<h4 id="split-by-responsibility"><a class="header" href="#split-by-responsibility">Split by Responsibility</a></h4>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Before: UserManager (god object)
struct UserManager { ... }

// After: Split into focused modules
struct AuthService { ... }
struct ProfileService { ... }
struct PermissionService { ... }
struct NotificationService { ... }
<span class="boring">}</span></code></pre></pre>
<h4 id="extract-common-functionality"><a class="header" href="#extract-common-functionality">Extract Common Functionality</a></h4>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Extract shared dependencies
struct ServiceContext {
    db: Database,
    cache: Cache,
    logger: Logger,
}

// Each service gets a reference
struct AuthService&lt;'a&gt; {
    context: &amp;'a ServiceContext,
}
<span class="boring">}</span></code></pre></pre>
<h4 id="use-composition"><a class="header" href="#use-composition">Use Composition</a></h4>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Compose services instead of inheriting
struct UserFacade {
    auth: AuthService,
    profile: ProfileService,
    permissions: PermissionService,
}

impl UserFacade {
    fn login(&amp;mut self, credentials: Credentials) -&gt; Result&lt;Session&gt; {
        self.auth.login(credentials)
    }
}
<span class="boring">}</span></code></pre></pre>
<h2 id="configuration-5"><a class="header" href="#configuration-5">Configuration</a></h2>
<h3 id="toml-configuration"><a class="header" href="#toml-configuration">TOML Configuration</a></h3>
<p>Add a <code>[god_object_detection]</code> section to your <code>.debtmap.toml</code>:</p>
<pre><code class="language-toml">[god_object_detection]
enabled = true

[god_object_detection.rust]
max_methods = 20
max_fields = 15
max_traits = 5      # max_traits = max responsibilities
max_lines = 1000
max_complexity = 200

[god_object_detection.python]
max_methods = 15
max_fields = 10
max_traits = 3
max_lines = 500
max_complexity = 150

[god_object_detection.javascript]
max_methods = 15
max_fields = 20
max_traits = 3
max_lines = 500
max_complexity = 150
</code></pre>
<p><strong>Note:</strong> <code>enabled</code> defaults to <code>true</code>. Set to <code>false</code> to disable god object detection entirely (equivalent to <code>--no-god-object</code> CLI flag).</p>
<p>See <code>src/config.rs:500-582</code>.</p>
<h3 id="tuning-for-your-project-1"><a class="header" href="#tuning-for-your-project-1">Tuning for Your Project</a></h3>
<p><strong>Strict mode (smaller modules):</strong></p>
<pre><code class="language-toml">[god_object_detection.rust]
max_methods = 15
max_fields = 10
max_traits = 3
</code></pre>
<p><strong>Lenient mode (larger modules acceptable):</strong></p>
<pre><code class="language-toml">[god_object_detection.rust]
max_methods = 30
max_fields = 20
max_traits = 7
</code></pre>
<h3 id="cli-options"><a class="header" href="#cli-options">CLI Options</a></h3>
<p>Debtmap provides several CLI flags to control god object detection behavior:</p>
<h4 id="--no-god-object"><a class="header" href="#--no-god-object"><code>--no-god-object</code></a></h4>
<p>Disables god object detection entirely.</p>
<pre><code class="language-bash">debtmap analyze . --no-god-object
</code></pre>
<p><strong>Use case:</strong> When you only want function-level complexity analysis without file-level aggregation.</p>
<h4 id="--aggregate-only"><a class="header" href="#--aggregate-only"><code>--aggregate-only</code></a></h4>
<p>Shows only file-level god object scores, hiding individual function details.</p>
<pre><code class="language-bash">debtmap analyze . --aggregate-only
</code></pre>
<p><strong>Use case:</strong> High-level overview of which files are god objects without function-by-function breakdowns.</p>
<h4 id="--no-aggregation"><a class="header" href="#--no-aggregation"><code>--no-aggregation</code></a></h4>
<p>Disables file-level aggregation, showing only individual function metrics.</p>
<pre><code class="language-bash">debtmap analyze . --no-aggregation
</code></pre>
<p><strong>Use case:</strong> Detailed function-level analysis without combining into file scores.</p>
<h4 id="--aggregation-method-method"><a class="header" href="#--aggregation-method-method"><code>--aggregation-method &lt;METHOD&gt;</code></a></h4>
<p>Chooses how to combine function scores into file-level scores:</p>
<ul>
<li><code>sum</code> - Add all function scores</li>
<li><code>weighted_sum</code> - Weight by complexity (default)</li>
<li><code>logarithmic_sum</code> - Logarithmic scaling for large files</li>
<li><code>max_plus_average</code> - Max score + average of others</li>
</ul>
<pre><code class="language-bash">debtmap analyze . --aggregation-method logarithmic_sum
</code></pre>
<h4 id="--min-problematic-n"><a class="header" href="#--min-problematic-n"><code>--min-problematic &lt;N&gt;</code></a></h4>
<p>Sets minimum number of problematic functions required for file-level aggregation.</p>
<pre><code class="language-bash">debtmap analyze . --min-problematic 3
</code></pre>
<p><strong>Use case:</strong> Avoid flagging files with only 1-2 complex functions as god objects.</p>
<p>See <code>features.json:65-71</code> and <code>features.json:507-512</code>.</p>
<h2 id="output-display"><a class="header" href="#output-display">Output Display</a></h2>
<h3 id="file-level-display"><a class="header" href="#file-level-display">File-Level Display</a></h3>
<p>When a god object is detected, Debtmap displays:</p>
<pre><code>âš ï¸ God Object: 270 methods, 0 fields, 8 responsibilities
Score: 1350 (Confidence: Definite)
</code></pre>
<h3 id="function-level-display"><a class="header" href="#function-level-display">Function-Level Display</a></h3>
<p>Within a god object file, individual functions show:</p>
<pre><code>â”œâ”€ âš ï¸ God Object: 45 methods, 20 fields, 5 responsibilities
â”‚      Score: 250 (Confidence: Probable)
</code></pre>
<p>The <code>âš ï¸ God Object</code> indicator makes it immediately clear which files need architectural refactoring.</p>
<h2 id="integration-with-file-level-scoring"><a class="header" href="#integration-with-file-level-scoring">Integration with File-Level Scoring</a></h2>
<p>God object detection affects the overall technical debt prioritization through a <strong>god object multiplier</strong>:</p>
<pre><code>god_object_multiplier = 2.0 + normalized_god_object_score
</code></pre>
<p>Where <code>normalized_god_object_score</code> is scaled to 0-1 range.</p>
<p>This means:</p>
<ol>
<li>God objects receive <strong>2-3Ã— higher priority</strong> in debt rankings</li>
<li>Functions within god objects may inherit elevated scores due to architectural concerns</li>
<li>The cascading impact ensures god objects surface in the â€œtop 10 most problematicâ€ lists</li>
</ol>
<p>This integration ensures that architectural debt (god objects) is weighted appropriately alongside function-level complexity.</p>
<p>See <code>features.json:570</code> and file-level scoring documentation.</p>
<h2 id="metrics-tracking-advanced"><a class="header" href="#metrics-tracking-advanced">Metrics Tracking (Advanced)</a></h2>
<p>For teams tracking god object evolution over time, Debtmap provides <code>GodObjectMetrics</code> with:</p>
<ul>
<li><strong>Snapshots</strong> - Historical god object data per file</li>
<li><strong>Trends</strong> - Improving/Stable/Worsening classification (based on Â±10 point score changes)</li>
<li><strong>New God Objects</strong> - Files that crossed the threshold</li>
<li><strong>Resolved God Objects</strong> - Files that were refactored below thresholds</li>
</ul>
<p>This enables longitudinal analysis: â€œAre we reducing god objects sprint-over-sprint?â€</p>
<p>See <code>src/organization/god_object_metrics.rs:1-228</code>.</p>
<h2 id="troubleshooting-6"><a class="header" href="#troubleshooting-6">Troubleshooting</a></h2>
<h3 id="why-is-my-functional-module-flagged-as-a-god-object"><a class="header" href="#why-is-my-functional-module-flagged-as-a-god-object">â€œWhy is my functional module flagged as a god object?â€</a></h3>
<p><strong>Answer:</strong> Debtmap aggregates standalone functions with struct methods. A file with 100 pure helper functions will be flagged, even though each function is simple.</p>
<p><strong>Solutions:</strong></p>
<ol>
<li>Use <strong>purity-weighted scoring</strong> (Rust only) via complexity-weighted analysis - pure functions contribute 0.3Ã— weight</li>
<li>Split the module into smaller, focused utility modules</li>
<li>Use <code>--min-problematic</code> to raise the threshold for file-level aggregation</li>
</ol>
<h3 id="my-god-object-score-seems-too-high"><a class="header" href="#my-god-object-score-seems-too-high">â€œMy god object score seems too highâ€</a></h3>
<p><strong>Answer:</strong> The scoring algorithm uses exponential scaling (<code>base_score Ã— 50 Ã— violation_count</code>) to ensure god objects are prioritized.</p>
<p><strong>Solutions:</strong></p>
<ol>
<li>Check the violation count - 5 violations means severe issues</li>
<li>Review each metric - are method count, field count, responsibilities, LOC, and complexity all high?</li>
<li>Consider if the score accurately reflects maintainability burden</li>
</ol>
<h3 id="can-i-disable-god-object-detection-for-specific-files"><a class="header" href="#can-i-disable-god-object-detection-for-specific-files">â€œCan I disable god object detection for specific files?â€</a></h3>
<p><strong>Answer:</strong> Currently, god object detection is global. However, you can:</p>
<ol>
<li>Use <code>--no-god-object</code> to disable entirely</li>
<li>Use <code>--no-aggregation</code> to skip file-level analysis</li>
<li>Adjust thresholds in <code>.debtmap.toml</code> to be more lenient</li>
</ol>
<h2 id="best-practices-6"><a class="header" href="#best-practices-6">Best Practices</a></h2>
<p>To avoid god objects:</p>
<ol>
<li><strong>Follow Single Responsibility Principle</strong> - Each module should have one clear purpose</li>
<li><strong>Regular Refactoring</strong> - Split modules before they reach thresholds</li>
<li><strong>Monitor Growth</strong> - Track method and field counts as modules evolve</li>
<li><strong>Use Composition</strong> - Prefer smaller, composable units over large monoliths</li>
<li><strong>Clear Boundaries</strong> - Define clear module interfaces and responsibilities</li>
<li><strong>Leverage Purity</strong> - Keep pure functions separate from stateful logic (reduces scores in Rust)</li>
<li><strong>Set Project Thresholds</strong> - Customize <code>.debtmap.toml</code> to match your teamâ€™s standards</li>
</ol>
<h2 id="configuration-tradeoffs"><a class="header" href="#configuration-tradeoffs">Configuration Tradeoffs</a></h2>
<p><strong>Strict Thresholds</strong> (e.g., Rust: 10 methods):</p>
<ul>
<li>âœ… Catch problems early</li>
<li>âœ… Enforce strong modularity</li>
<li>âŒ May flag legitimate large modules</li>
<li>âŒ More noise in reports</li>
</ul>
<p><strong>Lenient Thresholds</strong> (e.g., Rust: 50 methods):</p>
<ul>
<li>âœ… Reduce false positives</li>
<li>âœ… Focus on egregious violations</li>
<li>âŒ Miss real god objects</li>
<li>âŒ Allow technical debt to grow</li>
</ul>
<p><strong>Recommended:</strong> Start with defaults, then adjust based on your codebaseâ€™s characteristics. Use metrics tracking to monitor trends over time.</p>
<h2 id="related-documentation"><a class="header" href="#related-documentation">Related Documentation</a></h2>
<ul>
<li><a href="./file-level-scoring.html">File-Level Scoring</a> - How god objects affect overall file scores</li>
<li><a href="./configuration.html">Configuration</a> - Complete <code>.debtmap.toml</code> reference</li>
<li><a href="./cli-reference.html">CLI Reference</a> - All command-line options</li>
<li><a href="./tiered-prioritization.html">Tiered Prioritization</a> - How god objects are prioritized</li>
</ul>
<h2 id="summary-2"><a class="header" href="#summary-2">Summary</a></h2>
<p>God object detection is a powerful architectural analysis feature that:</p>
<ul>
<li>Identifies files/types violating single responsibility principle</li>
<li>Provides multiple scoring algorithms (simple, complexity-weighted, purity-weighted)</li>
<li>Generates actionable refactoring recommendations</li>
<li>Integrates with file-level scoring for holistic debt prioritization</li>
<li>Supports customization via TOML config and CLI flags</li>
</ul>
<p>By combining quantitative metrics (method count, LOC, complexity) with qualitative analysis (responsibility detection, purity), Debtmap helps teams systematically address architectural debt.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="parallel-processing"><a class="header" href="#parallel-processing">Parallel Processing</a></h1>
<p>Debtmap leverages Rustâ€™s powerful parallel processing capabilities to analyze large codebases efficiently. Built on Rayon for data parallelism and DashMap for lock-free concurrent data structures, debtmap achieves 10-100x faster performance than Java/Python-based competitors.</p>
<h2 id="overview-4"><a class="header" href="#overview-4">Overview</a></h2>
<p>Debtmapâ€™s parallel processing architecture uses a three-phase approach:</p>
<ol>
<li><strong>Parallel File Parsing</strong> - Parse source files concurrently across all available CPU cores</li>
<li><strong>Parallel Multi-File Extraction</strong> - Extract call graphs from parsed files in parallel</li>
<li><strong>Parallel Enhanced Analysis</strong> - Analyze trait dispatch, function pointers, and framework patterns</li>
</ol>
<p>This parallel pipeline is controlled by CLI flags that let you tune performance for your environment.</p>
<h2 id="performance-characteristics-3"><a class="header" href="#performance-characteristics-3">Performance Characteristics</a></h2>
<p><strong>Typical analysis times:</strong></p>
<ul>
<li>Small project (1k-5k LOC): &lt;1 second</li>
<li>Medium project (10k-50k LOC): 2-8 seconds</li>
<li>Large project (100k-500k LOC): 10-45 seconds</li>
</ul>
<p><strong>Comparison with other tools (medium-sized Rust project, ~50k LOC):</strong></p>
<ul>
<li>SonarQube: 3-4 minutes</li>
<li>CodeClimate: 2-3 minutes</li>
<li>Debtmap: 5-8 seconds</li>
</ul>
<h2 id="cli-flags-for-parallelization"><a class="header" href="#cli-flags-for-parallelization">CLI Flags for Parallelization</a></h2>
<p>Debtmap provides two flags to control parallel processing behavior:</p>
<h3 id="jobs---j"><a class="header" href="#jobs---j">â€“jobs / -j</a></h3>
<p>Control the number of worker threads for parallel processing:</p>
<pre><code class="language-bash"># Use all available CPU cores (default)
debtmap analyze --jobs 0

# Limit to 4 threads
debtmap analyze --jobs 4
debtmap analyze -j 4
</code></pre>
<p><strong>Behavior:</strong></p>
<ul>
<li><code>--jobs 0</code> (default): Auto-detects available CPU cores using <code>std::thread::available_parallelism()</code>. Falls back to 4 threads if detection fails.</li>
<li><code>--jobs N</code>: Explicitly sets the thread pool to N threads.</li>
</ul>
<p><strong>When to use:</strong></p>
<ul>
<li>Use <code>--jobs 0</code> for maximum performance on developer workstations</li>
<li>Use <code>--jobs 1-4</code> in memory-constrained environments like CI/CD</li>
<li>Use <code>--jobs 1</code> for deterministic analysis order during debugging</li>
</ul>
<p><strong>Environment Variable:</strong></p>
<p>You can also set the default via the <code>RAYON_NUM_THREADS</code> or <code>DEBTMAP_JOBS</code> environment variable:</p>
<pre><code class="language-bash">export RAYON_NUM_THREADS=4
debtmap analyze  # Uses 4 threads
</code></pre>
<p>The CLI flag takes precedence over the environment variable.</p>
<h3 id="no-parallel"><a class="header" href="#no-parallel">â€“no-parallel</a></h3>
<p>Disable parallel call graph construction entirely:</p>
<pre><code class="language-bash">debtmap analyze --no-parallel
</code></pre>
<p><strong>When to use:</strong></p>
<ul>
<li><strong>Debugging concurrency issues</strong>: Isolate whether a problem is parallelism-related</li>
<li><strong>Memory-constrained environments</strong>: Parallel processing increases memory usage</li>
<li><strong>Deterministic analysis</strong>: Ensures consistent ordering for reproducibility</li>
</ul>
<p><strong>Performance Impact:</strong></p>
<p>Disabling parallelization significantly increases analysis time:</p>
<ul>
<li>Small projects (&lt; 100 files): 2-3x slower</li>
<li>Medium projects (100-1000 files): 5-10x slower</li>
<li>Large projects (&gt; 1000 files): 10-50x slower</li>
</ul>
<p>For more details on both flags, see the <a href="./cli-reference.html#performance--caching">CLI Reference</a>.</p>
<h2 id="rayon-parallel-iterators"><a class="header" href="#rayon-parallel-iterators">Rayon Parallel Iterators</a></h2>
<p>Debtmap uses <a href="https://docs.rs/rayon">Rayon</a>, a data parallelism library for Rust, to parallelize file processing operations.</p>
<h3 id="thread-pool-configuration"><a class="header" href="#thread-pool-configuration">Thread Pool Configuration</a></h3>
<p>The global Rayon thread pool is configured at startup based on the <code>--jobs</code> parameter:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// From src/builders/parallel_call_graph.rs:46-52
if self.config.num_threads &gt; 0 {
    rayon::ThreadPoolBuilder::new()
        .num_threads(self.config.num_threads)
        .build_global()
        .ok(); // Ignore if already configured
}
<span class="boring">}</span></code></pre></pre>
<p>This configures Rayon to use a specific number of worker threads for all parallel operations throughout the analysis.</p>
<h3 id="worker-thread-selection"><a class="header" href="#worker-thread-selection">Worker Thread Selection</a></h3>
<p>The <code>get_worker_count()</code> function determines how many threads to use:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// From src/main.rs:660-669
fn get_worker_count(jobs: usize) -&gt; usize {
    if jobs == 0 {
        std::thread::available_parallelism()
            .map(|n| n.get())
            .unwrap_or(4)  // Fallback if detection fails
    } else {
        jobs  // Use explicit value
    }
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Auto-detection behavior:</strong></p>
<ul>
<li>Queries the OS for available parallelism (CPU cores)</li>
<li>Respects cgroup limits in containers (Docker, Kubernetes)</li>
<li>Falls back to 4 threads if detection fails (rare)</li>
</ul>
<p><strong>Manual configuration:</strong></p>
<ul>
<li>Useful in shared environments (CI/CD, shared build servers)</li>
<li>Prevents resource contention with other processes</li>
<li>Enables reproducible benchmarking</li>
</ul>
<h3 id="parallel-file-processing"><a class="header" href="#parallel-file-processing">Parallel File Processing</a></h3>
<p><strong>Phase 1: Parallel File Parsing</strong></p>
<p>Files are parsed concurrently using Rayonâ€™s parallel iterators:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// From src/builders/parallel_call_graph.rs:98-128
let parsed_files: Vec&lt;_&gt; = rust_files
    .par_iter()  // Convert to parallel iterator
    .filter_map(|file_path| {
        let content = io::read_file(file_path).ok()?;

        // Update progress atomically
        parallel_graph.stats().increment_files();

        Some((file_path.clone(), content))
    })
    .collect();
<span class="boring">}</span></code></pre></pre>
<p>Key features:</p>
<ul>
<li><code>.par_iter()</code> converts a sequential iterator to a parallel one</li>
<li>Each file is read independently on a worker thread</li>
<li>Progress tracking uses atomic counters (see <a href="parallel-processing.html#parallel-call-graph-statistics">Parallel Call Graph Statistics</a>)</li>
</ul>
<p><strong>Phase 2: Parallel Multi-File Extraction</strong></p>
<p>Files are grouped into chunks for optimal parallelization:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// From src/builders/parallel_call_graph.rs:130-161
let chunk_size = std::cmp::max(10, parsed_files.len() / rayon::current_num_threads());

parsed_files.par_chunks(chunk_size).for_each(|chunk| {
    // Parse syn files within each chunk
    let parsed_chunk: Vec&lt;_&gt; = chunk
        .iter()
        .filter_map(|(path, content)| {
            syn::parse_file(content)
                .ok()
                .map(|parsed| (parsed, path.clone()))
        })
        .collect();

    if !parsed_chunk.is_empty() {
        // Extract call graph for this chunk
        let chunk_graph = extract_call_graph_multi_file(&amp;parsed_chunk);

        // Merge into main graph concurrently
        parallel_graph.merge_concurrent(chunk_graph);
    }
});
<span class="boring">}</span></code></pre></pre>
<p>This chunking strategy balances parallelism with overhead:</p>
<ul>
<li>Minimum chunk size of 10 files prevents excessive overhead</li>
<li>Dynamic chunk sizing based on available threads</li>
<li>Each chunk produces a local call graph thatâ€™s merged concurrently</li>
</ul>
<p><strong>Phase 3: Enhanced Analysis</strong></p>
<p>The third phase analyzes trait dispatch, function pointers, and framework patterns. This phase is currently sequential due to complex shared state requirements, but benefits from the parallel foundation built in phases 1-2.</p>
<h3 id="parallel-architecture"><a class="header" href="#parallel-architecture">Parallel Architecture</a></h3>
<p>Debtmap processes files in parallel using Rayonâ€™s parallel iterators:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>files.par_iter()
    .map(|file| analyze_file(file))
    .collect()
<span class="boring">}</span></code></pre></pre>
<p>Each file is:</p>
<ol>
<li>Parsed independently</li>
<li>Analyzed for complexity</li>
<li>Scored and prioritized</li>
</ol>
<h2 id="dashmap-for-lock-free-concurrency"><a class="header" href="#dashmap-for-lock-free-concurrency">DashMap for Lock-Free Concurrency</a></h2>
<p>Debtmap uses <a href="https://docs.rs/dashmap">DashMap</a>, a concurrent hash map implementation, for lock-free data structures during parallel call graph construction.</p>
<h3 id="why-dashmap"><a class="header" href="#why-dashmap">Why DashMap?</a></h3>
<p>Traditional approaches to concurrent hash maps use a single <code>Mutex&lt;HashMap&gt;</code>, which creates contention:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// âŒ Traditional approach - serializes all access
let map = Arc&lt;Mutex&lt;HashMap&lt;K, V&gt;&gt;&gt;;

// Thread 1 blocks Thread 2, even for reads
let val = map.lock().unwrap().get(&amp;key);
<span class="boring">}</span></code></pre></pre>
<p>DashMap provides <strong>lock-free reads</strong> and <strong>fine-grained write locking</strong> through internal sharding:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// âœ… DashMap approach - concurrent reads, fine-grained writes
let map = Arc&lt;DashMap&lt;K, V&gt;&gt;;

// Multiple threads can read concurrently without blocking
let val = map.get(&amp;key);

// Writes only lock the specific shard, not the whole map
map.insert(key, value);
<span class="boring">}</span></code></pre></pre>
<h3 id="parallelcallgraph-implementation"><a class="header" href="#parallelcallgraph-implementation">ParallelCallGraph Implementation</a></h3>
<p>The <code>ParallelCallGraph</code> uses DashMap for all concurrent data structures:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// From src/priority/parallel_call_graph.rs:49-56
pub struct ParallelCallGraph {
    nodes: Arc&lt;DashMap&lt;FunctionId, NodeInfo&gt;&gt;,      // Functions
    edges: Arc&lt;DashSet&lt;FunctionCall&gt;&gt;,              // Calls
    caller_index: Arc&lt;DashMap&lt;FunctionId, DashSet&lt;FunctionId&gt;&gt;&gt;,  // Who calls this?
    callee_index: Arc&lt;DashMap&lt;FunctionId, DashSet&lt;FunctionId&gt;&gt;&gt;,  // Who does this call?
    stats: Arc&lt;ParallelStats&gt;,                      // Atomic counters
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Key components:</strong></p>
<ol>
<li><strong>nodes</strong>: Maps function identifiers to metadata (complexity, lines, flags)</li>
<li><strong>edges</strong>: Set of all function calls (deduplicated automatically)</li>
<li><strong>caller_index</strong>: Reverse index for â€œwho calls this function?â€</li>
<li><strong>callee_index</strong>: Forward index for â€œwhat does this function call?â€</li>
<li><strong>stats</strong>: Atomic counters for progress tracking</li>
</ol>
<h3 id="concurrent-operations"><a class="header" href="#concurrent-operations">Concurrent Operations</a></h3>
<p><strong>Adding Functions Concurrently</strong></p>
<p>Multiple analyzer threads can add functions simultaneously:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// From src/priority/parallel_call_graph.rs:78-96
pub fn add_function(
    &amp;self,
    id: FunctionId,
    is_entry_point: bool,
    is_test: bool,
    complexity: u32,
    lines: usize,
) {
    let node_info = NodeInfo {
        id: id.clone(),
        is_entry_point,
        is_test,
        complexity,
        lines,
    };
    self.nodes.insert(id, node_info);
    self.stats.add_nodes(1);  // Atomic increment
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Atomicity guarantees:</strong></p>
<ul>
<li><code>DashMap::insert()</code> is atomic - no data races</li>
<li><code>AtomicUsize</code> counters can be incremented from multiple threads safely</li>
<li>No locks required for reading existing nodes</li>
</ul>
<p><strong>Adding Calls Concurrently</strong></p>
<p>Function calls are added with automatic deduplication:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// From src/priority/parallel_call_graph.rs:98-117
pub fn add_call(&amp;self, caller: FunctionId, callee: FunctionId, call_type: CallType) {
    let call = FunctionCall {
        caller: caller.clone(),
        callee: callee.clone(),
        call_type,
    };

    if self.edges.insert(call) {  // DashSet deduplicates automatically
        // Update indices concurrently
        self.caller_index
            .entry(caller.clone())
            .or_default()
            .insert(callee.clone());

        self.callee_index.entry(callee).or_default().insert(caller);

        self.stats.add_edges(1);  // Only increment if actually inserted
    }
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Deduplication:</strong></p>
<ul>
<li><code>DashSet::insert()</code> returns <code>true</code> only for new items</li>
<li>Duplicate calls from multiple threads are safely ignored</li>
<li>Indices are updated atomically using <code>entry()</code> API</li>
</ul>
<h3 id="shared-read-only-data"><a class="header" href="#shared-read-only-data">Shared Read-Only Data</a></h3>
<p>Analysis configuration and indexes are shared across threads:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let coverage_index = Arc::new(build_coverage_index());

// All threads share the same index
files.par_iter()
    .map(|file| analyze_with_coverage(file, &amp;coverage_index))
<span class="boring">}</span></code></pre></pre>
<h3 id="memory-overhead"><a class="header" href="#memory-overhead">Memory Overhead</a></h3>
<p>DashMap uses internal sharding for parallelism, which has a memory overhead:</p>
<ul>
<li><strong>DashMap overhead</strong>: ~2x the memory of a regular <code>HashMap</code> due to sharding</li>
<li><strong>DashSet overhead</strong>: Similar to DashMap</li>
<li><strong>Benefit</strong>: Enables concurrent access without contention</li>
<li><strong>Trade-off</strong>: Debtmap prioritizes speed over memory for large codebases</li>
</ul>
<p>For memory-constrained environments, use <code>--jobs 2-4</code> or <code>--no-parallel</code> to reduce parallel overhead.</p>
<h2 id="parallel-call-graph-statistics"><a class="header" href="#parallel-call-graph-statistics">Parallel Call Graph Statistics</a></h2>
<p>Debtmap tracks parallel processing progress using atomic counters that can be safely updated from multiple threads.</p>
<h3 id="parallelstats-structure"><a class="header" href="#parallelstats-structure">ParallelStats Structure</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// From src/priority/parallel_call_graph.rs:7-47
pub struct ParallelStats {
    pub total_nodes: AtomicUsize,      // Functions processed
    pub total_edges: AtomicUsize,      // Calls discovered
    pub files_processed: AtomicUsize,  // Files completed
    pub total_files: AtomicUsize,      // Total files to process
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Atomic operations:</strong></p>
<ul>
<li><code>fetch_add()</code> - Atomically increment counters from any thread</li>
<li><code>load()</code> - Read current value without blocking</li>
<li><code>Ordering::Relaxed</code> - Sufficient for statistics (no synchronization needed)</li>
</ul>
<h3 id="progress-tracking"><a class="header" href="#progress-tracking">Progress Tracking</a></h3>
<p>Progress ratio calculation for long-running analysis:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// From src/priority/parallel_call_graph.rs:38-46
pub fn progress_ratio(&amp;self) -&gt; f64 {
    let processed = self.files_processed.load(Ordering::Relaxed) as f64;
    let total = self.total_files.load(Ordering::Relaxed) as f64;
    if total &gt; 0.0 {
        processed / total
    } else {
        0.0
    }
}
<span class="boring">}</span></code></pre></pre>
<p>This enables progress callbacks during analysis:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// From src/builders/parallel_call_graph.rs:110-121
parallel_graph.stats().increment_files();
if let Some(ref callback) = self.config.progress_callback {
    let processed = parallel_graph
        .stats()
        .files_processed
        .load(std::sync::atomic::Ordering::Relaxed);
    let total = parallel_graph
        .stats()
        .total_files
        .load(std::sync::atomic::Ordering::Relaxed);
    callback(processed, total);
}
<span class="boring">}</span></code></pre></pre>
<h3 id="log-output-format"><a class="header" href="#log-output-format">Log Output Format</a></h3>
<p>After analysis completes, debtmap reports final statistics:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// From src/builders/parallel_call_graph.rs:84-92
log::info!(
    "Parallel call graph complete: {} nodes, {} edges, {} files processed",
    stats.total_nodes.load(std::sync::atomic::Ordering::Relaxed),
    stats.total_edges.load(std::sync::atomic::Ordering::Relaxed),
    stats
        .files_processed
        .load(std::sync::atomic::Ordering::Relaxed),
);
<span class="boring">}</span></code></pre></pre>
<p><strong>Example output:</strong></p>
<pre><code>INFO - Processing 1247 Rust files in parallel
INFO - Progress: 100/1247 files processed
INFO - Progress: 500/1247 files processed
INFO - Progress: 1000/1247 files processed
INFO - Parallel call graph complete: 8942 nodes, 23451 edges, 1247 files processed
</code></pre>
<h2 id="concurrent-merging"><a class="header" href="#concurrent-merging">Concurrent Merging</a></h2>
<p>The <code>merge_concurrent()</code> method combines call graphs from different analysis phases using parallel iteration.</p>
<h3 id="implementation"><a class="header" href="#implementation">Implementation</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// From src/priority/parallel_call_graph.rs:119-138
pub fn merge_concurrent(&amp;self, other: CallGraph) {
    // Parallelize node merging
    let nodes_vec: Vec&lt;_&gt; = other.get_all_functions().collect();
    nodes_vec.par_iter().for_each(|func_id| {
        if let Some((is_entry, is_test, complexity, lines)) = other.get_function_info(func_id) {
            self.add_function((*func_id).clone(), is_entry, is_test, complexity, lines);
        }
    });

    // Parallelize edge merging
    let calls_vec: Vec&lt;_&gt; = other.get_all_calls();
    calls_vec.par_iter().for_each(|call| {
        self.add_call(
            call.caller.clone(),
            call.callee.clone(),
            call.call_type.clone(),
        );
    });
}
<span class="boring">}</span></code></pre></pre>
<p><strong>How it works:</strong></p>
<ol>
<li>Extract all nodes and edges from the source <code>CallGraph</code></li>
<li>Use <code>par_iter()</code> to merge nodes in parallel</li>
<li>Use <code>par_iter()</code> to merge edges in parallel</li>
<li>DashMap/DashSet automatically handle concurrent insertions</li>
</ol>
<h3 id="converting-between-representations"><a class="header" href="#converting-between-representations">Converting Between Representations</a></h3>
<p>Debtmap uses two call graph representations:</p>
<ul>
<li><strong>ParallelCallGraph</strong>: Concurrent data structures (DashMap/DashSet) for parallel construction</li>
<li><strong>CallGraph</strong>: Sequential data structures (HashMap/HashSet) for analysis algorithms</li>
</ul>
<p>Conversion happens at phase boundaries:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// From src/priority/parallel_call_graph.rs:140-162
pub fn to_call_graph(&amp;self) -&gt; CallGraph {
    let mut call_graph = CallGraph::new();

    // Add all nodes
    for entry in self.nodes.iter() {
        let node = entry.value();
        call_graph.add_function(
            node.id.clone(),
            node.is_entry_point,
            node.is_test,
            node.complexity,
            node.lines,
        );
    }

    // Add all edges
    for call in self.edges.iter() {
        call_graph.add_call(call.clone());
    }

    call_graph
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Why two representations?</strong></p>
<ul>
<li><strong>ParallelCallGraph</strong>: Optimized for concurrent writes during construction</li>
<li><strong>CallGraph</strong>: Optimized for graph algorithms (PageRank, connectivity, transitive reduction)</li>
<li>Conversion overhead is negligible compared to analysis time</li>
</ul>
<h2 id="performance-tuning-2"><a class="header" href="#performance-tuning-2">Performance Tuning</a></h2>
<h3 id="optimal-thread-count"><a class="header" href="#optimal-thread-count">Optimal Thread Count</a></h3>
<p><strong>General rule:</strong> Use physical core count, not logical cores.</p>
<pre><code class="language-bash"># Check physical core count
lscpu | grep "Core(s) per socket"

# macOS
sysctl hw.physicalcpu
</code></pre>
<p><strong>Recommended settings:</strong></p>
<div class="table-wrapper"><table><thead><tr><th>System</th><th>Cores</th><th>Recommended â€“jobs</th></tr></thead><tbody>
<tr><td>Laptop</td><td>4</td><td>Default or 4</td></tr>
<tr><td>Desktop</td><td>8</td><td>Default</td></tr>
<tr><td>Workstation</td><td>16+</td><td>Default</td></tr>
<tr><td>CI/CD</td><td>Varies</td><td>2-4 (shared resources)</td></tr>
</tbody></table>
</div>
<h3 id="memory-considerations"><a class="header" href="#memory-considerations">Memory Considerations</a></h3>
<p>Each thread requires memory for:</p>
<ul>
<li>AST parsing (~1-5 MB per file)</li>
<li>Analysis state (~500 KB per file)</li>
<li>Temporary buffers</li>
</ul>
<p><strong>Memory usage estimate:</strong></p>
<pre><code>Total Memory â‰ˆ (Thread Count) Ã— (Average File Size) Ã— 2-3
</code></pre>
<p><strong>Example (50 files, average 10 KB each, 8 threads):</strong></p>
<pre><code>Memory â‰ˆ 8 Ã— 10 KB Ã— 3 = 240 KB (negligible)
</code></pre>
<p>For very large files (&gt;1 MB), consider reducing thread count.</p>
<h3 id="memory-vs-speed-tradeoffs"><a class="header" href="#memory-vs-speed-tradeoffs">Memory vs Speed Tradeoffs</a></h3>
<p>Parallel processing uses more memory:</p>
<div class="table-wrapper"><table><thead><tr><th>Configuration</th><th>Memory Overhead</th><th>Speed Benefit</th></tr></thead><tbody>
<tr><td><code>--no-parallel</code></td><td>Baseline</td><td>Baseline</td></tr>
<tr><td><code>--jobs 1</code></td><td>+10% (data structures)</td><td>1x</td></tr>
<tr><td><code>--jobs 4</code></td><td>+30% (+ worker buffers)</td><td>4-6x</td></tr>
<tr><td><code>--jobs 8</code></td><td>+50% (+ worker buffers)</td><td>6-10x</td></tr>
<tr><td><code>--jobs 16</code></td><td>+80% (+ worker buffers)</td><td>10-15x</td></tr>
</tbody></table>
</div>
<p><strong>Memory overhead sources:</strong></p>
<ul>
<li>DashMap internal sharding (~2x HashMap)</li>
<li>Per-worker thread stacks and buffers</li>
<li>Parallel iterator intermediates</li>
</ul>
<h3 id="io-bound-vs-cpu-bound"><a class="header" href="#io-bound-vs-cpu-bound">I/O Bound vs CPU Bound</a></h3>
<p><strong>CPU-bound analysis (default):</strong></p>
<ul>
<li>Complexity calculations</li>
<li>Pattern detection</li>
<li>Risk scoring</li>
</ul>
<p>Parallel processing provides 4-8x speedup.</p>
<p><strong>I/O-bound operations:</strong></p>
<ul>
<li>Reading files from disk</li>
<li>Loading coverage data</li>
</ul>
<p>Limited speedup from parallelism (1.5-2x).</p>
<p><strong>If analysis is I/O-bound:</strong></p>
<ol>
<li>Move cache to SSD</li>
<li>Reduce thread count (less I/O contention)</li>
<li>Use <code>--max-files</code> to limit scope</li>
</ol>
<h2 id="scaling-strategies"><a class="header" href="#scaling-strategies">Scaling Strategies</a></h2>
<h3 id="small-projects-10k-loc"><a class="header" href="#small-projects-10k-loc">Small Projects (&lt;10k LOC)</a></h3>
<pre><code class="language-bash"># Default settings are fine
debtmap analyze .
</code></pre>
<p>Parallel overhead may exceed benefits. Consider <code>--no-parallel</code> if analysis is &lt;1 second.</p>
<h3 id="medium-projects-10k-100k-loc"><a class="header" href="#medium-projects-10k-100k-loc">Medium Projects (10k-100k LOC)</a></h3>
<pre><code class="language-bash"># Use all cores
debtmap analyze .
</code></pre>
<p>Optimal parallel efficiency. Expect 4-8x speedup from parallelism.</p>
<h3 id="large-projects-100k-loc"><a class="header" href="#large-projects-100k-loc">Large Projects (&gt;100k LOC)</a></h3>
<pre><code class="language-bash"># Use all cores with optimized cache
export DEBTMAP_CACHE_MAX_SIZE=5368709120  # 5GB
debtmap analyze . --jobs 0  # 0 = all cores
</code></pre>
<p>Maximize cache size to avoid re-analysis.</p>
<h3 id="cicd-environments"><a class="header" href="#cicd-environments">CI/CD Environments</a></h3>
<pre><code class="language-bash"># Limit threads to avoid resource contention
debtmap analyze . --jobs 2
</code></pre>
<p>CI environments often limit CPU cores per job.</p>
<h3 id="scaling-behavior"><a class="header" href="#scaling-behavior">Scaling Behavior</a></h3>
<p>Debtmapâ€™s parallel processing scales with CPU core count:</p>
<p><strong>Strong Scaling (Fixed Problem Size):</strong></p>
<div class="table-wrapper"><table><thead><tr><th>CPU Cores</th><th>Speedup</th><th>Efficiency</th></tr></thead><tbody>
<tr><td>1</td><td>1x</td><td>100%</td></tr>
<tr><td>2</td><td>1.8x</td><td>90%</td></tr>
<tr><td>4</td><td>3.4x</td><td>85%</td></tr>
<tr><td>8</td><td>6.2x</td><td>78%</td></tr>
<tr><td>16</td><td>10.5x</td><td>66%</td></tr>
<tr><td>32</td><td>16.8x</td><td>53%</td></tr>
</tbody></table>
</div>
<p>Efficiency decreases at higher core counts due to:</p>
<ul>
<li>Synchronization overhead (atomic operations, DashMap locking)</li>
<li>Memory bandwidth saturation</li>
<li>Diminishing returns from Amdahlâ€™s law (sequential portions)</li>
</ul>
<p><strong>Weak Scaling (Problem Size Grows with Cores):</strong></p>
<p>Debtmap maintains high efficiency when problem size scales with core count, making it ideal for analyzing larger codebases on more powerful machines.</p>
<h2 id="tuning-guidelines"><a class="header" href="#tuning-guidelines">Tuning Guidelines</a></h2>
<p><strong>Development Workstations:</strong></p>
<pre><code class="language-bash"># Use all cores for maximum speed
debtmap analyze --jobs 0
</code></pre>
<p><strong>CI/CD Environments:</strong></p>
<pre><code class="language-bash"># Limit threads to avoid resource contention
debtmap analyze --jobs 2

# Or disable parallelism on very constrained runners
debtmap analyze --no-parallel
</code></pre>
<p><strong>Containers:</strong></p>
<pre><code class="language-bash"># Auto-detection respects cgroup limits
debtmap analyze --jobs 0

# Or explicitly match container CPU allocation
debtmap analyze --jobs 4
</code></pre>
<p><strong>Benchmarking:</strong></p>
<pre><code class="language-bash"># Use fixed thread count for reproducible results
debtmap analyze --jobs 8
</code></pre>
<h2 id="profiling-and-debugging"><a class="header" href="#profiling-and-debugging">Profiling and Debugging</a></h2>
<h3 id="measure-analysis-time"><a class="header" href="#measure-analysis-time">Measure Analysis Time</a></h3>
<pre><code class="language-bash">time debtmap analyze .
</code></pre>
<h3 id="disable-parallelism-for-debugging"><a class="header" href="#disable-parallelism-for-debugging">Disable Parallelism for Debugging</a></h3>
<pre><code class="language-bash">debtmap analyze . --no-parallel -vv
</code></pre>
<p>Single-threaded mode with verbose output for debugging.</p>
<h3 id="profile-thread-usage"><a class="header" href="#profile-thread-usage">Profile Thread Usage</a></h3>
<p>Use system tools to monitor thread usage:</p>
<pre><code class="language-bash"># Linux
htop

# macOS
Activity Monitor (View &gt; CPU Usage &gt; Show Threads)
</code></pre>
<p>Look for:</p>
<ul>
<li>All cores at ~100% utilization (optimal)</li>
<li>Some cores idle (I/O bound or insufficient work)</li>
<li>Excessive context switching (too many threads)</li>
</ul>
<h3 id="finding-optimal-settings"><a class="header" href="#finding-optimal-settings">Finding Optimal Settings</a></h3>
<p><strong>Finding the optimal setting:</strong></p>
<pre><code class="language-bash"># Benchmark different configurations
time debtmap analyze --jobs 0  # Auto
time debtmap analyze --jobs 4  # 4 threads
time debtmap analyze --jobs 8  # 8 threads
time debtmap analyze --no-parallel  # Sequential
</code></pre>
<p>Monitor memory usage during analysis:</p>
<pre><code class="language-bash"># Monitor peak memory usage
/usr/bin/time -v debtmap analyze --jobs 8
</code></pre>
<h2 id="best-practices-7"><a class="header" href="#best-practices-7">Best Practices</a></h2>
<ol>
<li><strong>Use default settings</strong> - Debtmap auto-detects optimal thread count</li>
<li><strong>Limit threads in CI</strong> - Use <code>--jobs 2</code> or <code>--jobs 4</code> in shared environments</li>
<li><strong>Profile before tuning</strong> - Measure actual performance impact</li>
<li><strong>Consider I/O</strong> - If using slow storage, reduce thread count</li>
<li><strong>Cache aggressively</strong> - Large caches reduce repeated work</li>
</ol>
<h2 id="troubleshooting-7"><a class="header" href="#troubleshooting-7">Troubleshooting</a></h2>
<h3 id="analysis-is-slow-despite-parallelism"><a class="header" href="#analysis-is-slow-despite-parallelism">Analysis is Slow Despite Parallelism</a></h3>
<p><strong>Possible causes:</strong></p>
<ol>
<li>I/O bottleneck (slow disk)</li>
<li>Cache disabled or cleared</li>
<li>Excessive cache pruning</li>
<li>Memory pressure (swapping)</li>
</ol>
<p><strong>Solutions:</strong></p>
<ul>
<li>Move cache to SSD</li>
<li>Increase <code>DEBTMAP_CACHE_MAX_SIZE</code></li>
<li>Reduce thread count to avoid memory pressure</li>
</ul>
<h3 id="slow-analysis-performance"><a class="header" href="#slow-analysis-performance">Slow Analysis Performance</a></h3>
<p>If analysis is slower than expected:</p>
<ol>
<li>
<p><strong>Check thread count:</strong></p>
<pre><code class="language-bash"># Ensure you're using all cores
debtmap analyze --jobs 0 -vv | grep "threads"
</code></pre>
</li>
<li>
<p><strong>Check I/O bottleneck:</strong></p>
<pre><code class="language-bash"># Use iotop or similar to check disk saturation
# SSD storage significantly improves performance
</code></pre>
</li>
<li>
<p><strong>Check memory pressure:</strong></p>
<pre><code class="language-bash"># Monitor memory usage during analysis
top -p $(pgrep debtmap)
</code></pre>
</li>
<li>
<p><strong>Try different thread counts:</strong></p>
<pre><code class="language-bash"># Sometimes less threads = less contention
debtmap analyze --jobs 4
</code></pre>
</li>
</ol>
<h3 id="high-cpu-usage-but-no-progress"><a class="header" href="#high-cpu-usage-but-no-progress">High CPU Usage But No Progress</a></h3>
<p><strong>Possible cause:</strong> Analyzing very complex files (large ASTs)</p>
<p><strong>Solution:</strong></p>
<pre><code class="language-bash"># Reduce thread count to avoid memory thrashing
debtmap analyze . --jobs 2
</code></pre>
<h3 id="high-memory-usage"><a class="header" href="#high-memory-usage">High Memory Usage</a></h3>
<p>If debtmap uses too much memory:</p>
<ol>
<li>
<p><strong>Reduce parallelism:</strong></p>
<pre><code class="language-bash">debtmap analyze --jobs 2
</code></pre>
</li>
<li>
<p><strong>Disable parallel call graph:</strong></p>
<pre><code class="language-bash">debtmap analyze --no-parallel
</code></pre>
</li>
<li>
<p><strong>Analyze subdirectories separately:</strong></p>
<pre><code class="language-bash"># Process codebase in chunks
debtmap analyze src/module1
debtmap analyze src/module2
</code></pre>
</li>
</ol>
<h3 id="inconsistent-results-between-runs"><a class="header" href="#inconsistent-results-between-runs">Inconsistent Results Between Runs</a></h3>
<p><strong>Possible cause:</strong> Non-deterministic parallel aggregation (rare)</p>
<p><strong>Solution:</strong></p>
<pre><code class="language-bash"># Use single-threaded mode
debtmap analyze . --no-parallel
</code></pre>
<p>If results differ, report as a bug.</p>
<h3 id="debugging-concurrency-issues"><a class="header" href="#debugging-concurrency-issues">Debugging Concurrency Issues</a></h3>
<p>If you suspect a concurrency bug:</p>
<ol>
<li>
<p><strong>Run sequentially to isolate:</strong></p>
<pre><code class="language-bash">debtmap analyze --no-parallel
</code></pre>
</li>
<li>
<p><strong>Use deterministic mode:</strong></p>
<pre><code class="language-bash"># Single-threaded = deterministic order
debtmap analyze --jobs 1
</code></pre>
</li>
<li>
<p><strong>Enable verbose logging:</strong></p>
<pre><code class="language-bash">debtmap analyze -vvv --no-parallel &gt; debug.log 2&gt;&amp;1
</code></pre>
</li>
<li>
<p><strong>Report the issue:</strong>
If behavior differs between <code>--no-parallel</code> and parallel mode, please <a href="https://github.com/yourusername/debtmap/issues">report it</a> with:</p>
<ul>
<li>Command used</li>
<li>Platform (OS, CPU core count)</li>
<li>Debtmap version</li>
<li>Minimal reproduction case</li>
</ul>
</li>
</ol>
<h3 id="thread-contention-warning"><a class="header" href="#thread-contention-warning">Thread Contention Warning</a></h3>
<p>If you see warnings about thread contention:</p>
<pre><code>WARN - High contention detected on parallel call graph
</code></pre>
<p>This indicates too many threads competing for locks. Try:</p>
<pre><code class="language-bash"># Reduce thread count
debtmap analyze --jobs 4
</code></pre>
<h2 id="see-also-4"><a class="header" href="#see-also-4">See Also</a></h2>
<ul>
<li><a href="./cli-reference.html#performance--caching">CLI Reference - Performance &amp; Caching</a> - Complete flag documentation</li>
<li><a href="cache-management.html">Cache Management</a> - Cache configuration for performance</li>
<li><a href="configuration.html">Configuration</a> - Project-specific settings</li>
<li><a href="troubleshooting.html">Troubleshooting</a> - General troubleshooting guide</li>
<li><a href="./troubleshooting.html#slow-analysis-performance">Troubleshooting - Slow Analysis</a> - Performance debugging guide</li>
<li><a href="./troubleshooting.html#high-memory-usage">Troubleshooting - High Memory Usage</a> - Memory optimization tips</li>
<li><a href="./faq.html">FAQ - Reducing Parallelism</a> - Common questions about parallel processing</li>
<li><a href="./architecture.html">Architecture</a> - High-level system design</li>
</ul>
<h2 id="summary-3"><a class="header" href="#summary-3">Summary</a></h2>
<p>Debtmapâ€™s parallel processing architecture provides:</p>
<ul>
<li><strong>10-100x speedup</strong> over sequential analysis using Rayon parallel iterators</li>
<li><strong>Lock-free concurrency</strong> with DashMap for minimal contention</li>
<li><strong>Flexible configuration</strong> via <code>--jobs</code> and <code>--no-parallel</code> flags</li>
<li><strong>Automatic thread pool tuning</strong> that respects system resources</li>
<li><strong>Production-grade reliability</strong> with atomic progress tracking and concurrent merging</li>
</ul>
<p>The three-phase parallel pipeline (parse â†’ extract â†’ analyze) maximizes parallelism while maintaining correctness through carefully designed concurrent data structures.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="prodigy-integration"><a class="header" href="#prodigy-integration">Prodigy Integration</a></h1>
<p>Debtmap integrates with <a href="https://github.com/iepathos/prodigy">Prodigy</a> to provide fully automated technical debt reduction through AI-driven workflows. This chapter explains how to set up and use Prodigy workflows to automatically refactor code, add tests, and improve codebase quality.</p>
<h2 id="what-is-prodigy"><a class="header" href="#what-is-prodigy">What is Prodigy?</a></h2>
<p>Prodigy is an AI-powered workflow automation system that uses Claude to execute complex multi-step tasks. When integrated with Debtmap, it can:</p>
<ul>
<li><strong>Automatically refactor</strong> high-complexity functions identified by Debtmap</li>
<li><strong>Add unit tests</strong> for untested code</li>
<li><strong>Fix code duplication</strong> by extracting shared logic</li>
<li><strong>Improve code organization</strong> by addressing architectural issues</li>
<li><strong>Validate improvements</strong> with automated testing</li>
</ul>
<p>All changes are made in isolated git worktrees, validated with tests and linting, and only committed if all checks pass.</p>
<h2 id="benefits"><a class="header" href="#benefits">Benefits</a></h2>
<h3 id="automated-debt-reduction"><a class="header" href="#automated-debt-reduction">Automated Debt Reduction</a></h3>
<p>Instead of manually addressing each technical debt item, Prodigy can:</p>
<ol>
<li>Analyze Debtmapâ€™s output</li>
<li>Select high-priority items</li>
<li>Generate refactoring plans</li>
<li>Execute refactorings automatically</li>
<li>Validate with tests</li>
<li>Commit clean changes</li>
</ol>
<h3 id="iterative-improvement"><a class="header" href="#iterative-improvement">Iterative Improvement</a></h3>
<p>Prodigy supports <strong>iterative workflows</strong>:</p>
<ul>
<li>Run analysis â†’ fix top items â†’ re-analyze â†’ fix more</li>
<li>Configurable iteration count (default: 5 iterations)</li>
<li>Each iteration focuses on highest-priority remaining items</li>
</ul>
<h3 id="safe-experimentation"><a class="header" href="#safe-experimentation">Safe Experimentation</a></h3>
<p>All changes happen in <strong>isolated git worktrees</strong>:</p>
<ul>
<li>Original branch remains untouched</li>
<li>Failed attempts donâ€™t affect main codebase</li>
<li>Easy to review before merging</li>
<li>Automatic cleanup after workflow</li>
</ul>
<h2 id="prerequisites-1"><a class="header" href="#prerequisites-1">Prerequisites</a></h2>
<h3 id="install-prodigy"><a class="header" href="#install-prodigy">Install Prodigy</a></h3>
<pre><code class="language-bash"># Install Prodigy from GitHub repository
cargo install --git https://github.com/iepathos/prodigy prodigy

# Or if available on crates.io:
cargo install prodigy

# Verify installation
prodigy --version
</code></pre>
<p><strong>Requirements:</strong></p>
<ul>
<li>Rust 1.70 or later</li>
<li>Git (for worktree management)</li>
<li>Anthropic API key for Claude access</li>
</ul>
<h3 id="configure-claude-api"><a class="header" href="#configure-claude-api">Configure Claude API</a></h3>
<pre><code class="language-bash"># Set Claude API key
export ANTHROPIC_API_KEY="your-api-key-here"

# Or in ~/.prodigy/config.toml:
[api]
anthropic_key = "your-api-key-here"
</code></pre>
<h3 id="ensure-debtmap-is-installed"><a class="header" href="#ensure-debtmap-is-installed">Ensure Debtmap is Installed</a></h3>
<pre><code class="language-bash"># Install Debtmap
cargo install debtmap

# Verify installation
debtmap --version
</code></pre>
<h2 id="quick-start-2"><a class="header" href="#quick-start-2">Quick Start</a></h2>
<h3 id="1-initialize-workflow"><a class="header" href="#1-initialize-workflow">1. Initialize Workflow</a></h3>
<p>Create a workflow file <code>workflows/debtmap.yml</code>:</p>
<pre><code class="language-yaml"># Sequential workflow. Fix top technical debt item

# Phase 1: Generate coverage data
- shell: "just coverage-lcov"

# Phase 2: Analyze tech debt and capture baseline
- shell: "debtmap analyze . --lcov target/coverage/lcov.info --output .prodigy/debtmap-before.json --format json"

# Phase 3: Create implementation plan (PLANNING PHASE)
- claude: "/prodigy-debtmap-plan --before .prodigy/debtmap-before.json --output .prodigy/IMPLEMENTATION_PLAN.md"
  capture_output: true
  validate:
    commands:
      - claude: "/prodigy-validate-debtmap-plan --before .prodigy/debtmap-before.json --plan .prodigy/IMPLEMENTATION_PLAN.md --output .prodigy/plan-validation.json"
    result_file: ".prodigy/plan-validation.json"
    threshold: 75
    on_incomplete:
      commands:
        - claude: "/prodigy-revise-debtmap-plan --gaps ${validation.gaps} --plan .prodigy/IMPLEMENTATION_PLAN.md"
      max_attempts: 3
      fail_workflow: false

# Phase 4: Execute the plan (IMPLEMENTATION PHASE)
- claude: "/prodigy-debtmap-implement --plan .prodigy/IMPLEMENTATION_PLAN.md"
  commit_required: true
  validate:
    commands:
      - shell: "debtmap analyze . --lcov target/coverage/lcov.info --output .prodigy/debtmap-after.json --format json"
      - shell: "debtmap compare --before .prodigy/debtmap-before.json --after .prodigy/debtmap-after.json --plan .prodigy/IMPLEMENTATION_PLAN.md --output .prodigy/comparison.json --format json"
      - claude: "/prodigy-validate-debtmap-improvement --comparison .prodigy/comparison.json --output .prodigy/debtmap-validation.json"
    result_file: ".prodigy/debtmap-validation.json"
    threshold: 75
    on_incomplete:
      commands:
        - claude: "/prodigy-complete-debtmap-fix --gaps ${validation.gaps} --plan .prodigy/IMPLEMENTATION_PLAN.md"
          commit_required: true
        - shell: "just coverage-lcov"
        - shell: "debtmap analyze . --lcov target/coverage/lcov.info --output .prodigy/debtmap-after.json --format json"
        - shell: "debtmap compare --before .prodigy/debtmap-before.json --after .prodigy/debtmap-after.json --plan .prodigy/IMPLEMENTATION_PLAN.md --output .prodigy/comparison.json --format json"
      max_attempts: 5
      fail_workflow: true

# Phase 5: Run tests with automatic fixing
- shell: "just test"
  on_failure:
    claude: "/prodigy-debug-test-failure --output ${shell.output}"
    max_attempts: 5
    fail_workflow: true

# Phase 6: Run linting and formatting
- shell: "just fmt-check &amp;&amp; just lint"
  on_failure:
    claude: "/prodigy-lint ${shell.output}"
    max_attempts: 5
    fail_workflow: true
</code></pre>
<h3 id="2-run-workflow"><a class="header" href="#2-run-workflow">2. Run Workflow</a></h3>
<pre><code class="language-bash"># Run with auto-confirm, 5 iterations
prodigy run workflows/debtmap.yml -yn 5

# Run with custom iteration count
prodigy run workflows/debtmap.yml -yn 10

# Run single iteration for testing
prodigy run workflows/debtmap.yml -yn 1
</code></pre>
<p><strong>Command Flags:</strong></p>
<ul>
<li><code>-y</code> (<code>--yes</code>) - Auto-confirm workflow steps (skip prompts)</li>
<li><code>-n 5</code> (<code>--max-iterations 5</code>) - Run workflow for up to 5 iterations</li>
</ul>
<p><strong>Note</strong>: Worktrees are managed separately via the <code>prodigy worktree</code> command. In MapReduce mode, Prodigy automatically creates isolated worktrees for each parallel agent.</p>
<h3 id="3-review-results"><a class="header" href="#3-review-results">3. Review Results</a></h3>
<p>Prodigy creates a detailed report:</p>
<pre><code>ğŸ“Š WORKFLOW SUMMARY
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Iterations: 5
Items Fixed: 12
Tests Added: 8
Complexity Reduced: 145 â†’ 78 (-46%)
Coverage Improved: 45% â†’ 72% (+27%)

âœ… All validations passed
</code></pre>
<h2 id="useful-prodigy-commands"><a class="header" href="#useful-prodigy-commands">Useful Prodigy Commands</a></h2>
<p>Beyond <code>prodigy run</code>, several commands help manage workflows and sessions:</p>
<h3 id="resume-interrupted-workflows"><a class="header" href="#resume-interrupted-workflows">Resume Interrupted Workflows</a></h3>
<pre><code class="language-bash"># Resume an interrupted sequential workflow
prodigy resume &lt;SESSION_ID&gt;

# Resume an interrupted MapReduce job
prodigy resume-job &lt;JOB_ID&gt;

# List all sessions to find the SESSION_ID
prodigy sessions
</code></pre>
<p><strong>When to use</strong>: If a workflow is interrupted (Ctrl-C, system crash, network issues), you can resume from the last checkpoint rather than starting over.</p>
<h3 id="view-checkpoints"><a class="header" href="#view-checkpoints">View Checkpoints</a></h3>
<pre><code class="language-bash"># List all available checkpoints
prodigy checkpoints

# List checkpoints for specific session
prodigy checkpoints --session &lt;SESSION_ID&gt;
</code></pre>
<p><strong>When to use</strong>: To see available restore points for interrupted workflows.</p>
<h3 id="manage-worktrees"><a class="header" href="#manage-worktrees">Manage Worktrees</a></h3>
<pre><code class="language-bash"># List all Prodigy worktrees
prodigy worktree list

# Clean up old worktrees
prodigy worktree clean

# Remove specific worktree
prodigy worktree remove &lt;SESSION_ID&gt;
</code></pre>
<p><strong>When to use</strong>: MapReduce workflows create many worktrees. Clean them up periodically to save disk space.</p>
<h3 id="monitor-mapreduce-progress"><a class="header" href="#monitor-mapreduce-progress">Monitor MapReduce Progress</a></h3>
<pre><code class="language-bash"># View progress of running MapReduce job
prodigy progress &lt;JOB_ID&gt;

# View events and logs from MapReduce job
prodigy events &lt;JOB_ID&gt;

# Filter events by type
prodigy events &lt;JOB_ID&gt; --type agent_started
prodigy events &lt;JOB_ID&gt; --type agent_completed
prodigy events &lt;JOB_ID&gt; --type agent_failed
</code></pre>
<p><strong>When to use</strong>: Monitor long-running MapReduce jobs to see how many agents have completed, which are still running, and which have failed.</p>
<h3 id="manage-dead-letter-queue"><a class="header" href="#manage-dead-letter-queue">Manage Dead Letter Queue</a></h3>
<pre><code class="language-bash"># View failed MapReduce items in DLQ
prodigy dlq list &lt;JOB_ID&gt;

# Retry failed items from DLQ
prodigy dlq retry &lt;JOB_ID&gt; &lt;ITEM_ID&gt;

# Remove items from DLQ
prodigy dlq remove &lt;JOB_ID&gt; &lt;ITEM_ID&gt;
</code></pre>
<p><strong>When to use</strong>: When some MapReduce agents fail, their items go to the Dead Letter Queue. You can retry them individually or investigate why they failed.</p>
<h3 id="session-management"><a class="header" href="#session-management">Session Management</a></h3>
<pre><code class="language-bash"># List all workflow sessions
prodigy sessions

# Clean up old sessions
prodigy clean
</code></pre>
<p><strong>When to use</strong>: View history of workflow runs and clean up old data.</p>
<h2 id="workflow-configuration"><a class="header" href="#workflow-configuration">Workflow Configuration</a></h2>
<p>Prodigy workflows are defined as YAML lists of steps. Each step can be either a <code>shell</code> command or a <code>claude</code> slash command.</p>
<h3 id="workflow-step-types"><a class="header" href="#workflow-step-types">Workflow Step Types</a></h3>
<h4 id="shell-commands"><a class="header" href="#shell-commands">Shell Commands</a></h4>
<p>Execute shell commands directly:</p>
<pre><code class="language-yaml"># Simple shell command
- shell: "cargo test"

# With timeout (in seconds)
- shell: "just coverage-lcov"
  timeout: 900  # 15 minutes

# With error handling
- shell: "just test"
  on_failure:
    claude: "/prodigy-debug-test-failure --output ${shell.output}"
    max_attempts: 5
    fail_workflow: true
</code></pre>
<p><strong>Shell Command Fields:</strong></p>
<ul>
<li><code>shell</code>: Command to execute (string)</li>
<li><code>timeout</code>: Maximum execution time in seconds (optional)</li>
<li><code>on_failure</code>: Error handler configuration (optional)
<ul>
<li><code>claude</code>: Slash command to run on failure</li>
<li><code>max_attempts</code>: Maximum retry attempts</li>
<li><code>fail_workflow</code>: If true, fail entire workflow after max attempts</li>
</ul>
</li>
</ul>
<h4 id="claude-commands"><a class="header" href="#claude-commands">Claude Commands</a></h4>
<p>Execute Claude Code slash commands:</p>
<pre><code class="language-yaml"># Simple Claude command
- claude: "/prodigy-debtmap-plan --before .prodigy/debtmap-before.json --output .prodigy/IMPLEMENTATION_PLAN.md"

# With output capture (makes command output available in ${shell.output})
- claude: "/prodigy-debtmap-plan --before .prodigy/debtmap-before.json --output .prodigy/IMPLEMENTATION_PLAN.md"
  capture_output: true

# With commit requirement (workflow fails if no git commit made)
- claude: "/prodigy-debtmap-implement --plan .prodigy/IMPLEMENTATION_PLAN.md"
  commit_required: true

# With timeout and validation
- claude: "/prodigy-debtmap-implement --plan .prodigy/IMPLEMENTATION_PLAN.md"
  commit_required: true
  timeout: 1800  # 30 minutes
  validate:
    commands:
      - shell: "cargo test"
    result_file: ".prodigy/validation.json"
    threshold: 75
</code></pre>
<p><strong>Claude Command Fields:</strong></p>
<ul>
<li><code>claude</code>: Slash command to execute (string)</li>
<li><code>capture_output</code>: If true, command output is available in <code>${shell.output}</code> variable (optional)</li>
<li><code>commit_required</code>: If true, workflow fails if command doesnâ€™t create a git commit (optional)</li>
<li><code>timeout</code>: Maximum execution time in seconds (optional)</li>
<li><code>validate</code>: Validation configuration (optional, see Step-Level Validation below)</li>
</ul>
<h3 id="step-level-validation"><a class="header" href="#step-level-validation">Step-Level Validation</a></h3>
<p>Steps can include validation that must pass:</p>
<pre><code class="language-yaml">- claude: "/prodigy-debtmap-implement --plan .prodigy/IMPLEMENTATION_PLAN.md"
  commit_required: true
  validate:
    commands:
      - shell: "cargo test"
      - shell: "cargo clippy -- -D warnings"
    result_file: ".prodigy/validation.json"
    threshold: 75
    on_incomplete:
      commands:
        - claude: "/prodigy-complete-debtmap-fix --gaps ${validation.gaps} --plan .prodigy/IMPLEMENTATION_PLAN.md"
          commit_required: true
      max_attempts: 5
      fail_workflow: true
</code></pre>
<p><strong>Validation Options:</strong></p>
<ul>
<li><code>commands</code>: List of commands to run for validation</li>
<li><code>result_file</code>: JSON file containing validation results</li>
<li><code>threshold</code>: Minimum score (0-100) required to pass</li>
<li><code>on_incomplete</code>: Actions to take if validation score &lt; threshold</li>
<li><code>max_attempts</code>: Maximum retry attempts</li>
<li><code>fail_workflow</code>: Whether to fail entire workflow if validation never passes</li>
</ul>
<h3 id="error-handling"><a class="header" href="#error-handling">Error Handling</a></h3>
<p>Use <code>on_failure</code> to handle command failures:</p>
<pre><code class="language-yaml">- shell: "just fmt-check &amp;&amp; just lint"
  on_failure:
    claude: "/prodigy-lint ${shell.output}"
    max_attempts: 5
    fail_workflow: true
</code></pre>
<p><strong>Error Handling Options:</strong></p>
<ul>
<li><code>claude</code>: Slash command to fix the failure</li>
<li><code>max_attempts</code>: Maximum fix attempts</li>
<li><code>fail_workflow</code>: If true, workflow fails after max_attempts; if false, continues to next step</li>
</ul>
<h3 id="coverage-integration-2"><a class="header" href="#coverage-integration-2">Coverage Integration</a></h3>
<p>Generate and use coverage data in workflows:</p>
<pre><code class="language-yaml"># Generate coverage
- shell: "just coverage-lcov"

# Use coverage in analysis
- shell: "debtmap analyze . --lcov target/coverage/lcov.info --output .prodigy/debtmap-before.json --format json"
</code></pre>
<h2 id="claude-slash-commands"><a class="header" href="#claude-slash-commands">Claude Slash Commands</a></h2>
<p>Prodigy workflows use Claude Code slash commands to perform analysis, planning, and implementation. The key commands used in the debtmap workflow are:</p>
<h3 id="planning-commands"><a class="header" href="#planning-commands">Planning Commands</a></h3>
<h4 id="prodigy-debtmap-plan"><a class="header" href="#prodigy-debtmap-plan"><code>/prodigy-debtmap-plan</code></a></h4>
<p>Creates an implementation plan for the top priority debt item.</p>
<pre><code class="language-yaml">- claude: "/prodigy-debtmap-plan --before .prodigy/debtmap-before.json --output .prodigy/IMPLEMENTATION_PLAN.md"
  capture_output: true
</code></pre>
<p><strong>Parameters:</strong></p>
<ul>
<li><code>--before</code>: Path to debtmap analysis JSON file</li>
<li><code>--output</code>: Path to write implementation plan</li>
</ul>
<h4 id="prodigy-validate-debtmap-plan"><a class="header" href="#prodigy-validate-debtmap-plan"><code>/prodigy-validate-debtmap-plan</code></a></h4>
<p>Validates that the implementation plan is complete and addresses the debt item.</p>
<pre><code class="language-yaml">- claude: "/prodigy-validate-debtmap-plan --before .prodigy/debtmap-before.json --plan .prodigy/IMPLEMENTATION_PLAN.md --output .prodigy/plan-validation.json"
</code></pre>
<p><strong>Parameters:</strong></p>
<ul>
<li><code>--before</code>: Original debtmap analysis</li>
<li><code>--plan</code>: Implementation plan to validate</li>
<li><code>--output</code>: Validation results JSON (with score 0-100)</li>
</ul>
<h4 id="prodigy-revise-debtmap-plan"><a class="header" href="#prodigy-revise-debtmap-plan"><code>/prodigy-revise-debtmap-plan</code></a></h4>
<p>Revises an incomplete plan based on validation gaps.</p>
<pre><code class="language-yaml">- claude: "/prodigy-revise-debtmap-plan --gaps ${validation.gaps} --plan .prodigy/IMPLEMENTATION_PLAN.md"
</code></pre>
<p><strong>Parameters:</strong></p>
<ul>
<li><code>--gaps</code>: List of missing items from validation</li>
<li><code>--plan</code>: Plan file to update</li>
</ul>
<h3 id="implementation-commands"><a class="header" href="#implementation-commands">Implementation Commands</a></h3>
<h4 id="prodigy-debtmap-implement"><a class="header" href="#prodigy-debtmap-implement"><code>/prodigy-debtmap-implement</code></a></h4>
<p>Executes the implementation plan.</p>
<pre><code class="language-yaml">- claude: "/prodigy-debtmap-implement --plan .prodigy/IMPLEMENTATION_PLAN.md"
  commit_required: true
</code></pre>
<p><strong>Parameters:</strong></p>
<ul>
<li><code>--plan</code>: Path to implementation plan</li>
</ul>
<h4 id="prodigy-validate-debtmap-improvement-1"><a class="header" href="#prodigy-validate-debtmap-improvement-1"><code>/prodigy-validate-debtmap-improvement</code></a></h4>
<p>Validates that the implementation successfully addressed the debt item.</p>
<pre><code class="language-yaml">- claude: "/prodigy-validate-debtmap-improvement --comparison .prodigy/comparison.json --output .prodigy/debtmap-validation.json"
</code></pre>
<p><strong>Parameters:</strong></p>
<ul>
<li><code>--comparison</code>: Debtmap comparison results (before vs after)</li>
<li><code>--output</code>: Validation results JSON (with score 0-100)</li>
</ul>
<h4 id="prodigy-complete-debtmap-fix"><a class="header" href="#prodigy-complete-debtmap-fix"><code>/prodigy-complete-debtmap-fix</code></a></h4>
<p>Completes a partial fix based on validation gaps.</p>
<pre><code class="language-yaml">- claude: "/prodigy-complete-debtmap-fix --gaps ${validation.gaps} --plan .prodigy/IMPLEMENTATION_PLAN.md"
  commit_required: true
</code></pre>
<p><strong>Parameters:</strong></p>
<ul>
<li><code>--gaps</code>: Validation gaps to address</li>
<li><code>--plan</code>: Original implementation plan</li>
</ul>
<h3 id="testing-and-quality-commands"><a class="header" href="#testing-and-quality-commands">Testing and Quality Commands</a></h3>
<h4 id="prodigy-debug-test-failure"><a class="header" href="#prodigy-debug-test-failure"><code>/prodigy-debug-test-failure</code></a></h4>
<p>Automatically fixes failing tests.</p>
<pre><code class="language-yaml">- shell: "just test"
  on_failure:
    claude: "/prodigy-debug-test-failure --output ${shell.output}"
    max_attempts: 5
</code></pre>
<p><strong>Parameters:</strong></p>
<ul>
<li><code>--output</code>: Test failure output from shell command</li>
</ul>
<h4 id="prodigy-lint"><a class="header" href="#prodigy-lint"><code>/prodigy-lint</code></a></h4>
<p>Fixes linting and formatting issues.</p>
<pre><code class="language-yaml">- shell: "just fmt-check &amp;&amp; just lint"
  on_failure:
    claude: "/prodigy-lint ${shell.output}"
    max_attempts: 5
</code></pre>
<p><strong>Parameters:</strong></p>
<ul>
<li>Shell output with linting errors</li>
</ul>
<h2 id="target-selection"><a class="header" href="#target-selection">Target Selection</a></h2>
<p>Target selection happens through the debtmap analysis and slash commands, not through workflow configuration:</p>
<h3 id="how-targets-are-selected"><a class="header" href="#how-targets-are-selected">How Targets Are Selected</a></h3>
<ol>
<li><strong>Debtmap analyzes</strong> the codebase and scores all items by complexity, coverage, and risk</li>
<li><strong>Planning command</strong> (<code>/prodigy-debtmap-plan</code>) selects the highest priority item</li>
<li><strong>Implementation command</strong> (<code>/prodigy-debtmap-implement</code>) fixes that specific item</li>
<li><strong>Next iteration</strong> re-analyzes and selects the next highest priority item</li>
</ol>
<h3 id="factors-in-prioritization"><a class="header" href="#factors-in-prioritization">Factors in Prioritization</a></h3>
<ul>
<li><strong>Complexity score</strong>: Functions with cyclomatic complexity &gt; 10</li>
<li><strong>Coverage percentage</strong>: Lower coverage increases priority</li>
<li><strong>Risk score</strong>: Complexity Ã— (100 - coverage%)</li>
<li><strong>Debt type</strong>: Complexity, TestGap, Duplication, GodObject, DeepNesting</li>
</ul>
<h3 id="customizing-target-selection"><a class="header" href="#customizing-target-selection">Customizing Target Selection</a></h3>
<p>To focus on specific debt types or modules, modify the slash commands or create custom commands in <code>.claude/commands/</code></p>
<h2 id="mapreduce-workflows"><a class="header" href="#mapreduce-workflows">MapReduce Workflows</a></h2>
<p>Prodigy supports MapReduce workflows for processing multiple items in parallel. This is powerful for large-scale refactoring where you want to fix many debt items simultaneously.</p>
<h3 id="when-to-use-mapreduce"><a class="header" href="#when-to-use-mapreduce">When to Use MapReduce</a></h3>
<ul>
<li>Processing multiple independent debt items simultaneously (e.g., refactor 10 high-complexity functions in parallel)</li>
<li>Applying the same fix pattern across many files</li>
<li>Large-scale codebase cleanup tasks</li>
<li>Situations where sequential iteration would be too slow</li>
</ul>
<h3 id="mapreduce-vs-sequential-workflows"><a class="header" href="#mapreduce-vs-sequential-workflows">MapReduce vs Sequential Workflows</a></h3>
<p><strong>Sequential Workflow</strong> (<code>-n 5</code>):</p>
<ul>
<li>Runs entire workflow N times in sequence</li>
<li>Fixes one item per iteration</li>
<li>Each iteration re-analyzes the codebase</li>
<li>Total time: N Ã— workflow_duration</li>
</ul>
<p><strong>MapReduce Workflow</strong>:</p>
<ul>
<li>Processes multiple items in parallel in a single run</li>
<li>Setup phase runs once</li>
<li>Map phase spawns N parallel agents (each in isolated worktree)</li>
<li>Reduce phase aggregates results</li>
<li>Total time: setup + max(map_agent_durations) + reduce</li>
</ul>
<h3 id="complete-mapreduce-example"><a class="header" href="#complete-mapreduce-example">Complete MapReduce Example</a></h3>
<p>Create <code>workflows/debtmap-reduce.yml</code>:</p>
<pre><code class="language-yaml">name: debtmap-parallel-elimination
mode: mapreduce

# Setup phase: Analyze the codebase and generate debt items
setup:
  timeout: 900  # 15 minutes for coverage generation
  commands:
    # Generate coverage data with tarpaulin
    - shell: "just coverage-lcov"

    # Run debtmap with coverage data to establish baseline
    - shell: "debtmap analyze src --lcov target/coverage/lcov.info --output .prodigy/debtmap-before.json --format json"

# Map phase: Process each debt item in parallel with planning and validation
map:
  # Input configuration - debtmap-before.json contains items array
  input: .prodigy/debtmap-before.json
  json_path: "$.items[*]"

  # Commands to execute for each debt item
  agent_template:
    # Phase 1: Create implementation plan
    - claude: "/prodigy-debtmap-plan --item '${item}' --output .prodigy/plan-${item_id}.md"
      capture_output: true
      validate:
        commands:
          - claude: "/prodigy-validate-debtmap-plan --item '${item}' --plan .prodigy/plan-${item_id}.md --output .prodigy/validation-${item_id}.json"
        result_file: ".prodigy/validation-${item_id}.json"
        threshold: 75
        on_incomplete:
          commands:
            - claude: "/prodigy-revise-debtmap-plan --gaps ${validation.gaps} --plan .prodigy/plan-${item_id}.md"
          max_attempts: 3
          fail_workflow: false

    # Phase 2: Execute the plan
    - claude: "/prodigy-debtmap-implement --plan .prodigy/plan-${item_id}.md"
      commit_required: true
      validate:
        commands:
          - shell: "just coverage-lcov"
          - shell: "debtmap analyze src --lcov target/coverage/lcov.info --output .prodigy/debtmap-after-${item_id}.json --format json"
          - shell: "debtmap compare --before .prodigy/debtmap-before.json --after .prodigy/debtmap-after-${item_id}.json --plan .prodigy/plan-${item_id}.md --output .prodigy/comparison-${item_id}.json --format json"
          - claude: "/prodigy-validate-debtmap-improvement --comparison .prodigy/comparison-${item_id}.json --output .prodigy/debtmap-validation-${item_id}.json"
        result_file: ".prodigy/debtmap-validation-${item_id}.json"
        threshold: 75
        on_incomplete:
          commands:
            - claude: "/prodigy-complete-debtmap-fix --plan .prodigy/plan-${item_id}.md --validation .prodigy/debtmap-validation-${item_id}.json --attempt ${validation.attempt_number}"
              commit_required: true
            - shell: "just coverage-lcov"
            - shell: "debtmap analyze src --lcov target/coverage/lcov.info --output .prodigy/debtmap-after-${item_id}.json --format json"
            - shell: "debtmap compare --before .prodigy/debtmap-before.json --after .prodigy/debtmap-after-${item_id}.json --plan .prodigy/plan-${item_id}.md --output .prodigy/comparison-${item_id}.json --format json"
          max_attempts: 5
          fail_workflow: true

    # Phase 3: Verify tests pass
    - shell: "just test"
      on_failure:
        claude: "/prodigy-debug-test-failure --output ${shell.output}"
        max_attempts: 5
        fail_workflow: true

    # Phase 4: Check formatting and linting
    - shell: "just fmt-check &amp;&amp; just lint"
      on_failure:
        claude: "/prodigy-lint ${shell.output}"
        max_attempts: 5
        fail_workflow: true

  # Parallelization settings
  max_parallel: 5  # Run up to 5 agents in parallel

  # Filter and sort items
  filter: "File.score &gt;= 10 OR Function.unified_score.final_score &gt;= 10"
  sort_by: "File.score DESC, Function.unified_score.final_score DESC"
  max_items: 10  # Limit to 10 items per run

# Reduce phase: Aggregate results and verify overall improvements
reduce:
  # Phase 1: Run final tests across all changes
  - shell: "just test"
    on_failure:
      claude: "/prodigy-debug-test-failure --output ${shell.output}"
      max_attempts: 5
      fail_workflow: true

  # Phase 2: Check formatting and linting
  - shell: "just fmt-check &amp;&amp; just lint"
    on_failure:
      claude: "/prodigy-lint ${shell.output}"
      max_attempts: 5
      fail_workflow: true

  # Phase 3: Re-run debtmap to measure cumulative improvements
  - shell: "just coverage-lcov"
  - shell: "debtmap analyze src --lcov target/coverage/lcov.info --output .prodigy/debtmap-after.json --format json"

  # Phase 4: Create final commit with summary
  - write_file:
      path: ".prodigy/map-results.json"
      content: "${map.results}"
      format: json
      create_dirs: true

  - claude: |
      /prodigy-compare-debt-results \
        --before .prodigy/debtmap-before.json \
        --after .prodigy/debtmap-after.json \
        --map-results-file .prodigy/map-results.json \
        --successful ${map.successful} \
        --failed ${map.failed} \
        --total ${map.total}
    commit_required: true
</code></pre>
<h3 id="running-mapreduce-workflows"><a class="header" href="#running-mapreduce-workflows">Running MapReduce Workflows</a></h3>
<pre><code class="language-bash"># Run MapReduce workflow (single execution processes multiple items in parallel)
prodigy run workflows/debtmap-reduce.yml

# Run with auto-confirm
prodigy run workflows/debtmap-reduce.yml -y
</code></pre>
<p><strong>Note</strong>: MapReduce workflows donâ€™t typically use <code>-n</code> for iterations. Instead, they process multiple items in a single run through parallel map agents.</p>
<h3 id="mapreduce-configuration-options"><a class="header" href="#mapreduce-configuration-options">MapReduce Configuration Options</a></h3>
<h4 id="top-level-fields"><a class="header" href="#top-level-fields">Top-Level Fields</a></h4>
<ul>
<li><code>name</code>: Workflow name (string)</li>
<li><code>mode: mapreduce</code>: Enables MapReduce mode (required)</li>
<li><code>setup</code>: Commands to run once before map phase</li>
<li><code>map</code>: Map phase configuration</li>
<li><code>reduce</code>: Commands to run after all map agents complete</li>
</ul>
<h4 id="setup-phase-fields"><a class="header" href="#setup-phase-fields">Setup Phase Fields</a></h4>
<ul>
<li><code>timeout</code>: Maximum time in seconds for setup phase</li>
<li><code>commands</code>: List of shell or claude commands to run</li>
</ul>
<h4 id="map-phase-fields"><a class="header" href="#map-phase-fields">Map Phase Fields</a></h4>
<ul>
<li><code>input</code>: Path to JSON file containing items to process</li>
<li><code>json_path</code>: JSONPath expression to extract items array (e.g., <code>$.items[*]</code>)</li>
<li><code>agent_template</code>: List of commands to run for each item (each item gets its own agent in an isolated worktree)</li>
<li><code>max_parallel</code>: Maximum number of agents to run concurrently</li>
<li><code>filter</code>: Expression to filter which items to process (e.g., <code>"score &gt;= 10"</code>)</li>
<li><code>sort_by</code>: Expression to sort items (e.g., <code>"score DESC"</code>)</li>
<li><code>max_items</code>: Limit total items processed</li>
</ul>
<h4 id="mapreduce-specific-variables"><a class="header" href="#mapreduce-specific-variables">MapReduce-Specific Variables</a></h4>
<p>Available in <code>agent_template</code> commands:</p>
<ul>
<li><code>${item}</code>: The full JSON object for current item</li>
<li><code>${item_id}</code>: Unique ID for current item (auto-generated)</li>
<li><code>${validation.gaps}</code>: List of validation gaps from failed validation</li>
<li><code>${validation.attempt_number}</code>: Current retry attempt number (1, 2, 3, etc.)</li>
<li><code>${shell.output}</code>: Output from previous shell command</li>
<li><code>${map.results}</code>: All map agent results (available in reduce phase)</li>
<li><code>${map.successful}</code>: Count of successful map agents (reduce phase)</li>
<li><code>${map.failed}</code>: Count of failed map agents (reduce phase)</li>
<li><code>${map.total}</code>: Total number of map agents (reduce phase)</li>
</ul>
<h3 id="mapreduce-architecture"><a class="header" href="#mapreduce-architecture">MapReduce Architecture</a></h3>
<pre><code>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Setup Phase (main worktree)                            â”‚
â”‚ - Generate coverage data                               â”‚
â”‚ - Run debtmap analysis                                 â”‚
â”‚ - Output: .prodigy/debtmap-before.json                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Map Phase (parallel worktrees)                         â”‚
â”‚                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Agent 1      â”‚  â”‚ Agent 2      â”‚  â”‚ Agent 3      â”‚ â”‚
â”‚  â”‚ Item #1      â”‚  â”‚ Item #2      â”‚  â”‚ Item #3      â”‚ â”‚
â”‚  â”‚ Worktree A   â”‚  â”‚ Worktree B   â”‚  â”‚ Worktree C   â”‚ â”‚
â”‚  â”‚              â”‚  â”‚              â”‚  â”‚              â”‚ â”‚
â”‚  â”‚ Plan â†’ Fix   â”‚  â”‚ Plan â†’ Fix   â”‚  â”‚ Plan â†’ Fix   â”‚ â”‚
â”‚  â”‚ â†’ Validate   â”‚  â”‚ â†’ Validate   â”‚  â”‚ â†’ Validate   â”‚ â”‚
â”‚  â”‚ â†’ Test       â”‚  â”‚ â†’ Test       â”‚  â”‚ â†’ Test       â”‚ â”‚
â”‚  â”‚ â†’ Commit     â”‚  â”‚ â†’ Commit     â”‚  â”‚ â†’ Commit     â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  â”‚ Agent 4      â”‚  â”‚ Agent 5      â”‚                   â”‚
â”‚  â”‚ Item #4      â”‚  â”‚ Item #5      â”‚                   â”‚
â”‚  â”‚ Worktree D   â”‚  â”‚ Worktree E   â”‚                   â”‚
â”‚  â”‚              â”‚  â”‚              â”‚                   â”‚
â”‚  â”‚ Plan â†’ Fix   â”‚  â”‚ Plan â†’ Fix   â”‚                   â”‚
â”‚  â”‚ â†’ Validate   â”‚  â”‚ â†’ Validate   â”‚                   â”‚
â”‚  â”‚ â†’ Test       â”‚  â”‚ â†’ Test       â”‚                   â”‚
â”‚  â”‚ â†’ Commit     â”‚  â”‚ â†’ Commit     â”‚                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Reduce Phase (main worktree)                           â”‚
â”‚ - Merge all agent worktrees                            â”‚
â”‚ - Run final tests on merged code                       â”‚
â”‚ - Run final linting                                    â”‚
â”‚ - Re-analyze with debtmap                              â”‚
â”‚ - Generate summary commit                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre>
<p><strong>Key Concepts:</strong></p>
<ul>
<li><strong>Isolation</strong>: Each map agent works in its own git worktree</li>
<li><strong>Parallelism</strong>: Multiple agents process different items simultaneously</li>
<li><strong>Validation</strong>: Each agent validates its changes independently</li>
<li><strong>Merging</strong>: Reduce phase merges all successful agent worktrees</li>
<li><strong>Final Validation</strong>: Reduce phase ensures merged code passes all tests</li>
</ul>
<h2 id="iteration-strategy"><a class="header" href="#iteration-strategy">Iteration Strategy</a></h2>
<h3 id="how-iterations-work"><a class="header" href="#how-iterations-work">How Iterations Work</a></h3>
<p>When you run <code>prodigy run workflows/debtmap.yml -yn 5</code>, the workflow executes up to 5 times:</p>
<ol>
<li>
<p><strong>Iteration 1</strong>:</p>
<ul>
<li>Analyze codebase with debtmap</li>
<li>Select highest priority item</li>
<li>Create implementation plan</li>
<li>Execute plan and validate</li>
<li>Run tests and linting</li>
</ul>
</li>
<li>
<p><strong>Iteration 2</strong>:</p>
<ul>
<li>Re-analyze codebase (scores updated based on Iteration 1 changes)</li>
<li>Select next highest priority item</li>
<li>Repeat plan/implement/validate cycle</li>
</ul>
</li>
<li>
<p><strong>Continue</strong> until iteration limit reached or workflow completes without finding issues</p>
</li>
</ol>
<h3 id="controlling-iterations"><a class="header" href="#controlling-iterations">Controlling Iterations</a></h3>
<p>Iterations are controlled via the <code>-n</code> flag:</p>
<pre><code class="language-bash"># Single iteration (testing)
prodigy run workflows/debtmap.yml -yn 1

# Standard run (5 iterations)
prodigy run workflows/debtmap.yml -yn 5

# Deep cleanup (10+ iterations)
prodigy run workflows/debtmap.yml -yn 20
</code></pre>
<h3 id="what-happens-each-iteration"><a class="header" href="#what-happens-each-iteration">What Happens Each Iteration</a></h3>
<p>Each iteration runs the <strong>entire workflow from start to finish</strong>:</p>
<ol>
<li>Generate coverage data</li>
<li>Analyze technical debt</li>
<li>Create implementation plan</li>
<li>Execute plan</li>
<li>Validate improvement</li>
<li>Run tests (with auto-fixing)</li>
<li>Run linting (with auto-fixing)</li>
</ol>
<p>The workflow continues to the next iteration automatically if all steps succeed.</p>
<h3 id="example-output-2"><a class="header" href="#example-output-2">Example Output</a></h3>
<pre><code>Iteration 1:
  - Fixed: parse_expression() (9.2 â†’ 5.1)
  - Fixed: calculate_score() (8.8 â†’ 4.2)
  - Fixed: apply_weights() (8.5 â†’ 5.8)
  âœ“ Tests pass

Iteration 2:
  - Fixed: normalize_results() (7.5 â†’ 3.9)
  - Fixed: aggregate_data() (7.2 â†’ 4.1)
  âœ“ Tests pass

Iteration 3:
  - No items above threshold (6.0)
  âœ“ Early stop

Final Results:
  Items fixed: 5
  Average complexity: 15.2 â†’ 8.6
</code></pre>
<h2 id="validation-1"><a class="header" href="#validation-1">Validation</a></h2>
<p>Prodigy validates changes at the workflow step level, not as a standalone configuration.</p>
<h3 id="step-level-validation-1"><a class="header" href="#step-level-validation-1">Step-Level Validation</a></h3>
<p>Validation is attached to specific workflow steps:</p>
<pre><code class="language-yaml">- claude: "/prodigy-debtmap-implement --plan .prodigy/IMPLEMENTATION_PLAN.md"
  commit_required: true
  validate:
    commands:
      - shell: "debtmap analyze . --lcov target/coverage/lcov.info --output .prodigy/debtmap-after.json --format json"
      - shell: "debtmap compare --before .prodigy/debtmap-before.json --after .prodigy/debtmap-after.json --plan .prodigy/IMPLEMENTATION_PLAN.md --output .prodigy/comparison.json --format json"
      - claude: "/prodigy-validate-debtmap-improvement --comparison .prodigy/comparison.json --output .prodigy/debtmap-validation.json"
    result_file: ".prodigy/debtmap-validation.json"
    threshold: 75
    on_incomplete:
      commands:
        - claude: "/prodigy-complete-debtmap-fix --gaps ${validation.gaps} --plan .prodigy/IMPLEMENTATION_PLAN.md"
          commit_required: true
        - shell: "just coverage-lcov"
        - shell: "debtmap analyze . --lcov target/coverage/lcov.info --output .prodigy/debtmap-after.json --format json"
        - shell: "debtmap compare --before .prodigy/debtmap-before.json --after .prodigy/debtmap-after.json --plan .prodigy/IMPLEMENTATION_PLAN.md --output .prodigy/comparison.json --format json"
      max_attempts: 5
      fail_workflow: true
</code></pre>
<h3 id="validation-process"><a class="header" href="#validation-process">Validation Process</a></h3>
<ol>
<li><strong>Commands run</strong>: Execute validation commands (shell or claude)</li>
<li><strong>Check result file</strong>: Read JSON file specified in <code>result_file</code></li>
<li><strong>Compare to threshold</strong>: Score must be &gt;= threshold (0-100 scale)</li>
<li><strong>On incomplete</strong>: If score &lt; threshold, run <code>on_incomplete</code> commands</li>
<li><strong>Retry</strong>: Repeat up to <code>max_attempts</code> times</li>
<li><strong>Fail or continue</strong>: If <code>fail_workflow: true</code>, stop workflow; otherwise continue</li>
</ol>
<h3 id="validation-result-format"><a class="header" href="#validation-result-format">Validation Result Format</a></h3>
<p>The <code>result_file</code> JSON should contain:</p>
<pre><code class="language-json">{
  "score": 85,
  "passed": true,
  "gaps": [],
  "details": "All debt improvement criteria met"
}
</code></pre>
<h3 id="test-validation-with-auto-fix"><a class="header" href="#test-validation-with-auto-fix">Test Validation with Auto-Fix</a></h3>
<p>Tests are validated with automatic fixing on failure:</p>
<pre><code class="language-yaml">- shell: "just test"
  on_failure:
    claude: "/prodigy-debug-test-failure --output ${shell.output}"
    max_attempts: 5
    fail_workflow: true
</code></pre>
<p>If tests fail, Prodigy automatically attempts to fix them up to 5 times before failing the workflow.</p>
<h2 id="output-and-metrics"><a class="header" href="#output-and-metrics">Output and Metrics</a></h2>
<h3 id="workflow-report"><a class="header" href="#workflow-report">Workflow Report</a></h3>
<pre><code class="language-json">{
  "workflow": "debtmap-debt-reduction",
  "iterations": 5,
  "items_processed": 12,
  "items_fixed": 10,
  "items_failed": 2,
  "metrics": {
    "complexity_before": 145,
    "complexity_after": 78,
    "complexity_reduction": -46.2,
    "coverage_before": 45.3,
    "coverage_after": 72.1,
    "coverage_improvement": 26.8
  },
  "changes": [
    {
      "file": "src/parser.rs",
      "function": "parse_expression",
      "before_score": 9.2,
      "after_score": 5.1,
      "improvements": ["Reduced complexity", "Added tests"]
    }
  ]
}
</code></pre>
<h3 id="commit-messages"><a class="header" href="#commit-messages">Commit Messages</a></h3>
<p>Prodigy generates descriptive commit messages:</p>
<pre><code>refactor(parser): reduce complexity in parse_expression

- Extract nested conditionals to helper functions
- Add unit tests for edge cases
- Coverage: 0% â†’ 85%
- Complexity: 22 â†’ 8

Generated by Prodigy workflow: debtmap-debt-reduction
Iteration: 1/5
</code></pre>
<h2 id="integration-with-cicd"><a class="header" href="#integration-with-cicd">Integration with CI/CD</a></h2>
<h3 id="github-actions-1"><a class="header" href="#github-actions-1">GitHub Actions</a></h3>
<pre><code class="language-yaml">name: Prodigy Debt Reduction

on:
  schedule:
    - cron: '0 0 * * 0'  # Weekly on Sunday
  workflow_dispatch:

jobs:
  reduce-debt:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Setup Rust
        uses: actions-rs/toolchain@v1
        with:
          toolchain: stable

      - name: Install Prodigy
        run: cargo install prodigy

      - name: Install dependencies
        run: |
          cargo install debtmap
          cargo install just

      - name: Run Prodigy workflow
        env:
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
        run: prodigy run workflows/debtmap.yml -yn 5

      - name: Create PR
        uses: peter-evans/create-pull-request@v5
        with:
          title: "chore: automated debt reduction via Prodigy"
          body: |
            Automated technical debt reduction using Prodigy workflow.

            This PR was generated by the weekly debt reduction workflow.
            Review changes carefully before merging.
          branch: prodigy-debt-reduction
</code></pre>
<h3 id="gitlab-ci-1"><a class="header" href="#gitlab-ci-1">GitLab CI</a></h3>
<pre><code class="language-yaml">prodigy-debt-reduction:
  stage: quality
  rules:
    - if: '$CI_PIPELINE_SOURCE == "schedule"'
  script:
    - cargo install prodigy
    - cargo install debtmap
    - cargo install just
    - prodigy run workflows/debtmap.yml -yn 5
  artifacts:
    paths:
      - .prodigy/debtmap-*.json
      - .prodigy/comparison.json
</code></pre>
<h3 id="important-ci-considerations"><a class="header" href="#important-ci-considerations">Important CI Considerations</a></h3>
<ul>
<li><strong>API Keys</strong>: Store <code>ANTHROPIC_API_KEY</code> as a secret</li>
<li><strong>Worktrees</strong>: MapReduce mode creates isolated worktrees automatically for parallel processing</li>
<li><strong>Dependencies</strong>: Install <code>prodigy</code>, <code>debtmap</code>, and <code>just</code> (or your build tool)</li>
<li><strong>Timeout</strong>: CI jobs may need extended timeout for multiple iterations</li>
<li><strong>Review</strong>: Always create a PR for human review before merging automated changes</li>
</ul>
<h2 id="best-practices-8"><a class="header" href="#best-practices-8">Best Practices</a></h2>
<h3 id="1-start-small"><a class="header" href="#1-start-small">1. Start Small</a></h3>
<p>Begin with low iteration counts:</p>
<pre><code class="language-bash"># First run: 1 iteration to test workflow
prodigy run workflows/debtmap.yml -yn 1

# Standard run: 3-5 iterations
prodigy run workflows/debtmap.yml -yn 5
</code></pre>
<h3 id="2-focus-on-high-priority-items"><a class="header" href="#2-focus-on-high-priority-items">2. Focus on High-Priority Items</a></h3>
<p>The debtmap analysis automatically prioritizes by:</p>
<ul>
<li>Complexity score (cyclomatic complexity)</li>
<li>Coverage percentage (lower coverage = higher priority)</li>
<li>Risk score (complexity Ã— (100 - coverage%))</li>
</ul>
<p>To focus on specific areas, create custom slash commands in <code>.claude/commands/</code> that filter by:</p>
<ul>
<li>Module/file patterns</li>
<li>Specific debt types (Complexity, TestGap, Duplication)</li>
<li>Score thresholds</li>
</ul>
<h3 id="3-validate-thoroughly"><a class="header" href="#3-validate-thoroughly">3. Validate Thoroughly</a></h3>
<p>Use comprehensive validation in your workflow:</p>
<pre><code class="language-yaml">- shell: "just test"
  on_failure:
    claude: "/prodigy-debug-test-failure --output ${shell.output}"
    max_attempts: 5
    fail_workflow: true

- shell: "just fmt-check &amp;&amp; just lint"
  on_failure:
    claude: "/prodigy-lint ${shell.output}"
    max_attempts: 5
    fail_workflow: true
</code></pre>
<h3 id="4-review-before-merging"><a class="header" href="#4-review-before-merging">4. Review Before Merging</a></h3>
<p>Always review Prodigyâ€™s changes:</p>
<pre><code class="language-bash"># Find your worktree
ls ~/.prodigy/worktrees/

# Check changes
cd ~/.prodigy/worktrees/session-xxx
git diff main

# Review commit history
git log --oneline

# Run full test suite
cargo test --all-features
</code></pre>
<h3 id="5-monitor-progress"><a class="header" href="#5-monitor-progress">5. Monitor Progress</a></h3>
<p>Track debt reduction over iterations:</p>
<pre><code class="language-bash"># Compare before and after
debtmap compare --before .prodigy/debtmap-before.json --after .prodigy/debtmap-after.json

# View detailed metrics
cat .prodigy/comparison.json | jq
</code></pre>
<h2 id="troubleshooting-8"><a class="header" href="#troubleshooting-8">Troubleshooting</a></h2>
<h3 id="workflow-fails-to-start"><a class="header" href="#workflow-fails-to-start">Workflow Fails to Start</a></h3>
<p><strong>Issue</strong>: â€œProdigy not foundâ€ or â€œAPI key missingâ€</p>
<p><strong>Solution</strong>:</p>
<pre><code class="language-bash"># Install Prodigy
cargo install prodigy

# Set API key
export ANTHROPIC_API_KEY="your-key"

# Verify installation
prodigy --version
</code></pre>
<h3 id="validation-failures"><a class="header" href="#validation-failures">Validation Failures</a></h3>
<p><strong>Issue</strong>: Validation score below threshold</p>
<p><strong>Solution</strong>: Check validation results:</p>
<pre><code class="language-bash"># View validation details
cat .prodigy/debtmap-validation.json

# Check what gaps remain
cat .prodigy/debtmap-validation.json | jq '.gaps'

# Review comparison results
cat .prodigy/comparison.json
</code></pre>
<p>The workflow will automatically retry up to <code>max_attempts</code> times with <code>/prodigy-complete-debtmap-fix</code>.</p>
<h3 id="test-failures"><a class="header" href="#test-failures">Test Failures</a></h3>
<p><strong>Issue</strong>: Tests fail after implementation</p>
<p><strong>Solution</strong>: The workflow includes automatic test fixing:</p>
<pre><code class="language-yaml">- shell: "just test"
  on_failure:
    claude: "/prodigy-debug-test-failure --output ${shell.output}"
    max_attempts: 5
    fail_workflow: true
</code></pre>
<p>If tests still fail after 5 attempts, review manually:</p>
<pre><code class="language-bash"># Check test output
just test

# Review recent changes
git diff HEAD~1
</code></pre>
<h3 id="no-items-processed"><a class="header" href="#no-items-processed">No Items Processed</a></h3>
<p><strong>Issue</strong>: Workflow completes but doesnâ€™t find debt to fix</p>
<p><strong>Possible Causes</strong>:</p>
<ol>
<li>Codebase has very low debt scores (below selection threshold)</li>
<li>Coverage data not generated properly</li>
<li>Debtmap analysis found no high-priority items</li>
</ol>
<p><strong>Solution</strong>:</p>
<pre><code class="language-bash"># Check debtmap analysis results
cat .prodigy/debtmap-before.json | jq '.items | sort_by(-.unified_score.final_score) | .[0:5]'

# Verify coverage was generated
ls -lh target/coverage/lcov.info

# Run debtmap manually to see what's detected
debtmap analyze . --lcov target/coverage/lcov.info
</code></pre>
<h3 id="workflow-hangs-or-times-out"><a class="header" href="#workflow-hangs-or-times-out">Workflow Hangs or Times Out</a></h3>
<p><strong>Issue</strong>: Workflow takes too long or appears stuck</p>
<p><strong>Possible Causes</strong>:</p>
<ul>
<li>Large codebase with many files</li>
<li>Complex refactoring requiring extensive analysis</li>
<li>Network issues with Claude API</li>
</ul>
<p><strong>Solution</strong>:</p>
<ul>
<li>Reduce iteration count for testing (<code>-n 1</code>)</li>
<li>Check Claude API connectivity</li>
<li>Monitor worktree for progress: <code>cd ~/.prodigy/worktrees/session-xxx &amp;&amp; git log</code></li>
</ul>
<h3 id="mapreduce-specific-troubleshooting"><a class="header" href="#mapreduce-specific-troubleshooting">MapReduce-Specific Troubleshooting</a></h3>
<h4 id="resuming-failed-mapreduce-jobs"><a class="header" href="#resuming-failed-mapreduce-jobs">Resuming Failed MapReduce Jobs</a></h4>
<p><strong>Issue</strong>: MapReduce job was interrupted or failed</p>
<p><strong>Solution</strong>:</p>
<pre><code class="language-bash"># Find the job ID from recent sessions
prodigy sessions

# Resume the MapReduce job from checkpoint
prodigy resume-job &lt;JOB_ID&gt;
</code></pre>
<p>The job will resume from where it left off, skipping already-completed items.</p>
<h4 id="checking-mapreduce-progress"><a class="header" href="#checking-mapreduce-progress">Checking MapReduce Progress</a></h4>
<p><strong>Issue</strong>: Want to monitor long-running MapReduce job</p>
<p><strong>Solution</strong>:</p>
<pre><code class="language-bash"># View overall progress
prodigy progress &lt;JOB_ID&gt;

# View detailed events
prodigy events &lt;JOB_ID&gt;

# Filter for specific event types
prodigy events &lt;JOB_ID&gt; --type agent_completed
prodigy events &lt;JOB_ID&gt; --type agent_failed
</code></pre>
<p><strong>Output example</strong>:</p>
<pre><code>MapReduce Job: job-abc123
Status: running
Progress: 7/10 items (70%)
- Completed: 5
- Running: 2
- Failed: 3
</code></pre>
<h4 id="managing-failed-mapreduce-items"><a class="header" href="#managing-failed-mapreduce-items">Managing Failed MapReduce Items</a></h4>
<p><strong>Issue</strong>: Some agents failed, items in Dead Letter Queue</p>
<p><strong>Solution</strong>:</p>
<pre><code class="language-bash"># View failed items
prodigy dlq list &lt;JOB_ID&gt;

# Review why an item failed (check events)
prodigy events &lt;JOB_ID&gt; --item &lt;ITEM_ID&gt;

# Retry specific failed item
prodigy dlq retry &lt;JOB_ID&gt; &lt;ITEM_ID&gt;

# Remove unfixable items from DLQ
prodigy dlq remove &lt;JOB_ID&gt; &lt;ITEM_ID&gt;
</code></pre>
<p><strong>Common failure reasons</strong>:</p>
<ul>
<li>Validation threshold not met after max_attempts</li>
<li>Tests fail and canâ€™t be fixed automatically</li>
<li>Merge conflicts with other agentsâ€™ changes</li>
<li>Timeout exceeded for complex refactoring</li>
</ul>
<h4 id="cleaning-up-mapreduce-worktrees"><a class="header" href="#cleaning-up-mapreduce-worktrees">Cleaning Up MapReduce Worktrees</a></h4>
<p><strong>Issue</strong>: Disk space consumed by many MapReduce worktrees</p>
<p><strong>Solution</strong>:</p>
<pre><code class="language-bash"># List all worktrees
prodigy worktree list

# Clean up completed job worktrees
prodigy worktree clean

# Remove specific session's worktrees
prodigy worktree remove &lt;SESSION_ID&gt;

# Manual cleanup (if Prodigy commands don't work)
rm -rf ~/.prodigy/worktrees/session-xxx
</code></pre>
<p><strong>When to clean</strong>:</p>
<ul>
<li>After successful job completion and merge</li>
<li>When disk space is low</li>
<li>After abandoned or failed jobs</li>
</ul>
<h4 id="mapreduce-merge-conflicts"><a class="header" href="#mapreduce-merge-conflicts">MapReduce Merge Conflicts</a></h4>
<p><strong>Issue</strong>: Reduce phase fails due to merge conflicts between agent worktrees</p>
<p><strong>Possible Causes</strong>:</p>
<ul>
<li>Multiple agents modified overlapping code</li>
<li>Agents made conflicting architectural changes</li>
<li>Shared dependencies updated differently</li>
</ul>
<p><strong>Solution</strong>:</p>
<pre><code class="language-bash"># Review which agents succeeded
prodigy events &lt;JOB_ID&gt; --type agent_completed

# Check merge conflicts
cd ~/.prodigy/worktrees/session-xxx
git status

# Manually resolve conflicts
# Edit conflicting files
git add .
git commit -m "Resolve MapReduce merge conflicts"

# Resume the job
prodigy resume-job &lt;JOB_ID&gt;
</code></pre>
<p><strong>Prevention</strong>:</p>
<ul>
<li>Use <code>filter</code> to ensure agents work on independent items</li>
<li>Reduce <code>max_parallel</code> to minimize conflicts</li>
<li>Design debt items to be truly independent</li>
</ul>
<h4 id="understanding-mapreduce-variables"><a class="header" href="#understanding-mapreduce-variables">Understanding MapReduce Variables</a></h4>
<p>If youâ€™re debugging workflow files, these variables are available:</p>
<p><strong>In map phase (agent_template)</strong>:</p>
<ul>
<li><code>${item}</code>: Full JSON of current item being processed</li>
<li><code>${item_id}</code>: Unique ID for current item</li>
<li><code>${validation.gaps}</code>: Validation gaps from validation result</li>
<li><code>${validation.attempt_number}</code>: Current retry attempt (1, 2, 3â€¦)</li>
<li><code>${shell.output}</code>: Output from previous shell command</li>
</ul>
<p><strong>In reduce phase</strong>:</p>
<ul>
<li><code>${map.results}</code>: All map agent results as JSON</li>
<li><code>${map.successful}</code>: Count of successful agents</li>
<li><code>${map.failed}</code>: Count of failed agents</li>
<li><code>${map.total}</code>: Total number of agents</li>
</ul>
<p><strong>Example debug command</strong>:</p>
<pre><code class="language-yaml"># In agent_template, log the item being processed
- shell: "echo 'Processing item: ${item_id}' &gt;&gt; .prodigy/debug.log"
</code></pre>
<h2 id="example-workflows"><a class="header" href="#example-workflows">Example Workflows</a></h2>
<h3 id="full-repository-cleanup"><a class="header" href="#full-repository-cleanup">Full Repository Cleanup</a></h3>
<p>For comprehensive debt reduction, use a higher iteration count:</p>
<pre><code class="language-bash"># Run 10 iterations for deeper cleanup
prodigy run workflows/debtmap.yml -yn 10

# Run 20 iterations for major refactoring
prodigy run workflows/debtmap.yml -yn 20
</code></pre>
<p>The workflow automatically:</p>
<ol>
<li>Selects highest priority items each iteration</li>
<li>Addresses different debt types (Complexity, TestGap, Duplication)</li>
<li>Validates all changes with tests and linting</li>
<li>Commits only successful improvements</li>
</ol>
<h3 id="custom-workflow-for-specific-focus"><a class="header" href="#custom-workflow-for-specific-focus">Custom Workflow for Specific Focus</a></h3>
<p>Create a custom workflow file for focused improvements:</p>
<p><strong><code>workflows/add-tests.yml</code></strong> - Focus on test coverage:</p>
<pre><code class="language-yaml"># Generate coverage
- shell: "just coverage-lcov"

# Analyze with focus on test gaps
- shell: "debtmap analyze . --lcov target/coverage/lcov.info --output .prodigy/debtmap-before.json --format json"

# Create plan (slash command will prioritize TestGap items)
- claude: "/prodigy-debtmap-plan --before .prodigy/debtmap-before.json --output .prodigy/IMPLEMENTATION_PLAN.md"

# ... rest of standard workflow steps
</code></pre>
<p>Run with:</p>
<pre><code class="language-bash">prodigy run workflows/add-tests.yml -yn 5
</code></pre>
<h3 id="targeted-module-cleanup"><a class="header" href="#targeted-module-cleanup">Targeted Module Cleanup</a></h3>
<p>Create a custom slash command to focus on specific modules:</p>
<p><strong><code>.claude/commands/refactor-module.md</code></strong>:</p>
<pre><code class="language-markdown"># /refactor-module

Refactor the highest complexity item in the specified module.

Arguments: --module &lt;module_name&gt;

... implementation details ...
</code></pre>
<p>Then create a workflow using this command for targeted refactoring.</p>
<h2 id="see-also-5"><a class="header" href="#see-also-5">See Also</a></h2>
<ul>
<li><a href="./cli-reference.html">Debtmap CLI Reference</a> - Debtmap command options</li>
<li><a href="./configuration.html">Configuration</a> - Debtmap configuration</li>
<li><a href="./tiered-prioritization.html">Tiered Prioritization</a> - Understanding priority tiers</li>
<li><a href="https://github.com/iepathos/prodigy">Prodigy Documentation</a> - Full Prodigy reference</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="scoring-strategies"><a class="header" href="#scoring-strategies">Scoring Strategies</a></h1>
<p>Debtmap provides two complementary scoring approaches: <strong>file-level</strong> and <strong>function-level</strong>. Understanding when to use each approach helps you make better refactoring decisions and prioritize work effectively.</p>
<h2 id="overview-5"><a class="header" href="#overview-5">Overview</a></h2>
<p>Different refactoring scenarios require different levels of granularity:</p>
<ul>
<li><strong>File-level scoring</strong>: Identifies architectural issues and planning major refactoring initiatives</li>
<li><strong>Function-level scoring</strong>: Pinpoints specific hot spots for targeted improvements</li>
</ul>
<p>This chapter explains both approaches, when to use each, and how to interpret the results.</p>
<h2 id="file-level-scoring"><a class="header" href="#file-level-scoring">File-Level Scoring</a></h2>
<p>File-level scoring aggregates metrics across all functions in a file to identify architectural problems and module-level refactoring opportunities.</p>
<h3 id="formula-1"><a class="header" href="#formula-1">Formula</a></h3>
<pre><code>File Score = Size Ã— Complexity Ã— Coverage Multiplier Ã— Density Ã— GodObject Ã— FunctionScores
</code></pre>
<p>Where each factor is calculated as:</p>
<ul>
<li><strong>Size</strong> = <code>sqrt(total_lines / 100)</code></li>
<li><strong>Complexity</strong> = <code>(avg_complexity / 5.0) Ã— sqrt(total_complexity / 50.0)</code></li>
<li><strong>Coverage Multiplier</strong> = <code>1.0 - coverage_percent</code></li>
<li><strong>Density</strong> = <code>1.0 + ((function_count - 50) Ã— 0.02)</code> if function_count &gt; 50, else 1.0</li>
<li><strong>GodObject</strong> = <code>2.0 + god_object_score</code> if detected</li>
<li><strong>FunctionScores</strong> = <code>sum(function_scores) / 10</code></li>
</ul>
<h3 id="factors"><a class="header" href="#factors">Factors</a></h3>
<p><strong>Size Factor</strong>: <code>sqrt(total_lines / 100)</code></p>
<ul>
<li>Larger files have higher impact</li>
<li>Square root dampens the effect to avoid over-penalizing large files</li>
<li>Rationale: Refactoring a 1000-line file affects more code than a 100-line file</li>
</ul>
<p><strong>Complexity Factor</strong>: Combines average and total complexity</p>
<ul>
<li><code>(average_cyclomatic + total_cyclomatic / function_count) / 2</code></li>
<li>Balances per-function and aggregate complexity</li>
<li>Rationale: Both concentrated complexity and spread-out complexity matter</li>
</ul>
<p><strong>Coverage Multiplier</strong>: <code>1.0 - coverage_percent</code></p>
<ul>
<li>Lower coverage increases score multiplicatively</li>
<li>Range: 0.0 (100% coverage) to 1.0 (0% coverage)</li>
<li>Rationale: Untested files amplify existing complexity and risk</li>
<li>Note: Earlier versions used an additive â€œCoverage Factorâ€ formula; current implementation uses multiplicative dampening</li>
</ul>
<p><strong>Density Factor</strong>: Penalizes files with excessive function count</p>
<ul>
<li>Triggers when function count &gt; 50</li>
<li><code>1.0 + ((function_count - 50) Ã— 0.02)</code> for function_count &gt; 50, else 1.0</li>
<li>Creates a gradual linear increase: 51 functions = 1.02x, 75 functions = 1.50x, 100 functions = 2.0x</li>
<li>Rationale: Files with many functions likely violate single responsibility</li>
</ul>
<p><strong>God Object Multiplier</strong>: <code>2.0 + god_object_score</code> when detected</p>
<ul>
<li>Applies when god object detection flags the file</li>
<li>Range: 2.0 (borderline) to 3.0 (severe god object)</li>
<li>Rationale: God objects need immediate architectural attention</li>
</ul>
<p><strong>Function Scores</strong>: <code>sum(all_function_scores) / 10</code></p>
<ul>
<li>Normalized sum of individual function debt scores</li>
<li>Provides baseline before modifiers</li>
</ul>
<h3 id="use-cases-3"><a class="header" href="#use-cases-3">Use Cases</a></h3>
<p><strong>1. Planning Major Refactoring Initiatives</strong></p>
<pre><code class="language-bash"># Show top 10 files needing architectural refactoring
debtmap analyze . --aggregate-only --top 10
</code></pre>
<p>Use when:</p>
<ul>
<li>Planning sprint or quarterly refactoring work</li>
<li>Deciding which modules to split</li>
<li>Prioritizing architectural improvements</li>
<li>Allocating team resources</li>
</ul>
<p><strong>Note</strong>: File-level scoring is enabled with the <code>--aggregate-only</code> flag, which changes output to show only file-level metrics instead of function-level details.</p>
<p><strong>2. Identifying Architectural Issues</strong></p>
<p>File-level scoring excels at finding:</p>
<ul>
<li>God objects with too many responsibilities</li>
<li>Files with poor cohesion</li>
<li>Modules that should be split</li>
<li>Files with too many functions</li>
</ul>
<pre><code class="language-bash"># Focus on architectural problems
debtmap analyze . --aggregate-only --filter Architecture
</code></pre>
<p><strong>3. Breaking Up Monolithic Modules</strong></p>
<pre><code class="language-bash"># Find files with excessive function counts
debtmap analyze . --aggregate-only --min-problematic 50
</code></pre>
<p><strong>4. Evaluating Overall Codebase Health</strong></p>
<pre><code class="language-bash"># Generate file-level report for executive summary
debtmap analyze . --aggregate-only --format markdown -o report.md
</code></pre>
<h3 id="aggregation-methods"><a class="header" href="#aggregation-methods">Aggregation Methods</a></h3>
<p>Debtmap supports multiple aggregation methods for file-level scores, configurable via CLI or configuration file.</p>
<h4 id="weighted-sum-default"><a class="header" href="#weighted-sum-default">Weighted Sum (Default)</a></h4>
<p><strong>Formula</strong>: <code>Î£(function_score Ã— complexity_weight Ã— coverage_weight)</code></p>
<pre><code class="language-bash">debtmap analyze . --aggregation-method weighted_sum
</code></pre>
<p>Or via configuration:</p>
<pre><code class="language-toml">[aggregation]
method = "weighted_sum"
</code></pre>
<p><strong>Characteristics</strong>:</p>
<ul>
<li>Weights functions by their complexity and coverage gaps</li>
<li>Emphasizes high-impact functions over trivial ones</li>
<li>Best for most use cases where you want to focus on significant issues</li>
</ul>
<p><strong>Best for</strong>: Standard codebases where you want proportional emphasis on complex, untested code</p>
<h4 id="simple-sum"><a class="header" href="#simple-sum">Simple Sum</a></h4>
<p><strong>Formula</strong>: <code>Î£(function_scores)</code></p>
<pre><code class="language-toml">[aggregation]
method = "sum"
</code></pre>
<p><strong>Characteristics</strong>:</p>
<ul>
<li>Adds all function scores directly without weighting</li>
<li>Treats all functions equally regardless of complexity</li>
<li>Useful for broad overview and trend analysis</li>
</ul>
<p><strong>Best for</strong>: Getting a raw count-based view of technical debt across all functions</p>
<h4 id="logarithmic-sum"><a class="header" href="#logarithmic-sum">Logarithmic Sum</a></h4>
<p><strong>Formula</strong>: <code>log(1 + Î£(function_scores))</code></p>
<pre><code class="language-toml">[aggregation]
method = "logarithmic_sum"
</code></pre>
<p><strong>Characteristics</strong>:</p>
<ul>
<li>Dampens impact of many small issues to prevent score explosion</li>
<li>Prevents files with hundreds of minor issues from dominating</li>
<li>Creates more balanced comparisons across files of different sizes</li>
</ul>
<p><strong>Best for</strong>: Legacy codebases with many small issues where you want to avoid extreme scores</p>
<h4 id="max-plus-average"><a class="header" href="#max-plus-average">Max Plus Average</a></h4>
<p><strong>Formula</strong>: <code>max_score Ã— 0.6 + avg_score Ã— 0.4</code></p>
<pre><code class="language-toml">[aggregation]
method = "max_plus_average"
</code></pre>
<p><strong>Characteristics</strong>:</p>
<ul>
<li>Considers worst function (60%) plus average of all functions (40%)</li>
<li>Balances worst-case and typical-case scenarios</li>
<li>Highlights files with both a critical hot spot and general issues</li>
</ul>
<p><strong>Best for</strong>: Identifying files with concentrated complexity alongside general code quality concerns</p>
<h4 id="choosing-an-aggregation-method"><a class="header" href="#choosing-an-aggregation-method">Choosing an Aggregation Method</a></h4>
<div class="table-wrapper"><table><thead><tr><th>Codebase Type</th><th>Recommended Method</th><th>Rationale</th></tr></thead><tbody>
<tr><td>New/Modern</td><td><code>weighted_sum</code></td><td>Proportional emphasis on real issues</td></tr>
<tr><td>Legacy with many small issues</td><td><code>logarithmic_sum</code></td><td>Prevents score explosion</td></tr>
<tr><td>Mixed quality</td><td><code>max_plus_average</code></td><td>Balances hot spots with overall quality</td></tr>
<tr><td>Trend analysis</td><td><code>sum</code></td><td>Simple, consistent metric over time</td></tr>
</tbody></table>
</div>
<h3 id="configuration-6"><a class="header" href="#configuration-6">Configuration</a></h3>
<pre><code class="language-toml">[aggregation]
method = "weighted_sum"
min_problematic = 3              # Need 3+ problematic functions for file-level score

[god_object_detection]
enabled = true
max_methods = 20
max_fields = 15
max_responsibilities = 5
</code></pre>
<h2 id="function-level-scoring"><a class="header" href="#function-level-scoring">Function-Level Scoring</a></h2>
<p>Function-level scoring identifies specific functions needing attention for targeted improvements.</p>
<h3 id="formula-2"><a class="header" href="#formula-2">Formula</a></h3>
<pre><code>Base Score = (Complexity Ã— 0.40) + (Dependency Ã— 0.20)
Coverage Multiplier = 1.0 - coverage_percent
Final Score = Base Score Ã— Coverage Multiplier Ã— Role Multiplier
</code></pre>
<p><strong>Note</strong>: Coverage acts as a dampening multiplier rather than an additive factor. Lower coverage (higher multiplier) increases the final score, making untested complex code a higher priority. The weights for complexity and dependency are configurable via the <code>[scoring]</code> section in <code>.debtmap.toml</code>. See <a href="scoring-strategies.html#configuration">Configuration</a> below.</p>
<p><strong>Migration Note</strong>: Earlier versions used an additive model with weights (Complexity Ã— 0.35) + (Coverage Ã— 0.50) + (Dependency Ã— 0.15). The current model uses coverage as a multiplicative dampener, which better reflects that testing gaps amplify existing complexity rather than adding to it.</p>
<h3 id="metrics"><a class="header" href="#metrics">Metrics</a></h3>
<p><strong>Cyclomatic Complexity</strong></p>
<ul>
<li>Counts decision points (if, match, loops)</li>
<li>Guides test case count</li>
</ul>
<p><strong>Cognitive Complexity</strong></p>
<ul>
<li>Measures understanding difficulty</li>
<li>Accounts for nesting depth</li>
</ul>
<p><strong>Coverage Percentage</strong></p>
<ul>
<li>Direct line coverage from LCOV</li>
<li>0% coverage = maximum urgency</li>
</ul>
<p><strong>Dependency Count</strong></p>
<ul>
<li>Upstream callers + downstream callees</li>
<li>Higher dependencies = higher impact</li>
</ul>
<p><strong>Role Multiplier</strong></p>
<p>Functions are classified by role, and each role receives a multiplier based on its architectural importance:</p>
<div class="table-wrapper"><table><thead><tr><th>Role</th><th>Multiplier</th><th>Description</th></tr></thead><tbody>
<tr><td><strong>Pure logic</strong></td><td>1.2x</td><td>Core business rules and algorithms</td></tr>
<tr><td><strong>Unknown</strong></td><td>1.0x</td><td>Functions without clear classification</td></tr>
<tr><td><strong>Entry point</strong></td><td>0.9x</td><td>Public APIs, main functions, HTTP handlers</td></tr>
<tr><td><strong>Orchestrator</strong></td><td>0.8x</td><td>Functions that coordinate other functions</td></tr>
<tr><td><strong>IO wrapper</strong></td><td>0.7x</td><td>Simple file/network I/O wrappers</td></tr>
<tr><td><strong>Pattern match</strong></td><td>0.6x</td><td>Functions primarily doing pattern matching</td></tr>
</tbody></table>
</div>
<p><strong>Note</strong>: Role multipliers are configurable via the <code>[role_multipliers]</code> section in <code>.debtmap.toml</code>. The multipliers have been rebalanced to be less extreme than earlier versions - pure logic was reduced from 1.5x to 1.2x, while orchestrator and IO wrapper were increased to better reflect their importance in modern codebases.</p>
<h3 id="use-cases-4"><a class="header" href="#use-cases-4">Use Cases</a></h3>
<p><strong>1. Identifying Specific Hot Spots</strong></p>
<pre><code class="language-bash"># Show top 20 functions needing attention
debtmap analyze . --top 20
</code></pre>
<p>Use when:</p>
<ul>
<li>Planning individual developer tasks</li>
<li>Assigning specific refactoring work</li>
<li>Identifying functions to test first</li>
<li>Code review focus</li>
</ul>
<p><strong>2. Sprint Planning for Developers</strong></p>
<pre><code class="language-bash"># Get function-level tasks for this sprint
debtmap analyze . --top 10 --format json -o sprint-tasks.json
</code></pre>
<p><strong>3. Writing Unit Tests</strong></p>
<pre><code class="language-bash"># Find untested complex functions
debtmap analyze . --lcov coverage.lcov --filter Testing --top 15
</code></pre>
<p><strong>4. Targeted Performance Optimization</strong></p>
<pre><code class="language-bash"># Find complex hot paths
debtmap analyze . --filter Performance --context --top 10
</code></pre>
<h3 id="configuration-7"><a class="header" href="#configuration-7">Configuration</a></h3>
<p>Complete configuration file example (<code>.debtmap.toml</code>):</p>
<pre><code class="language-toml"># Scoring weights (current model uses coverage as multiplier, not additive)
[scoring]
complexity = 0.40            # Weight for complexity in base score
dependency = 0.20            # Weight for dependency impact in base score

# Role multipliers (applied to final score)
[role_multipliers]
pure_logic = 1.2             # Core business rules
unknown = 1.0                # Unclassified functions
entry_point = 0.9            # Public APIs, main functions
orchestrator = 0.8           # Coordination functions
io_wrapper = 0.7             # File/network I/O wrappers
pattern_match = 0.6          # Pattern matching functions

# Aggregation settings (for file-level scoring)
[aggregation]
method = "weighted_sum"      # Options: weighted_sum, sum, logarithmic_sum, max_plus_average
min_problematic = 3          # Minimum number of problematic functions to report file

# Normalization settings
[normalization]
linear_threshold = 10.0      # Scores below this use linear scaling
logarithmic_threshold = 100.0 # Scores above this use logarithmic dampening
sqrt_multiplier = 3.33       # Applied to mid-range scores
log_multiplier = 10.0        # Applied to high scores
show_raw_scores = true       # Display both normalized and raw scores
</code></pre>
<p><strong>Note</strong>: The configuration file must be named <code>.debtmap.toml</code> and placed in your project root.</p>
<h2 id="when-to-use-each-approach"><a class="header" href="#when-to-use-each-approach">When to Use Each Approach</a></h2>
<h3 id="use-file-level-scoring-when"><a class="header" href="#use-file-level-scoring-when">Use File-Level Scoring When:</a></h3>
<p>âœ… Planning architectural refactoring
âœ… Quarterly or annual planning
âœ… Deciding which modules to split
âœ… Executive summaries and high-level reports
âœ… Team capacity planning
âœ… Identifying god objects
âœ… Module reorganization</p>
<p><strong>Command:</strong></p>
<pre><code class="language-bash">debtmap analyze . --aggregate-only
</code></pre>
<h3 id="use-function-level-scoring-when"><a class="header" href="#use-function-level-scoring-when">Use Function-Level Scoring When:</a></h3>
<p>âœ… Sprint planning
âœ… Individual developer task assignment
âœ… Writing specific unit tests
âœ… Code review preparation
âœ… Pair programming sessions
âœ… Daily or weekly development work
âœ… Targeted hot spot fixes</p>
<p><strong>Command:</strong></p>
<pre><code class="language-bash">debtmap analyze . --top 20
</code></pre>
<h3 id="use-both-together"><a class="header" href="#use-both-together">Use Both Together:</a></h3>
<p>Many workflows benefit from both views:</p>
<pre><code class="language-bash"># Step 1: Identify problematic files
debtmap analyze . --aggregate-only --top 5 -o files.json

# Step 2: Drill into specific file
debtmap analyze src/problematic/module.rs --format terminal
</code></pre>
<h2 id="comparison-examples"><a class="header" href="#comparison-examples">Comparison Examples</a></h2>
<h3 id="example-1-god-object-detection"><a class="header" href="#example-1-god-object-detection">Example 1: God Object Detection</a></h3>
<p><strong>File-Level View:</strong></p>
<pre><code>src/services/user_service.rs - Score: 245.8
  - 850 lines, 45 methods
  - God Object: 78% score
  - Action: Split into UserAuth, UserProfile, UserNotifications
</code></pre>
<p><strong>Function-Level View:</strong></p>
<pre><code>src/services/user_service.rs:142 - authenticate_user() - Score: 8.5
src/services/user_service.rs:298 - update_profile() - Score: 7.2
src/services/user_service.rs:456 - send_notification() - Score: 6.8
</code></pre>
<p><strong>Decision</strong>: File-level score (245.8) correctly identifies architectural issue. Individual functions arenâ€™t exceptionally complex, but the file has too many responsibilities. <strong>Solution</strong>: Split the file.</p>
<h3 id="example-2-targeted-function-fix"><a class="header" href="#example-2-targeted-function-fix">Example 2: Targeted Function Fix</a></h3>
<p><strong>File-Level View:</strong></p>
<pre><code>src/parsers/expression.rs - Score: 45.2
  - 320 lines, 12 functions
  - No god object detected
</code></pre>
<p><strong>Function-Level View:</strong></p>
<pre><code>src/parsers/expression.rs:89 - parse_complex_expression() - Score: 9.1
  - Cyclomatic: 22, Cognitive: 35
  - Coverage: 0%
  - Action: Add tests and refactor
</code></pre>
<p><strong>Decision</strong>: File as a whole is acceptable, but one function needs attention. <strong>Solution</strong>: Focus on that specific function.</p>
<h3 id="example-3-balanced-refactoring"><a class="header" href="#example-3-balanced-refactoring">Example 3: Balanced Refactoring</a></h3>
<p><strong>File-Level View:</strong></p>
<pre><code>src/analysis/scoring.rs - Score: 125.6
  - 580 lines, 18 functions
  - High complexity, low coverage
</code></pre>
<p><strong>Function-Level View:</strong></p>
<pre><code>calculate_score() - Score: 8.8 (15% coverage)
apply_weights() - Score: 8.2 (10% coverage)
normalize_results() - Score: 7.5 (0% coverage)
</code></pre>
<p><strong>Decision</strong>: Both file and functions need work. <strong>Solution</strong>: Add tests first (function-level), then consider splitting if complexity persists (file-level).</p>
<h2 id="score-normalization"><a class="header" href="#score-normalization">Score Normalization</a></h2>
<p>Both scoring approaches normalize to a 0-10 scale for consistency.</p>
<h3 id="normalization-strategies"><a class="header" href="#normalization-strategies">Normalization Strategies</a></h3>
<p><strong>Default: Linear Clamping</strong></p>
<p>The default normalization uses simple linear clamping:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>score_normalized = raw_score.clamp(0.0, 100.0)
<span class="boring">}</span></code></pre></pre>
<p>This ensures scores stay within the expected range without additional transformations.</p>
<p><strong>Advanced: Multi-Phase Normalization</strong></p>
<p>For more sophisticated normalization, debtmap provides <code>normalize_final_score_with_metadata</code> which uses different scaling for different score ranges:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>score_normalized = if raw_score &lt; 10.0 {
    raw_score  // Linear below 10
} else if raw_score &lt; 100.0 {
    10.0 + (raw_score - 10.0).sqrt() Ã— 3.33  // Square root 10-100
} else {
    41.59 + (raw_score / 100.0).ln() Ã— 10.0  // Logarithmic above 100
}
<span class="boring">}</span></code></pre></pre>
<p>This multi-phase approach dampens extreme values while preserving distinctions in the normal range.</p>
<h3 id="configuration-8"><a class="header" href="#configuration-8">Configuration</a></h3>
<pre><code class="language-toml">[normalization]
linear_threshold = 10.0       # Scores below this use linear scaling (1:1 mapping)
logarithmic_threshold = 100.0 # Scores above this use logarithmic dampening
sqrt_multiplier = 3.33        # Applied to scores between linear and log thresholds
log_multiplier = 10.0         # Applied to scores above logarithmic threshold
show_raw_scores = true        # Display both normalized (0-10) and raw scores in output
</code></pre>
<p><strong>Explanation</strong>:</p>
<ul>
<li><strong>linear_threshold</strong>: Scores below this value are mapped 1:1 (no scaling)</li>
<li><strong>logarithmic_threshold</strong>: Scores above this value are dampened logarithmically to prevent extreme values</li>
<li><strong>sqrt_multiplier</strong>: Square root scaling applied to mid-range scores (between linear and logarithmic thresholds)</li>
<li><strong>log_multiplier</strong>: Logarithmic dampening factor for very high scores</li>
<li><strong>show_raw_scores</strong>: When enabled, output includes both the normalized 0-10 score and the raw calculated score</li>
</ul>
<h2 id="best-practices-9"><a class="header" href="#best-practices-9">Best Practices</a></h2>
<h3 id="workflow-integration"><a class="header" href="#workflow-integration">Workflow Integration</a></h3>
<p><strong>Week 1: File-Level Assessment</strong></p>
<pre><code class="language-bash"># Identify architectural problems
debtmap analyze . --aggregate-only --top 10
</code></pre>
<p><strong>Week 2-4: Function-Level Work</strong></p>
<pre><code class="language-bash"># Work through specific functions
debtmap analyze src/target/module.rs
</code></pre>
<p><strong>Monthly: Compare Progress</strong></p>
<pre><code class="language-bash">debtmap compare --before baseline.json --after current.json
</code></pre>
<h3 id="team-collaboration"><a class="header" href="#team-collaboration">Team Collaboration</a></h3>
<ul>
<li><strong>Architects</strong>: Use file-level scores for strategic planning</li>
<li><strong>Tech Leads</strong>: Use both for sprint planning</li>
<li><strong>Developers</strong>: Use function-level for daily work</li>
<li><strong>QA</strong>: Use function-level for test prioritization</li>
</ul>
<h3 id="cicd-integration-1"><a class="header" href="#cicd-integration-1">CI/CD Integration</a></h3>
<pre><code class="language-bash"># Gate: No new file-level regressions
debtmap analyze . --aggregate-only --format json -o file-scores.json

# Gate: No new critical function-level issues
debtmap analyze . --min-priority critical --format json -o critical-items.json
</code></pre>
<h2 id="troubleshooting-9"><a class="header" href="#troubleshooting-9">Troubleshooting</a></h2>
<p><strong>Issue</strong>: File-level scores seem too high</p>
<p><strong>Solution</strong>: Check aggregation method:</p>
<pre><code class="language-toml">[aggregation]
method = "logarithmic_sum"  # Dampen scores
</code></pre>
<p><strong>Issue</strong>: Function-level scores all similar</p>
<p><strong>Solution</strong>: Adjust scoring weights to emphasize different factors:</p>
<pre><code class="language-toml">[scoring]
coverage = 0.60    # Emphasize testing gaps more
complexity = 0.30
dependency = 0.10
</code></pre>
<p><strong>Issue</strong>: Too many low-priority items</p>
<p><strong>Solution</strong>: Use minimum thresholds:</p>
<pre><code class="language-toml">[thresholds]
minimum_debt_score = 3.0
</code></pre>
<h2 id="see-also-6"><a class="header" href="#see-also-6">See Also</a></h2>
<ul>
<li><a href="./tiered-prioritization.html">Tiered Prioritization</a> - Understanding tier-based classification</li>
<li><a href="./configuration.html">Configuration</a> - Scoring and aggregation configuration</li>
<li><a href="./analysis-guide.html">Analysis Guide</a> - Detailed metric explanations</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="tiered-prioritization-1"><a class="header" href="#tiered-prioritization-1">Tiered Prioritization</a></h1>
<p>Debtmap uses a sophisticated tiered prioritization system to surface critical architectural issues above simple testing gaps. This chapter explains the tier strategy, how to interpret tier classifications, and how to customize tier thresholds for your project.</p>
<h2 id="overview-6"><a class="header" href="#overview-6">Overview</a></h2>
<p>The tiered prioritization system organizes technical debt into four distinct tiers based on impact, urgency, and architectural significance. This prevents â€œwalls of similar-scored itemsâ€ and ensures critical issues donâ€™t get lost among minor problems.</p>
<h2 id="the-four-tiers"><a class="header" href="#the-four-tiers">The Four Tiers</a></h2>
<h3 id="tier-1-critical-architecture"><a class="header" href="#tier-1-critical-architecture">Tier 1: Critical Architecture</a></h3>
<p><strong>Description</strong>: God Objects, God Modules, excessive complexity requiring immediate architectural attention</p>
<p><strong>Priority</strong>: Must address before adding new features</p>
<p><strong>Weight</strong>: 1.5x (highest priority multiplier)</p>
<p><strong>Impact</strong>: High impact on maintainability and team velocity</p>
<p><strong>Examples</strong>:</p>
<ul>
<li>Files with 15+ responsibilities</li>
<li>Modules with 50+ methods</li>
<li>God objects flagged by detection algorithms</li>
<li>Circular dependencies affecting core modules</li>
</ul>
<p><strong>When to Address</strong>: Immediately, before sprint work begins. These issues compound over time and block progress.</p>
<pre><code class="language-bash"># Focus on Tier 1 items
debtmap analyze . --filter Architecture --min-priority high
</code></pre>
<h3 id="tier-2-complex-untested"><a class="header" href="#tier-2-complex-untested">Tier 2: Complex Untested</a></h3>
<p><strong>Description</strong>: Untested code with high complexity or critical dependencies</p>
<p><strong>Priority</strong>: Risk of bugs in critical paths</p>
<p><strong>Weight</strong>: 1.0x (standard multiplier)</p>
<p><strong>Action</strong>: Should be tested before refactoring to prevent regressions</p>
<p><strong>Examples</strong>:</p>
<ul>
<li>Functions with cyclomatic complexity &gt; 15 and 0% coverage</li>
<li>Business logic entry points without tests</li>
<li>Complex error handling without validation</li>
</ul>
<p><strong>When to Address</strong>: Within current sprint. Add tests before making changes.</p>
<pre><code class="language-bash"># See Tier 2 testing gaps
debtmap analyze . --lcov coverage.lcov --filter Testing
</code></pre>
<h3 id="tier-3-testing-gaps"><a class="header" href="#tier-3-testing-gaps">Tier 3: Testing Gaps</a></h3>
<p><strong>Description</strong>: Untested code with moderate complexity</p>
<p><strong>Priority</strong>: Improve coverage to prevent future issues</p>
<p><strong>Weight</strong>: 0.7x (reduced multiplier)</p>
<p><strong>Action</strong>: Add tests opportunistically or during related changes</p>
<p><strong>Examples</strong>:</p>
<ul>
<li>Functions with cyclomatic complexity 10-15 and low coverage</li>
<li>Utility functions without edge case tests</li>
<li>Moderate complexity with partial coverage</li>
</ul>
<p><strong>When to Address</strong>: Next sprint or when touching related code.</p>
<h3 id="tier-4-maintenance"><a class="header" href="#tier-4-maintenance">Tier 4: Maintenance</a></h3>
<p><strong>Description</strong>: Low-complexity issues and code quality improvements</p>
<p><strong>Priority</strong>: Address opportunistically during other work</p>
<p><strong>Weight</strong>: 0.3x (lowest multiplier)</p>
<p><strong>Action</strong>: Fix when convenient, low urgency</p>
<p><strong>Examples</strong>:</p>
<ul>
<li>Simple functions with minor code quality issues</li>
<li>TODO markers in well-tested code</li>
<li>Minor duplication in test code</li>
</ul>
<p><strong>When to Address</strong>: During cleanup sprints or when refactoring nearby code.</p>
<h2 id="configuration-9"><a class="header" href="#configuration-9">Configuration</a></h2>
<h3 id="default-tier-thresholds"><a class="header" href="#default-tier-thresholds">Default Tier Thresholds</a></h3>
<pre><code class="language-toml">[tiers]
# Tier 2 thresholds (Complex Untested)
t2_complexity_threshold = 15         # Cyclomatic complexity cutoff
t2_dependency_threshold = 10         # Dependency count cutoff

# Tier 3 thresholds (Testing Gaps)
t3_complexity_threshold = 10         # Lower complexity threshold

# Display options
show_t4_in_main_report = false      # Hide Tier 4 from main output

# Tier weights
[tiers.tier_weights]
t1 = 1.5    # Critical architecture
t2 = 1.0    # Complex untested
t3 = 0.7    # Testing gaps
t4 = 0.3    # Maintenance
</code></pre>
<h3 id="customizing-tier-thresholds"><a class="header" href="#customizing-tier-thresholds">Customizing Tier Thresholds</a></h3>
<p>Adjust thresholds to match your teamâ€™s standards:</p>
<pre><code class="language-toml"># Stricter thresholds for high-quality codebases
[tiers]
t2_complexity_threshold = 12
t3_complexity_threshold = 8

# More lenient for legacy codebases
[tiers]
t2_complexity_threshold = 20
t3_complexity_threshold = 15
</code></pre>
<h3 id="tier-weight-customization"><a class="header" href="#tier-weight-customization">Tier Weight Customization</a></h3>
<p>Adjust weights based on your priorities:</p>
<pre><code class="language-toml"># Emphasize testing over architecture
[tiers.tier_weights]
t1 = 1.2    # Reduce architecture weight
t2 = 1.3    # Increase testing weight
t3 = 0.8
t4 = 0.3

# Focus on architecture first
[tiers.tier_weights]
t1 = 2.0    # Maximize architecture weight
t2 = 1.0
t3 = 0.5
t4 = 0.2
</code></pre>
<h2 id="use-cases-5"><a class="header" href="#use-cases-5">Use Cases</a></h2>
<h3 id="sprint-planning"><a class="header" href="#sprint-planning">Sprint Planning</a></h3>
<p>Use tiered prioritization to allocate work:</p>
<pre><code class="language-bash"># See Tier 1 items for architectural planning
debtmap analyze . --filter Architecture --top 5

# See Tier 2/3 for testing sprint work
debtmap analyze . --filter Testing --min-priority medium
</code></pre>
<h3 id="code-review-focus"><a class="header" href="#code-review-focus">Code Review Focus</a></h3>
<p>Prioritize review attention based on tiers:</p>
<ul>
<li><strong>Tier 1</strong>: Architectural review required, senior dev attention</li>
<li><strong>Tier 2</strong>: Test coverage validation critical</li>
<li><strong>Tier 3</strong>: Standard review process</li>
<li><strong>Tier 4</strong>: Quick review or automated checks</li>
</ul>
<h3 id="refactoring-strategy"><a class="header" href="#refactoring-strategy">Refactoring Strategy</a></h3>
<pre><code class="language-bash"># Phase 1: Address Tier 1 architectural issues
debtmap analyze . --filter Architecture

# Phase 2: Add tests for Tier 2 complex code
debtmap analyze . --lcov coverage.lcov --min-priority high

# Phase 3: Improve Tier 3 coverage
debtmap analyze . --lcov coverage.lcov --min-priority medium
</code></pre>
<h2 id="best-practices-10"><a class="header" href="#best-practices-10">Best Practices</a></h2>
<ol>
<li><strong>Always address Tier 1 before feature work</strong> - Architectural issues compound</li>
<li><strong>Test Tier 2 items before refactoring</strong> - Avoid regressions</li>
<li><strong>Batch Tier 3 items</strong> - Address multiple in one sprint</li>
<li><strong>Defer Tier 4 items</strong> - Only fix during cleanup or when convenient</li>
<li><strong>Track tier distribution over time</strong> - Aim to reduce Tier 1/2 counts</li>
</ol>
<h2 id="interpreting-tier-output"><a class="header" href="#interpreting-tier-output">Interpreting Tier Output</a></h2>
<h3 id="terminal-output-1"><a class="header" href="#terminal-output-1">Terminal Output</a></h3>
<pre><code>â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    TIERED TECHNICAL DEBT REPORT
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ”´ TIER 1: CRITICAL ARCHITECTURE (3 items)
  1. src/services.rs - God Object (85% god score, 52 methods)
  2. src/core/engine.rs - Circular dependency with parsers module
  3. src/api/handlers.rs - God Module (15 responsibilities)

ğŸŸ  TIER 2: COMPLEX UNTESTED (12 items)
  1. src/processing/transform.rs:145 - Complexity 18, Coverage 0%
  ...

ğŸŸ¡ TIER 3: TESTING GAPS (45 items)
  ...

âšª TIER 4: MAINTENANCE (120 items) [hidden]
  Use --show-t4 to display maintenance items
</code></pre>
<h3 id="json-output-1"><a class="header" href="#json-output-1">JSON Output</a></h3>
<pre><code class="language-json">{
  "tier_distribution": {
    "t1_count": 3,
    "t2_count": 12,
    "t3_count": 45,
    "t4_count": 120
  },
  "items": [
    {
      "tier": "T1_CriticalArchitecture",
      "priority_weight": 1.5,
      "base_score": 8.5,
      "final_score": 12.75
    }
  ]
}
</code></pre>
<h2 id="troubleshooting-10"><a class="header" href="#troubleshooting-10">Troubleshooting</a></h2>
<p><strong>Issue</strong>: Too many Tier 1 items</p>
<p><strong>Solution</strong>: Lower tier weights or increase thresholds temporarily:</p>
<pre><code class="language-toml">[tiers.tier_weights]
t1 = 1.2    # Reduce from 1.5
</code></pre>
<p><strong>Issue</strong>: Not enough items in Tier 1</p>
<p><strong>Solution</strong>: Check if god object detection is enabled:</p>
<pre><code class="language-toml">[god_object_detection]
enabled = true
</code></pre>
<p><strong>Issue</strong>: All items in Tier 4</p>
<p><strong>Solution</strong>: Lower minimum thresholds:</p>
<pre><code class="language-toml">[thresholds]
minimum_debt_score = 1.0
minimum_cyclomatic_complexity = 2
</code></pre>
<h2 id="see-also-7"><a class="header" href="#see-also-7">See Also</a></h2>
<ul>
<li><a href="./scoring-strategies.html">Scoring Strategies</a> - Understanding file-level vs function-level scoring</li>
<li><a href="./configuration.html">Configuration</a> - Complete configuration reference</li>
<li><a href="./analysis-guide.html">Analysis Guide</a> - Detailed metric explanations</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="examples-1"><a class="header" href="#examples-1">Examples</a></h1>
<p>This chapter provides practical, real-world examples of using Debtmap across different project types and workflows. All examples use current CLI syntax verified against the source code.</p>
<blockquote>
<p><strong>Quick Start</strong>: New to Debtmap? Start with <a href="examples.html#basic-rust-analysis">Basic Rust Analysis</a> for the simplest introduction, then explore <a href="examples.html#coverage-integration-with-cargo-tarpaulin">Coverage Integration</a> for risk-based prioritization.</p>
</blockquote>
<blockquote>
<p><strong>Quick Navigation</strong>: For detailed explanations of all CLI options, see the <a href="cli-reference.html">CLI Reference</a> chapter.</p>
</blockquote>
<h2 id="overview-7"><a class="header" href="#overview-7">Overview</a></h2>
<p>This chapter demonstrates:</p>
<ul>
<li><strong>Language-specific analysis</strong>: Rust, Python, JavaScript/TypeScript with their respective testing tools</li>
<li><strong>CI/CD integration</strong>: GitHub Actions, GitLab CI, CircleCI with validation gates</li>
<li><strong>Output formats</strong>: Terminal, JSON, and Markdown with interpretation guidance</li>
<li><strong>Advanced features</strong>: Context-aware analysis, multi-pass processing, cache management</li>
<li><strong>Configuration patterns</strong>: Tailored settings for different project types</li>
<li><strong>Progress tracking</strong>: Using the <code>compare</code> command to validate refactoring improvements</li>
</ul>
<p>All examples are copy-paste ready and tested against the current Debtmap implementation.</p>
<h2 id="table-of-contents"><a class="header" href="#table-of-contents">Table of Contents</a></h2>
<ul>
<li><a href="examples.html#analyzing-rust-projects">Analyzing Rust Projects</a></li>
<li><a href="examples.html#python-analysis">Python Analysis</a></li>
<li><a href="examples.html#javascripttypescript">JavaScript/TypeScript</a></li>
<li><a href="examples.html#ci-integration">CI Integration</a></li>
<li><a href="examples.html#output-formats">Output Formats</a></li>
<li><a href="examples.html#advanced-usage">Advanced Usage</a></li>
<li><a href="examples.html#configuration-examples">Configuration Examples</a></li>
<li><a href="examples.html#compare-command">Compare Command</a></li>
</ul>
<h2 id="analyzing-rust-projects"><a class="header" href="#analyzing-rust-projects">Analyzing Rust Projects</a></h2>
<h3 id="basic-rust-analysis"><a class="header" href="#basic-rust-analysis">Basic Rust Analysis</a></h3>
<p>Start with a simple analysis of your Rust project:</p>
<pre><code class="language-bash"># Analyze all Rust files in current directory
debtmap analyze .

# Analyze specific directory
debtmap analyze ./src

# Analyze with custom complexity threshold
debtmap analyze ./src --threshold-complexity 15
</code></pre>
<h3 id="coverage-integration-with-cargo-tarpaulin"><a class="header" href="#coverage-integration-with-cargo-tarpaulin">Coverage Integration with cargo-tarpaulin</a></h3>
<p>Combine complexity analysis with test coverage for risk-based prioritization:</p>
<pre><code class="language-bash"># Generate LCOV coverage data
cargo tarpaulin --out lcov --output-dir target/coverage

# Analyze with coverage data
debtmap analyze . --lcov target/coverage/lcov.info

# Or use the shorter alias
debtmap analyze . --coverage-file target/coverage/lcov.info
</code></pre>
<blockquote>
<p><strong>Note</strong>: <code>--lcov</code> is an alias for <code>--coverage-file</code> - both work identically.</p>
</blockquote>
<p><strong>What this does:</strong></p>
<ul>
<li>Functions with 0% coverage and high complexity get marked as <code>[CRITICAL]</code></li>
<li>Well-tested functions (&gt;80% coverage) are deprioritized</li>
<li>Shows risk reduction potential for each untested function</li>
</ul>
<h3 id="custom-thresholds"><a class="header" href="#custom-thresholds">Custom Thresholds</a></h3>
<p>Configure thresholds to match your project standards:</p>
<pre><code class="language-bash"># Set both complexity and duplication thresholds
debtmap analyze . \
  --threshold-complexity 15 \
  --threshold-duplication 50

# Use preset configurations for quick setup
debtmap analyze . --threshold-preset strict    # Strict standards
debtmap analyze . --threshold-preset balanced  # Default balanced
debtmap analyze . --threshold-preset lenient   # Lenient for legacy code
</code></pre>
<p><strong>Preset configurations:</strong></p>
<ul>
<li><strong>Strict</strong>: Lower thresholds for high quality standards (good for new projects)</li>
<li><strong>Balanced</strong>: Default thresholds suitable for typical projects</li>
<li><strong>Lenient</strong>: Higher thresholds designed for legacy codebases with existing technical debt</li>
</ul>
<h3 id="god-object-detection-2"><a class="header" href="#god-object-detection-2">God Object Detection</a></h3>
<p>Identify classes and modules with too many responsibilities:</p>
<pre><code class="language-bash"># Standard analysis includes god object detection
debtmap analyze .

# Disable god object detection for specific run
debtmap analyze . --no-god-object
</code></pre>
<p>God objects are flagged with detailed metrics:</p>
<ul>
<li>Number of methods and fields</li>
<li>Responsibility count (grouped by naming patterns)</li>
<li>God object score (0-100%)</li>
<li>Recommendations for splitting</li>
</ul>
<h4 id="purity-weighted-god-object-scoring"><a class="header" href="#purity-weighted-god-object-scoring">Purity-Weighted God Object Scoring</a></h4>
<p>Debtmap uses purity analysis to distinguish functional programming patterns from actual god objects. Enable verbose mode to see purity distribution:</p>
<pre><code class="language-bash"># See purity distribution in god object analysis
debtmap analyze . -v
</code></pre>
<p><strong>Example Output:</strong></p>
<pre><code>GOD OBJECT ANALYSIS: src/core/processor.rs
  Total functions: 107
  PURITY DISTRIBUTION:
    Pure: 70 functions (65%) â†’ complexity weight: 6.3
    Impure: 37 functions (35%) â†’ complexity weight: 14.0
    Total weighted complexity: 20.3
  God object score: 12.0 (threshold: 70.0)
  Status: âœ“ Not a god object (functional design)
</code></pre>
<p>This shows:</p>
<ul>
<li><strong>Pure functions</strong> (no side effects, immutable) receive 0.3Ã— weight</li>
<li><strong>Impure functions</strong> (I/O, mutations, side effects) receive 1.0Ã— weight</li>
<li>Functional modules with many pure helpers avoid false positives</li>
<li>Focus shifts to modules with excessive stateful code</li>
</ul>
<p><strong>Why This Matters:</strong></p>
<p>Without purity weighting:</p>
<pre><code>Module with 100 pure helpers â†’ Flagged as god object âŒ
</code></pre>
<p>With purity weighting:</p>
<pre><code>Module with 100 pure helpers â†’ Normal (functional design) âœ…
Module with 100 impure functions â†’ God object detected âœ…
</code></pre>
<p><strong>Compare Two Modules:</strong></p>
<p>Functional module (70 pure, 30 impure):</p>
<pre><code>Pure:    70 Ã— 0.3 = 21.0
Impure:  30 Ã— 1.0 = 30.0
Score: 35.0 â†’ Not a god object âœ“
</code></pre>
<p>Procedural module (100 impure):</p>
<pre><code>Impure: 100 Ã— 1.0 = 100.0
Score: 125.0 â†’ God object detected âœ—
</code></pre>
<h3 id="filtering-and-focusing"><a class="header" href="#filtering-and-focusing">Filtering and Focusing</a></h3>
<pre><code class="language-bash"># Analyze only Rust files
debtmap analyze . --languages rust

# Focus on architecture issues (god objects, complexity)
debtmap analyze . --filter Architecture

# Focus on testing gaps
debtmap analyze . --filter Testing

# Filter by multiple categories
debtmap analyze . --filter Architecture,Testing

# Show only top 10 issues
debtmap analyze . --top 10

# Show only high-priority items
debtmap analyze . --min-priority high
</code></pre>
<p><strong>Valid filter categories:</strong></p>
<ul>
<li><code>Architecture</code> - God objects, high complexity, structural issues</li>
<li><code>Testing</code> - Test coverage gaps, untested critical code</li>
<li><code>Duplication</code> - Code duplication and similar patterns</li>
<li><code>Maintainability</code> - Long functions, deep nesting, readability issues</li>
</ul>
<h3 id="output-formats-2"><a class="header" href="#output-formats-2">Output Formats</a></h3>
<pre><code class="language-bash"># JSON output for CI integration
debtmap analyze . --format json --output report.json

# Markdown report
debtmap analyze . --format markdown --output DEBT_REPORT.md

# Terminal output (default) - prettified
debtmap analyze .
</code></pre>
<h3 id="multi-pass-analysis"><a class="header" href="#multi-pass-analysis">Multi-Pass Analysis</a></h3>
<p>For deeper analysis with context awareness:</p>
<pre><code class="language-bash"># Enable context-aware analysis with multiple providers
debtmap analyze . \
  --context \
  --context-providers critical_path,dependency,git_history

# Multi-pass analysis with attribution
debtmap analyze . --multi-pass --attribution
</code></pre>
<h3 id="complete-ci-example"><a class="header" href="#complete-ci-example">Complete CI Example</a></h3>
<p>This is from Debtmapâ€™s own <code>.github/workflows/debtmap.yml</code>:</p>
<pre><code class="language-bash"># 1. Install cargo-tarpaulin
cargo install cargo-tarpaulin

# 2. Build debtmap
cargo build --release

# 3. Generate coverage
cargo tarpaulin --config .tarpaulin.toml --out Lcov --timeout 300

# 4. Run validation with coverage
./target/release/debtmap validate . \
  --coverage-file target/coverage/lcov.info \
  --format json \
  --output debtmap-report.json
</code></pre>
<h2 id="python-analysis"><a class="header" href="#python-analysis">Python Analysis</a></h2>
<h3 id="basic-python-analysis"><a class="header" href="#basic-python-analysis">Basic Python Analysis</a></h3>
<pre><code class="language-bash"># Analyze Python files only
debtmap analyze . --languages python

# Analyze specific Python directory
debtmap analyze src --languages python
</code></pre>
<h3 id="coverage-integration-with-pytest"><a class="header" href="#coverage-integration-with-pytest">Coverage Integration with pytest</a></h3>
<p>Generate coverage and analyze risk:</p>
<pre><code class="language-bash"># Generate LCOV coverage with pytest
pytest --cov --cov-report=lcov

# Analyze with coverage data
debtmap analyze . \
  --languages python \
  --lcov coverage.lcov
</code></pre>
<h3 id="python-specific-patterns"><a class="header" href="#python-specific-patterns">Python-Specific Patterns</a></h3>
<pre><code class="language-bash"># Focus on testing gaps in Python code
debtmap analyze . \
  --languages python \
  --filter Testing

# Find god objects in Python modules
debtmap analyze . \
  --languages python \
  --filter Architecture
</code></pre>
<h3 id="example-configuration-for-python-projects"><a class="header" href="#example-configuration-for-python-projects">Example Configuration for Python Projects</a></h3>
<p>Create <code>.debtmap.toml</code>:</p>
<pre><code class="language-toml">[languages]
enabled = ["python"]

[thresholds]
complexity = 12
max_function_lines = 40

[ignore]
patterns = [
  "**/*_test.py",
  "tests/**",
  ".venv/**",
  "**/__pycache__/**",
]

[god_object]
enabled = true
max_methods = 15
max_responsibilities = 4
</code></pre>
<h2 id="javascripttypescript-1"><a class="header" href="#javascripttypescript-1">JavaScript/TypeScript</a></h2>
<h3 id="analyzing-jsts-projects"><a class="header" href="#analyzing-jsts-projects">Analyzing JS/TS Projects</a></h3>
<pre><code class="language-bash"># Analyze JavaScript and TypeScript
debtmap analyze . --languages javascript,typescript

# TypeScript only
debtmap analyze . --languages typescript
</code></pre>
<h3 id="coverage-integration-with-jest"><a class="header" href="#coverage-integration-with-jest">Coverage Integration with Jest</a></h3>
<pre><code class="language-bash"># Generate LCOV with Jest
jest --coverage --coverageReporters=lcov

# Analyze with coverage
debtmap analyze . \
  --languages javascript,typescript \
  --lcov coverage/lcov.info
</code></pre>
<h3 id="nodejs-project-patterns"><a class="header" href="#nodejs-project-patterns">Node.js Project Patterns</a></h3>
<pre><code class="language-bash"># Exclude node_modules and focus on source
debtmap analyze src --languages javascript,typescript

# With custom complexity thresholds for JS
debtmap analyze . \
  --languages javascript,typescript \
  --threshold-complexity 10
</code></pre>
<h3 id="typescript-configuration-example"><a class="header" href="#typescript-configuration-example">TypeScript Configuration Example</a></h3>
<p>Create <code>.debtmap.toml</code>:</p>
<pre><code class="language-toml">[languages]
enabled = ["typescript", "javascript"]

[thresholds]
complexity = 10
max_function_lines = 50

[ignore]
patterns = [
  "node_modules/**",
  "**/*.test.ts",
  "**/*.spec.ts",
  "dist/**",
  "build/**",
  "**/*.d.ts",
]
</code></pre>
<h3 id="monorepo-analysis"><a class="header" href="#monorepo-analysis">Monorepo Analysis</a></h3>
<pre><code class="language-bash"># Analyze specific package
debtmap analyze packages/api --languages typescript

# Analyze all packages, grouped by category
debtmap analyze packages \
  --languages typescript \
  --group-by-category
</code></pre>
<h2 id="ci-integration-1"><a class="header" href="#ci-integration-1">CI Integration</a></h2>
<h3 id="github-actions-2"><a class="header" href="#github-actions-2">GitHub Actions</a></h3>
<p>Complete workflow example (from <code>.github/workflows/debtmap.yml</code>):</p>
<pre><code class="language-yaml">name: Debtmap

on:
  push:
    branches: [ main, master ]
  pull_request:
    branches: [ main, master ]
  workflow_dispatch:

env:
  CARGO_TERM_COLOR: always
  RUST_BACKTRACE: 1

jobs:
  validate:
    name: Technical Debt Validation
    runs-on: ubuntu-latest

    steps:
    - name: Checkout repository
      uses: actions/checkout@v5
      with:
        fetch-depth: 0

    - name: Setup Rust
      uses: dtolnay/rust-toolchain@stable
      with:
        components: rustfmt, clippy

    - name: Cache cargo dependencies
      uses: actions/cache@v4
      with:
        path: |
          ~/.cargo/bin/
          ~/.cargo/registry/index/
          ~/.cargo/registry/cache/
          ~/.cargo/git/db/
          target/
        key: ${{ runner.os }}-cargo-${{ hashFiles('**/Cargo.lock') }}
        restore-keys: |
          ${{ runner.os }}-cargo-

    - name: Install cargo-tarpaulin
      run: |
        if ! command -v cargo-tarpaulin &amp;&gt; /dev/null; then
          cargo install cargo-tarpaulin
        else
          echo "cargo-tarpaulin already installed"
        fi

    - name: Build debtmap
      run: cargo build --release

    - name: Generate coverage data
      run: cargo tarpaulin --config .tarpaulin.toml --out Lcov --timeout 300

    - name: Run debtmap validation with coverage
      run: |
        if [ -f "target/coverage/lcov.info" ]; then
          ./target/release/debtmap validate . --coverage-file target/coverage/lcov.info --format json --output debtmap-report.json
        else
          echo "Warning: LCOV file not found, running validation without coverage data"
          ./target/release/debtmap validate . --format json --output debtmap-report.json
        fi

    - name: Upload debtmap report and coverage
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: debtmap-analysis-artifacts
        path: |
          debtmap-report.json
          target/coverage/lcov.info
        retention-days: 7
</code></pre>
<h3 id="gitlab-ci-2"><a class="header" href="#gitlab-ci-2">GitLab CI</a></h3>
<pre><code class="language-yaml">debtmap:
  stage: quality
  image: rust:latest
  script:
    # Install debtmap
    - cargo install debtmap

    # Run tests with coverage (generates LCOV format)
    - cargo install cargo-tarpaulin
    - cargo tarpaulin --out Lcov

    # Validate with debtmap (using LCOV format)
    - debtmap validate .
        --coverage-file lcov.info
        --format json
        --output debtmap-report.json
  artifacts:
    paths:
      - lcov.info
      - debtmap-report.json
    expire_in: 1 week
</code></pre>
<h3 id="circleci"><a class="header" href="#circleci">CircleCI</a></h3>
<pre><code class="language-yaml">version: 2.1

jobs:
  debtmap:
    docker:
      - image: cimg/rust:1.75
    steps:
      - checkout

      - run:
          name: Install debtmap
          command: cargo install debtmap

      - run:
          name: Generate coverage
          command: |
            cargo install cargo-tarpaulin
            cargo tarpaulin --out Lcov

      - run:
          name: Run debtmap
          command: |
            debtmap validate . \
              --coverage-file lcov.info \
              --format json \
              --output debtmap.json

      - store_artifacts:
          path: debtmap.json

workflows:
  version: 2
  build:
    jobs:
      - debtmap
</code></pre>
<h3 id="using-debtmap-validate-for-pr-gates"><a class="header" href="#using-debtmap-validate-for-pr-gates">Using debtmap validate for PR Gates</a></h3>
<pre><code class="language-bash"># Fail build if thresholds are exceeded
debtmap validate . --coverage-file lcov.info

# With custom thresholds
debtmap validate . \
  --coverage-file lcov.info \
  --threshold-complexity 15

# Exit code 0 if passing, 1 if failing
</code></pre>
<h3 id="compare-command-in-ci"><a class="header" href="#compare-command-in-ci">Compare Command in CI</a></h3>
<p>Track technical debt trends over time:</p>
<pre><code class="language-bash"># Generate baseline (on main branch)
debtmap analyze . --format json --output baseline.json

# After PR changes
debtmap analyze . --format json --output current.json

# Compare and fail if regressions detected
debtmap compare \
  --before baseline.json \
  --after current.json \
  --format json
</code></pre>
<h2 id="output-formats-3"><a class="header" href="#output-formats-3">Output Formats</a></h2>
<h3 id="terminal-output-default"><a class="header" href="#terminal-output-default">Terminal Output (Default)</a></h3>
<p>The default terminal output is prettified with colors and priorities:</p>
<pre><code class="language-bash">debtmap analyze . --lcov coverage.lcov --top 3
</code></pre>
<p>Example output:</p>
<pre><code>â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    PRIORITY TECHNICAL DEBT FIXES
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ¯ TOP 3 RECOMMENDATIONS (by unified priority)

#1 SCORE: 8.9 [CRITICAL]
â”œâ”€ TEST GAP: ./src/analyzers/rust.rs:38 parse_function()
â”œâ”€ ACTION: Add 6 unit tests for full coverage
â”œâ”€ IMPACT: Full test coverage, -3.7 risk
â”œâ”€ COMPLEXITY: cyclomatic=6, cognitive=8, nesting=2, lines=32
â”œâ”€ DEPENDENCIES: 0 upstream, 11 downstream
â””â”€ WHY: Business logic with 0% coverage, manageable complexity

ğŸ“Š TOTAL DEBT SCORE: 4907
ğŸ“ˆ OVERALL COVERAGE: 67.12%
</code></pre>
<h3 id="json-output-2"><a class="header" href="#json-output-2">JSON Output</a></h3>
<p>Machine-readable format for CI/CD integration:</p>
<pre><code class="language-bash">debtmap analyze . --format json --output report.json
</code></pre>
<p><strong>Using JSON output programmatically:</strong></p>
<pre><code class="language-bash"># Extract total debt score
debtmap analyze . --format json | jq '.total_debt_score'

# Count critical items
debtmap analyze . --format json | jq '[.items[] | select(.unified_score.final_score &gt;= 8)] | length'

# Get top 5 functions by score
debtmap analyze . --format json | jq '.items | sort_by(-.unified_score.final_score) | .[0:5] | .[].location'

# Extract all test gap items
debtmap analyze . --format json | jq '[.items[] | select(.debt_type == "TestGap")]'
</code></pre>
<p>Structure:</p>
<pre><code class="language-json">{
  "items": [
    {
      "location": {
        "file": "src/main.rs",
        "function": "process_data",
        "line": 42
      },
      "debt_type": "TestGap",
      "unified_score": {
        "complexity_factor": 3.2,
        "coverage_factor": 10.0,
        "dependency_factor": 2.5,
        "role_multiplier": 1.2,
        "final_score": 9.4
      },
      "function_role": "BusinessLogic",
      "recommendation": {
        "action": "Add unit tests",
        "details": "Add 6 unit tests for full coverage",
        "effort_estimate": "2-3 hours"
      },
      "expected_impact": {
        "risk_reduction": 3.9,
        "complexity_reduction": 0,
        "coverage_improvement": 100
      }
    }
  ],
  "overall_coverage": 67.12,
  "total_debt_score": 4907
}
</code></pre>
<h3 id="markdown-report"><a class="header" href="#markdown-report">Markdown Report</a></h3>
<pre><code class="language-bash">debtmap analyze . --format markdown --output DEBT_REPORT.md
</code></pre>
<p>Great for documentation or PR comments.</p>
<h3 id="understanding-output-formats-1"><a class="header" href="#understanding-output-formats-1">Understanding Output Formats</a></h3>
<pre><code class="language-bash"># JSON output (default is legacy format)
debtmap analyze . --format json

# Unified JSON format (alternative to legacy)
debtmap analyze . --format json --output-format unified

# Legacy JSON format (default, for backward compatibility)
debtmap analyze . --format json --output-format legacy

# Output format options: terminal, json, markdown
debtmap analyze . --format terminal
</code></pre>
<p><strong>Unified vs Legacy JSON Formats:</strong></p>
<p>The unified format provides a consistent structure with a <code>type</code> field to distinguish between different item types, replacing the File/Function wrapper objects used in legacy format.</p>
<ul>
<li><strong>Unified format</strong>: Cleaner schema, consistent structure across all items, easier to parse programmatically</li>
<li><strong>Legacy format</strong>: Default for backward compatibility with existing tooling and scripts</li>
</ul>
<p>Use unified format for new integrations and tools. Use legacy format when working with existing debtmap analysis pipelines.</p>
<h2 id="advanced-usage-1"><a class="header" href="#advanced-usage-1">Advanced Usage</a></h2>
<h3 id="context-aware-analysis-3"><a class="header" href="#context-aware-analysis-3">Context-Aware Analysis</a></h3>
<p>Enable advanced context providers for more accurate prioritization:</p>
<pre><code class="language-bash"># Enable all context providers
debtmap analyze . \
  --context \
  --context-providers critical_path,dependency,git_history

# Disable specific providers
debtmap analyze . \
  --context \
  --disable-context git_history
</code></pre>
<h3 id="multi-pass-analysis-1"><a class="header" href="#multi-pass-analysis-1">Multi-Pass Analysis</a></h3>
<pre><code class="language-bash"># Multi-pass with attribution tracking
debtmap analyze . --multi-pass --attribution

# Shows which functions contribute to which patterns
</code></pre>
<h3 id="cache-management-2"><a class="header" href="#cache-management-2">Cache Management</a></h3>
<pre><code class="language-bash"># Show cache statistics
debtmap analyze . --cache-stats

# Clear cache before analysis
debtmap analyze . --clear-cache

# Force cache rebuild
debtmap analyze . --force-cache-rebuild
</code></pre>
<h3 id="aggregation-methods-1"><a class="header" href="#aggregation-methods-1">Aggregation Methods</a></h3>
<pre><code class="language-bash"># Use logarithmic sum for aggregation
debtmap analyze . --aggregation-method logarithmic_sum

# Standard sum (default)
debtmap analyze . --aggregation-method sum
</code></pre>
<h3 id="filtering-and-grouping"><a class="header" href="#filtering-and-grouping">Filtering and Grouping</a></h3>
<pre><code class="language-bash"># Group results by debt category
debtmap analyze . --group-by-category

# Filter specific categories
debtmap analyze . --filter Architecture,Testing

# Show only high-priority items
debtmap analyze . --min-priority high --top 10
</code></pre>
<h3 id="verbosity-levels-1"><a class="header" href="#verbosity-levels-1">Verbosity Levels</a></h3>
<pre><code class="language-bash"># Level 1: Show main score factors
debtmap analyze . -v

# Level 2: Show detailed calculations
debtmap analyze . -vv

# Level 3: Show all debug information
debtmap analyze . -vvv

# Long form also available
debtmap analyze . --verbose

# Show macro expansion details (Rust)
debtmap analyze . --verbose-macro-warnings --show-macro-stats
</code></pre>
<h3 id="parallel-processing-control"><a class="header" href="#parallel-processing-control">Parallel Processing Control</a></h3>
<pre><code class="language-bash"># Use 8 parallel jobs
debtmap analyze . --jobs 8

# Disable parallel processing
debtmap analyze . --no-parallel
</code></pre>
<h2 id="configuration-examples-1"><a class="header" href="#configuration-examples-1">Configuration Examples</a></h2>
<h3 id="basic-configuration"><a class="header" href="#basic-configuration">Basic Configuration</a></h3>
<p>Create <code>.debtmap.toml</code>:</p>
<pre><code class="language-toml">[thresholds]
complexity = 15
duplication = 25
max_function_lines = 50
max_nesting_depth = 4

[languages]
enabled = ["rust", "python"]

[ignore]
patterns = [
  "tests/**/*",
  "**/*.test.rs",
  "target/**",
]
</code></pre>
<h3 id="entropy-based-complexity-1"><a class="header" href="#entropy-based-complexity-1">Entropy-Based Complexity</a></h3>
<pre><code class="language-toml">[entropy]
enabled = true
weight = 0.5
use_classification = true
pattern_threshold = 0.7
entropy_threshold = 0.4
branch_threshold = 0.8
max_combined_reduction = 0.3
</code></pre>
<p>This reduces false positives for repetitive code patterns.</p>
<h3 id="custom-scoring-weights"><a class="header" href="#custom-scoring-weights">Custom Scoring Weights</a></h3>
<pre><code class="language-toml">[scoring]
coverage = 0.40      # Test coverage gaps
complexity = 0.40    # Code complexity
dependency = 0.20    # Dependency criticality
</code></pre>
<h3 id="god-object-detection-tuning"><a class="header" href="#god-object-detection-tuning">God Object Detection Tuning</a></h3>
<pre><code class="language-toml">[god_object]
enabled = true

# Rust-specific thresholds
[god_object.rust]
max_methods = 25
max_fields = 15
max_traits = 5
max_lines = 400
max_complexity = 50

# Python-specific thresholds
[god_object.python]
max_methods = 20
max_fields = 12
max_lines = 350
max_complexity = 45

# JavaScript/TypeScript-specific thresholds
[god_object.javascript]
max_methods = 20
max_fields = 12
max_lines = 300
max_complexity = 40
</code></pre>
<h3 id="external-api-configuration-1"><a class="header" href="#external-api-configuration-1">External API Configuration</a></h3>
<p>For libraries (not CLI tools):</p>
<pre><code class="language-toml">[external_api]
detect_external_api = true

api_functions = [
  "parse",
  "Parser::new",
  "client::connect",
]

api_files = [
  "src/lib.rs",
  "src/api.rs",
  "src/public/*.rs",
]
</code></pre>
<h3 id="complete-multi-language-configuration"><a class="header" href="#complete-multi-language-configuration">Complete Multi-Language Configuration</a></h3>
<pre><code class="language-toml">[thresholds]
complexity = 12
duplication = 30
max_file_lines = 400
max_function_lines = 40
minimum_debt_score = 1.0
minimum_cyclomatic_complexity = 2

[entropy]
enabled = true
weight = 0.5

[scoring]
coverage = 0.40
complexity = 0.40
dependency = 0.20

[languages]
enabled = ["rust", "python", "javascript", "typescript"]

[ignore]
patterns = [
  # Tests
  "tests/**/*",
  "**/*.test.*",
  "**/*_test.*",

  # Build artifacts
  "target/**",
  "dist/**",
  "build/**",
  "node_modules/**",

  # Python
  ".venv/**",
  "**/__pycache__/**",

  # Generated code
  "*.generated.*",
  "*.pb.*",
]

[god_object]
enabled = true
max_methods = 18
max_fields = 12
</code></pre>
<h2 id="compare-command"><a class="header" href="#compare-command">Compare Command</a></h2>
<p>The <code>compare</code> command helps validate that refactoring achieved its goals.</p>
<h3 id="basic-comparison-workflow"><a class="header" href="#basic-comparison-workflow">Basic Comparison Workflow</a></h3>
<pre><code class="language-bash"># 1. Generate baseline before refactoring
debtmap analyze . --format json --output before.json

# 2. Make your code improvements
#    ... refactor, add tests, etc ...

# 3. Generate new analysis
debtmap analyze . --format json --output after.json

# 4. Compare and verify improvements
debtmap compare --before before.json --after after.json
</code></pre>
<h3 id="target-specific-comparison"><a class="header" href="#target-specific-comparison">Target-Specific Comparison</a></h3>
<p>Focus on whether a specific function improved:</p>
<pre><code class="language-bash"># Target format: file:function:line
debtmap compare \
  --before before.json \
  --after after.json \
  --target-location src/main.rs:process_data:100
</code></pre>
<h3 id="using-with-implementation-plans"><a class="header" href="#using-with-implementation-plans">Using with Implementation Plans</a></h3>
<p>Extract target automatically from plan files:</p>
<pre><code class="language-bash"># If IMPLEMENTATION_PLAN.md contains:
# **Target**: src/parser.rs:parse_expression:45

debtmap compare \
  --before before.json \
  --after after.json \
  --plan IMPLEMENTATION_PLAN.md
</code></pre>
<h3 id="output-formats-4"><a class="header" href="#output-formats-4">Output Formats</a></h3>
<pre><code class="language-bash"># JSON output (default)
debtmap compare --before before.json --after after.json

# Terminal output (explicit)
debtmap compare \
  --before before.json \
  --after after.json \
  --format terminal

# JSON for CI integration (explicit output file)
debtmap compare \
  --before before.json \
  --after after.json \
  --format json \
  --output comparison.json

# Markdown report
debtmap compare \
  --before before.json \
  --after after.json \
  --format markdown \
  --output COMPARISON.md
</code></pre>
<h3 id="interpreting-results-1"><a class="header" href="#interpreting-results-1">Interpreting Results</a></h3>
<p><strong>Target Status:</strong></p>
<ul>
<li><strong>Resolved</strong>: Function no longer appears (complexity reduced below threshold)</li>
<li><strong>Improved</strong>: Metrics improved (complexity down, coverage up)</li>
<li><strong>Unchanged</strong>: No significant change</li>
<li><strong>Regressed</strong>: Metrics got worse</li>
<li><strong>Not Found</strong>: Target not found in baseline</li>
</ul>
<p><strong>Overall Trend:</strong></p>
<ul>
<li><strong>Improving</strong>: More items resolved/improved than regressed</li>
<li><strong>Stable</strong>: No significant changes</li>
<li><strong>Regressing</strong>: New critical debt introduced</li>
</ul>
<p><strong>Example Output:</strong></p>
<pre><code>Target Status: Resolved âœ…
- src/parser.rs:parse_expression:45 reduced from complexity 22 to 8
- Coverage improved from 0% to 85%

Overall Trend: Improving
- 3 items resolved
- 2 items improved
- 0 regressions
- Total debt score: 450 â†’ 285 (-37%)
</code></pre>
<h3 id="ci-integration-2"><a class="header" href="#ci-integration-2">CI Integration</a></h3>
<p>Use in pull request validation:</p>
<pre><code class="language-bash"># In CI script
debtmap compare \
  --before baseline.json \
  --after current.json \
  --format json | jq -e '.overall_trend == "Improving"'

# Exit code 0 if improving, 1 otherwise
</code></pre>
<h2 id="tips-and-best-practices"><a class="header" href="#tips-and-best-practices">Tips and Best Practices</a></h2>
<ol>
<li><strong>Start Simple</strong>: Begin with basic analysis, add coverage later</li>
<li><strong>Use Filters</strong>: Focus on one category at a time (Architecture, Testing)</li>
<li><strong>Iterate</strong>: Run analysis, fix top items, repeat</li>
<li><strong>CI Integration</strong>: Automate validation in your build pipeline</li>
<li><strong>Track Progress</strong>: Use <code>compare</code> command to validate improvements</li>
<li><strong>Configure Thresholds</strong>: Adjust to match your teamâ€™s standards</li>
<li><strong>Leverage Coverage</strong>: Always include coverage data for accurate risk assessment</li>
</ol>
<h2 id="next-steps-3"><a class="header" href="#next-steps-3">Next Steps</a></h2>
<ul>
<li><a href="cli-reference.html">CLI Reference</a> - Complete CLI documentation</li>
<li><a href="analysis-guide.html">Analysis Guide</a> - Understanding analysis results</li>
<li><a href="configuration.html">Configuration</a> - Advanced configuration options</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="frequently-asked-questions"><a class="header" href="#frequently-asked-questions">Frequently Asked Questions</a></h1>
<p>Common questions about debtmapâ€™s features, usage, and comparison with other tools.</p>
<h2 id="features--capabilities"><a class="header" href="#features--capabilities">Features &amp; Capabilities</a></h2>
<h3 id="what-is-entropy-based-complexity-analysis"><a class="header" href="#what-is-entropy-based-complexity-analysis">What is entropy-based complexity analysis?</a></h3>
<p>Entropy analysis uses information theory to distinguish between genuinely complex code and repetitive pattern-based code. Traditional cyclomatic complexity counts branches, but not all branches are equal in cognitive load.</p>
<p>For example, a function with 20 identical if/return validation checks has the same cyclomatic complexity as a function with 20 diverse conditional branches handling different business logic. Entropy analysis gives the validation function a much lower effective complexity score because it follows a simple, repetitive pattern.</p>
<p><strong>Result:</strong> 60-75% reduction in false positives compared to traditional complexity metrics.</p>
<p><a href="why-debtmap.html#entropy-based-complexity-analysis">Read more in Why Debtmap?</a></p>
<h3 id="how-does-coverage-integration-work"><a class="header" href="#how-does-coverage-integration-work">How does coverage integration work?</a></h3>
<p>Debtmap reads LCOV format coverage data (generated by tools like <code>cargo-tarpaulin</code>, <code>pytest-cov</code>, or <code>jest</code>) and maps it to specific functions and branches. It then combines coverage percentages with complexity metrics to calculate risk scores.</p>
<p><strong>Key insight:</strong> A complex function with good test coverage is lower risk than a moderately complex function with no tests.</p>
<p><strong>Example workflow:</strong></p>
<pre><code class="language-bash"># Generate coverage data
cargo tarpaulin --out lcov --output-dir target/coverage

# Analyze with coverage integration
debtmap analyze . --lcov target/coverage/lcov.info
</code></pre>
<p><a href="analysis-guide.html#coverage-integrated-analysis">See examples in Analysis Guide</a></p>
<h3 id="what-languages-are-supported"><a class="header" href="#what-languages-are-supported">What languages are supported?</a></h3>
<p><strong>Full support:</strong></p>
<ul>
<li>Rust (via <code>syn</code> crate - complete AST analysis)</li>
<li>Python (via <code>rustpython</code> - full Python 3.x support)</li>
</ul>
<p><strong>Partial support:</strong></p>
<ul>
<li>JavaScript (via <code>tree-sitter</code> - ES6+, JSX)</li>
<li>TypeScript (via <code>tree-sitter</code> - basic support)</li>
</ul>
<p><strong>Planned:</strong></p>
<ul>
<li>Go (target: Q2 2025)</li>
<li>Java (target: Q3 2025)</li>
<li>C/C++ (target: Q4 2025)</li>
</ul>
<p>Language support means: AST parsing, metric extraction, complexity calculation, and pattern detection.</p>
<h3 id="can-i-customize-the-complexity-thresholds"><a class="header" href="#can-i-customize-the-complexity-thresholds">Can I customize the complexity thresholds?</a></h3>
<p>Yes! Configure thresholds in <code>.debtmap.toml</code>:</p>
<pre><code class="language-toml">[thresholds]
cyclomatic_complexity = 10      # Flag functions above this
nesting_depth = 3               # Maximum nesting levels
loc = 200                       # Maximum lines per function
parameter_count = 4             # Maximum parameters

[scoring]
critical_threshold = 8.0        # Risk score for Critical tier
high_threshold = 5.0            # Risk score for High tier
moderate_threshold = 2.0        # Risk score for Moderate tier
</code></pre>
<p>See <a href="configuration.html#thresholds">Configuration</a> for all available options.</p>
<h3 id="does-debtmap-integrate-with-cicd"><a class="header" href="#does-debtmap-integrate-with-cicd">Does debtmap integrate with CI/CD?</a></h3>
<p>Yes! Use the <code>validate</code> command to enforce quality gates:</p>
<pre><code class="language-bash"># Fail build if critical or high-tier debt detected
debtmap validate . --max-critical 0 --max-high 5

# Exit codes:
# 0 = validation passed
# 1 = validation failed (debt exceeds thresholds)
# 2 = analysis error
</code></pre>
<p><strong>GitHub Actions example:</strong></p>
<pre><code class="language-yaml">- name: Check technical debt
  run: |
    cargo tarpaulin --out lcov --output-dir target/coverage
    debtmap validate . --lcov target/coverage/lcov.info \
      --max-critical 0 --max-high 10 \
      --format json --output debt-report.json

- name: Comment on PR
  uses: actions/github-script@v6
  with:
    script: |
      const report = require('./debt-report.json');
      // Post report as PR comment
</code></pre>
<p>See <a href="prodigy-integration.html">Prodigy Integration</a> for more CI/CD patterns.</p>
<h2 id="comparison-with-other-tools"><a class="header" href="#comparison-with-other-tools">Comparison with Other Tools</a></h2>
<h3 id="how-is-debtmap-different-from-sonarqube"><a class="header" href="#how-is-debtmap-different-from-sonarqube">How is debtmap different from SonarQube?</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Aspect</th><th>Debtmap</th><th>SonarQube</th></tr></thead><tbody>
<tr><td><strong>Speed</strong></td><td>10-100x faster (Rust)</td><td>Slower (JVM overhead)</td></tr>
<tr><td><strong>Coverage Integration</strong></td><td>âœ… Built-in LCOV</td><td>âš ï¸ Enterprise only</td></tr>
<tr><td><strong>Entropy Analysis</strong></td><td>âœ… Unique feature</td><td>âŒ No</td></tr>
<tr><td><strong>Language Support</strong></td><td>Rust, Python, JS/TS</td><td>25+ languages</td></tr>
<tr><td><strong>Setup</strong></td><td>Single binary</td><td>JVM + server setup</td></tr>
<tr><td><strong>Cost</strong></td><td>Free, open-source</td><td>Free (basic) / Paid (advanced)</td></tr>
<tr><td><strong>Use Case</strong></td><td>Fast local analysis</td><td>Enterprise dashboards</td></tr>
</tbody></table>
</div>
<p><strong>When to use SonarQube:</strong> Multi-language monorepos, enterprise compliance, centralized quality dashboards.</p>
<p><strong>When to use debtmap:</strong> Rust-focused projects, local development workflow, coverage-driven prioritization.</p>
<h3 id="how-is-debtmap-different-from-codeclimate"><a class="header" href="#how-is-debtmap-different-from-codeclimate">How is debtmap different from CodeClimate?</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Aspect</th><th>Debtmap</th><th>CodeClimate</th></tr></thead><tbody>
<tr><td><strong>Deployment</strong></td><td>Local binary</td><td>Cloud service</td></tr>
<tr><td><strong>Coverage</strong></td><td>Built-in integration</td><td>Separate tool</td></tr>
<tr><td><strong>Entropy</strong></td><td>âœ… Yes</td><td>âŒ No</td></tr>
<tr><td><strong>Speed</strong></td><td>Seconds</td><td>Minutes (uploads code)</td></tr>
<tr><td><strong>Privacy</strong></td><td>Code stays local</td><td>Code uploaded to cloud</td></tr>
<tr><td><strong>Cost</strong></td><td>Free</td><td>Free (open source) / Paid</td></tr>
</tbody></table>
</div>
<p><strong>When to use CodeClimate:</strong> Multi-language projects, prefer SaaS solutions, want maintainability ratings.</p>
<p><strong>When to use debtmap:</strong> Rust projects, privacy-sensitive code, fast local analysis, entropy-based scoring.</p>
<h3 id="should-i-replace-clippy-with-debtmap"><a class="header" href="#should-i-replace-clippy-with-debtmap">Should I replace clippy with debtmap?</a></h3>
<p><strong>Noâ€”use both!</strong> They serve different purposes:</p>
<p><strong>clippy:</strong></p>
<ul>
<li>Focuses on idiomatic Rust patterns</li>
<li>Catches common mistakes (e.g., unnecessary clones, inefficient iterators)</li>
<li>Suggests Rust-specific best practices</li>
<li>Runs in milliseconds</li>
</ul>
<p><strong>debtmap:</strong></p>
<ul>
<li>Focuses on technical debt prioritization</li>
<li>Identifies untested complex code</li>
<li>Combines complexity with test coverage</li>
<li>Provides quantified recommendations</li>
</ul>
<p><strong>Recommended workflow:</strong></p>
<pre><code class="language-bash"># Fix clippy issues first (quick wins)
cargo clippy --all-targets --all-features -- -D warnings

# Then prioritize debt with debtmap
debtmap analyze . --lcov coverage/lcov.info --top 10
</code></pre>
<h3 id="should-i-replace-cargo-audit-with-debtmap"><a class="header" href="#should-i-replace-cargo-audit-with-debtmap">Should I replace cargo-audit with debtmap?</a></h3>
<p><strong>Noâ€”different focus.</strong> cargo-audit scans for security vulnerabilities in dependencies. Debtmap analyzes code complexity and test coverage.</p>
<p><strong>Use both:</strong></p>
<ul>
<li><code>cargo-audit</code> - Security vulnerabilities in dependencies</li>
<li><code>cargo-geiger</code> - Unsafe code detection</li>
<li><code>debtmap</code> - Technical debt and test gaps</li>
</ul>
<h3 id="how-does-debtmap-compare-to-traditional-code-coverage-tools"><a class="header" href="#how-does-debtmap-compare-to-traditional-code-coverage-tools">How does debtmap compare to traditional code coverage tools?</a></h3>
<p>Debtmap doesnâ€™t replace coverage toolsâ€”it <strong>augments</strong> them.</p>
<p><strong>Coverage tools (tarpaulin, pytest-cov, jest):</strong></p>
<ul>
<li>Measure what % of code is executed by tests</li>
<li>Tell you â€œyou have 75% coverageâ€</li>
</ul>
<p><strong>Debtmap:</strong></p>
<ul>
<li>Reads coverage data from these tools</li>
<li>Prioritizes gaps based on code complexity</li>
<li>Tells you â€œfunction X has 0% coverage and complexity 12â€”fix this firstâ€</li>
</ul>
<p><strong>Value:</strong> Debtmap answers â€œwhich 25% should I test first?â€ instead of just â€œ75% is tested.â€</p>
<h2 id="usage--configuration"><a class="header" href="#usage--configuration">Usage &amp; Configuration</a></h2>
<h3 id="why-dont-entry-points-need-100-coverage"><a class="header" href="#why-dont-entry-points-need-100-coverage">Why donâ€™t entry points need 100% coverage?</a></h3>
<p>Entry points (main functions, CLI handlers, framework integration code) are typically tested via <strong>integration tests</strong>, not unit tests. Unit testing them would mean mocking the entire runtime environment, which is brittle and low-value.</p>
<p>Debtmap recognizes common entry point patterns and lowers their priority for unit test coverage:</p>
<pre><pre class="playground"><code class="language-rust">// Entry point - integration test coverage expected
fn main() {
    // Debtmap: LOW priority for unit tests
}

// HTTP handler - integration test coverage expected
async fn handle_request(req: Request) -&gt; Response {
    // Debtmap: LOW priority for unit tests
}

// Core business logic - unit test coverage expected
fn calculate_discount(cart: &amp;Cart) -&gt; Discount {
    // Debtmap: HIGH priority for unit tests if uncovered
}</code></pre></pre>
<p>You can configure entry point detection in <code>.debtmap.toml</code>:</p>
<pre><code class="language-toml">[analysis]
entry_point_patterns = [
    "main",
    "handle_*",
    "run_*",
    "*_handler",
]
</code></pre>
<h3 id="how-do-i-exclude-test-files-from-analysis"><a class="header" href="#how-do-i-exclude-test-files-from-analysis">How do I exclude test files from analysis?</a></h3>
<p>By default, debtmap excludes common test directories. To customize:</p>
<p><strong>.debtmap.toml:</strong></p>
<pre><code class="language-toml">[analysis]
exclude_patterns = [
    "**/tests/**",
    "**/*_test.rs",
    "**/test_*.py",
    "**/*.test.ts",
    "**/target/**",
    "**/node_modules/**",
]
</code></pre>
<p><strong>Command line:</strong></p>
<pre><code class="language-bash">debtmap analyze . --exclude '**/tests/**' --exclude '**/*_test.rs'
</code></pre>
<h3 id="can-i-analyze-only-specific-files-or-directories"><a class="header" href="#can-i-analyze-only-specific-files-or-directories">Can I analyze only specific files or directories?</a></h3>
<p>Yes! Use the <code>--include</code> flag:</p>
<pre><code class="language-bash"># Analyze only src/ directory
debtmap analyze . --include 'src/**'

# Analyze specific files
debtmap analyze . --include 'src/main.rs' --include 'src/lib.rs'

# Combine include and exclude
debtmap analyze . --include 'src/**' --exclude 'src/generated/**'
</code></pre>
<h3 id="how-do-i-configure-ignore-patterns-for-generated-code"><a class="header" href="#how-do-i-configure-ignore-patterns-for-generated-code">How do I configure ignore patterns for generated code?</a></h3>
<p>Add to <code>.debtmap.toml</code>:</p>
<pre><code class="language-toml">[analysis]
exclude_patterns = [
    "**/generated/**",
    "**/*.g.rs",           # Generated Rust
    "**/*_pb.py",          # Protobuf generated Python
    "**/*.generated.ts",   # Generated TypeScript
]
</code></pre>
<p>Or use comments in source files:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// debtmap:ignore-file - entire file ignored

fn complex_function() {
    // debtmap:ignore-start
    // ... complex generated code ...
    // debtmap:ignore-end
}
<span class="boring">}</span></code></pre></pre>
<h3 id="what-if-debtmap-reports-false-positives"><a class="header" href="#what-if-debtmap-reports-false-positives">What if debtmap reports false positives?</a></h3>
<p><strong>1. Verify entropy analysis is enabled</strong> (default in v0.2.8+):</p>
<pre><code class="language-toml">[analysis]
enable_entropy_analysis = true
</code></pre>
<p><strong>2. Adjust thresholds</strong> for your projectâ€™s needs:</p>
<pre><code class="language-toml">[thresholds]
cyclomatic_complexity = 15  # Increase if you have many validation functions
</code></pre>
<p><strong>3. Use ignore comments</strong> for specific functions:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// debtmap:ignore - explanation for why this is acceptable
fn complex_but_acceptable() {
    // ...
}
<span class="boring">}</span></code></pre></pre>
<p><strong>4. Report false positives:</strong> If you believe debtmapâ€™s analysis is incorrect, please <a href="https://github.com/prodigy-tools/debtmap/issues">open an issue</a> with a code example. This helps improve the tool!</p>
<h3 id="how-accurate-is-the-risk-scoring"><a class="header" href="#how-accurate-is-the-risk-scoring">How accurate is the risk scoring?</a></h3>
<p>Risk scores are <strong>relative prioritization metrics</strong>, not absolute measures. They help you answer â€œwhich code should I focus on first?â€ rather than â€œexactly how risky is this code?â€</p>
<p><strong>Factors affecting accuracy:</strong></p>
<ul>
<li><strong>Coverage data quality:</strong> Accurate if your tests exercise realistic scenarios</li>
<li><strong>Entropy analysis:</strong> Effective for common patterns; may miss domain-specific patterns</li>
<li><strong>Call graph:</strong> More accurate within single files than across modules</li>
<li><strong>Context:</strong> Cannot account for business criticality (you know your domain best)</li>
</ul>
<p><strong>Best practice:</strong> Use risk scores for prioritization, but apply your domain knowledge when deciding what to actually refactor or test.</p>
<h3 id="can-i-run-debtmap-on-a-ci-server"><a class="header" href="#can-i-run-debtmap-on-a-ci-server">Can I run debtmap on a CI server?</a></h3>
<p>Yes! Debtmap is designed for CI/CD pipelines:</p>
<p><strong>Performance:</strong></p>
<ul>
<li>Statically linked binary (no runtime dependencies)</li>
<li>Fast analysis (seconds, not minutes)</li>
<li>Low memory footprint</li>
</ul>
<p><strong>Exit codes:</strong></p>
<ul>
<li><code>0</code> - Analysis succeeded, validation passed</li>
<li><code>1</code> - Analysis succeeded, validation failed (debt thresholds exceeded)</li>
<li><code>2</code> - Analysis error (parse failure, invalid config, etc.)</li>
</ul>
<p><strong>Example CI configuration:</strong></p>
<pre><code class="language-yaml"># .github/workflows/debt-check.yml
name: Technical Debt Check

on: [pull_request]

jobs:
  debt-analysis:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Install debtmap
        run: cargo install debtmap

      - name: Generate coverage
        run: cargo tarpaulin --out lcov

      - name: Analyze debt
        run: debtmap validate . --lcov lcov.info --max-critical 0
</code></pre>
<h2 id="troubleshooting-11"><a class="header" href="#troubleshooting-11">Troubleshooting</a></h2>
<h3 id="analysis-is-slow-on-my-large-codebase"><a class="header" href="#analysis-is-slow-on-my-large-codebase">Analysis is slow on my large codebase</a></h3>
<p><strong>Optimization strategies:</strong></p>
<p><strong>1. Exclude unnecessary files:</strong></p>
<pre><code class="language-toml">[analysis]
exclude_patterns = [
    "**/target/**",
    "**/node_modules/**",
    "**/vendor/**",
    "**/.git/**",
]
</code></pre>
<p><strong>2. Use incremental mode</strong> (cache results for unchanged files):</p>
<pre><code class="language-bash">debtmap analyze . --incremental --cache-dir .debtmap-cache
</code></pre>
<p><strong>3. Analyze specific directories:</strong></p>
<pre><code class="language-bash"># Only analyze src/, skip examples and benches
debtmap analyze src/
</code></pre>
<p><strong>4. Reduce parallelism</strong> if memory-constrained:</p>
<pre><code class="language-bash">debtmap analyze . --jobs 4
</code></pre>
<p><strong>Expected performance:</strong></p>
<ul>
<li>50k LOC: 5-15 seconds</li>
<li>200k LOC: 30-90 seconds</li>
<li>1M+ LOC: 3-8 minutes</li>
</ul>
<p>If analysis is significantly slower, please <a href="https://github.com/prodigy-tools/debtmap/issues">report a performance issue</a>.</p>
<h3 id="debtmap-crashes-with-stack-overflow"><a class="header" href="#debtmap-crashes-with-stack-overflow">Debtmap crashes with â€œstack overflowâ€</a></h3>
<p>This typically happens with extremely deep call stacks or heavily nested code.</p>
<p><strong>Solutions:</strong></p>
<p><strong>1. Increase stack size:</strong></p>
<pre><code class="language-bash"># Linux/macOS
RUST_MIN_STACK=8388608 debtmap analyze .

# Windows PowerShell
$env:RUST_MIN_STACK=8388608; debtmap analyze .
</code></pre>
<p><strong>2. Exclude problematic files:</strong></p>
<pre><code class="language-bash">debtmap analyze . --exclude 'path/to/deeply/nested/file.rs'
</code></pre>
<p><strong>3. Report the issue:</strong> If you encounter stack overflows, please report with a minimal reproducible example.</p>
<h3 id="coverage-data-isnt-being-applied"><a class="header" href="#coverage-data-isnt-being-applied">Coverage data isnâ€™t being applied</a></h3>
<p><strong>Check:</strong></p>
<p><strong>1. LCOV file path is correct:</strong></p>
<pre><code class="language-bash">debtmap analyze . --lcov target/coverage/lcov.info
</code></pre>
<p><strong>2. LCOV file contains data:</strong></p>
<pre><code class="language-bash">grep -c "^SF:" target/coverage/lcov.info  # Should be &gt; 0
</code></pre>
<p><strong>3. Source paths match:</strong>
LCOV file paths must match your source file paths. If you generate coverage in a different directory:</p>
<pre><code class="language-toml">[coverage]
source_root = "/path/to/project"  # Rewrite LCOV paths
</code></pre>
<p><strong>4. Enable debug logging:</strong></p>
<pre><code class="language-bash">RUST_LOG=debug debtmap analyze . --lcov lcov.info 2&gt;&amp;1 | grep -i coverage
</code></pre>
<h3 id="debtmap-reports-no-functions-found"><a class="header" href="#debtmap-reports-no-functions-found">Debtmap reports â€œNo functions foundâ€</a></h3>
<p><strong>Common causes:</strong></p>
<p><strong>1. Wrong language detection:</strong></p>
<pre><code class="language-bash"># Verify file extensions are recognized
debtmap analyze . --verbose
</code></pre>
<p><strong>2. Syntax errors preventing parsing:</strong></p>
<pre><code class="language-bash"># Check for parse errors
RUST_LOG=warn debtmap analyze .
</code></pre>
<p><strong>3. All files excluded by ignore patterns:</strong></p>
<pre><code class="language-bash"># List files being analyzed
debtmap analyze . --dry-run
</code></pre>
<p><strong>4. Unsupported language features:</strong>
Some cutting-edge syntax may not parse correctly. Report parsing issues with code examples.</p>
<h3 id="how-do-i-report-a-bug-or-request-a-feature"><a class="header" href="#how-do-i-report-a-bug-or-request-a-feature">How do I report a bug or request a feature?</a></h3>
<p><strong>Bug reports:</strong></p>
<ol>
<li>Check <a href="https://github.com/prodigy-tools/debtmap/issues">existing issues</a></li>
<li>Provide minimal reproducible example</li>
<li>Include debtmap version: <code>debtmap --version</code></li>
<li>Include OS and Rust version: <code>rustc --version</code></li>
</ol>
<p><strong>Feature requests:</strong></p>
<ol>
<li>Describe the use case (what problem does it solve?)</li>
<li>Provide example of desired behavior</li>
<li>Explain why existing features donâ€™t address the need</li>
</ol>
<p><strong>Contributions:</strong>
Debtmap is open-source and welcomes contributions! See <a href="https://github.com/prodigy-tools/debtmap/blob/master/CONTRIBUTING.md">CONTRIBUTING.md</a> for guidelines.</p>
<h2 id="advanced-topics"><a class="header" href="#advanced-topics">Advanced Topics</a></h2>
<h3 id="can-i-extend-debtmap-with-custom-analyzers"><a class="header" href="#can-i-extend-debtmap-with-custom-analyzers">Can I extend debtmap with custom analyzers?</a></h3>
<p>Not yet, but planned for v0.3.0. Youâ€™ll be able to implement the <code>Analyzer</code> trait for custom language support or domain-specific pattern detection.</p>
<p><strong>Roadmap:</strong></p>
<ul>
<li>v0.3.0: Plugin API for custom analyzers</li>
<li>v0.4.0: Plugin API for custom scoring strategies</li>
<li>v0.5.0: Plugin API for custom output formatters</li>
</ul>
<p>Track progress in <a href="https://github.com/prodigy-tools/debtmap/issues/42">issue #42</a>.</p>
<h3 id="how-does-debtmap-handle-monorepos"><a class="header" href="#how-does-debtmap-handle-monorepos">How does debtmap handle monorepos?</a></h3>
<p><strong>Workspace support:</strong>
Debtmap analyzes each workspace member independently by default:</p>
<pre><code class="language-bash"># Analyze entire workspace
debtmap analyze .

# Analyze specific member
debtmap analyze packages/api

# Combined report for all members
debtmap analyze . --workspace-mode combined
</code></pre>
<p><strong>Configuration:</strong></p>
<pre><code class="language-toml">[workspace]
members = ["packages/*", "services/*"]
exclude = ["examples/*"]
</code></pre>
<h3 id="can-i-compare-debt-between-branches-or-commits"><a class="header" href="#can-i-compare-debt-between-branches-or-commits">Can I compare debt between branches or commits?</a></h3>
<p>Yes! Use the <code>compare</code> command:</p>
<pre><code class="language-bash"># Compare current branch with main
debtmap compare main

# Compare two specific commits
debtmap compare abc123..def456

# Show only new debt introduced
debtmap compare main --show-new-only
</code></pre>
<p>Output shows:</p>
<ul>
<li>New debt items (introduced since base)</li>
<li>Resolved debt items (fixed since base)</li>
<li>Changed debt items (score increased/decreased)</li>
</ul>
<p>See <a href="examples.html#comparing-branches">Examples - Comparing Branches</a> for details.</p>
<h3 id="how-do-i-integrate-debtmap-with-my-editor"><a class="header" href="#how-do-i-integrate-debtmap-with-my-editor">How do I integrate debtmap with my editor?</a></h3>
<p><strong>VS Code:</strong></p>
<ul>
<li>Install the â€œDebtmapâ€ extension (planned for Q2 2025)</li>
<li>Inline warnings in editor for high-risk code</li>
<li>Quick fixes to generate test stubs</li>
</ul>
<p><strong>Vim/Neovim:</strong></p>
<ul>
<li>Use ALE or vim-lsp with debtmapâ€™s LSP mode (planned)</li>
</ul>
<p><strong>IntelliJ/RustRover:</strong></p>
<ul>
<li>Use external tools integration:
<ul>
<li>Settings â†’ Tools â†’ External Tools</li>
<li>Add debtmap command</li>
<li>Configure keyboard shortcut</li>
</ul>
</li>
</ul>
<p>Track editor integration progress in <a href="https://github.com/prodigy-tools/debtmap/issues/38">issue #38</a>.</p>
<h2 id="need-more-help"><a class="header" href="#need-more-help">Need More Help?</a></h2>
<ul>
<li><strong>Documentation:</strong> <a href="https://debtmap.dev">debtmap.dev</a></li>
<li><strong>GitHub Issues:</strong> <a href="https://github.com/prodigy-tools/debtmap/issues">Report bugs or request features</a></li>
<li><strong>Discussions:</strong> <a href="https://github.com/prodigy-tools/debtmap/discussions">Ask questions</a></li>
<li><strong>Examples:</strong> See <a href="examples.html">Examples</a> for real-world use cases</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="troubleshooting-12"><a class="header" href="#troubleshooting-12">Troubleshooting</a></h1>
<p>Common issues and solutions for using debtmap effectively.</p>
<h2 id="quick-fixes-for-common-issues"><a class="header" href="#quick-fixes-for-common-issues">Quick Fixes for Common Issues</a></h2>
<p>If youâ€™re experiencing problems, try these first:</p>
<ol>
<li><strong>Analysis is slow</strong>: Check <code>--cache-stats</code>, ensure caching is enabled, adjust threads with <code>--jobs</code></li>
<li><strong>Parse errors</strong>: Use <code>--semantic-off</code> for faster fallback mode or exclude problematic files</li>
<li><strong>No output</strong>: Increase verbosity with <code>-v</code> or lower <code>--min-priority</code></li>
<li><strong>Cache corruption</strong>: Run with <code>--clear-cache</code> to rebuild</li>
<li><strong>Inconsistent results</strong>: Check if coverage file changed or context providers are enabled</li>
</ol>
<h2 id="common-issues-2"><a class="header" href="#common-issues-2">Common Issues</a></h2>
<h3 id="parse-errors"><a class="header" href="#parse-errors">Parse Errors</a></h3>
<p><strong>Problem</strong>: Encountering â€œParse error in file:line:columnâ€ messages</p>
<p><strong>Causes</strong>:</p>
<ul>
<li>Unsupported language syntax or version</li>
<li>Complex macro expansions (Rust)</li>
<li>Type inference edge cases (Python, TypeScript)</li>
</ul>
<p><strong>Solutions</strong>:</p>
<pre><code class="language-bash"># Try fallback mode without semantic analysis
debtmap --semantic-off

# For Rust macro issues, see detailed warnings
debtmap --verbose-macro-warnings --show-macro-stats

# Exclude specific problematic files
# Add to .debtmap/config.toml:
# exclude = ["path/to/problematic/file.rs"]
</code></pre>
<h3 id="out-of-memory-errors"><a class="header" href="#out-of-memory-errors">Out of Memory Errors</a></h3>
<p><strong>Problem</strong>: Analysis crashes or runs out of memory on large codebases</p>
<p><strong>Solutions</strong>:</p>
<pre><code class="language-bash"># Limit parallel processing
debtmap --jobs 2

# Disable parallel processing entirely
debtmap --no-parallel

# Test with limited files first
debtmap --max-files 100

# Analyze subdirectories separately
debtmap path/to/subset
</code></pre>
<h3 id="performance-issues-1"><a class="header" href="#performance-issues-1">Performance Issues</a></h3>
<p><strong>Problem</strong>: Analysis takes too long to complete</p>
<p><strong>Solutions</strong>:</p>
<pre><code class="language-bash"># Check cache statistics
debtmap --cache-stats

# Ensure caching is enabled (it is by default)
# If cache was disabled, remove --no-cache flag

# Use all available CPU cores
debtmap --jobs 0

# Try faster fallback mode (less accurate)
debtmap --semantic-off

# Use plain output for faster terminal rendering
debtmap --plain
</code></pre>
<p>See <a href="troubleshooting.html#performance-tips">Performance Tips</a> for detailed optimization strategies.</p>
<h3 id="cache-corruption"><a class="header" href="#cache-corruption">Cache Corruption</a></h3>
<p><strong>Problem</strong>: Getting â€œCache errorâ€ messages or stale results</p>
<p><strong>Solutions</strong>:</p>
<pre><code class="language-bash"># Clear cache and rebuild
debtmap --clear-cache

# Force cache rebuild
debtmap --force-cache-rebuild

# Check cache status
debtmap --cache-stats

# Use different cache location
debtmap --cache-location /path/to/cache
</code></pre>
<p>See <a href="troubleshooting.html#cache-troubleshooting">Cache Troubleshooting</a> for more details.</p>
<h3 id="file-permission-errors"><a class="header" href="#file-permission-errors">File Permission Errors</a></h3>
<p><strong>Problem</strong>: â€œFile system errorâ€ when accessing files</p>
<p><strong>Solutions</strong>:</p>
<ul>
<li>Ensure you have read permissions for all source files</li>
<li>Check that the project directory is accessible</li>
<li>Verify cache directory is writable (default: <code>.debtmap/cache</code>)</li>
<li>Use <code>--cache-location</code> to specify an accessible cache directory</li>
</ul>
<h3 id="git-history-errors"><a class="header" href="#git-history-errors">Git History Errors</a></h3>
<p><strong>Problem</strong>: Errors when using <code>git_history</code> context provider</p>
<p><strong>Causes</strong>:</p>
<ul>
<li>Not running in a git repository</li>
<li>Git history not available for files</li>
<li>Insufficient git permissions</li>
</ul>
<p><strong>Solutions</strong>:</p>
<pre><code class="language-bash"># Disable git_history context provider
debtmap --context --disable-context git_history

# Disable all context providers
debtmap --no-context-aware

# Check if in git repository
git status
</code></pre>
<h3 id="coverage-file-issues"><a class="header" href="#coverage-file-issues">Coverage File Issues</a></h3>
<p><strong>Problem</strong>: Coverage file not being processed or causing errors</p>
<p><strong>Causes</strong>:</p>
<ul>
<li>Non-LCOV format coverage file</li>
<li>Malformed coverage data</li>
<li>Path mismatches between coverage and source files</li>
</ul>
<p><strong>Solutions</strong>:</p>
<pre><code class="language-bash"># Verify coverage file format (must be LCOV)
head coverage.info

# Check coverage file path
debtmap --coverage-file path/to/coverage.info -v

# Ensure paths in coverage file match source paths
# Coverage paths are relative to project root
</code></pre>
<h3 id="threshold-and-preset-confusion"><a class="header" href="#threshold-and-preset-confusion">Threshold and Preset Confusion</a></h3>
<p><strong>Problem</strong>: Unexpected filtering or priority levels</p>
<p><strong>Solutions</strong>:</p>
<pre><code class="language-bash"># Check what threshold preset does
debtmap --threshold-preset strict --help

# Override specific thresholds
debtmap --min-priority 3

# See all items regardless of thresholds
debtmap --min-priority 0

# Use category filters instead
debtmap --filter "complexity,debt"
</code></pre>
<h3 id="json-format-issues"><a class="header" href="#json-format-issues">JSON Format Issues</a></h3>
<p><strong>Problem</strong>: JSON output parsing errors or unexpected structure</p>
<p><strong>Understanding the Two Formats</strong>:</p>
<p><strong>Legacy format</strong> wraps items in variant-specific objects:</p>
<pre><code class="language-json">{"File": {"path": "src/main.rs", "score": 7.5, ...}}
{"Function": {"name": "parse", "score": 8.2, ...}}
</code></pre>
<p><strong>Unified format</strong> uses consistent structure with <code>type</code> field:</p>
<pre><code class="language-json">{"type": "File", "path": "src/main.rs", "score": 7.5, ...}
{"type": "Function", "name": "parse", "score": 8.2, ...}
</code></pre>
<p>The unified format is <strong>recommended</strong> for parsing and tool integration as it provides a consistent structure across all item types.</p>
<p><strong>Solutions</strong>:</p>
<pre><code class="language-bash"># Use unified JSON format (consistent structure, recommended)
debtmap --format json --output-format unified

# Legacy format (default, uses {File: {...}} structure)
debtmap --format json --output-format legacy

# Validate JSON output
debtmap --format json | jq .

# Write to file for easier inspection
debtmap --format json --output results.json
</code></pre>
<p>See the <a href="./configuration.html#output-formats">Configuration/Output Formats</a> chapter for detailed JSON structure documentation.</p>
<h3 id="context-provider-errors"><a class="header" href="#context-provider-errors">Context Provider Errors</a></h3>
<p><strong>Problem</strong>: Errors with critical_path, dependency, or git_history providers</p>
<p><strong>Solutions</strong>:</p>
<pre><code class="language-bash"># Enable specific providers only
debtmap --context --context-providers critical_path,dependency

# Disable problematic provider
debtmap --context --disable-context git_history

# Disable context-aware filtering
debtmap --no-context-aware

# Check context provider details
debtmap --context -vvv
</code></pre>
<p>See <a href="troubleshooting.html#context-provider-troubleshooting">Context Provider Troubleshooting</a> for details.</p>
<h2 id="debug-mode"><a class="header" href="#debug-mode">Debug Mode</a></h2>
<p>Use verbosity flags to diagnose issues and understand analysis behavior.</p>
<h3 id="verbosity-levels-2"><a class="header" href="#verbosity-levels-2">Verbosity Levels</a></h3>
<pre><code class="language-bash"># Level 1: Show main score factors
debtmap -v

# Level 2: Show detailed calculations
debtmap -vv

# Level 3: Show all debug information
debtmap -vvv
</code></pre>
<p><strong>What each level shows</strong>:</p>
<ul>
<li><code>-v</code>: Score breakdowns, main contributing factors</li>
<li><code>-vv</code>: Detailed metric calculations, file processing</li>
<li><code>-vvv</code>: Full debug output, context provider details, cache operations</li>
</ul>
<h3 id="diagnostic-options"><a class="header" href="#diagnostic-options">Diagnostic Options</a></h3>
<pre><code class="language-bash"># Show macro parsing warnings (Rust)
debtmap --verbose-macro-warnings

# Show macro expansion statistics (Rust)
debtmap --show-macro-stats

# Disable semantic analysis (fallback mode)
debtmap --semantic-off

# Validate LOC consistency
debtmap --validate-loc

# Show cache statistics
debtmap --cache-stats
</code></pre>
<h3 id="debugging-score-calculations"><a class="header" href="#debugging-score-calculations">Debugging Score Calculations</a></h3>
<pre><code class="language-bash"># Use verbosity levels to see score breakdown
debtmap -v    # Shows score factors

# See how coverage affects scores
debtmap --coverage-file coverage.info -v

# See how context affects scores
debtmap --context --context-providers critical_path -v
</code></pre>
<h3 id="example-debug-session"><a class="header" href="#example-debug-session">Example Debug Session</a></h3>
<pre><code class="language-bash"># Step 1: Run with verbosity to see what's happening
debtmap -vv

# Step 2: Check cache stats
debtmap --cache-stats

# Step 3: Try without semantic analysis
debtmap --semantic-off -v

# Step 4: Check specific file
debtmap path/to/file.rs -vvv

# Step 5: Validate results
debtmap --validate-loc
</code></pre>
<h2 id="performance-tips"><a class="header" href="#performance-tips">Performance Tips</a></h2>
<p>Optimize debtmap analysis speed and resource usage.</p>
<h3 id="parallel-processing-1"><a class="header" href="#parallel-processing-1">Parallel Processing</a></h3>
<pre><code class="language-bash"># Use all CPU cores (default)
debtmap --jobs 0

# Limit to 4 threads
debtmap --jobs 4

# Disable parallel processing (debugging)
# Note: --no-parallel is equivalent to --jobs 1 (single-threaded)
debtmap --no-parallel
</code></pre>
<p><strong>When to adjust parallelism</strong>:</p>
<ul>
<li><strong>Use <code>--jobs 0</code></strong> (default): Maximum performance on dedicated machine</li>
<li><strong>Use <code>--jobs N</code></strong>: Limit resource usage while other tasks run</li>
<li><strong>Use <code>--no-parallel</code></strong>: Debugging concurrency issues</li>
</ul>
<h3 id="caching-strategies"><a class="header" href="#caching-strategies">Caching Strategies</a></h3>
<p>Caching is <strong>enabled by default</strong> and provides the biggest performance improvement.</p>
<p><strong>Note</strong>: The <code>--cache</code> flag (to enable caching) is deprecated and hidden. Caching is now always enabled by default; use <code>--no-cache</code> to disable it.</p>
<pre><code class="language-bash"># Check cache effectiveness
debtmap --cache-stats

# Clear cache if corrupted
debtmap --clear-cache

# Force cache rebuild
debtmap --force-cache-rebuild

# Disable cache (not recommended)
debtmap --no-cache
</code></pre>
<p><strong>Cache locations</strong>:</p>
<pre><code class="language-bash"># Local cache (default): .debtmap/cache
debtmap

# Shared cache for multiple projects
debtmap --cache-location ~/.cache/debtmap

# Migrate existing cache to shared location
debtmap --migrate-cache

# Set via environment variable
export DEBTMAP_CACHE_DIR=~/.cache/debtmap
debtmap
</code></pre>
<p><strong>Cache best practices</strong>:</p>
<ol>
<li>Use shared cache for multiple similar projects</li>
<li>Monitor cache size with <code>--cache-stats</code> periodically</li>
<li>Clear cache after major refactorings</li>
<li>Use local cache for project-specific configurations</li>
</ol>
<h3 id="analysis-optimizations"><a class="header" href="#analysis-optimizations">Analysis Optimizations</a></h3>
<pre><code class="language-bash"># Fast mode: disable semantic analysis
debtmap --semantic-off

# Plain output: faster terminal rendering
debtmap --plain

# Limit files for testing
debtmap --max-files 100

# Analyze subdirectory only
debtmap src/specific/module

# Reduce output with filters
debtmap --min-priority 4 --top 20
</code></pre>
<h3 id="performance-comparison"><a class="header" href="#performance-comparison">Performance Comparison</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Configuration</th><th>Speed</th><th>Accuracy</th></tr></thead><tbody>
<tr><td>Default (cached)</td><td>Fast</td><td>High</td></tr>
<tr><td><code>--no-cache</code></td><td>Slow</td><td>High</td></tr>
<tr><td><code>--semantic-off</code></td><td>Fastest</td><td>Medium</td></tr>
<tr><td><code>--no-parallel</code></td><td>Slowest</td><td>High</td></tr>
<tr><td><code>--jobs 4</code></td><td>Medium</td><td>High</td></tr>
</tbody></table>
</div>
<h3 id="monitoring-performance"><a class="header" href="#monitoring-performance">Monitoring Performance</a></h3>
<pre><code class="language-bash"># Time analysis
time debtmap

# Check cache hit rate
debtmap --cache-stats

# Profile with verbosity
debtmap -vv 2&gt;&amp;1 | grep "processed in"
</code></pre>
<h2 id="cache-troubleshooting"><a class="header" href="#cache-troubleshooting">Cache Troubleshooting</a></h2>
<p>Detailed guidance for cache-related issues.</p>
<h3 id="check-cache-status"><a class="header" href="#check-cache-status">Check Cache Status</a></h3>
<pre><code class="language-bash"># View cache statistics
debtmap --cache-stats

# Sample output:
# Cache location: .debtmap/cache
# Cache entries: 1,234
# Cache size: 45.2 MB
# Hit rate: 87.3%
</code></pre>
<h3 id="clear-corrupted-cache"><a class="header" href="#clear-corrupted-cache">Clear Corrupted Cache</a></h3>
<pre><code class="language-bash"># Clear all cache entries
debtmap --clear-cache

# Force rebuild on next run
debtmap --force-cache-rebuild

# Manual cache deletion
rm -rf .debtmap/cache
# or for shared cache:
rm -rf ~/.cache/debtmap
</code></pre>
<h3 id="cache-location-management"><a class="header" href="#cache-location-management">Cache Location Management</a></h3>
<pre><code class="language-bash"># Use local cache (default)
debtmap
# Cache at: .debtmap/cache

# Use shared cache
debtmap --cache-location ~/.cache/debtmap

# Set permanently via environment
export DEBTMAP_CACHE_DIR=~/.cache/debtmap
debtmap

# Migrate existing cache
debtmap --migrate-cache
</code></pre>
<h3 id="cache-pruning-configuration"><a class="header" href="#cache-pruning-configuration">Cache Pruning Configuration</a></h3>
<p>Debtmap automatically manages cache size and age to prevent unbounded growth:</p>
<p><strong>Environment Variables</strong>:</p>
<pre><code class="language-bash"># Enable/disable automatic cache pruning (default: true)
export DEBTMAP_CACHE_AUTO_PRUNE=true

# Maximum cache size in bytes (default: 1GB)
export DEBTMAP_CACHE_MAX_SIZE=1073741824

# Maximum cache entry age in days (default: 30)
export DEBTMAP_CACHE_MAX_AGE_DAYS=30

# Maximum number of cache entries (default: 10000)
export DEBTMAP_CACHE_MAX_ENTRIES=10000

# Percentage of entries to remove when pruning (default: 0.25)
export DEBTMAP_CACHE_PRUNE_PERCENTAGE=0.25

# Pruning strategy: lru, lfu, fifo, age_based (default: lru)
export DEBTMAP_CACHE_STRATEGY=lru
</code></pre>
<p><strong>Pruning Strategies</strong>:</p>
<ul>
<li><strong>LRU (Least Recently Used)</strong>: Removes oldest accessed entries (recommended for general use)</li>
<li><strong>LFU (Least Frequently Used)</strong>: Removes least accessed entries (best for stable workloads)</li>
<li><strong>FIFO (First In First Out)</strong>: Removes oldest created entries (simple, useful for testing)</li>
<li><strong>Age-based</strong>: Removes entries older than MAX_AGE_DAYS (useful for compliance requirements)</li>
</ul>
<p><strong>Performance Implications</strong>:</p>
<ul>
<li>LRU/LFU provide better cache hit rates than FIFO</li>
<li>Age-based strategy ensures data freshness but may reduce hit rate</li>
<li>Adjust MAX_SIZE based on available disk space and project count</li>
<li>Lower PRUNE_PERCENTAGE means more frequent but smaller cleanup operations</li>
</ul>
<h3 id="cache-strategies"><a class="header" href="#cache-strategies">Cache Strategies</a></h3>
<p><strong>Local cache</strong> (<code>.debtmap/cache</code>):</p>
<ul>
<li><strong>Pros</strong>: Isolated per project, automatically managed</li>
<li><strong>Cons</strong>: Duplicates across projects</li>
</ul>
<p><strong>Shared cache</strong> (<code>~/.cache/debtmap</code>):</p>
<ul>
<li><strong>Pros</strong>: Shared across projects, saves disk space</li>
<li><strong>Cons</strong>: Requires manual management, may mix unrelated projects</li>
</ul>
<h3 id="cache-consistency"><a class="header" href="#cache-consistency">Cache Consistency</a></h3>
<pre><code class="language-bash"># Validate LOC consistency
debtmap --validate-loc

# Cache automatically invalidates on file changes
# Uses file hashes to detect modifications

# Force fresh analysis
debtmap --no-cache
</code></pre>
<h3 id="cache-size-monitoring"><a class="header" href="#cache-size-monitoring">Cache Size Monitoring</a></h3>
<pre><code class="language-bash"># Check cache size
debtmap --cache-stats

# Clean up old entries (manual)
# No automatic cleanup - manage cache size manually
# Consider clearing cache periodically for large projects
</code></pre>
<h2 id="context-provider-troubleshooting"><a class="header" href="#context-provider-troubleshooting">Context Provider Troubleshooting</a></h2>
<p>Diagnose and fix issues with context providers (critical_path, dependency, git_history).</p>
<h3 id="enable-context-analysis"><a class="header" href="#enable-context-analysis">Enable Context Analysis</a></h3>
<pre><code class="language-bash"># Enable with default providers
debtmap --context

# Or use explicit flag
debtmap --enable-context

# Specify specific providers
debtmap --context --context-providers critical_path,dependency,git_history
</code></pre>
<h3 id="disable-specific-providers-1"><a class="header" href="#disable-specific-providers-1">Disable Specific Providers</a></h3>
<pre><code class="language-bash"># Disable git_history only
debtmap --context --disable-context git_history

# Disable multiple providers
debtmap --context --disable-context git_history,dependency

# Disable context-aware filtering
debtmap --no-context-aware
</code></pre>
<h3 id="git-history-provider-issues"><a class="header" href="#git-history-provider-issues">Git History Provider Issues</a></h3>
<p><strong>Problem</strong>: â€œGit history errorâ€ when running analysis</p>
<p><strong>Causes</strong>:</p>
<ul>
<li>Not in a git repository</li>
<li>No git history for files</li>
<li>Git not installed or accessible</li>
</ul>
<p><strong>Solutions</strong>:</p>
<pre><code class="language-bash"># Verify in git repository
git status

# Disable git_history provider
debtmap --context --disable-context git_history

# Initialize git repo if needed
git init
</code></pre>
<h3 id="dependency-provider-issues"><a class="header" href="#dependency-provider-issues">Dependency Provider Issues</a></h3>
<p><strong>Problem</strong>: â€œDependency errorâ€ or incomplete dependency graph</p>
<p><strong>Causes</strong>:</p>
<ul>
<li>Complex import structures</li>
<li>Circular dependencies</li>
<li>Unsupported dependency patterns</li>
</ul>
<p><strong>Solutions</strong>:</p>
<pre><code class="language-bash"># Disable dependency provider
debtmap --context --disable-context dependency

# Try with verbosity to see details
debtmap --context -vvv

# Use without context
debtmap
</code></pre>
<h3 id="critical-path-provider-issues"><a class="header" href="#critical-path-provider-issues">Critical Path Provider Issues</a></h3>
<p><strong>Problem</strong>: Critical path analysis fails or produces unexpected results</p>
<p><strong>Causes</strong>:</p>
<ul>
<li>Invalid call graph</li>
<li>Missing function definitions</li>
<li>Complex control flow</li>
</ul>
<p><strong>Solutions</strong>:</p>
<pre><code class="language-bash"># Disable critical_path provider
debtmap --context --disable-context critical_path

# Try with semantic analysis disabled
debtmap --context --semantic-off

# Debug with verbosity
debtmap --context --context-providers critical_path -vvv
</code></pre>
<h3 id="context-impact-on-scoring"><a class="header" href="#context-impact-on-scoring">Context Impact on Scoring</a></h3>
<p>Context providers add additional risk factors to scoring:</p>
<pre><code class="language-bash"># See context contribution to scores
debtmap --context -v

# Compare with and without context
debtmap --output baseline.json
debtmap --context --output with_context.json
debtmap compare --before baseline.json --after with_context.json
</code></pre>
<h3 id="performance-impact"><a class="header" href="#performance-impact">Performance Impact</a></h3>
<p>Context analysis adds processing overhead:</p>
<pre><code class="language-bash"># Faster: no context
debtmap

# Slower: with all context providers
debtmap --context --context-providers critical_path,dependency,git_history

# Medium: selective providers
debtmap --context --context-providers dependency
</code></pre>
<h3 id="debug-context-providers"><a class="header" href="#debug-context-providers">Debug Context Providers</a></h3>
<pre><code class="language-bash"># See detailed context provider output
debtmap --context -vvv

# Check which providers are active
debtmap --context -v | grep "context provider"
</code></pre>
<h2 id="advanced-analysis-troubleshooting"><a class="header" href="#advanced-analysis-troubleshooting">Advanced Analysis Troubleshooting</a></h2>
<p>Advanced CLI flags for specialized analysis scenarios.</p>
<h3 id="multi-pass-analysis-2"><a class="header" href="#multi-pass-analysis-2">Multi-Pass Analysis</a></h3>
<p><strong>Flag</strong>: <code>--multi-pass</code></p>
<p>Multi-pass analysis performs multiple iterations to refine results.</p>
<pre><code class="language-bash"># Enable multi-pass analysis
debtmap --multi-pass

# Useful for complex projects with intricate dependencies
# May increase analysis time but improve accuracy
</code></pre>
<p><strong>When to use</strong>:</p>
<ul>
<li>Complex dependency graphs</li>
<li>Large codebases with deep nesting</li>
<li>When single-pass results seem incomplete</li>
</ul>
<h3 id="attribution-output"><a class="header" href="#attribution-output">Attribution Output</a></h3>
<p><strong>Flag</strong>: <code>--attribution</code></p>
<p>Shows attribution information for detected issues.</p>
<pre><code class="language-bash"># Enable attribution output
debtmap --attribution

# Combine with verbosity for details
debtmap --attribution -v
</code></pre>
<p><strong>Troubleshooting</strong>:</p>
<ul>
<li>Requires git history provider for author information</li>
<li>May slow down analysis</li>
<li>Use <code>--disable-context git_history</code> if causing errors</li>
</ul>
<h3 id="aggregation-methods-2"><a class="header" href="#aggregation-methods-2">Aggregation Methods</a></h3>
<p><strong>Flag</strong>: <code>--aggregation-method &lt;method&gt;</code></p>
<p>Controls how results are aggregated across files.</p>
<pre><code class="language-bash"># Available aggregation methods:
debtmap --aggregation-method weighted_sum  # (default)
debtmap --aggregation-method sum
debtmap --aggregation-method logarithmic_sum
debtmap --aggregation-method max_plus_average
</code></pre>
<p><strong>Common issues</strong>:</p>
<ul>
<li>Different methods produce different result structures</li>
<li>Choose method based on your reporting needs</li>
<li>Use consistent method for comparison over time</li>
</ul>
<h3 id="minimum-problematic-threshold"><a class="header" href="#minimum-problematic-threshold">Minimum Problematic Threshold</a></h3>
<p><strong>Flag</strong>: <code>--min-problematic &lt;number&gt;</code></p>
<p>Sets the minimum score for an item to be considered problematic.</p>
<pre><code class="language-bash"># Default threshold
debtmap --min-problematic 3

# More strict (show more issues)
debtmap --min-problematic 1

# Less strict (show only serious issues)
debtmap --min-problematic 5
</code></pre>
<p><strong>Relationship to other filters</strong>:</p>
<ul>
<li>Works alongside <code>--min-priority</code></li>
<li>Filters at analysis level vs display level</li>
<li>Lower values = more issues shown</li>
</ul>
<h3 id="god-object-detection-3"><a class="header" href="#god-object-detection-3">God Object Detection</a></h3>
<p><strong>Flag</strong>: <code>--no-god-object</code></p>
<p>Disables god object (large class/module) detection.</p>
<pre><code class="language-bash"># Disable god object detection
debtmap --no-god-object

# Useful if false positives on legitimately large modules
# Or if your architecture uses centralized classes
</code></pre>
<p><strong>When to use</strong>:</p>
<ul>
<li>False positives on framework files</li>
<li>Intentional large aggregator classes</li>
<li>Reducing noise in results</li>
</ul>
<h3 id="detail-level-control"><a class="header" href="#detail-level-control">Detail Level Control</a></h3>
<p><strong>Flag</strong>: <code>--detail-level &lt;level&gt;</code></p>
<p>Controls the level of detail in analysis output.</p>
<pre><code class="language-bash"># Available detail levels:
debtmap --detail-level summary        # High-level overview only
debtmap --detail-level standard       # (default) Balanced detail
debtmap --detail-level comprehensive  # Detailed analysis
debtmap --detail-level debug         # Full debug information
</code></pre>
<p><strong>When to use</strong>:</p>
<ul>
<li><code>summary</code>: Quick overview for large codebases</li>
<li><code>standard</code>: Default, appropriate for most use cases</li>
<li><code>comprehensive</code>: Deep dive into specific issues</li>
<li><code>debug</code>: Troubleshooting analysis behavior</li>
</ul>
<h3 id="aggregation-control"><a class="header" href="#aggregation-control">Aggregation Control</a></h3>
<p><strong>Flags</strong>: <code>--aggregate-only</code>, <code>--no-aggregation</code></p>
<p>Control file-level score aggregation.</p>
<pre><code class="language-bash"># Show only aggregated file-level scores
debtmap --aggregate-only

# Disable file-level aggregation entirely
debtmap --no-aggregation

# Default: show both individual items and file aggregates
debtmap
</code></pre>
<p><strong>Use cases</strong>:</p>
<ul>
<li><code>--aggregate-only</code>: Focus on file-level technical debt</li>
<li><code>--no-aggregation</code>: See individual functions/classes only</li>
<li>Default: Full picture with both levels</li>
</ul>
<h3 id="tiered-prioritization-issues"><a class="header" href="#tiered-prioritization-issues">Tiered Prioritization Issues</a></h3>
<p><strong>Overview</strong>: Debtmap uses a 4-tier system to classify technical debt items by priority:</p>
<p><strong>Tier Classification</strong>:</p>
<ul>
<li><strong>Tier 1 (Critical Architecture)</strong>: High complexity, low coverage, high dependencies, entry points</li>
<li><strong>Tier 2 (Complex Untested)</strong>: Significant complexity or coverage gaps</li>
<li><strong>Tier 3 (Testing Gaps)</strong>: Moderate issues that need attention</li>
<li><strong>Tier 4 (Maintenance)</strong>: Low-priority items, routine maintenance</li>
</ul>
<p><strong>Tier Weights</strong>:</p>
<ul>
<li>Tier 1: 1.5Ã— multiplier (highest priority)</li>
<li>Tier 2: 1.0Ã— multiplier (standard priority)</li>
<li>Tier 3: 0.7Ã— multiplier (reduced priority)</li>
<li>Tier 4: 0.3Ã— multiplier (lowest priority)</li>
</ul>
<p><strong>Configuration</strong>:</p>
<pre><code class="language-toml"># In .debtmap.toml
[tiers]
# Tier 2 requires EITHER high complexity OR high dependencies
t2_complexity_threshold = 15
t2_dependency_threshold = 10

# Tier 3 requires moderate complexity
t3_complexity_threshold = 8

# Control Tier 4 visibility in main report
show_t4_in_main_report = false
</code></pre>
<p><strong>Common Issues</strong>:</p>
<p><strong>Q: Why is my item in Tier 3 instead of Tier 2?</strong></p>
<p>A: Check if it meets Tier 2 thresholds:</p>
<pre><code class="language-bash"># See tier classification with verbosity
debtmap -v

# Check current thresholds
cat .debtmap.toml | grep -A 5 "\[tiers\]"

# Lower thresholds to promote more items to Tier 2
# In .debtmap.toml:
# t2_complexity_threshold = 10  (default: 15)
# t2_dependency_threshold = 5   (default: 10)
</code></pre>
<p><strong>Q: How do I hide Tier 4 items from the main report?</strong></p>
<p>A: Use the <code>show_t4_in_main_report</code> configuration:</p>
<pre><code class="language-toml"># In .debtmap.toml
[tiers]
show_t4_in_main_report = false
</code></pre>
<p>Tier 4 items will still appear in detailed output but wonâ€™t clutter the main summary.</p>
<h3 id="file-level-scoring-issues"><a class="header" href="#file-level-scoring-issues">File-Level Scoring Issues</a></h3>
<p><strong>Overview</strong>: Debtmap aggregates function/class scores into file-level scores using a multi-factor formula.</p>
<p><strong>File Score Formula</strong>:</p>
<pre><code>File Score = size_factor Ã— complexity_factor Ã— coverage_factor Ã—
             density_factor Ã— god_object_multiplier Ã— function_scores
</code></pre>
<p><strong>Factors</strong>:</p>
<ul>
<li><strong>size_factor</strong>: Based on lines of code (larger files = higher factor)</li>
<li><strong>complexity_factor</strong>: Average complexity across functions</li>
<li><strong>coverage_factor</strong>: File-level test coverage (from LCOV file)</li>
<li><strong>density_factor</strong>: Penalizes files with &gt;50 functions (indicates code smell)</li>
<li><strong>god_object_multiplier</strong>: 1.5Ã— for files exceeding god object thresholds</li>
</ul>
<p><strong>Aggregation Methods</strong>:</p>
<pre><code class="language-bash"># Weighted sum (default) - considers complexity weights
debtmap --aggregation-method weighted_sum

# Simple sum - adds all function scores
debtmap --aggregation-method sum

# Logarithmic sum - dampens very high scores
debtmap --aggregation-method logarithmic_sum

# Max plus average - highlights worst function + context
debtmap --aggregation-method max_plus_average
</code></pre>
<p><strong>When to use each method</strong>:</p>
<ul>
<li><strong>weighted_sum</strong>: Default, balances individual and collective impact</li>
<li><strong>sum</strong>: When you want raw cumulative debt</li>
<li><strong>logarithmic_sum</strong>: For very large files to prevent score explosion</li>
<li><strong>max_plus_average</strong>: Focus on worst offender while considering overall file health</li>
</ul>
<p><strong>Aggregation Control Flags</strong>:</p>
<pre><code class="language-bash"># Show only aggregated file-level scores
debtmap --aggregate-only

# Disable file-level aggregation entirely
debtmap --no-aggregation

# Default: show both individual items and file aggregates
debtmap
</code></pre>
<p><strong>Troubleshooting High File Scores</strong>:</p>
<p><strong>Q: Why does this file have such a high score?</strong></p>
<p>A: Check contributing factors with verbosity:</p>
<pre><code class="language-bash"># See file-level score breakdown
debtmap path/to/file.rs -vv

# Look for:
# - High function count (density_factor kicks in at 50+)
# - God object detection (1.5Ã— multiplier)
# - Low coverage (high coverage_factor)
# - Large file size (size_factor)
# - Multiple high-complexity functions

# Disable god object detection if false positive
debtmap --no-god-object path/to/file.rs
</code></pre>
<h3 id="combining-advanced-flags"><a class="header" href="#combining-advanced-flags">Combining Advanced Flags</a></h3>
<pre><code class="language-bash"># Comprehensive analysis with all features
debtmap --multi-pass --attribution --context -vv

# Minimal filtering for exploration
debtmap --min-problematic 1 --min-priority 0 --no-god-object

# Performance-focused advanced analysis
debtmap --multi-pass --jobs 8 --cache-location ~/.cache/debtmap

# Summary view with aggregated scores
debtmap --detail-level summary --aggregate-only
</code></pre>
<h2 id="error-messages-reference"><a class="header" href="#error-messages-reference">Error Messages Reference</a></h2>
<p>Understanding common error messages and how to resolve them.</p>
<h3 id="file-system-errors"><a class="header" href="#file-system-errors">File System Errors</a></h3>
<p><strong>Message</strong>: <code>File system error: Permission denied</code></p>
<p><strong>Meaning</strong>: Cannot read file or directory due to permissions</p>
<p><strong>Solutions</strong>:</p>
<ul>
<li>Check file permissions: <code>ls -la &lt;file&gt;</code></li>
<li>Ensure user has read access</li>
<li>Verify cache directory is writable</li>
<li>Use <code>--cache-location</code> for accessible directory</li>
</ul>
<hr />
<p><strong>Message</strong>: <code>File system error: No such file or directory</code></p>
<p><strong>Meaning</strong>: File or directory does not exist</p>
<p><strong>Solutions</strong>:</p>
<ul>
<li>Verify path is correct</li>
<li>Check current working directory: <code>pwd</code></li>
<li>Use absolute paths if needed</li>
<li>Ensure files werenâ€™t moved or deleted</li>
</ul>
<h3 id="parse-errors-1"><a class="header" href="#parse-errors-1">Parse Errors</a></h3>
<p><strong>Message</strong>: <code>Parse error in file.rs:line:column: unexpected token</code></p>
<p><strong>Meaning</strong>: Syntax debtmap cannot parse</p>
<p><strong>Solutions</strong>:</p>
<pre><code class="language-bash"># Try fallback mode
debtmap --semantic-off

# For Rust macros
debtmap --verbose-macro-warnings --show-macro-stats

# Exclude problematic file
# In .debtmap/config.toml:
# exclude = ["path/to/file.rs"]
</code></pre>
<h3 id="analysis-errors"><a class="header" href="#analysis-errors">Analysis Errors</a></h3>
<p><strong>Message</strong>: <code>Analysis error: internal analysis failure</code></p>
<p><strong>Meaning</strong>: Internal error during analysis phase</p>
<p><strong>Solutions</strong>:</p>
<pre><code class="language-bash"># Try fallback mode
debtmap --semantic-off

# Report with debug info
debtmap -vvv 2&gt;&amp;1 | tee error.log

# Isolate problem file
debtmap --max-files 1 path/to/suspected/file
</code></pre>
<h3 id="configuration-errors"><a class="header" href="#configuration-errors">Configuration Errors</a></h3>
<p><strong>Message</strong>: <code>Configuration error: invalid config value</code></p>
<p><strong>Meaning</strong>: Invalid configuration in <code>.debtmap/config.toml</code> or CLI</p>
<p><strong>Solutions</strong>:</p>
<ul>
<li>Check <code>.debtmap/config.toml</code> syntax</li>
<li>Validate TOML format: <code>cat .debtmap/config.toml</code></li>
<li>Review CLI flag values</li>
<li>Check for typos in flag names</li>
</ul>
<h3 id="cache-errors"><a class="header" href="#cache-errors">Cache Errors</a></h3>
<p><strong>Message</strong>: <code>Cache error: corrupted cache entry</code></p>
<p><strong>Meaning</strong>: Cache data is invalid or corrupted</p>
<p><strong>Solutions</strong>:</p>
<pre><code class="language-bash"># Clear cache
debtmap --clear-cache

# Force rebuild
debtmap --force-cache-rebuild

# Use different cache location
debtmap --cache-location /tmp/debtmap-cache
</code></pre>
<h3 id="validation-errors"><a class="header" href="#validation-errors">Validation Errors</a></h3>
<p><strong>Message</strong>: <code>Validation error: threshold validation failed</code></p>
<p><strong>Meaning</strong>: Threshold configuration is invalid</p>
<p><strong>Solutions</strong>:</p>
<ul>
<li>Check threshold values in config</li>
<li>Ensure <code>--min-priority</code> is in valid range (0-10)</li>
<li>Verify threshold preset exists</li>
<li>Use <code>--threshold-preset</code> with valid preset name</li>
</ul>
<h3 id="dependency-errors"><a class="header" href="#dependency-errors">Dependency Errors</a></h3>
<p><strong>Message</strong>: <code>Dependency error: cannot resolve dependency graph</code></p>
<p><strong>Meaning</strong>: Cannot build dependency relationships</p>
<p><strong>Solutions</strong>:</p>
<pre><code class="language-bash"># Disable dependency provider
debtmap --context --disable-context dependency

# Try without context
debtmap

# Debug with verbosity
debtmap -vvv
</code></pre>
<h3 id="concurrency-errors"><a class="header" href="#concurrency-errors">Concurrency Errors</a></h3>
<p><strong>Message</strong>: <code>Concurrency error: parallel processing failure</code></p>
<p><strong>Meaning</strong>: Error during parallel execution</p>
<p><strong>Solutions</strong>:</p>
<pre><code class="language-bash"># Disable parallel processing
debtmap --no-parallel

# Reduce thread count
debtmap --jobs 1

# Report issue with debug output
debtmap -vvv 2&gt;&amp;1 | tee error.log
</code></pre>
<h3 id="unsupported-errors"><a class="header" href="#unsupported-errors">Unsupported Errors</a></h3>
<p><strong>Message</strong>: <code>Unsupported: feature not available for &lt;language&gt;</code></p>
<p><strong>Meaning</strong>: Language or construct not supported</p>
<p><strong>Solutions</strong>:</p>
<ul>
<li>Use supported languages: Rust, Python, JavaScript, TypeScript</li>
<li>Check if language is enabled in config</li>
<li>Some advanced features may not be available for all languages</li>
<li>Try <code>--semantic-off</code> for basic analysis</li>
</ul>
<h3 id="pattern-errors"><a class="header" href="#pattern-errors">Pattern Errors</a></h3>
<p><strong>Message</strong>: <code>Pattern error: invalid glob pattern</code></p>
<p><strong>Meaning</strong>: Invalid glob pattern in configuration or CLI</p>
<p><strong>Solutions</strong>:</p>
<ul>
<li>Check glob pattern syntax</li>
<li>Escape special characters if needed</li>
<li>Test pattern with shell glob: <code>ls &lt;pattern&gt;</code></li>
<li>Use simpler patterns or path prefixes</li>
</ul>
<h2 id="language-specific-issues"><a class="header" href="#language-specific-issues">Language-Specific Issues</a></h2>
<h3 id="rust-1"><a class="header" href="#rust-1">Rust</a></h3>
<p><strong>Macro Expansion Issues</strong></p>
<pre><code class="language-bash"># See macro warnings
debtmap --verbose-macro-warnings

# Show macro statistics
debtmap --show-macro-stats

# Common issue: Complex macros may not expand correctly
# Solution: Use --semantic-off for faster fallback
</code></pre>
<p><strong>Trait and Generic Complexity</strong></p>
<p>Complex trait bounds and generic constraints may affect analysis accuracy:</p>
<pre><code class="language-bash"># Full semantic analysis (default)
debtmap

# Fallback mode for edge cases
debtmap --semantic-off
</code></pre>
<h3 id="python-1"><a class="header" href="#python-1">Python</a></h3>
<p><strong>Type Inference Limitations</strong></p>
<p>Dynamic typing makes some analysis challenging:</p>
<pre><code class="language-bash"># Best effort type inference (default)
debtmap

# Fallback mode if issues
debtmap --semantic-off
</code></pre>
<p><strong>Import Resolution</strong></p>
<p>Complex import structures may not resolve fully:</p>
<ul>
<li>Relative imports usually work</li>
<li>Dynamic imports may not be detected</li>
<li><code>__init__.py</code> packages are supported</li>
</ul>
<h3 id="javascripttypescript-2"><a class="header" href="#javascripttypescript-2">JavaScript/TypeScript</a></h3>
<p><strong>JSX/TSX Parsing</strong></p>
<p>Ensure files have correct extensions:</p>
<ul>
<li><code>.jsx</code> for JavaScript + JSX</li>
<li><code>.tsx</code> for TypeScript + JSX</li>
<li>Configure extensions in <code>.debtmap/config.toml</code> if needed</li>
</ul>
<p><strong>Type Resolution</strong></p>
<p>TypeScript type resolution in complex projects:</p>
<pre><code class="language-bash"># Full type checking (default for .ts files)
debtmap

# Fallback if type issues
debtmap --semantic-off
</code></pre>
<h3 id="mixed-language-projects"><a class="header" href="#mixed-language-projects">Mixed Language Projects</a></h3>
<pre><code class="language-bash"># Analyze all supported languages (default)
debtmap

# Filter specific languages
# In .debtmap/config.toml:
# languages = ["rust", "python"]
</code></pre>
<h3 id="unsupported-language-constructs"><a class="header" href="#unsupported-language-constructs">Unsupported Language Constructs</a></h3>
<p>Some advanced language features may show as â€œUnsupportedâ€:</p>
<ul>
<li>Rust: Some macro patterns, const generics edge cases</li>
<li>Python: Some metaclass patterns, dynamic code generation</li>
<li>JavaScript: Some advanced AST manipulation</li>
</ul>
<p><strong>Solutions</strong>:</p>
<ul>
<li>Use <code>--semantic-off</code> for basic analysis</li>
<li>Exclude problematic files if needed</li>
<li>Report unsupported patterns as feature requests</li>
</ul>
<h3 id="false-positives"><a class="header" href="#false-positives">False Positives</a></h3>
<p>Reduce false positives for validation functions and repetitive code patterns using entropy analysis:</p>
<p><strong>Enable and Configure Entropy Analysis</strong>:</p>
<pre><code class="language-toml"># In .debtmap.toml
[entropy]
enabled = true                    # Enable entropy-based dampening
weight = 0.3                      # Weight in complexity adjustment (0.0-1.0)
min_tokens = 50                   # Minimum tokens for entropy calculation
pattern_threshold = 0.7           # Pattern similarity threshold (0.0-1.0)
use_classification = true         # Enable advanced token classification
entropy_threshold = 0.5           # Entropy level for dampening (0.0-1.0)
branch_threshold = 0.8            # Branch similarity threshold (0.0-1.0)
max_combined_reduction = 0.5      # Max reduction percentage (0.0-1.0)
</code></pre>
<p><strong>When to Adjust Parameters</strong>:</p>
<ul>
<li><strong>Increase <code>pattern_threshold</code></strong> (e.g., 0.8-0.9): Be more strict, reduce dampening for truly unique code</li>
<li><strong>Decrease <code>entropy_threshold</code></strong> (e.g., 0.3-0.4): Apply dampening more broadly to catch more repetitive patterns</li>
<li><strong>Increase <code>weight</code></strong> (e.g., 0.4-0.5): Make entropy have stronger impact on final scores</li>
<li><strong>Increase <code>min_tokens</code></strong> (e.g., 100): Only apply entropy analysis to larger functions</li>
<li><strong>Increase <code>branch_threshold</code></strong> (e.g., 0.9): Be more strict about branching pattern similarity</li>
</ul>
<p>Entropy analysis can reduce false positives by up to 70% for validation functions, error handling, and other repetitive patterns.</p>
<p><strong>Other False Positive Reduction Strategies</strong>:</p>
<pre><code class="language-bash"># Use context-aware analysis
debtmap --context

# Adjust thresholds
debtmap --threshold-preset lenient

# Disable context-aware filtering if too aggressive
debtmap --no-context-aware
</code></pre>
<h3 id="missing-detections"><a class="header" href="#missing-detections">Missing Detections</a></h3>
<pre><code class="language-bash"># Ensure semantic analysis is enabled
debtmap  # (default, semantic ON)

# Increase verbosity to see what's detected
debtmap -vv

# Check if files are being analyzed
debtmap -v 2&gt;&amp;1 | grep "Processing"
</code></pre>
<h2 id="output-formatting-issues"><a class="header" href="#output-formatting-issues">Output Formatting Issues</a></h2>
<h3 id="choose-output-format"><a class="header" href="#choose-output-format">Choose Output Format</a></h3>
<pre><code class="language-bash"># Terminal format (default, human-readable)
debtmap

# JSON format
debtmap --format json

# Markdown format
debtmap --format markdown
</code></pre>
<h3 id="json-format-options"><a class="header" href="#json-format-options">JSON Format Options</a></h3>
<pre><code class="language-bash"># Legacy format (default): {File: {...}}
debtmap --format json --output-format legacy

# Unified format: consistent structure with 'type' field
debtmap --format json --output-format unified

# Validate JSON
debtmap --format json | jq .

# Write to file
debtmap --format json --output results.json
</code></pre>
<h3 id="plain-output-mode"><a class="header" href="#plain-output-mode">Plain Output Mode</a></h3>
<p>For environments without color/emoji support:</p>
<pre><code class="language-bash"># ASCII-only, no colors, no emoji
debtmap --plain

# Or set environment variable
export NO_EMOJI=1
debtmap
</code></pre>
<h3 id="terminal-color-issues"><a class="header" href="#terminal-color-issues">Terminal Color Issues</a></h3>
<p><strong>Problem</strong>: Colors not rendering or showing escape codes</p>
<p><strong>Solutions</strong>:</p>
<pre><code class="language-bash"># Use plain mode
debtmap --plain

# Check TERM environment variable
echo $TERM

# Set appropriate TERM
export TERM=xterm-256color
</code></pre>
<h3 id="emoji-issues"><a class="header" href="#emoji-issues">Emoji Issues</a></h3>
<p><strong>Problem</strong>: Emojis showing as boxes or ??</p>
<p><strong>Solutions</strong>:</p>
<pre><code class="language-bash"># Disable emojis
debtmap --plain

# Or environment variable
export NO_EMOJI=1
debtmap
</code></pre>
<h3 id="markdown-rendering"><a class="header" href="#markdown-rendering">Markdown Rendering</a></h3>
<p>Ensure viewer supports GitHub-flavored markdown:</p>
<ul>
<li>Tables</li>
<li>Code blocks with syntax highlighting</li>
<li>Task lists</li>
</ul>
<h3 id="write-output-to-file"><a class="header" href="#write-output-to-file">Write Output to File</a></h3>
<pre><code class="language-bash"># JSON to file
debtmap --format json --output results.json

# Markdown to file
debtmap --format markdown --output report.md

# Terminal format to file (preserves colors)
debtmap --output results.txt

# Plain format to file
debtmap --plain --output results.txt
</code></pre>
<h3 id="summary-vs-full-output"><a class="header" href="#summary-vs-full-output">Summary vs Full Output</a></h3>
<pre><code class="language-bash"># Summary mode (compact)
debtmap --summary
debtmap -s

# Full output (default)
debtmap

# Limit number of items
debtmap --top 10       # Top 10 by priority
debtmap --tail 10      # Bottom 10 by priority
</code></pre>
<h3 id="filtering-output"><a class="header" href="#filtering-output">Filtering Output</a></h3>
<pre><code class="language-bash"># Minimum priority level
debtmap --min-priority 5

# Category filters
debtmap --filter "complexity,debt"

# Combine filters
debtmap --min-priority 3 --top 20 --filter complexity
</code></pre>
<h2 id="compare-command-issues"><a class="header" href="#compare-command-issues">Compare Command Issues</a></h2>
<p>The <code>compare</code> command helps track changes in technical debt over time.</p>
<h3 id="basic-usage-2"><a class="header" href="#basic-usage-2">Basic Usage</a></h3>
<p><strong>Note</strong>: The <code>compare</code> command defaults to JSON output format (unlike <code>analyze</code> which defaults to terminal). Use <code>--format terminal</code> or <code>--format markdown</code> if you need different output.</p>
<pre><code class="language-bash"># Save baseline results
debtmap --format json --output before.json

# Make code changes...

# Save new results
debtmap --format json --output after.json

# Compare results (outputs JSON by default)
debtmap compare --before before.json --after after.json

# Compare with terminal output
debtmap compare --before before.json --after after.json --format terminal
</code></pre>
<h3 id="targeted-comparison"><a class="header" href="#targeted-comparison">Targeted Comparison</a></h3>
<p>Use <code>--plan</code> and <code>--target-location</code> for focused debt analysis:</p>
<pre><code class="language-bash"># Compare based on implementation plan
debtmap compare --before before.json --after after.json --plan implementation-plan.json

# Compare specific code location
debtmap compare --before before.json --after after.json \
  --target-location src/main.rs:calculate_score:42

# Combine both for precise tracking
debtmap compare --before before.json --after after.json \
  --plan implementation-plan.json \
  --target-location src/analyzers/complexity.rs:analyze_function:128
</code></pre>
<p><strong>Use cases</strong>:</p>
<ul>
<li><code>--plan</code>: Track debt changes for planned refactoring tasks</li>
<li><code>--target-location</code>: Focus on specific function or code location</li>
<li>Combine for granular technical debt tracking</li>
</ul>
<h3 id="incompatible-format-errors"><a class="header" href="#incompatible-format-errors">Incompatible Format Errors</a></h3>
<p><strong>Problem</strong>: â€œIncompatible formatsâ€ error when comparing files</p>
<p><strong>Causes</strong>:</p>
<ul>
<li>Mixing legacy and unified JSON formats</li>
<li>Files from different debtmap versions</li>
<li>Corrupted JSON files</li>
</ul>
<p><strong>Solutions</strong>:</p>
<pre><code class="language-bash"># Ensure both files use same output format
debtmap --format json --output-format unified --output before.json
# ... make changes ...
debtmap --format json --output-format unified --output after.json
debtmap compare --before before.json --after after.json

# Validate JSON files are well-formed
jq . before.json &gt; /dev/null
jq . after.json &gt; /dev/null
</code></pre>
<h3 id="comparing-across-branches"><a class="header" href="#comparing-across-branches">Comparing Across Branches</a></h3>
<pre><code class="language-bash"># Save baseline on main branch
git checkout main
debtmap --format json --output main.json

# Switch to feature branch
git checkout feature-branch
debtmap --format json --output feature.json

# Compare branches
debtmap compare --before main.json --after feature.json
</code></pre>
<h3 id="missing-files-error"><a class="header" href="#missing-files-error">Missing Files Error</a></h3>
<p><strong>Problem</strong>: â€œFile not foundâ€ when running compare</p>
<p><strong>Solutions</strong>:</p>
<ul>
<li>Verify file paths are correct (use absolute paths if needed)</li>
<li>Ensure JSON files werenâ€™t moved or deleted</li>
<li>Check current working directory with <code>pwd</code></li>
</ul>
<h3 id="format-mismatch-issues"><a class="header" href="#format-mismatch-issues">Format Mismatch Issues</a></h3>
<p><strong>Problem</strong>: Compare shows unexpected differences or errors</p>
<p><strong>Solutions</strong>:</p>
<pre><code class="language-bash"># Regenerate both files with same debtmap version
debtmap --format json --output before.json
# ... make changes ...
debtmap --format json --output after.json

# Use same output format for both
debtmap --format json --output-format unified --output before.json
debtmap --format json --output-format unified --output after.json
</code></pre>
<h2 id="validate-command-issues"><a class="header" href="#validate-command-issues">Validate Command Issues</a></h2>
<p>The <code>validate</code> command checks if a codebase meets specified quality thresholds, useful for CI/CD pipelines.</p>
<h3 id="basic-validation"><a class="header" href="#basic-validation">Basic Validation</a></h3>
<pre><code class="language-bash"># Validate codebase passes default thresholds
debtmap validate /path/to/project

# Exit code 0 if passes, non-zero if validation fails
</code></pre>
<h3 id="debt-density-validation"><a class="header" href="#debt-density-validation">Debt Density Validation</a></h3>
<p><strong>Flag</strong>: <code>--max-debt-density &lt;number&gt;</code></p>
<p>Sets the maximum acceptable technical debt per 1000 lines of code.</p>
<pre><code class="language-bash"># Set maximum acceptable debt density (per 1000 LOC)
debtmap validate /path/to/project --max-debt-density 10.0

# Stricter threshold for critical projects
debtmap validate /path/to/project --max-debt-density 5.0

# Lenient threshold for legacy code
debtmap validate /path/to/project --max-debt-density 20.0
</code></pre>
<p><strong>Troubleshooting validation failures</strong>:</p>
<pre><code class="language-bash"># See which files exceed threshold
debtmap validate /path/to/project --max-debt-density 10.0 -v

# Get detailed breakdown
debtmap validate /path/to/project --max-debt-density 10.0 -vv

# Analyze specific files that failed
debtmap /path/to/problematic/file.rs -v
</code></pre>
<h3 id="cicd-integration-2"><a class="header" href="#cicd-integration-2">CI/CD Integration</a></h3>
<pre><code class="language-bash"># In CI pipeline (fails build if validation fails)
debtmap validate . --max-debt-density 10.0 || exit 1

# With verbose output for debugging
debtmap validate . --max-debt-density 10.0 -v

# Save validation report
debtmap validate . --max-debt-density 10.0 --format json --output validation.json
</code></pre>
<p><strong>Use cases</strong>:</p>
<ul>
<li>Enforce quality gates in CI/CD pipelines</li>
<li>Prevent accumulation of technical debt over time</li>
<li>Track debt density trends across releases</li>
<li>Set different thresholds for different parts of codebase</li>
</ul>
<h2 id="faq-1"><a class="header" href="#faq-1">FAQ</a></h2>
<h3 id="general-questions"><a class="header" href="#general-questions">General Questions</a></h3>
<p><strong>Q: Why is my analysis slow?</strong></p>
<p>A: Check several factors:</p>
<pre><code class="language-bash"># Check cache status
debtmap --cache-stats

# Ensure caching is enabled (default)
# Remove --no-cache if present

# Use all CPU cores
debtmap --jobs 0

# Check for large files or complex macros
debtmap -vv
</code></pre>
<p><strong>Q: What does â€˜Parse errorâ€™ mean?</strong></p>
<p>A: File contains syntax debtmap cannot parse. Solutions:</p>
<ul>
<li>Try <code>--semantic-off</code> for fallback mode</li>
<li>Use <code>--verbose-macro-warnings</code> for Rust macros</li>
<li>Exclude problematic files in <code>.debtmap/config.toml</code></li>
<li>Report parse errors as potential bugs</li>
</ul>
<p><strong>Q: Why do scores differ between runs?</strong></p>
<p>A: Several factors affect scores:</p>
<ul>
<li>Coverage file changed (use <code>--coverage-file</code>)</li>
<li>Context providers enabled/disabled (<code>--context</code>)</li>
<li>Cache was cleared (<code>--clear-cache</code>)</li>
<li>Code changes (intended behavior)</li>
<li>Different threshold settings</li>
</ul>
<p><strong>Q: How do I reduce noise in results?</strong></p>
<p>A: Use filtering options:</p>
<pre><code class="language-bash"># Increase minimum priority
debtmap --min-priority 5

# Use threshold preset
debtmap --threshold-preset strict

# Filter categories
debtmap --filter "complexity,debt"

# Limit output
debtmap --top 20
</code></pre>
<h3 id="format-and-output"><a class="header" href="#format-and-output">Format and Output</a></h3>
<p><strong>Q: Whatâ€™s the difference between legacy and unified JSON?</strong></p>
<p>A: Two JSON output formats:</p>
<ul>
<li><strong>Legacy</strong>: <code>{File: {...}}</code> - nested file-based structure</li>
<li><strong>Unified</strong>: Consistent structure with <code>type</code> field for each item</li>
</ul>
<pre><code class="language-bash"># Legacy (default)
debtmap --format json --output-format legacy

# Unified (recommended for parsing)
debtmap --format json --output-format unified
</code></pre>
<p><strong>Q: Can I analyze partial codebases?</strong></p>
<p>A: Yes, several approaches:</p>
<pre><code class="language-bash"># Limit file count
debtmap --max-files 100

# Analyze specific directory
debtmap src/specific/module

# Use filters in config
# .debtmap/config.toml:
# include = ["src/**/*.rs"]
</code></pre>
<p><strong>Q: How is the 0-10 priority score calculated?</strong></p>
<p>A: Debtmap uses a multi-factor unified scoring formula to compute priority scores:</p>
<p><strong>Base Score Formula</strong>:</p>
<pre><code>Base Score = (Complexity Factor Ã— 0.40) +
             (Coverage Factor Ã— 0.40) +
             (Dependency Factor Ã— 0.20)
</code></pre>
<p><strong>Where</strong>:</p>
<ul>
<li><strong>Complexity Factor</strong>: Normalized cognitive complexity (0.0-10.0 scale)</li>
<li><strong>Coverage Factor</strong>: <code>(1 - coverage_percentage) Ã— 2 + 1</code> (ranges from 1.0-3.0)</li>
<li><strong>Dependency Factor</strong>: Based on incoming/outgoing dependencies and coupling</li>
</ul>
<p><strong>Role Multipliers</strong> adjust the base score by function role:</p>
<ul>
<li><strong>Entry points</strong>: 1.5Ã— (critical path, highest priority)</li>
<li><strong>Business logic</strong>: 1.2Ã— (core functionality)</li>
<li><strong>Data access</strong>: 1.0Ã— (important but stable)</li>
<li><strong>Infrastructure</strong>: 0.8Ã— (lower priority)</li>
<li><strong>Utilities</strong>: 0.5Ã— (minimal priority)</li>
<li><strong>Test code</strong>: 0.1Ã— (lowest priority)</li>
</ul>
<p><strong>Final Score</strong>:</p>
<pre><code>Final Score = Base Score Ã— Role Multiplier
</code></pre>
<p>This ensures functions are prioritized by true impact, not just raw complexity. Entry points with high complexity and low coverage get the highest scores, while well-tested utility functions get lower scores.</p>
<p><strong>Example</strong>:</p>
<ul>
<li>Function with complexity=20, coverage=0%, dependencies=5, role=entry_point</li>
<li>Complexity factor: 8.0 (normalized)</li>
<li>Coverage factor: 3.0 (untested)</li>
<li>Dependency factor: 2.5 (moderate coupling)</li>
<li>Base score: (8.0 Ã— 0.40) + (3.0 Ã— 0.40) + (2.5 Ã— 0.20) = 4.9</li>
<li>Final score: 4.9 Ã— 1.5 (entry point) = <strong>7.35</strong></li>
</ul>
<pre><code class="language-bash"># See score breakdown with verbosity
debtmap -v

# See detailed factor calculations
debtmap -vv
</code></pre>
<h3 id="coverage-and-testing"><a class="header" href="#coverage-and-testing">Coverage and Testing</a></h3>
<p><strong>Q: How does coverage affect scores?</strong></p>
<p>A: Coverage is integrated into the unified scoring system as a factor:</p>
<ul>
<li><strong>Coverage Factor Formula</strong>: <code>(1 - coverage_percentage) Ã— 2 + 1</code></li>
<li>This factor is weighted at 40% in the unified scoring formula</li>
<li><strong>Base Score</strong> = (Complexity Ã— 0.40) + (Coverage Ã— 0.40) + (Dependency Ã— 0.20)</li>
<li><strong>Final Score</strong> = Base Score Ã— Role Multiplier</li>
</ul>
<p>This means untested code (0% coverage) gets the highest coverage factor (3.0), while fully tested code (100% coverage) gets the lowest (1.0), ensuring untested complex code rises to the top.</p>
<pre><code class="language-bash"># Use coverage file
debtmap --coverage-file coverage.info

# See coverage impact
debtmap --coverage-file coverage.info -v
</code></pre>
<p>See the FAQ entry â€œHow is the 0-10 priority score calculated?â€ for complete scoring formula details.</p>
<h3 id="context-and-analysis"><a class="header" href="#context-and-analysis">Context and Analysis</a></h3>
<p><strong>Q: What are context providers?</strong></p>
<p>A: Additional analysis for prioritization:</p>
<ul>
<li><strong>critical_path</strong>: Call graph analysis, entry point distance</li>
<li><strong>dependency</strong>: Dependency relationships and coupling</li>
<li><strong>git_history</strong>: Change frequency and authorship</li>
</ul>
<pre><code class="language-bash"># Enable all
debtmap --context

# Specific providers
debtmap --context --context-providers critical_path,dependency

# See context impact
debtmap --context -v
</code></pre>
<h3 id="results-and-comparison"><a class="header" href="#results-and-comparison">Results and Comparison</a></h3>
<p><strong>Q: Why no output?</strong></p>
<p>A: Check verbosity and filtering:</p>
<pre><code class="language-bash"># Increase verbosity
debtmap -v

# Lower priority threshold
debtmap --min-priority 0

# Check if files were analyzed
debtmap -vv 2&gt;&amp;1 | grep "Processed"

# Ensure not using strict threshold
debtmap --threshold-preset lenient
</code></pre>
<p><strong>Q: How to compare results over time?</strong></p>
<p>A: Use the <code>compare</code> command:</p>
<pre><code class="language-bash"># Save baseline
debtmap --format json --output before.json

# Make changes...

# Analyze again
debtmap --format json --output after.json

# Compare
debtmap compare --before before.json --after after.json
</code></pre>
<p><strong>Q: Why does compare fail with â€˜incompatible formatsâ€™?</strong></p>
<p>A: The JSON files must use the same output format:</p>
<pre><code class="language-bash"># Use unified format for both
debtmap --format json --output-format unified --output before.json
# ... make changes ...
debtmap --format json --output-format unified --output after.json
debtmap compare --before before.json --after after.json

# Or use legacy format for both (but unified is recommended)
debtmap --format json --output-format legacy --output before.json
debtmap --format json --output-format legacy --output after.json
</code></pre>
<p><strong>Q: How do I compare results from different branches?</strong></p>
<p>A: Generate JSON output on each branch and compare:</p>
<pre><code class="language-bash"># On main branch
git checkout main
debtmap --format json --output main.json

# On feature branch
git checkout feature-branch
debtmap --format json --output feature.json

# Compare (from either branch)
debtmap compare --before main.json --after feature.json
</code></pre>
<p><strong>Q: Can I compare legacy and unified JSON formats?</strong></p>
<p>A: No, both files must use the same format. Regenerate with matching formats:</p>
<pre><code class="language-bash"># Convert both to unified format
debtmap --format json --output-format unified --output before.json
debtmap --format json --output-format unified --output after.json
debtmap compare --before before.json --after after.json
</code></pre>
<h3 id="performance-and-optimization"><a class="header" href="#performance-and-optimization">Performance and Optimization</a></h3>
<p><strong>Q: How many threads should I use?</strong></p>
<p>A: Depends on your machine:</p>
<pre><code class="language-bash"># Use all cores (default, recommended)
debtmap --jobs 0

# Limit to 4 threads (if other work running)
debtmap --jobs 4

# Single threaded (debugging only)
debtmap --no-parallel
</code></pre>
<p><strong>Q: Should I use shared or local cache?</strong></p>
<p>A: Depends on your workflow:</p>
<ul>
<li><strong>Local cache</strong> (<code>.debtmap/cache</code>): Isolated, automatic</li>
<li><strong>Shared cache</strong> (<code>~/.cache/debtmap</code>): Saves space across projects</li>
</ul>
<pre><code class="language-bash"># Shared cache
debtmap --cache-location ~/.cache/debtmap

# Set permanently
export DEBTMAP_CACHE_DIR=~/.cache/debtmap
</code></pre>
<h2 id="when-to-file-bug-reports"><a class="header" href="#when-to-file-bug-reports">When to File Bug Reports</a></h2>
<p>File a bug report when:</p>
<p>âœ… <strong>These are bugs</strong>:</p>
<ul>
<li>Parse errors on valid syntax</li>
<li>Crashes or panics</li>
<li>Incorrect complexity calculations</li>
<li>Cache corruption</li>
<li>Concurrency errors</li>
<li>Incorrect error messages</li>
</ul>
<p>âŒ <strong>These are not bugs</strong>:</p>
<ul>
<li>Unsupported language constructs (file feature request)</li>
<li>Disagreement with complexity scores (subjective)</li>
<li>Performance on very large codebases (optimization request)</li>
<li>Missing documentation (docs issue, not code bug)</li>
</ul>
<h3 id="how-to-report-issues"><a class="header" href="#how-to-report-issues">How to Report Issues</a></h3>
<ol>
<li><strong>Reproduce with minimal example</strong></li>
<li><strong>Include debug output</strong>: <code>debtmap -vvv 2&gt;&amp;1 | tee error.log</code></li>
<li><strong>Include version</strong>: <code>debtmap --version</code></li>
<li><strong>Include platform</strong>: OS, Rust version if relevant</li>
<li><strong>Include configuration</strong>: <code>.debtmap/config.toml</code> if used</li>
<li><strong>Expected vs actual behavior</strong></li>
</ol>
<h3 id="before-filing"><a class="header" href="#before-filing">Before Filing</a></h3>
<ol>
<li>Check this troubleshooting guide</li>
<li>Try <code>--semantic-off</code> fallback mode</li>
<li>Clear cache with <code>--clear-cache</code></li>
<li>Update to latest version</li>
<li>Search existing issues on GitHub</li>
</ol>
<h2 id="related-documentation-1"><a class="header" href="#related-documentation-1">Related Documentation</a></h2>
<ul>
<li><strong><a href="./configuration.html">Configuration Guide</a></strong>: Configure debtmap behavior</li>
<li><strong><a href="./cli-reference.html">CLI Reference</a></strong>: Complete CLI flag documentation</li>
<li><strong><a href="./analysis-guide.html">Analysis Guide</a></strong>: Understanding analysis results</li>
<li><strong><a href="./examples.html">Examples</a></strong>: Practical usage examples</li>
<li><strong><a href="./api/index.html">API Documentation</a></strong>: Rust API documentation</li>
</ul>
<h2 id="troubleshooting-checklist"><a class="header" href="#troubleshooting-checklist">Troubleshooting Checklist</a></h2>
<p>When debugging issues, work through this checklist:</p>
<ul>
<li><input disabled="" type="checkbox"/>
Run with <code>-vv</code> to see detailed output</li>
<li><input disabled="" type="checkbox"/>
Check <code>--cache-stats</code> for cache issues</li>
<li><input disabled="" type="checkbox"/>
Try <code>--clear-cache</code> to rule out cache corruption</li>
<li><input disabled="" type="checkbox"/>
Try <code>--semantic-off</code> to use fallback mode</li>
<li><input disabled="" type="checkbox"/>
Check file permissions and paths</li>
<li><input disabled="" type="checkbox"/>
Verify configuration in <code>.debtmap/config.toml</code></li>
<li><input disabled="" type="checkbox"/>
Test with <code>--max-files 10</code> to isolate issues</li>
<li><input disabled="" type="checkbox"/>
Try <code>--no-parallel</code> to rule out concurrency</li>
<li><input disabled="" type="checkbox"/>
Check <code>debtmap --version</code> for updates</li>
<li><input disabled="" type="checkbox"/>
Review error messages in this guide</li>
<li><input disabled="" type="checkbox"/>
Search GitHub issues for similar problems</li>
<li><input disabled="" type="checkbox"/>
Create minimal reproduction case</li>
<li><input disabled="" type="checkbox"/>
File bug report with debug output</li>
</ul>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->

        <script>
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>


    </div>
    </body>
</html>
