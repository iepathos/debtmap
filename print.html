<!DOCTYPE HTML>
<html lang="en" class="rust sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Debtmap Documentation</title>
        <meta name="robots" content="noindex">


        <!-- Custom HTML head -->

        <meta name="description" content="Comprehensive guide to Debtmap code complexity and technical debt analyzer">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon-de23e50b.svg">
        <link rel="shortcut icon" href="favicon-8114d1fc.png">
        <link rel="stylesheet" href="css/variables-8adf115d.css">
        <link rel="stylesheet" href="css/general-2459343d.css">
        <link rel="stylesheet" href="css/chrome-ae938929.css">
        <link rel="stylesheet" href="css/print-9e4910d8.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="fonts/fonts-9644e21d.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="mdbook-highlight-css" href="highlight-493f70e1.css">
        <link rel="stylesheet" id="mdbook-tomorrow-night-css" href="tomorrow-night-4c0ae647.css">
        <link rel="stylesheet" id="mdbook-ayu-highlight-css" href="ayu-highlight-3fdfc3ac.css">

        <!-- Custom theme stylesheets -->


        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "";
            const default_light_theme = "rust";
            const default_dark_theme = "navy";
            window.path_to_searchindex_js = "searchindex-08efffac.js";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="toc-d4b9eccb.js"></script>
    </head>
    <body>
    <div id="mdbook-help-container">
        <div id="mdbook-help-popup">
            <h2 class="mdbook-help-title">Keyboard shortcuts</h2>
            <div>
                <p>Press <kbd>←</kbd> or <kbd>→</kbd> to navigate between chapters</p>
                <p>Press <kbd>S</kbd> or <kbd>/</kbd> to search in the book</p>
                <p>Press <kbd>?</kbd> to show this help</p>
                <p>Press <kbd>Esc</kbd> to hide this help</p>
            </div>
        </div>
    </div>
    <div id="mdbook-body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('rust')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="mdbook-sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            let sidebar = null;
            const sidebar_toggle = document.getElementById("mdbook-sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
                sidebar_toggle.checked = false;
            }
            if (sidebar === 'visible') {
                sidebar_toggle.checked = true;
            } else {
                html.classList.remove('sidebar-visible');
            }
        </script>

        <nav id="mdbook-sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="toc.html"></iframe>
            </noscript>
            <div id="mdbook-sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="mdbook-page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="mdbook-menu-bar-hover-placeholder"></div>
                <div id="mdbook-menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="mdbook-sidebar-toggle" class="icon-button" for="mdbook-sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="mdbook-sidebar">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"/></svg></span>
                        </label>
                        <button id="mdbook-theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="mdbook-theme-list">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M371.3 367.1c27.3-3.9 51.9-19.4 67.2-42.9L600.2 74.1c12.6-19.5 9.4-45.3-7.6-61.2S549.7-4.4 531.1 9.6L294.4 187.2c-24 18-38.2 46.1-38.4 76.1L371.3 367.1zm-19.6 25.4l-116-104.4C175.9 290.3 128 339.6 128 400c0 3.9 .2 7.8 .6 11.6c1.8 17.5-10.2 36.4-27.8 36.4H96c-17.7 0-32 14.3-32 32s14.3 32 32 32H240c61.9 0 112-50.1 112-112c0-2.5-.1-5-.2-7.5z"/></svg></span>
                        </button>
                        <ul id="mdbook-theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-default_theme">Auto</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-ayu">Ayu</button></li>
                        </ul>
                        <button id="mdbook-search-toggle" class="icon-button" type="button" title="Search (`/`)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="/ s" aria-controls="mdbook-searchbar">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M416 208c0 45.9-14.9 88.3-40 122.7L502.6 457.4c12.5 12.5 12.5 32.8 0 45.3s-32.8 12.5-45.3 0L330.7 376c-34.4 25.2-76.8 40-122.7 40C93.1 416 0 322.9 0 208S93.1 0 208 0S416 93.1 416 208zM208 352c79.5 0 144-64.5 144-144s-64.5-144-144-144S64 128.5 64 208s64.5 144 144 144z"/></svg></span>
                        </button>
                    </div>

                    <h1 class="menu-title">Debtmap Documentation</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <span class=fa-svg id="print-button"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M128 0C92.7 0 64 28.7 64 64v96h64V64H354.7L384 93.3V160h64V93.3c0-17-6.7-33.3-18.7-45.3L400 18.7C388 6.7 371.7 0 354.7 0H128zM384 352v32 64H128V384 368 352H384zm64 32h32c17.7 0 32-14.3 32-32V256c0-35.3-28.7-64-64-64H64c-35.3 0-64 28.7-64 64v96c0 17.7 14.3 32 32 32H64v64c0 35.3 28.7 64 64 64H384c35.3 0 64-28.7 64-64V384zm-16-88c-13.3 0-24-10.7-24-24s10.7-24 24-24s24 10.7 24 24s-10.7 24-24 24z"/></svg></span>
                        </a>
                        <a href="https://github.com/iepathos/debtmap" title="Git repository" aria-label="Git repository">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span>
                        </a>

                    </div>
                </div>

                <div id="mdbook-search-wrapper" class="hidden">
                    <form id="mdbook-searchbar-outer" class="searchbar-outer">
                        <div class="search-wrapper">
                            <input type="search" id="mdbook-searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="mdbook-searchresults-outer" aria-describedby="searchresults-header">
                            <div class="spinner-wrapper">
                                <span class=fa-svg id="fa-spin"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M304 48c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zm0 416c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zM48 304c26.5 0 48-21.5 48-48s-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48zm464-48c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zM142.9 437c18.7-18.7 18.7-49.1 0-67.9s-49.1-18.7-67.9 0s-18.7 49.1 0 67.9s49.1 18.7 67.9 0zm0-294.2c18.7-18.7 18.7-49.1 0-67.9S93.7 56.2 75 75s-18.7 49.1 0 67.9s49.1 18.7 67.9 0zM369.1 437c18.7 18.7 49.1 18.7 67.9 0s18.7-49.1 0-67.9s-49.1-18.7-67.9 0s-18.7 49.1 0 67.9z"/></svg></span>
                            </div>
                        </div>
                    </form>
                    <div id="mdbook-searchresults-outer" class="searchresults-outer hidden">
                        <div id="mdbook-searchresults-header" class="searchresults-header"></div>
                        <ul id="mdbook-searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('mdbook-sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('mdbook-sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#mdbook-sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="mdbook-content" class="content">
                    <main>
                        <h1 id="introduction"><a class="header" href="#introduction">Introduction</a></h1>
<p>Debtmap is a <strong>code complexity sensor</strong> for AI-assisted development. It identifies technical debt hotspots and provides the structured data AI coding tools need to understand and fix them.</p>
<h2 id="what-is-debtmap"><a class="header" href="#what-is-debtmap">What is Debtmap?</a></h2>
<p>Debtmap is different from traditional static analysis tools. Instead of telling you what to fix, it provides <strong>signals</strong> that AI assistants can use to make informed decisions:</p>
<ol>
<li><strong>Where to look</strong> - Prioritized list of debt items with exact file locations</li>
<li><strong>What to read</strong> - Context suggestions (callers, callees, test files)</li>
<li><strong>What signals matter</strong> - Complexity, coverage, coupling metrics</li>
</ol>
<p>The key insight: AI coding assistants are great at fixing code, but they need guidance on <em>where</em> to focus and <em>what</em> to read. Debtmap provides that guidance.</p>
<h2 id="the-ai-sensor-model"><a class="header" href="#the-ai-sensor-model">The AI Sensor Model</a></h2>
<p>Debtmap is a <strong>sensor</strong>, not a prescriber. It measures and reports; it doesn’t tell you what to do.</p>
<p><strong>What Debtmap provides:</strong></p>
<ul>
<li>Quantified complexity signals (cyclomatic, cognitive, nesting)</li>
<li>Test coverage gaps with risk prioritization</li>
<li>Context suggestions for AI consumption</li>
<li>Structured output (JSON, LLM-markdown) for machine consumption</li>
</ul>
<p><strong>What Debtmap doesn’t provide:</strong></p>
<ul>
<li>“Fix this by doing X” recommendations</li>
<li>“You should consider Y” advice</li>
<li>Template-based refactoring suggestions</li>
</ul>
<p>This design is intentional. AI assistants can consider business context, team preferences, and constraints that Debtmap can’t know. The AI decides what to do; Debtmap tells it where to look.</p>
<h2 id="quick-start"><a class="header" href="#quick-start">Quick Start</a></h2>
<pre><code class="language-bash"># Install
cargo install debtmap

# Analyze and pipe to Claude Code
debtmap analyze . --format llm-markdown --top 3 | claude "Fix the top item"

# Get structured signals for your AI workflow
debtmap analyze . --format json --top 10 &gt; debt.json

# With coverage data for accurate risk assessment
cargo llvm-cov --lcov --output-path coverage.lcov
debtmap analyze . --lcov coverage.lcov --format llm-markdown
</code></pre>
<h2 id="key-features"><a class="header" href="#key-features">Key Features</a></h2>
<h3 id="signal-generation"><a class="header" href="#signal-generation">Signal Generation</a></h3>
<ul>
<li><strong>Complexity signals</strong> - Cyclomatic, cognitive, nesting depth, lines of code</li>
<li><strong>Coverage signals</strong> - Line coverage, branch coverage, function coverage</li>
<li><strong>Coupling signals</strong> - Fan-in, fan-out, call graph depth</li>
<li><strong>Quality signals</strong> - Entropy (false positive reduction), purity (testability)</li>
</ul>
<h3 id="ai-optimized-output"><a class="header" href="#ai-optimized-output">AI-Optimized Output</a></h3>
<ul>
<li><strong>LLM markdown format</strong> - Minimal tokens, maximum information</li>
<li><strong>Context suggestions</strong> - File ranges the AI should read</li>
<li><strong>Structured JSON</strong> - Stable schema for programmatic access</li>
<li><strong>Deterministic output</strong> - Same input = same output</li>
</ul>
<h3 id="analysis-capabilities"><a class="header" href="#analysis-capabilities">Analysis Capabilities</a></h3>
<ul>
<li><strong>Rust-first analysis</strong> - Full AST parsing, macro expansion, trait resolution</li>
<li><strong>Coverage integration</strong> - Native LCOV support for risk assessment</li>
<li><strong>Entropy analysis</strong> - Reduces false positives from repetitive code</li>
<li><strong>Parallel processing</strong> - Fast analysis (10-100x faster than Java/Python tools)</li>
</ul>
<h3 id="workflow-integration"><a class="header" href="#workflow-integration">Workflow Integration</a></h3>
<ul>
<li><strong>Direct piping</strong> - Pipe output to Claude, Cursor, or custom agents</li>
<li><strong>CI/CD gates</strong> - Validate debt thresholds in pipelines</li>
<li><strong>Progress tracking</strong> - Compare debt across commits</li>
</ul>
<h2 id="current-status"><a class="header" href="#current-status">Current Status</a></h2>
<p>Debtmap focuses exclusively on Rust. This focused approach allows us to:</p>
<ul>
<li>Build deep Rust-specific analysis (macros, traits, lifetimes)</li>
<li>Perfect core algorithms before expanding</li>
<li>Deliver the best possible AI sensor for Rust</li>
</ul>
<p>Multi-language support (Python, JavaScript/TypeScript, Go) is planned for future releases.</p>
<h2 id="target-audience"><a class="header" href="#target-audience">Target Audience</a></h2>
<p>Debtmap is designed for:</p>
<ul>
<li><strong>AI-assisted developers</strong> - Get signals that help AI assistants make better decisions</li>
<li><strong>Development teams</strong> - Prioritize debt remediation with quantified metrics</li>
<li><strong>CI/CD engineers</strong> - Enforce quality gates with coverage-aware thresholds</li>
<li><strong>Legacy codebase maintainers</strong> - Identify where AI can help most</li>
</ul>
<h2 id="getting-started"><a class="header" href="#getting-started">Getting Started</a></h2>
<p>Ready to start? Check out:</p>
<ul>
<li><a href="#getting-started-1">Getting Started</a> - Installation and first analysis</li>
<li><a href="#llm-integration-guide">LLM Integration</a> - AI workflow patterns</li>
<li><a href="#why-debtmap">Why Debtmap?</a> - The AI sensor model explained</li>
</ul>
<p><strong>Quick tip:</strong> Start with <code>debtmap analyze . --format llm-markdown --top 5</code> to see the top priority items with context suggestions.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="why-debtmap"><a class="header" href="#why-debtmap">Why Debtmap?</a></h1>
<p>Debtmap is a <strong>code complexity sensor</strong> designed for AI-assisted development workflows. It identifies technical debt hotspots and provides the structured data AI coding tools need to understand and fix them.</p>
<h2 id="the-ai-development-paradox"><a class="header" href="#the-ai-development-paradox">The AI Development Paradox</a></h2>
<p>AI coding assistants like Claude Code, GitHub Copilot, and Cursor are transforming software development. They can write code faster than ever before. But this creates a paradox:</p>
<p><strong>AI creates technical debt faster than humans can manage it.</strong></p>
<p>When an AI generates hundreds of lines of code per hour, traditional code review and refactoring processes break down. Teams accumulate debt faster than they can pay it down.</p>
<p>At the same time, AI assistants struggle to fix the debt they create:</p>
<ul>
<li><strong>Limited context window</strong> - They can’t see the entire codebase at once</li>
<li><strong>No test awareness</strong> - They don’t know which code is tested vs untested</li>
<li><strong>No prioritization</strong> - They can’t identify what matters most</li>
<li><strong>Wasted tokens</strong> - They read irrelevant code while missing critical context</li>
</ul>
<h2 id="what-ai-coding-tools-need"><a class="header" href="#what-ai-coding-tools-need">What AI Coding Tools Need</a></h2>
<p>For an AI assistant to effectively fix technical debt, it needs:</p>
<h3 id="1-prioritized-targets"><a class="header" href="#1-prioritized-targets">1. Prioritized Targets</a></h3>
<p>Not “here are 500 complex functions,” but “here are the 10 functions that matter most, ranked by severity.”</p>
<p>Debtmap provides a severity score (0-10) that combines:</p>
<ul>
<li>Complexity metrics (cyclomatic, cognitive, nesting)</li>
<li>Test coverage gaps</li>
<li>Coupling and dependency impact</li>
<li>Pattern-based false positive reduction</li>
</ul>
<h3 id="2-context-suggestions"><a class="header" href="#2-context-suggestions">2. Context Suggestions</a></h3>
<p>Not “this function is complex,” but “read lines 38-85 of parser.rs, plus lines 100-120 of handler.rs where it’s called, and lines 50-75 of the test file.”</p>
<p>Debtmap’s context suggestions tell the AI exactly which code to read:</p>
<pre><code>CONTEXT:
├─ Primary: src/parser.rs:38-85 (the debt item)
├─ Caller: src/handler.rs:100-120 (usage context)
└─ Tests: tests/parser_test.rs:50-75 (expected behavior)
</code></pre>
<h3 id="3-quantified-signals"><a class="header" href="#3-quantified-signals">3. Quantified Signals</a></h3>
<p>Not “this code is bad,” but “cyclomatic complexity: 12, cognitive complexity: 18, test coverage: 0%, called by 8 functions.”</p>
<p>These signals let the AI make informed decisions about the best approach:</p>
<ul>
<li>High complexity + good coverage = risky to refactor</li>
<li>Low complexity + no coverage = easy test target</li>
<li>High coupling + high complexity = incremental approach needed</li>
</ul>
<h3 id="4-structured-output"><a class="header" href="#4-structured-output">4. Structured Output</a></h3>
<p>Not free-form text, but JSON and markdown optimized for LLM consumption:</p>
<ul>
<li>Consistent structure across all debt items</li>
<li>Minimal tokens for maximum information</li>
<li>Deterministic output for reproducible workflows</li>
<li>Stable IDs for referencing items across runs</li>
</ul>
<h2 id="what-debtmap-provides"><a class="header" href="#what-debtmap-provides">What Debtmap Provides</a></h2>
<h3 id="complexity-signals"><a class="header" href="#complexity-signals">Complexity Signals</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Signal</th><th>What It Measures</th><th>Why It Matters</th></tr>
</thead>
<tbody>
<tr><td>Cyclomatic</td><td>Decision points (if, match, loop)</td><td>Number of execution paths</td></tr>
<tr><td>Cognitive</td><td>Readability difficulty</td><td>How hard code is to understand</td></tr>
<tr><td>Nesting</td><td>Indentation depth</td><td>Compound complexity</td></tr>
<tr><td>Lines</td><td>Function length</td><td>Scope of changes needed</td></tr>
</tbody>
</table>
</div>
<h3 id="coverage-signals"><a class="header" href="#coverage-signals">Coverage Signals</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Signal</th><th>What It Measures</th><th>Why It Matters</th></tr>
</thead>
<tbody>
<tr><td>Line coverage</td><td>% of lines executed by tests</td><td>Basic test coverage</td></tr>
<tr><td>Branch coverage</td><td>% of branches taken</td><td>Edge case coverage</td></tr>
<tr><td>Function coverage</td><td>Whether function is tested at all</td><td>Critical gap detection</td></tr>
</tbody>
</table>
</div>
<h3 id="coupling-signals"><a class="header" href="#coupling-signals">Coupling Signals</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Signal</th><th>What It Measures</th><th>Why It Matters</th></tr>
</thead>
<tbody>
<tr><td>Fan-in</td><td>Functions that call this function</td><td>Impact of changes</td></tr>
<tr><td>Fan-out</td><td>Functions this function calls</td><td>Dependency risk</td></tr>
<tr><td>Call depth</td><td>Distance from entry points</td><td>Integration complexity</td></tr>
</tbody>
</table>
</div>
<h3 id="quality-signals"><a class="header" href="#quality-signals">Quality Signals</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Signal</th><th>What It Measures</th><th>Why It Matters</th></tr>
</thead>
<tbody>
<tr><td>Entropy</td><td>Pattern variety in code</td><td>False positive filtering</td></tr>
<tr><td>Purity</td><td>Side effect presence</td><td>Testability indicator</td></tr>
<tr><td>Dead code</td><td>Unused functions</td><td>Cleanup candidates</td></tr>
</tbody>
</table>
</div>
<h2 id="what-debtmap-doesnt-do"><a class="header" href="#what-debtmap-doesnt-do">What Debtmap Doesn’t Do</a></h2>
<p>Debtmap is a <strong>sensor</strong>, not a prescriber. It measures and reports; it doesn’t tell you what to do.</p>
<h3 id="no-fix-suggestions"><a class="header" href="#no-fix-suggestions">No Fix Suggestions</a></h3>
<p>Debtmap doesn’t say “split this function into 5 modules” or “add 8 unit tests.” Those decisions require understanding the business context, architectural constraints, and team preferences that only humans (or AI with proper context) can evaluate.</p>
<h3 id="no-should-statements"><a class="header" href="#no-should-statements">No “Should” Statements</a></h3>
<p>Debtmap doesn’t say “you should refactor this” or “consider extracting a helper function.” It reports facts: “complexity: 18, coverage: 0%, called by 12 functions.” The AI or developer decides what to do with that information.</p>
<h3 id="no-impact-predictions"><a class="header" href="#no-impact-predictions">No Impact Predictions</a></h3>
<p>Debtmap doesn’t claim “refactoring this will reduce bugs by 40%.” Such predictions are speculative. Debtmap reports what it can measure accurately and leaves interpretation to the consumer.</p>
<h2 id="comparison-with-alternatives"><a class="header" href="#comparison-with-alternatives">Comparison with Alternatives</a></h2>
<h3 id="vs-static-analysis-tools-sonarqube-codeclimate"><a class="header" href="#vs-static-analysis-tools-sonarqube-codeclimate">vs Static Analysis Tools (SonarQube, CodeClimate)</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Aspect</th><th>Traditional Tools</th><th>Debtmap</th></tr>
</thead>
<tbody>
<tr><td>Output</td><td>Recommendations</td><td>Signals</td></tr>
<tr><td>Audience</td><td>Humans</td><td>AI + Humans</td></tr>
<tr><td>Format</td><td>Dashboards</td><td>JSON/Markdown</td></tr>
<tr><td>Speed</td><td>Minutes</td><td>Seconds</td></tr>
<tr><td>Focus</td><td>“Fix this”</td><td>“Here’s what exists”</td></tr>
</tbody>
</table>
</div>
<p>Traditional tools are designed for human code review workflows. Debtmap is designed for AI-assisted development.</p>
<h3 id="vs-linters-clippy-eslint"><a class="header" href="#vs-linters-clippy-eslint">vs Linters (Clippy, ESLint)</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Aspect</th><th>Linters</th><th>Debtmap</th></tr>
</thead>
<tbody>
<tr><td>Focus</td><td>Style/idioms</td><td>Complexity/debt</td></tr>
<tr><td>Scope</td><td>Line-level</td><td>Function-level</td></tr>
<tr><td>Output</td><td>Warnings</td><td>Prioritized signals</td></tr>
<tr><td>Coverage</td><td>Not integrated</td><td>Core feature</td></tr>
</tbody>
</table>
</div>
<p>Linters catch code style issues. Debtmap identifies complexity hotspots. Use both.</p>
<h3 id="vs-coverage-tools-tarpaulin-pytest-cov"><a class="header" href="#vs-coverage-tools-tarpaulin-pytest-cov">vs Coverage Tools (Tarpaulin, pytest-cov)</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Aspect</th><th>Coverage Tools</th><th>Debtmap</th></tr>
</thead>
<tbody>
<tr><td>Output</td><td>Coverage %</td><td>Risk-prioritized gaps</td></tr>
<tr><td>Complexity</td><td>Not considered</td><td>Core metric</td></tr>
<tr><td>Context</td><td>None</td><td>File ranges for AI</td></tr>
</tbody>
</table>
</div>
<p>Coverage tools tell you what’s tested. Debtmap tells you what untested code is most risky.</p>
<h2 id="how-debtmap-fits-in-your-workflow"><a class="header" href="#how-debtmap-fits-in-your-workflow">How Debtmap Fits in Your Workflow</a></h2>
<h3 id="ai-assisted-development"><a class="header" href="#ai-assisted-development">AI-Assisted Development</a></h3>
<pre><code class="language-bash"># Generate debt signals
debtmap analyze . --format llm-markdown --lcov coverage.lcov

# Pipe to AI
cat debt.md | claude "Fix the top item, read the suggested context first"
</code></pre>
<h3 id="cicd-integration"><a class="header" href="#cicd-integration">CI/CD Integration</a></h3>
<pre><code class="language-bash"># Fail build if debt exceeds thresholds
debtmap validate . --max-debt-density 10.0

# Generate report for PR review
debtmap analyze . --format json --output debt-report.json
</code></pre>
<h3 id="exploratory-analysis"><a class="header" href="#exploratory-analysis">Exploratory Analysis</a></h3>
<pre><code class="language-bash"># Quick overview
debtmap analyze . --top 10

# Deep dive with coverage
debtmap analyze . --lcov coverage.lcov --format terminal -vv
</code></pre>
<h2 id="key-insights"><a class="header" href="#key-insights">Key Insights</a></h2>
<ol>
<li><strong>Debtmap is a sensor</strong> - It measures, it doesn’t prescribe</li>
<li><strong>AI does the thinking</strong> - Debtmap provides data, AI decides action</li>
<li><strong>Context is key</strong> - Knowing what to read is as valuable as what to fix</li>
<li><strong>Signals over interpretations</strong> - Raw metrics, not template advice</li>
<li><strong>Speed matters</strong> - Fast enough for local development loops</li>
</ol>
<h2 id="next-steps"><a class="header" href="#next-steps">Next Steps</a></h2>
<p>Ready to try it? Head to <a href="#getting-started-1">Getting Started</a> to install debtmap and run your first analysis.</p>
<p>Want to integrate with your AI workflow? See <a href="#llm-integration-guide">LLM Integration</a> for detailed guidance.</p>
<p>Want to understand how it works under the hood? See <a href="#architecture">Architecture</a> for the analysis pipeline.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="getting-started-1"><a class="header" href="#getting-started-1">Getting Started</a></h1>
<p>This guide will help you install Debtmap and run your first analysis in just a few minutes.</p>
<h2 id="prerequisites"><a class="header" href="#prerequisites">Prerequisites</a></h2>
<p>Before installing Debtmap, you’ll need:</p>
<ul>
<li><strong>For pre-built binaries</strong>: No prerequisites! The install script handles everything.</li>
<li><strong>For cargo install or building from source</strong>:
<ul>
<li>Rust toolchain (rustc and cargo)</li>
<li>Supported platforms: Linux, macOS, Windows</li>
<li>Rust edition 2021 or later</li>
</ul>
</li>
</ul>
<p><strong>Optional</strong> (for coverage-based risk analysis):</p>
<ul>
<li><strong>Rust projects</strong>: <code>cargo-tarpaulin</code> or <code>cargo-llvm-cov</code> for coverage data</li>
</ul>
<h2 id="installation"><a class="header" href="#installation">Installation</a></h2>
<h3 id="quick-install-recommended"><a class="header" href="#quick-install-recommended">Quick Install (Recommended)</a></h3>
<p>Install the latest release with a single command:</p>
<pre><code class="language-bash">curl -sSL https://raw.githubusercontent.com/iepathos/debtmap/master/install.sh | bash
</code></pre>
<p>Or with wget:</p>
<pre><code class="language-bash">wget -qO- https://raw.githubusercontent.com/iepathos/debtmap/master/install.sh | bash
</code></pre>
<p>This will:</p>
<ul>
<li>Automatically detect your OS and architecture</li>
<li>Download the appropriate pre-built binary from the latest GitHub release</li>
<li>Install debtmap to <code>~/.cargo/bin</code> if it exists, otherwise <code>~/.local/bin</code></li>
<li>Offer to automatically add the install directory to your PATH if needed</li>
</ul>
<h3 id="using-cargo"><a class="header" href="#using-cargo">Using Cargo</a></h3>
<p>If you have Rust installed:</p>
<pre><code class="language-bash">cargo install debtmap
</code></pre>
<h3 id="from-source"><a class="header" href="#from-source">From Source</a></h3>
<p>For the latest development version:</p>
<pre><code class="language-bash"># Clone the repository
git clone https://github.com/iepathos/debtmap.git
cd debtmap

# Build and install
cargo install --path .
</code></pre>
<h3 id="verify-installation"><a class="header" href="#verify-installation">Verify Installation</a></h3>
<p>After installation, verify Debtmap is working:</p>
<pre><code class="language-bash"># Check version
debtmap --version

# See available commands
debtmap --help
</code></pre>
<h2 id="quick-start-1"><a class="header" href="#quick-start-1">Quick Start</a></h2>
<p>Here are the most common commands to get you started:</p>
<pre><code class="language-bash"># Basic analysis (simplest command)
debtmap analyze .

# LLM-optimized output for AI workflows (recommended)
debtmap analyze . --format llm-markdown

# Pipe directly to Claude Code
debtmap analyze . --format llm-markdown --top 3 | claude "Fix the top item"

# JSON output for programmatic access
debtmap analyze . --format json --top 10 &gt; debt.json

# With coverage data for accurate risk assessment
cargo llvm-cov --lcov --output-path coverage.lcov
debtmap analyze . --lcov coverage.lcov

# Show only critical/high priority items
debtmap analyze . --min-priority high --top 10

# Terminal output for human exploration
debtmap analyze . --format terminal
</code></pre>
<h2 id="first-analysis"><a class="header" href="#first-analysis">First Analysis</a></h2>
<p>Let’s run your first analysis! Navigate to a project directory and run:</p>
<pre><code class="language-bash">debtmap analyze .
</code></pre>
<p><strong>What happens during analysis:</strong></p>
<ol>
<li><strong>File Discovery</strong> - Debtmap scans your project for Rust source files (<code>.rs</code>)</li>
<li><strong>Parsing</strong> - Each file is parsed into an Abstract Syntax Tree (AST)</li>
<li><strong>Metric Extraction</strong> - Complexity, coverage gaps, and coupling are measured</li>
<li><strong>Prioritization</strong> - Results are ranked by severity (CRITICAL, HIGH, MEDIUM, LOW)</li>
<li><strong>Context Generation</strong> - File ranges are suggested for each debt item</li>
<li><strong>Output</strong> - Results are displayed in your chosen format</li>
</ol>
<p><strong>Expected timing</strong>: Analyzing a 10,000 LOC project typically takes 2-5 seconds.</p>
<h2 id="example-output"><a class="header" href="#example-output">Example Output</a></h2>
<p>When you run <code>debtmap analyze . --format llm-markdown</code>, you’ll see output like this:</p>
<pre><code class="language-markdown"># Technical Debt Analysis

## Summary
- Total items: 47
- Critical: 3, High: 12, Moderate: 20, Low: 12

## #1 [CRITICAL] parse_complex_input
**Location:** src/parser.rs:38-85
**Score:** 8.9/10

**Signals:**
| Metric | Value |
|--------|-------|
| Cyclomatic | 12 |
| Cognitive | 18 |
| Coverage | 0% |
| Nesting | 4 |

**Context:**
- Primary: src/parser.rs:38-85
- Caller: src/handler.rs:100-120
- Test: tests/parser_test.rs:50-75
</code></pre>
<h2 id="understanding-the-output"><a class="header" href="#understanding-the-output">Understanding the Output</a></h2>
<h3 id="priority-tiers"><a class="header" href="#priority-tiers">Priority Tiers</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Tier</th><th>Score</th><th>Meaning</th></tr>
</thead>
<tbody>
<tr><td>CRITICAL</td><td>8.0-10.0</td><td>High complexity with no test coverage</td></tr>
<tr><td>HIGH</td><td>5.0-7.9</td><td>Moderate complexity with coverage gaps</td></tr>
<tr><td>MODERATE</td><td>2.0-4.9</td><td>Lower risk, monitor</td></tr>
<tr><td>LOW</td><td>0.0-1.9</td><td>Acceptable state</td></tr>
</tbody>
</table>
</div>
<h3 id="key-signals"><a class="header" href="#key-signals">Key Signals</a></h3>
<p><strong>Complexity signals:</strong></p>
<ul>
<li><strong>Cyclomatic</strong>: Decision points (if, match, loop)</li>
<li><strong>Cognitive</strong>: How hard code is to understand</li>
<li><strong>Nesting</strong>: Indentation depth</li>
<li><strong>Lines</strong>: Function length</li>
</ul>
<p><strong>Coverage signals:</strong></p>
<ul>
<li><strong>Line coverage</strong>: % of lines executed by tests</li>
<li><strong>Branch coverage</strong>: % of branches taken</li>
</ul>
<p><strong>Coupling signals:</strong></p>
<ul>
<li><strong>Fan-in</strong>: Functions that call this function</li>
<li><strong>Fan-out</strong>: Functions this function calls</li>
</ul>
<h3 id="context-suggestions"><a class="header" href="#context-suggestions">Context Suggestions</a></h3>
<p>Each debt item includes file ranges the AI should read:</p>
<pre><code>Context:
├─ Primary: src/parser.rs:38-85 (the debt item)
├─ Caller: src/handler.rs:100-120 (usage context)
└─ Test: tests/parser_test.rs:50-75 (expected behavior)
</code></pre>
<p>These suggestions help AI assistants understand the code before making changes.</p>
<h2 id="output-formats"><a class="header" href="#output-formats">Output Formats</a></h2>
<h3 id="llm-markdown-for-ai-workflows"><a class="header" href="#llm-markdown-for-ai-workflows">LLM Markdown (for AI workflows)</a></h3>
<pre><code class="language-bash">debtmap analyze . --format llm-markdown
</code></pre>
<p>Optimized for minimal token usage while providing all necessary context.</p>
<h3 id="json-for-programmatic-access"><a class="header" href="#json-for-programmatic-access">JSON (for programmatic access)</a></h3>
<pre><code class="language-bash">debtmap analyze . --format json --output debt.json
</code></pre>
<p>Structured data for CI/CD integration and custom tooling.</p>
<h3 id="terminal-for-human-exploration"><a class="header" href="#terminal-for-human-exploration">Terminal (for human exploration)</a></h3>
<pre><code class="language-bash">debtmap analyze . --format terminal
</code></pre>
<p>Color-coded, interactive output for manual review.</p>
<h2 id="adding-coverage-data"><a class="header" href="#adding-coverage-data">Adding Coverage Data</a></h2>
<p>Coverage data enables accurate risk assessment:</p>
<pre><code class="language-bash"># Generate coverage with cargo-llvm-cov
cargo llvm-cov --lcov --output-path coverage.lcov

# Or with cargo-tarpaulin
cargo tarpaulin --out lcov --output-dir target/coverage

# Analyze with coverage
debtmap analyze . --lcov coverage.lcov
</code></pre>
<p>With coverage data:</p>
<ul>
<li>Complex code with good tests = lower priority</li>
<li>Simple code with no tests = higher priority</li>
<li>Untested error paths are identified</li>
</ul>
<h2 id="ai-workflow-examples"><a class="header" href="#ai-workflow-examples">AI Workflow Examples</a></h2>
<h3 id="claude-code"><a class="header" href="#claude-code">Claude Code</a></h3>
<pre><code class="language-bash"># Direct piping
debtmap analyze . --format llm-markdown --top 3 | claude "Fix the top item"

# With coverage
cargo llvm-cov --lcov --output-path coverage.lcov
debtmap analyze . --format llm-markdown --lcov coverage.lcov --top 1 | \
  claude "Add tests for this function"
</code></pre>
<h3 id="cursor"><a class="header" href="#cursor">Cursor</a></h3>
<pre><code class="language-bash"># Generate report for Cursor to reference
debtmap analyze . --format llm-markdown --top 10 &gt; debt-report.md

# In Cursor: @debt-report.md Fix the top critical item
</code></pre>
<h3 id="custom-pipelines"><a class="header" href="#custom-pipelines">Custom Pipelines</a></h3>
<pre><code class="language-bash"># Get JSON for programmatic processing
debtmap analyze . --format json --top 5 | \
  jq '.items[0].context.primary' | \
  xargs -I {} echo "Read {}"
</code></pre>
<h2 id="configuration"><a class="header" href="#configuration">Configuration</a></h2>
<p>Create a project-specific configuration:</p>
<pre><code class="language-bash">debtmap init
</code></pre>
<p>This creates <code>.debtmap.toml</code>:</p>
<pre><code class="language-toml">[thresholds]
complexity = 10
duplication = 40

[tiers]
critical = 9.0
high = 7.0
medium = 5.0

[ignore]
patterns = ["**/target/**", "**/tests/**"]
</code></pre>
<h2 id="cli-options-reference"><a class="header" href="#cli-options-reference">CLI Options Reference</a></h2>
<h3 id="analysis-options"><a class="header" href="#analysis-options">Analysis Options</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Option</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>--format &lt;FORMAT&gt;</code></td><td>Output format: terminal, json, markdown, llm-markdown</td></tr>
<tr><td><code>--output &lt;FILE&gt;</code></td><td>Write to file instead of stdout</td></tr>
<tr><td><code>--lcov &lt;FILE&gt;</code></td><td>LCOV coverage file for risk analysis</td></tr>
<tr><td><code>--top &lt;N&gt;</code></td><td>Show only top N priority items</td></tr>
<tr><td><code>--min-priority &lt;TIER&gt;</code></td><td>Filter by minimum priority (low, medium, high, critical)</td></tr>
<tr><td><code>--min-score &lt;N&gt;</code></td><td>Filter items below score N</td></tr>
</tbody>
</table>
</div>
<h3 id="verbosity-options"><a class="header" href="#verbosity-options">Verbosity Options</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Option</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>-v</code></td><td>Show main score factors</td></tr>
<tr><td><code>-vv</code></td><td>Show detailed calculations</td></tr>
<tr><td><code>-vvv</code></td><td>Show all debug information</td></tr>
<tr><td><code>--quiet</code></td><td>Suppress progress output</td></tr>
</tbody>
</table>
</div>
<h3 id="performance-options"><a class="header" href="#performance-options">Performance Options</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Option</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>--jobs &lt;N&gt;</code></td><td>Number of threads (0 = all cores)</td></tr>
<tr><td><code>--no-parallel</code></td><td>Disable parallel processing</td></tr>
<tr><td><code>--max-files &lt;N&gt;</code></td><td>Limit analysis to N files</td></tr>
</tbody>
</table>
</div>
<h2 id="troubleshooting"><a class="header" href="#troubleshooting">Troubleshooting</a></h2>
<h3 id="installation-issues"><a class="header" href="#installation-issues">Installation Issues</a></h3>
<ul>
<li><strong>Binary not in PATH</strong>: Add <code>~/.cargo/bin</code> or <code>~/.local/bin</code> to your PATH
<pre><code class="language-bash">export PATH="$HOME/.cargo/bin:$PATH"  # Add to ~/.bashrc or ~/.zshrc
</code></pre>
</li>
<li><strong>Permission issues</strong>: Run the install script with your current user (don’t use sudo)</li>
<li><strong>Cargo not found</strong>: Install Rust from https://rustup.rs</li>
</ul>
<h3 id="analysis-issues"><a class="header" href="#analysis-issues">Analysis Issues</a></h3>
<ul>
<li><strong>Empty output</strong>: Check that your project contains Rust source files (<code>.rs</code>)</li>
<li><strong>Parser failures</strong>: Run with <code>-vv</code> for debug output</li>
<li><strong>Performance issues</strong>: Limit parallel jobs with <code>--jobs 4</code></li>
</ul>
<h3 id="coverage-issues"><a class="header" href="#coverage-issues">Coverage Issues</a></h3>
<ul>
<li><strong>Coverage not applied</strong>: Verify LCOV file path is correct</li>
<li><strong>Low coverage detected</strong>: Ensure tests actually run during coverage generation</li>
</ul>
<h2 id="whats-next"><a class="header" href="#whats-next">What’s Next?</a></h2>
<p>Now that you’ve run your first analysis:</p>
<ul>
<li><strong>Integrate with AI</strong>: See <a href="#llm-integration-guide">LLM Integration</a> for AI workflow patterns</li>
<li><strong>Understand metrics</strong>: See <a href="#metrics-reference">Metrics Reference</a> for signal definitions</li>
<li><strong>Configure thresholds</strong>: See <a href="#configuration-2">Configuration</a> for customization</li>
<li><strong>CI/CD integration</strong>: See <a href="#prodigy-integration">Prodigy Integration</a> for automation</li>
</ul>
<hr>
<p><strong>Need help?</strong> Report issues at https://github.com/iepathos/debtmap/issues</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="llm-integration-guide"><a class="header" href="#llm-integration-guide">LLM Integration Guide</a></h1>
<p>How to use debtmap output in AI coding workflows.</p>
<h2 id="overview"><a class="header" href="#overview">Overview</a></h2>
<p>Debtmap is designed to provide AI coding assistants with the signals they need to understand and fix technical debt. This guide covers:</p>
<ol>
<li>Output formats optimized for LLMs</li>
<li>Context suggestions and how to use them</li>
<li>Example workflows for different AI tools</li>
<li>Interpreting signals for effective prompts</li>
</ol>
<h2 id="output-formats-1"><a class="header" href="#output-formats-1">Output Formats</a></h2>
<h3 id="llm-markdown-recommended"><a class="header" href="#llm-markdown-recommended">LLM Markdown (Recommended)</a></h3>
<p>The <code>llm-markdown</code> format is specifically designed for LLM consumption:</p>
<pre><code class="language-bash">debtmap analyze . --format llm-markdown --top 5
</code></pre>
<p><strong>Output structure:</strong></p>
<pre><code class="language-markdown"># Technical Debt Analysis

## Summary
- Total items: 47
- Critical: 3
- High: 12
- Moderate: 20
- Low: 12

## Top Priority Items

### #1 [CRITICAL] parse_complex_input
**Location:** src/parser.rs:38-85
**Score:** 8.9/10

**Signals:**
| Metric | Value | Threshold |
|--------|-------|-----------|
| Cyclomatic | 12 | 10 |
| Cognitive | 18 | 15 |
| Coverage | 0% | 80% |
| Nesting | 4 | 3 |

**Context to read:**
- Primary: src/parser.rs:38-85
- Caller: src/handler.rs:100-120
- Caller: src/api.rs:45-60
- Test: tests/parser_test.rs:50-75

---

### #2 [CRITICAL] validate_auth
...
</code></pre>
<p><strong>Why this format works:</strong></p>
<ul>
<li>Consistent structure across all items</li>
<li>Tables for easy metric comparison</li>
<li>Context suggestions inline</li>
<li>Minimal tokens for maximum information</li>
</ul>
<h3 id="json"><a class="header" href="#json">JSON</a></h3>
<p>For programmatic access and CI/CD integration:</p>
<pre><code class="language-bash">debtmap analyze . --format json --output debt.json
</code></pre>
<p><strong>Structure:</strong></p>
<pre><code class="language-json">{
  "version": "1.0",
  "timestamp": "2024-01-15T10:30:00Z",
  "summary": {
    "total_items": 47,
    "by_tier": {
      "critical": 3,
      "high": 12,
      "moderate": 20,
      "low": 12
    },
    "total_loc": 15420
  },
  "items": [
    {
      "rank": 1,
      "id": "parse_complex_input_38",
      "tier": "critical",
      "score": 8.9,
      "location": {
        "file": "src/parser.rs",
        "line_start": 38,
        "line_end": 85,
        "function": "parse_complex_input"
      },
      "metrics": {
        "cyclomatic": 12,
        "cognitive": 18,
        "nesting": 4,
        "loc": 47
      },
      "coverage": {
        "line_percent": 0.0,
        "branch_percent": 0.0
      },
      "context": {
        "primary": "src/parser.rs:38-85",
        "callers": [
          {"file": "src/handler.rs", "lines": "100-120", "calls": 12},
          {"file": "src/api.rs", "lines": "45-60", "calls": 8}
        ],
        "tests": [
          {"file": "tests/parser_test.rs", "lines": "50-75"}
        ]
      }
    }
  ]
}
</code></pre>
<h3 id="terminal"><a class="header" href="#terminal">Terminal</a></h3>
<p>For human exploration (not recommended for AI piping):</p>
<pre><code class="language-bash">debtmap analyze . --format terminal
</code></pre>
<h2 id="context-suggestions-1"><a class="header" href="#context-suggestions-1">Context Suggestions</a></h2>
<p>Each debt item includes a <code>context</code> field that tells the AI exactly what code to read:</p>
<pre><code>CONTEXT:
├─ Primary: src/parser.rs:38-85 (the debt item)
├─ Caller: src/handler.rs:100-120 (understands usage)
├─ Caller: src/api.rs:45-60 (understands usage)
├─ Callee: src/tokenizer.rs:15-40 (understands dependencies)
└─ Test: tests/parser_test.rs:50-75 (understands expected behavior)
</code></pre>
<h3 id="context-types"><a class="header" href="#context-types">Context Types</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Type</th><th>What It Contains</th><th>Why It Matters</th></tr>
</thead>
<tbody>
<tr><td>Primary</td><td>The function with debt</td><td>Core code to understand/fix</td></tr>
<tr><td>Caller</td><td>Functions that call this</td><td>Usage patterns, constraints</td></tr>
<tr><td>Callee</td><td>Functions this calls</td><td>Dependencies, side effects</td></tr>
<tr><td>Test</td><td>Related test files</td><td>Expected behavior, test patterns</td></tr>
<tr><td>Type</td><td>Struct/enum definitions</td><td>Data structures being used</td></tr>
</tbody>
</table>
</div>
<h3 id="using-context-effectively"><a class="header" href="#using-context-effectively">Using Context Effectively</a></h3>
<p><strong>Minimal context (fastest):</strong></p>
<pre><code class="language-bash"># Just get the primary location
debtmap analyze . --format json | jq '.items[0].location'
</code></pre>
<p><strong>Full context (most accurate):</strong></p>
<pre><code class="language-bash"># Read all suggested files
debtmap analyze . --format llm-markdown --top 1
# Then have the AI read each file in the context section
</code></pre>
<h2 id="example-workflows"><a class="header" href="#example-workflows">Example Workflows</a></h2>
<h3 id="claude-code-integration"><a class="header" href="#claude-code-integration">Claude Code Integration</a></h3>
<p><strong>Direct piping:</strong></p>
<pre><code class="language-bash">debtmap analyze . --format llm-markdown --top 3 | claude "Fix the top item. Read the context files first."
</code></pre>
<p><strong>Two-step workflow:</strong></p>
<pre><code class="language-bash"># Step 1: Get the analysis
debtmap analyze . --format llm-markdown --lcov coverage.lcov --top 5 &gt; debt.md

# Step 2: Send to Claude with context
cat debt.md | claude "
Read the context files for item #1 before making changes.
Then fix the debt item following these rules:
1. Add tests first
2. Refactor only after tests pass
3. Keep functions under 20 lines
"
</code></pre>
<p><strong>Focused fix with coverage:</strong></p>
<pre><code class="language-bash"># Generate fresh coverage
cargo llvm-cov --lcov --output-path coverage.lcov

# Analyze with coverage
debtmap analyze . --format llm-markdown --lcov coverage.lcov --top 1

# Send the top item to Claude
debtmap analyze . --format llm-markdown --lcov coverage.lcov --top 1 | \
  claude "Add tests for this function to reach 80% coverage"
</code></pre>
<h3 id="cursor-integration"><a class="header" href="#cursor-integration">Cursor Integration</a></h3>
<p>Cursor works best with file-based context:</p>
<pre><code class="language-bash"># Generate debt report
debtmap analyze . --format llm-markdown --top 10 &gt; .cursor/debt-report.md

# In Cursor, reference the report:
# @debt-report.md Fix the top critical item
</code></pre>
<h3 id="custom-agent-workflow"><a class="header" href="#custom-agent-workflow">Custom Agent Workflow</a></h3>
<p>For building your own AI pipeline:</p>
<pre><code class="language-python">import json
import subprocess

# Run debtmap analysis
result = subprocess.run(
    ["debtmap", "analyze", ".", "--format", "json", "--top", "10"],
    capture_output=True,
    text=True
)
debt_data = json.loads(result.stdout)

# Process each item
for item in debt_data["items"]:
    # Extract context files
    context_files = []
    context_files.append(item["context"]["primary"])
    context_files.extend([c["file"] for c in item["context"].get("callers", [])])

    # Read context files
    context_content = ""
    for file_spec in context_files:
        file_path, lines = parse_file_spec(file_spec)
        context_content += read_file_lines(file_path, lines)

    # Build prompt
    prompt = f"""
    Fix this technical debt item:

    Location: {item["location"]["file"]}:{item["location"]["line_start"]}
    Function: {item["location"]["function"]}
    Score: {item["score"]}/10

    Signals:
    - Cyclomatic complexity: {item["metrics"]["cyclomatic"]}
    - Test coverage: {item["coverage"]["line_percent"]}%

    Context code:
    {context_content}

    Instructions:
    1. Add tests first
    2. Refactor to reduce complexity
    3. Keep the same public API
    """

    # Send to your LLM
    response = call_llm(prompt)
</code></pre>
<h3 id="cicd-integration-1"><a class="header" href="#cicd-integration-1">CI/CD Integration</a></h3>
<p><strong>GitHub Actions example:</strong></p>
<pre><code class="language-yaml">name: Debt Analysis

on: [pull_request]

jobs:
  analyze:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Install debtmap
        run: cargo install debtmap

      - name: Generate coverage
        run: cargo llvm-cov --lcov --output-path coverage.lcov

      - name: Analyze debt
        run: |
          debtmap analyze . --format json --lcov coverage.lcov &gt; debt.json

      - name: Check for critical items
        run: |
          CRITICAL=$(jq '.summary.by_tier.critical' debt.json)
          if [ "$CRITICAL" -gt 0 ]; then
            echo "::warning::Found $CRITICAL critical debt items"
          fi

      - name: Upload report
        uses: actions/upload-artifact@v3
        with:
          name: debt-report
          path: debt.json
</code></pre>
<h2 id="interpreting-signals"><a class="header" href="#interpreting-signals">Interpreting Signals</a></h2>
<h3 id="severity-score-0-10"><a class="header" href="#severity-score-0-10">Severity Score (0-10)</a></h3>
<p>The severity score combines multiple signals:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Score</th><th>Tier</th><th>Interpretation</th></tr>
</thead>
<tbody>
<tr><td>8.0-10.0</td><td>Critical</td><td>High complexity, no tests, high coupling</td></tr>
<tr><td>5.0-7.9</td><td>High</td><td>Moderate risk, coverage gaps</td></tr>
<tr><td>2.0-4.9</td><td>Moderate</td><td>Lower risk, monitor</td></tr>
<tr><td>0.0-1.9</td><td>Low</td><td>Acceptable state</td></tr>
</tbody>
</table>
</div>
<h3 id="complexity-signals-1"><a class="header" href="#complexity-signals-1">Complexity Signals</a></h3>
<p><strong>Cyclomatic complexity:</strong></p>
<ul>
<li>1-5: Simple, easy to test</li>
<li>6-10: Moderate, manageable</li>
<li>11-20: Complex, consider splitting</li>
<li>21+: Very complex, high priority</li>
</ul>
<p><strong>Cognitive complexity:</strong></p>
<ul>
<li>Measures how hard code is to understand</li>
<li>Penalizes nesting more than cyclomatic</li>
<li>Higher values = harder to reason about</li>
</ul>
<p><strong>Nesting depth:</strong></p>
<ul>
<li>1-2: Normal</li>
<li>3: Getting complex</li>
<li>4+: Strongly consider refactoring</li>
</ul>
<h3 id="coverage-signals-1"><a class="header" href="#coverage-signals-1">Coverage Signals</a></h3>
<p><strong>Line coverage:</strong></p>
<ul>
<li>0%: Critical gap, no tests at all</li>
<li>1-50%: Poor coverage</li>
<li>51-80%: Moderate coverage</li>
<li>81%+: Good coverage</li>
</ul>
<p><strong>Branch coverage:</strong></p>
<ul>
<li>More important than line coverage</li>
<li>Missing branches = missing edge cases</li>
<li>0% branch = high risk</li>
</ul>
<h3 id="coupling-signals-1"><a class="header" href="#coupling-signals-1">Coupling Signals</a></h3>
<p><strong>Fan-in (callers):</strong></p>
<ul>
<li>High fan-in = many dependents</li>
<li>Changes affect many places</li>
<li>Higher priority for stability</li>
</ul>
<p><strong>Fan-out (callees):</strong></p>
<ul>
<li>High fan-out = many dependencies</li>
<li>Complex testing requirements</li>
<li>Consider dependency injection</li>
</ul>
<h2 id="best-practices"><a class="header" href="#best-practices">Best Practices</a></h2>
<h3 id="for-effective-ai-prompts"><a class="header" href="#for-effective-ai-prompts">For Effective AI Prompts</a></h3>
<ol>
<li>
<p><strong>Always include context files</strong></p>
<ul>
<li>AI makes better decisions with more context</li>
<li>Context suggestions are curated for relevance</li>
</ul>
</li>
<li>
<p><strong>Specify your constraints</strong></p>
<ul>
<li>“Keep the same public API”</li>
<li>“Add tests before refactoring”</li>
<li>“Functions must be under 20 lines”</li>
</ul>
</li>
<li>
<p><strong>One item at a time</strong></p>
<ul>
<li>Focus on top priority item</li>
<li>Complete fix before moving on</li>
<li>Re-run analysis after changes</li>
</ul>
</li>
<li>
<p><strong>Verify with coverage</strong></p>
<ul>
<li>Regenerate coverage after changes</li>
<li>Run debtmap again to confirm improvement</li>
<li>Track score reduction</li>
</ul>
</li>
</ol>
<h3 id="for-cicd-integration"><a class="header" href="#for-cicd-integration">For CI/CD Integration</a></h3>
<ol>
<li>
<p><strong>Set appropriate thresholds</strong></p>
<ul>
<li>Don’t fail on existing debt</li>
<li>Fail on new critical items</li>
<li>Track trends over time</li>
</ul>
</li>
<li>
<p><strong>Cache analysis results</strong></p>
<ul>
<li>Use git-based caching</li>
<li>Only re-analyze changed files</li>
</ul>
</li>
<li>
<p><strong>Integrate with PR comments</strong></p>
<ul>
<li>Show debt impact of changes</li>
<li>Suggest focus areas for review</li>
</ul>
</li>
</ol>
<h2 id="troubleshooting-1"><a class="header" href="#troubleshooting-1">Troubleshooting</a></h2>
<h3 id="empty-context-suggestions"><a class="header" href="#empty-context-suggestions">Empty context suggestions</a></h3>
<p><strong>Cause:</strong> Call graph analysis couldn’t resolve callers/callees</p>
<p><strong>Solution:</strong> Ensure file parsing succeeded:</p>
<pre><code class="language-bash">debtmap analyze . -vv  # Verbose mode shows parsing issues
</code></pre>
<h3 id="inconsistent-scores-between-runs"><a class="header" href="#inconsistent-scores-between-runs">Inconsistent scores between runs</a></h3>
<p><strong>Cause:</strong> Non-deterministic analysis (should not happen)</p>
<p><strong>Solution:</strong> Report as a bug with reproducible example</p>
<h3 id="large-context-suggestions"><a class="header" href="#large-context-suggestions">Large context suggestions</a></h3>
<p><strong>Cause:</strong> High coupling in codebase</p>
<p><strong>Solution:</strong> Use <code>--max-context-lines</code> to limit:</p>
<pre><code class="language-bash">debtmap analyze . --format llm-markdown --max-context-lines 300
</code></pre>
<h2 id="api-reference"><a class="header" href="#api-reference">API Reference</a></h2>
<h3 id="cli-options-for-llm-integration"><a class="header" href="#cli-options-for-llm-integration">CLI Options for LLM Integration</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Option</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>--format llm-markdown</code></td><td>LLM-optimized markdown output</td></tr>
<tr><td><code>--format json</code></td><td>Structured JSON output</td></tr>
<tr><td><code>--top N</code></td><td>Limit to top N items</td></tr>
<tr><td><code>--lcov FILE</code></td><td>Include coverage data</td></tr>
<tr><td><code>--min-score N</code></td><td>Filter items below score N</td></tr>
<tr><td><code>--output FILE</code></td><td>Write to file instead of stdout</td></tr>
</tbody>
</table>
</div>
<h3 id="json-schema"><a class="header" href="#json-schema">JSON Schema</a></h3>
<p>The JSON output follows a stable schema. See <a href="#output-formats-3">Output Formats</a> for the complete schema definition.</p>
<h2 id="next-steps-1"><a class="header" href="#next-steps-1">Next Steps</a></h2>
<ul>
<li><strong>Configure analysis:</strong> See <a href="#configuration-2">Configuration</a></li>
<li><strong>Understand metrics:</strong> See <a href="#metrics-reference">Metrics Reference</a></li>
<li><strong>View examples:</strong> See <a href="#examples-5">Examples</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="cli-reference"><a class="header" href="#cli-reference">CLI Reference</a></h1>
<p>Complete reference for Debtmap command-line interface.</p>
<h2 id="quick-start-2"><a class="header" href="#quick-start-2">Quick Start</a></h2>
<pre><code class="language-bash"># Basic analysis
debtmap analyze src/

# With coverage integration
debtmap analyze src/ --coverage-file coverage.lcov

# Generate JSON report
debtmap analyze . --format json --output report.json

# Show top 10 priority items only
debtmap analyze . --top 10 --min-priority high

# Initialize configuration and validate
debtmap init
debtmap validate . --config debtmap.toml
</code></pre>
<h2 id="commands"><a class="header" href="#commands">Commands</a></h2>
<p>Debtmap provides six main commands: five for analysis and validation, plus one debugging tool.</p>
<h3 id="analyze"><a class="header" href="#analyze"><code>analyze</code></a></h3>
<p>Analyze code for complexity and technical debt.</p>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash">debtmap analyze &lt;PATH&gt; [OPTIONS]
</code></pre>
<p><strong>Arguments:</strong></p>
<ul>
<li><code>&lt;PATH&gt;</code> - Path to analyze (file or directory)</li>
</ul>
<p><strong>Description:</strong>
Primary command for code analysis. Supports multiple output formats (json, markdown, terminal), coverage file integration, parallel processing, context-aware risk analysis, and comprehensive filtering options.</p>
<p>See <a href="#options">Options</a> section below for all available flags.</p>
<h3 id="init"><a class="header" href="#init"><code>init</code></a></h3>
<p>Initialize a Debtmap configuration file.</p>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash">debtmap init [OPTIONS]
</code></pre>
<p><strong>Options:</strong></p>
<ul>
<li><code>-f, --force</code> - Force overwrite existing config</li>
</ul>
<p><strong>Description:</strong>
Creates a <code>debtmap.toml</code> configuration file in the current directory with default settings. Use <code>--force</code> to overwrite an existing configuration file.</p>
<h3 id="validate"><a class="header" href="#validate"><code>validate</code></a></h3>
<p>Validate code against thresholds defined in configuration file.</p>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash">debtmap validate &lt;PATH&gt; [OPTIONS]
</code></pre>
<p><strong>Arguments:</strong></p>
<ul>
<li><code>&lt;PATH&gt;</code> - Path to analyze</li>
</ul>
<p><strong>Options:</strong></p>
<p><em>Configuration &amp; Output:</em></p>
<ul>
<li><code>-c, --config &lt;CONFIG&gt;</code> - Configuration file path</li>
<li><code>-f, --format &lt;FORMAT&gt;</code> - Output format: json, markdown, terminal</li>
<li><code>-o, --output &lt;OUTPUT&gt;</code> - Output file path (defaults to stdout)</li>
</ul>
<p><em>Coverage &amp; Context:</em></p>
<ul>
<li><code>--coverage-file &lt;PATH&gt;</code> / <code>--lcov &lt;PATH&gt;</code> - LCOV coverage file for risk analysis</li>
<li><code>--context</code> / <code>--enable-context</code> - Enable context-aware risk analysis</li>
<li><code>--context-providers &lt;PROVIDERS&gt;</code> - Context providers to use (comma-separated)</li>
<li><code>--disable-context &lt;PROVIDERS&gt;</code> - Disable specific context providers</li>
</ul>
<p><em>Thresholds &amp; Validation:</em></p>
<ul>
<li><code>--max-debt-density &lt;N&gt;</code> - Maximum debt density allowed (per 1000 LOC)</li>
</ul>
<p><em>Display Filtering:</em></p>
<ul>
<li><code>--top &lt;N&gt;</code> / <code>--head &lt;N&gt;</code> - Show only top N priority items</li>
<li><code>--tail &lt;N&gt;</code> - Show only bottom N priority items</li>
<li><code>-s, --summary</code> - Use summary format with tiered priority display</li>
</ul>
<p><em>Analysis Control:</em></p>
<ul>
<li><code>--semantic-off</code> - Disable semantic analysis (fallback mode)</li>
</ul>
<p><em>Performance Control:</em></p>
<ul>
<li><code>--no-parallel</code> - Disable parallel call graph construction (enabled by default)</li>
<li><code>-j, --jobs &lt;N&gt;</code> - Number of threads for parallel processing (0 = use all cores)
<ul>
<li>Can also use <code>DEBTMAP_JOBS</code> environment variable</li>
</ul>
</li>
</ul>
<p><em>Debugging &amp; Verbosity:</em></p>
<ul>
<li><code>-v, --verbose</code> - Increase verbosity level (can be repeated: -v, -vv, -vvv)</li>
</ul>
<p><strong>Description:</strong>
Similar to <code>analyze</code> but enforces thresholds defined in configuration file. Returns non-zero exit code if thresholds are exceeded, making it suitable for CI/CD integration.</p>
<p>The <code>validate</code> command supports a focused subset of <code>analyze</code> options, primarily for output control, coverage integration, context-aware analysis, and display filtering.</p>
<p><strong>Note:</strong> The following <code>analyze</code> options are NOT available in the <code>validate</code> command:</p>
<ul>
<li><code>--threshold-complexity</code>, <code>--threshold-duplication</code>, <code>--threshold-preset</code> (configure these in <code>.debtmap.toml</code> instead)</li>
<li><code>--languages</code> (language filtering)</li>
</ul>
<p><strong>Note:</strong> The <code>--explain-score</code> flag exists in the <code>validate</code> command but is deprecated (hidden). Use <code>-v</code>, <code>-vv</code>, or <code>-vvv</code> for verbosity instead.</p>
<p>Configure analysis thresholds in your <code>.debtmap.toml</code> configuration file for use with the <code>validate</code> command.</p>
<p><strong>Exit Codes:</strong></p>
<ul>
<li><code>0</code> - Success (no errors, all thresholds passed)</li>
<li>Non-zero - Failure (errors occurred or thresholds exceeded)</li>
</ul>
<h3 id="compare"><a class="header" href="#compare"><code>compare</code></a></h3>
<p>Compare two analysis results and generate a diff report.</p>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash">debtmap compare --before &lt;FILE&gt; --after &lt;FILE&gt; [OPTIONS]
</code></pre>
<p><strong>Required Options:</strong></p>
<ul>
<li><code>--before &lt;FILE&gt;</code> - Path to “before” analysis JSON</li>
<li><code>--after &lt;FILE&gt;</code> - Path to “after” analysis JSON</li>
</ul>
<p><strong>Optional Target Location:</strong></p>
<ul>
<li><code>--plan &lt;FILE&gt;</code> - Path to implementation plan (to extract target location)</li>
<li><code>--target-location &lt;LOCATION&gt;</code> - Target location in format <code>file:function:line</code></li>
</ul>
<p><strong>Note:</strong> <code>--plan</code> and <code>--target-location</code> are mutually exclusive options. Using both together will cause a CLI error:</p>
<pre><code>error: the argument '--plan &lt;FILE&gt;' cannot be used with '--target-location &lt;LOCATION&gt;'
</code></pre>
<p>Use one or the other to specify the target location.</p>
<p><strong>Output Options:</strong></p>
<ul>
<li><code>-f, --format &lt;FORMAT&gt;</code> - Output format: json, markdown, terminal (default: json)</li>
<li><code>-o, --output &lt;OUTPUT&gt;</code> - Output file (defaults to stdout)</li>
</ul>
<p><strong>Description:</strong>
Compares two analysis results and generates a diff showing improvements or regressions in code quality metrics.</p>
<h3 id="validate-improvement"><a class="header" href="#validate-improvement"><code>validate-improvement</code></a></h3>
<p>Validate that technical debt improvements meet quality thresholds.</p>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash">debtmap validate-improvement --comparison &lt;FILE&gt; [OPTIONS]
</code></pre>
<p><strong>Required Options:</strong></p>
<ul>
<li><code>--comparison &lt;FILE&gt;</code> - Path to comparison JSON file (from <code>debtmap compare</code>)</li>
</ul>
<p><strong>Optional Options:</strong></p>
<ul>
<li><code>-o, --output &lt;FILE&gt;</code> - Output file path for validation results (default: <code>.prodigy/debtmap-validation.json</code>)</li>
<li><code>--previous-validation &lt;FILE&gt;</code> - Path to previous validation result for trend tracking</li>
<li><code>--threshold &lt;N&gt;</code> - Improvement threshold percentage (default: 75.0)</li>
<li><code>-f, --format &lt;FORMAT&gt;</code> - Output format: json, markdown, terminal (default: json)</li>
<li><code>--quiet</code> - Suppress console output (useful for automation)</li>
</ul>
<p><strong>Description:</strong>
Validates improvement quality by analyzing comparison output from <code>debtmap compare</code>. Calculates a composite improvement score based on weighted components:</p>
<p><strong>Composite Score Calculation:</strong></p>
<ol>
<li>
<p><strong>Target Item Improvement (50% weight)</strong> - Measures direct improvement to the specific target item being fixed</p>
<ul>
<li>Compares complexity, debt score, and other metrics before/after</li>
<li>Weight: 0.5 × (target improvement percentage)</li>
</ul>
</li>
<li>
<p><strong>Overall Project Health (30% weight)</strong> - Evaluates project-wide quality changes</p>
<ul>
<li>Analyzes aggregate metrics across entire codebase</li>
<li>Considers new issues introduced, total debt changes</li>
<li>Weight: 0.3 × (project health percentage)</li>
</ul>
</li>
<li>
<p><strong>Absence of Regressions (20% weight)</strong> - Penalizes introduction of new technical debt</p>
<ul>
<li>Checks for new high-priority issues in other parts of codebase</li>
<li>Ensures improvements don’t shift complexity elsewhere</li>
<li>Weight: 0.2 × (regression-free percentage)</li>
</ul>
</li>
</ol>
<p><strong>Final Score:</strong> <code>composite_score = (0.5 × target) + (0.3 × health) + (0.2 × no_regressions)</code></p>
<p>The default threshold (75%) requires strong improvement in the target item while maintaining overall project quality.</p>
<p>When <code>--previous-validation</code> is provided, tracks progress trends across multiple attempts and provides recommendations for continuing or adjusting the improvement approach.</p>
<p><strong>Example:</strong></p>
<pre><code class="language-bash"># Basic validation
debtmap validate-improvement --comparison comparison.json

# With trend tracking and custom threshold
debtmap validate-improvement \
  --comparison .prodigy/comparison.json \
  --previous-validation .prodigy/validation.json \
  --output .prodigy/validation.json \
  --threshold 80.0
</code></pre>
<h3 id="explain-coverage-debugging"><a class="header" href="#explain-coverage-debugging"><code>explain-coverage</code> (Debugging)</a></h3>
<p>Explain coverage detection for a specific function.</p>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash">debtmap explain-coverage &lt;PATH&gt; --coverage-file &lt;FILE&gt; --function &lt;NAME&gt; [OPTIONS]
</code></pre>
<p><strong>Arguments:</strong></p>
<ul>
<li><code>&lt;PATH&gt;</code> - Path to the codebase to analyze</li>
</ul>
<p><strong>Required Options:</strong></p>
<ul>
<li><code>--coverage-file &lt;FILE&gt;</code> / <code>--lcov &lt;FILE&gt;</code> - LCOV coverage file</li>
<li><code>--function &lt;NAME&gt;</code> - Function name to explain (e.g., “create_auto_commit”)</li>
</ul>
<p><strong>Optional Options:</strong></p>
<ul>
<li><code>--file &lt;PATH&gt;</code> - File path containing the function (helps narrow search)</li>
<li><code>-v, --verbose</code> - Show all attempted matching strategies</li>
<li><code>-f, --format &lt;FORMAT&gt;</code> - Output format: text or json (default: text)</li>
</ul>
<p><strong>Description:</strong>
Debugging tool that explains how coverage detection works for a specific function. Shows all attempted matching strategies and helps diagnose coverage mapping issues. This command is particularly useful when:</p>
<ul>
<li>Coverage appears incorrect for specific functions</li>
<li>You need to understand why a function isn’t matched in coverage data</li>
<li>Debugging LCOV line mapping issues</li>
</ul>
<p><strong>Example:</strong></p>
<pre><code class="language-bash"># Explain coverage for a specific function
debtmap explain-coverage src/ --coverage-file coverage.lcov --function "process_file"

# Narrow search to specific file with verbose output
debtmap explain-coverage . --lcov lcov.info --function "analyze_complexity" --file "src/analyzer.rs" -v

# JSON output for automation
debtmap explain-coverage . --coverage-file coverage.lcov --function "my_function" --format json
</code></pre>
<h2 id="options"><a class="header" href="#options">Options</a></h2>
<p>Options are organized by category for clarity. Most options apply to the <code>analyze</code> command, with a subset available for <code>validate</code>.</p>
<h3 id="output-control"><a class="header" href="#output-control">Output Control</a></h3>
<p>Control how analysis results are formatted and displayed.</p>
<p><strong>Format Options:</strong></p>
<ul>
<li><code>-f, --format &lt;FORMAT&gt;</code> - Output format: json, markdown, terminal (default: terminal for analyze)</li>
<li><code>-o, --output &lt;OUTPUT&gt;</code> - Output file path (defaults to stdout)</li>
<li><code>--plain</code> - Plain output mode: ASCII-only, no colors, no emoji, machine-parseable</li>
</ul>
<p><strong>Display Filtering:</strong></p>
<ul>
<li><code>--top &lt;N&gt;</code> / <code>--head &lt;N&gt;</code> - Show only top N priority items</li>
<li><code>--tail &lt;N&gt;</code> - Show only bottom N priority items (lowest priority)</li>
<li><code>-s, --summary</code> - Use summary format with tiered priority display (compact output)</li>
<li><code>-c, --compact</code> - Use compact output format (minimal details, top metrics only). Conflicts with verbosity flags (-v, -vv, -vvv). Only available in <code>analyze</code> command (note: <code>validate</code> uses <code>-c</code> for <code>--config</code>)</li>
<li><code>--min-priority &lt;PRIORITY&gt;</code> - Minimum priority to display: low, medium, high, critical</li>
<li><code>--filter &lt;CATEGORIES&gt;</code> - Filter by debt categories (comma-separated)</li>
<li><code>--aggregate-only</code> - Show only aggregated file-level scores</li>
<li><code>--group-by-category</code> - Group output by debt category</li>
</ul>
<p><strong>Dependency Display Options:</strong></p>
<ul>
<li><code>--show-dependencies</code> - Show caller/callee information in output</li>
<li><code>--no-dependencies</code> - Hide dependency information (conflicts with –show-dependencies)</li>
<li><code>--max-callers &lt;N&gt;</code> - Maximum number of callers to display (default: 5)</li>
<li><code>--max-callees &lt;N&gt;</code> - Maximum number of callees to display (default: 5)</li>
<li><code>--show-external-calls</code> - Include external crate calls in dependencies</li>
<li><code>--show-std-lib-calls</code> - Include standard library calls in dependencies</li>
</ul>
<h3 id="analysis-control"><a class="header" href="#analysis-control">Analysis Control</a></h3>
<p>Configure analysis behavior, thresholds, and language selection.</p>
<p><strong>Thresholds:</strong></p>
<ul>
<li><code>--threshold-complexity &lt;N&gt;</code> - Complexity threshold (default: 10) [analyze command]</li>
<li><code>--threshold-duplication &lt;N&gt;</code> - Duplication threshold in lines (default: 50) [analyze command]</li>
<li><code>--threshold-preset &lt;PRESET&gt;</code> - Complexity threshold preset: strict, balanced, lenient [analyze command]
<ul>
<li><code>strict</code> - Strict thresholds for high code quality standards</li>
<li><code>balanced</code> - Balanced thresholds for typical projects (default)</li>
<li><code>lenient</code> - Lenient thresholds for legacy or complex domains</li>
</ul>
</li>
<li><code>--max-debt-density &lt;N&gt;</code> - Maximum debt density allowed per 1000 LOC [validate command]</li>
</ul>
<p><strong>Note:</strong> Threshold options (<code>--threshold-complexity</code>, <code>--threshold-duplication</code>, <code>--threshold-preset</code>) are command-line options for the <code>analyze</code> command. For the <code>validate</code> command, these thresholds are configured via the <code>--config</code> file (<code>debtmap.toml</code>) rather than as command-line flags.</p>
<p><strong>Language Selection:</strong></p>
<ul>
<li><code>--languages &lt;LANGS&gt;</code> - Comma-separated list of languages to analyze
<ul>
<li>Example: <code>--languages rust,python,javascript</code></li>
<li>Supported: rust, python, javascript, typescript</li>
</ul>
</li>
</ul>
<p><strong>Analysis Modes:</strong></p>
<ul>
<li><code>--semantic-off</code> - Disable semantic analysis (fallback mode)</li>
<li><code>--no-context-aware</code> - Disable context-aware false positive reduction (enabled by default)</li>
<li><code>--no-multi-pass</code> - Disable multi-pass analysis (use single-pass for performance)</li>
<li><code>--attribution</code> - Show complexity attribution details (requires multi-pass, which is the default)</li>
</ul>
<p><strong>Functional Programming Analysis:</strong></p>
<ul>
<li><code>--ast-functional-analysis</code> - Enable AST-based functional composition analysis (spec 111)
<ul>
<li>Analyzes code for functional programming patterns and composition</li>
<li>Detects pure functions, immutability, and side effects</li>
</ul>
</li>
<li><code>--functional-analysis-profile &lt;PROFILE&gt;</code> - Set functional analysis profile
<ul>
<li><code>strict</code> - Strict functional purity requirements (for pure FP codebases)</li>
<li><code>balanced</code> - Balanced analysis suitable for mixed paradigms (default)</li>
<li><code>lenient</code> - Lenient thresholds for imperative codebases</li>
</ul>
</li>
</ul>
<h3 id="context--coverage"><a class="header" href="#context--coverage">Context &amp; Coverage</a></h3>
<p>Enable context-aware risk analysis and integrate test coverage data.</p>
<p><strong>Context-Aware Risk Analysis:</strong></p>
<ul>
<li><code>--context</code> / <code>--enable-context</code> - Enable context-aware risk analysis</li>
<li><code>--context-providers &lt;PROVIDERS&gt;</code> - Context providers to use (comma-separated)
<ul>
<li>Available: <code>critical_path</code>, <code>dependency</code>, <code>git_history</code></li>
<li>Example: <code>--context-providers critical_path,git_history</code></li>
</ul>
</li>
<li><code>--disable-context &lt;PROVIDERS&gt;</code> - Disable specific context providers (comma-separated)</li>
</ul>
<p><strong>Coverage Integration:</strong></p>
<ul>
<li><code>--coverage-file &lt;PATH&gt;</code> / <code>--lcov &lt;PATH&gt;</code> - LCOV coverage file for risk analysis
<ul>
<li>Coverage data dampens debt scores for well-tested code (multiplier = 1.0 - coverage)</li>
<li>Surfaces untested complex functions as higher priority</li>
<li>Total debt score with coverage ≤ score without coverage</li>
</ul>
</li>
<li><code>--validate-loc</code> - Validate LOC consistency across analysis modes (with/without coverage)</li>
</ul>
<h3 id="performance"><a class="header" href="#performance">Performance</a></h3>
<p>Optimize analysis performance through parallelization.</p>
<p><strong>Parallel Processing:</strong></p>
<ul>
<li><code>--no-parallel</code> - Disable parallel call graph construction (enabled by default)</li>
<li><code>-j, --jobs &lt;N&gt;</code> - Number of threads for parallel processing
<ul>
<li><code>0</code> = use all available CPU cores (default)</li>
<li>Specify number to limit thread count</li>
</ul>
</li>
</ul>
<p><strong>Other Performance:</strong></p>
<ul>
<li><code>--max-files &lt;N&gt;</code> - Maximum number of files to analyze (0 = no limit)</li>
</ul>
<h3 id="debugging--verbosity"><a class="header" href="#debugging--verbosity">Debugging &amp; Verbosity</a></h3>
<p>Control diagnostic output and debugging information.</p>
<p><strong>Verbosity Levels:</strong></p>
<ul>
<li><code>-v, --verbose</code> - Increase verbosity level (can be repeated: -v, -vv, -vvv)
<ul>
<li><code>-v</code> - Show main score factors</li>
<li><code>-vv</code> - Show detailed calculations</li>
<li><code>-vvv</code> - Show all debug information</li>
</ul>
</li>
</ul>
<p><strong>Specialized Debugging:</strong></p>
<ul>
<li><code>--explain-metrics</code> - Explain metric definitions and formulas (measured vs estimated)</li>
<li><code>--verbose-macro-warnings</code> - Show verbose macro parsing warnings (Rust analysis)</li>
<li><code>--show-macro-stats</code> - Show macro expansion statistics at end of analysis</li>
<li><code>--detail-level &lt;LEVEL&gt;</code> - Detail level for diagnostic reports
<ul>
<li>Options: summary, standard, comprehensive, debug (default: standard)</li>
</ul>
</li>
</ul>
<p><strong>Call Graph Debugging:</strong></p>
<ul>
<li><code>--debug-call-graph</code> - Enable detailed call graph debugging with resolution information</li>
<li><code>--trace-function &lt;FUNCTIONS&gt;</code> - Trace specific functions during call resolution (comma-separated)
<ul>
<li>Example: <code>--trace-function 'my_function,another_function'</code></li>
</ul>
</li>
<li><code>--call-graph-stats</code> - Show only call graph statistics (no detailed failure list)</li>
<li><code>--validate-call-graph</code> - Validate call graph structure and report issues</li>
<li><code>--debug-format &lt;FORMAT&gt;</code> - Debug output format: text or json (default: text)
<ul>
<li>Use with call graph debugging flags to control output format</li>
</ul>
</li>
</ul>
<h3 id="aggregation"><a class="header" href="#aggregation">Aggregation</a></h3>
<p>Control file-level aggregation and god object detection.</p>
<p><strong>File Aggregation:</strong></p>
<ul>
<li><code>--aggregate-only</code> - Show only aggregated file-level scores</li>
<li><code>--no-aggregation</code> - Disable file-level aggregation</li>
<li><code>--aggregation-method &lt;METHOD&gt;</code> - File aggregation method (default: weighted_sum)
<ul>
<li>Options: sum, weighted_sum, logarithmic_sum, max_plus_average</li>
</ul>
</li>
<li><code>--min-problematic &lt;N&gt;</code> - Minimum number of problematic functions for file aggregation</li>
<li><code>--no-god-object</code> - Disable god object detection</li>
</ul>
<h3 id="option-aliases"><a class="header" href="#option-aliases">Option Aliases</a></h3>
<p>Common option shortcuts and aliases for convenience:</p>
<ul>
<li><code>--lcov</code> is alias for <code>--coverage-file</code></li>
<li><code>--enable-context</code> is alias for <code>--context</code></li>
<li><code>--head</code> is alias for <code>--top</code></li>
<li><code>-s</code> is short form for <code>--summary</code></li>
<li><code>-v</code> is short form for <code>--verbose</code></li>
<li><code>-f</code> is short form for <code>--format</code></li>
<li><code>-o</code> is short form for <code>--output</code></li>
<li><code>-c</code> is short form for <code>--config</code></li>
<li><code>-j</code> is short form for <code>--jobs</code></li>
</ul>
<h3 id="deprecated-options"><a class="header" href="#deprecated-options">Deprecated Options</a></h3>
<p>The following options are deprecated and should be migrated:</p>
<ul>
<li><code>--explain-score</code> (hidden) - <strong>Deprecated:</strong> use <code>-v</code> instead
<ul>
<li><strong>Migration:</strong> Use <code>-v</code>, <code>-vv</code>, or <code>-vvv</code> for increasing verbosity levels</li>
</ul>
</li>
</ul>
<h2 id="configuration-1"><a class="header" href="#configuration-1">Configuration</a></h2>
<h3 id="configuration-file"><a class="header" href="#configuration-file">Configuration File</a></h3>
<p>Created via <code>debtmap init</code> command. The configuration file (<code>debtmap.toml</code>) is used by the <code>validate</code> command for threshold enforcement and default settings.</p>
<p><strong>Creating Configuration:</strong></p>
<pre><code class="language-bash"># Create new config
debtmap init

# Overwrite existing config
debtmap init --force
</code></pre>
<h3 id="environment-variables"><a class="header" href="#environment-variables">Environment Variables</a></h3>
<ul>
<li>
<p><code>DEBTMAP_CONFIG</code> - Custom config file path (same as <code>--config</code> global flag)</p>
<ul>
<li>Example: <code>export DEBTMAP_CONFIG=/path/to/debtmap.toml</code></li>
<li>Overrides default configuration file locations</li>
<li>Useful for CI/CD environments with centralized config</li>
<li>Source: src/cli.rs:45</li>
</ul>
</li>
<li>
<p><code>DEBTMAP_JOBS</code> - Number of threads for parallel processing (same as <code>--jobs</code> / <code>-j</code> flag)</p>
<ul>
<li>Example: <code>export DEBTMAP_JOBS=8  # Same as --jobs 8</code></li>
<li>Use <code>0</code> to utilize all available CPU cores</li>
<li>Controls thread pool size for parallel call graph construction</li>
</ul>
</li>
<li>
<p><code>DEBTMAP_SINGLE_PASS</code> - Disable multi-pass analysis globally (same as <code>--no-multi-pass</code> flag)</p>
<ul>
<li>Example: <code>export DEBTMAP_SINGLE_PASS=1  # Disable multi-pass analysis</code></li>
<li>Set to <code>1</code> or <code>true</code> to disable multi-pass analysis by default</li>
<li>Useful for CI/CD environments where performance is critical</li>
<li>Can be overridden by command-line flags</li>
</ul>
</li>
<li>
<p><code>PRODIGY_AUTOMATION</code> - Enable automation mode for <code>validate-improvement</code> command</p>
<ul>
<li>Example: <code>export PRODIGY_AUTOMATION=true</code></li>
<li>Set to <code>true</code> to enable automation mode (suppresses interactive prompts)</li>
<li>Used in automated workflows for continuous improvement validation</li>
<li>Source: src/main.rs:549</li>
</ul>
</li>
<li>
<p><code>PRODIGY_VALIDATION</code> - Alternative flag for automation mode (same effect as <code>PRODIGY_AUTOMATION</code>)</p>
<ul>
<li>Example: <code>export PRODIGY_VALIDATION=true</code></li>
<li>Set to <code>true</code> to enable automation mode</li>
<li>Provided for backward compatibility with existing workflows</li>
<li>Source: src/main.rs:552</li>
</ul>
</li>
</ul>
<h3 id="getting-help"><a class="header" href="#getting-help">Getting Help</a></h3>
<p>Get help for any command:</p>
<pre><code class="language-bash"># General help
debtmap --help

# Command-specific help
debtmap analyze --help
debtmap validate --help
debtmap compare --help
debtmap init --help
</code></pre>
<h2 id="common-workflows"><a class="header" href="#common-workflows">Common Workflows</a></h2>
<h3 id="basic-analysis"><a class="header" href="#basic-analysis">Basic Analysis</a></h3>
<p>Analyze a project and view results in terminal:</p>
<pre><code class="language-bash">debtmap analyze src/
</code></pre>
<p>Generate JSON report for further processing:</p>
<pre><code class="language-bash">debtmap analyze . --format json --output report.json
</code></pre>
<p>Generate Markdown report:</p>
<pre><code class="language-bash">debtmap analyze . --format markdown --output report.md
</code></pre>
<h3 id="coverage-integrated-analysis"><a class="header" href="#coverage-integrated-analysis">Coverage-Integrated Analysis</a></h3>
<p>Analyze with test coverage to surface untested complex code:</p>
<pre><code class="language-bash"># Generate coverage file first (example for Rust)
cargo tarpaulin --out lcov

# Run analysis with coverage
debtmap analyze src/ --coverage-file lcov.info
</code></pre>
<p>Coverage dampens debt scores for well-tested code, making untested complex functions more visible.</p>
<h3 id="context-aware-analysis"><a class="header" href="#context-aware-analysis">Context-Aware Analysis</a></h3>
<p>Enable context providers for risk-aware prioritization:</p>
<pre><code class="language-bash"># Use all context providers
debtmap analyze . --context

# Use specific context providers
debtmap analyze . --context --context-providers critical_path,git_history
</code></pre>
<p>Context-aware analysis reduces false positives and prioritizes code based on:</p>
<ul>
<li>Critical execution paths</li>
<li>Dependency relationships</li>
<li>Git history (change frequency)</li>
</ul>
<h3 id="filtered--focused-analysis"><a class="header" href="#filtered--focused-analysis">Filtered &amp; Focused Analysis</a></h3>
<p>Show only top priority items:</p>
<pre><code class="language-bash">debtmap analyze . --top 10 --min-priority high
</code></pre>
<p>Filter by specific debt categories:</p>
<pre><code class="language-bash">debtmap analyze . --filter complexity,duplication
</code></pre>
<p>Use summary mode for compact output:</p>
<pre><code class="language-bash">debtmap analyze . --summary
</code></pre>
<p>Show only file-level aggregations:</p>
<pre><code class="language-bash">debtmap analyze . --aggregate-only
</code></pre>
<h3 id="performance-tuning"><a class="header" href="#performance-tuning">Performance Tuning</a></h3>
<p>Control parallelization:</p>
<pre><code class="language-bash"># Use 8 threads
debtmap analyze . --jobs 8

# Disable parallel processing
debtmap analyze . --no-parallel
</code></pre>
<p>Limit analysis scope:</p>
<pre><code class="language-bash"># Analyze maximum 100 files
debtmap analyze . --max-files 100

# Analyze specific languages only
debtmap analyze . --languages rust,python
</code></pre>
<h3 id="cicd-integration-2"><a class="header" href="#cicd-integration-2">CI/CD Integration</a></h3>
<p>Use the <code>validate</code> command in CI/CD pipelines:</p>
<pre><code class="language-bash"># Initialize configuration (one time)
debtmap init

# Edit debtmap.toml to set thresholds
# ...

# In CI pipeline: validate against thresholds
debtmap validate . --config debtmap.toml --max-debt-density 50
</code></pre>
<p>The <code>validate</code> command returns non-zero exit code if thresholds are exceeded, failing the build.</p>
<h3 id="comparison--tracking"><a class="header" href="#comparison--tracking">Comparison &amp; Tracking</a></h3>
<p>Compare analysis results before and after changes:</p>
<pre><code class="language-bash"># Before changes
debtmap analyze . --format json --output before.json

# Make code changes...

# After changes
debtmap analyze . --format json --output after.json

# Generate comparison report
debtmap compare --before before.json --after after.json --format markdown
</code></pre>
<p>With implementation plan:</p>
<pre><code class="language-bash">debtmap compare --before before.json --after after.json --plan IMPLEMENTATION_PLAN.md
</code></pre>
<h3 id="debugging-analysis"><a class="header" href="#debugging-analysis">Debugging Analysis</a></h3>
<p>Increase verbosity to understand scoring:</p>
<pre><code class="language-bash"># Show main score factors
debtmap analyze src/ -v

# Show detailed calculations
debtmap analyze src/ -vv

# Show all debug information
debtmap analyze src/ -vvv
</code></pre>
<p>Debug call graph resolution issues:</p>
<pre><code class="language-bash"># Enable call graph debugging
debtmap analyze . --debug-call-graph

# Trace specific functions
debtmap analyze . --debug-call-graph --trace-function 'problematic_function'

# Validate call graph structure
debtmap analyze . --validate-call-graph --debug-format json
</code></pre>
<p>Show macro expansion statistics (Rust):</p>
<pre><code class="language-bash">debtmap analyze . --show-macro-stats --verbose-macro-warnings
</code></pre>
<p>Use detailed diagnostic reports:</p>
<pre><code class="language-bash">debtmap analyze . --detail-level comprehensive
</code></pre>
<p>Analyze functional programming patterns:</p>
<pre><code class="language-bash"># Enable functional analysis
debtmap analyze . --ast-functional-analysis

# Use strict profile for pure FP codebases
debtmap analyze . --ast-functional-analysis --functional-analysis-profile strict
</code></pre>
<h2 id="examples"><a class="header" href="#examples">Examples</a></h2>
<h3 id="basic-analysis-1"><a class="header" href="#basic-analysis-1">Basic Analysis</a></h3>
<pre><code class="language-bash"># Analyze current directory
debtmap analyze .

# Analyze specific directory
debtmap analyze src/

# Generate JSON output
debtmap analyze . --format json --output report.json
</code></pre>
<h3 id="with-coverage"><a class="header" href="#with-coverage">With Coverage</a></h3>
<pre><code class="language-bash"># Analyze with LCOV coverage file
debtmap analyze src/ --coverage-file coverage.lcov

# Alternative alias
debtmap analyze src/ --lcov coverage.lcov
</code></pre>
<h3 id="context-aware-analysis-1"><a class="header" href="#context-aware-analysis-1">Context-Aware Analysis</a></h3>
<pre><code class="language-bash"># Enable all context providers
debtmap analyze . --context

# Use specific context providers
debtmap analyze . --context --context-providers critical_path,git_history

# Disable specific providers
debtmap analyze . --context --disable-context dependency
</code></pre>
<h3 id="filtered-output"><a class="header" href="#filtered-output">Filtered Output</a></h3>
<pre><code class="language-bash"># Top 10 priority items only
debtmap analyze . --top 10

# High priority and above
debtmap analyze . --min-priority high

# Specific categories
debtmap analyze . --filter complexity,duplication

# Summary format
debtmap analyze . --summary

# Group by category
debtmap analyze . --group-by-category
</code></pre>
<h3 id="performance-tuning-1"><a class="header" href="#performance-tuning-1">Performance Tuning</a></h3>
<pre><code class="language-bash"># Use 8 threads
debtmap analyze . --jobs 8

# Disable parallelization
debtmap analyze . --no-parallel

# Limit file count
debtmap analyze . --max-files 100
</code></pre>
<h3 id="validation"><a class="header" href="#validation">Validation</a></h3>
<pre><code class="language-bash"># Initialize config
debtmap init --force

# Validate against config
debtmap validate . --config debtmap.toml

# With max debt density threshold
debtmap validate . --max-debt-density 50
</code></pre>
<h3 id="comparison"><a class="header" href="#comparison">Comparison</a></h3>
<pre><code class="language-bash"># Compare two analyses
debtmap compare --before before.json --after after.json

# With markdown output
debtmap compare --before before.json --after after.json --format markdown

# With implementation plan
debtmap compare --before before.json --after after.json --plan IMPLEMENTATION_PLAN.md

# With target location
debtmap compare --before before.json --after after.json --target-location "src/main.rs:process_file:42"
</code></pre>
<h3 id="language-selection"><a class="header" href="#language-selection">Language Selection</a></h3>
<pre><code class="language-bash"># Analyze only Rust files
debtmap analyze . --languages rust

# Multiple languages
debtmap analyze . --languages rust,python,javascript
</code></pre>
<h3 id="threshold-configuration"><a class="header" href="#threshold-configuration">Threshold Configuration</a></h3>
<pre><code class="language-bash"># Custom complexity threshold
debtmap analyze . --threshold-complexity 15

# Use preset
debtmap analyze . --threshold-preset strict

# Custom duplication threshold
debtmap analyze . --threshold-duplication 100
</code></pre>
<h3 id="plainmachine-readable-output"><a class="header" href="#plainmachine-readable-output">Plain/Machine-Readable Output</a></h3>
<pre><code class="language-bash"># Plain output (no colors, no emoji)
debtmap analyze . --plain

# Combine with JSON for CI
debtmap analyze . --format json --plain --output report.json
</code></pre>
<h3 id="advanced-debugging"><a class="header" href="#advanced-debugging">Advanced Debugging</a></h3>
<pre><code class="language-bash"># Call graph debugging with detailed information
debtmap analyze . --debug-call-graph --debug-format json

# Trace specific functions during call resolution
debtmap analyze . --debug-call-graph --trace-function 'process_file,analyze_complexity'

# Validate call graph structure
debtmap analyze . --validate-call-graph

# Show only call graph statistics
debtmap analyze . --debug-call-graph --call-graph-stats

# Functional programming analysis with strict profile
debtmap analyze . --ast-functional-analysis --functional-analysis-profile strict

# Explain metric definitions
debtmap analyze . --explain-metrics -v
</code></pre>
<h2 id="command-compatibility-matrix"><a class="header" href="#command-compatibility-matrix">Command Compatibility Matrix</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Option</th><th>analyze</th><th>validate</th><th>compare</th><th>init</th><th>explain-coverage</th></tr>
</thead>
<tbody>
<tr><td><code>&lt;PATH&gt;</code> argument</td><td>✓</td><td>✓</td><td>✗</td><td>✗</td><td>✓</td></tr>
<tr><td><code>--format</code></td><td>✓</td><td>✓</td><td>✓</td><td>✗</td><td>✓</td></tr>
<tr><td><code>--output</code></td><td>✓</td><td>✓</td><td>✓</td><td>✗</td><td>✗</td></tr>
<tr><td><code>--coverage-file</code></td><td>✓</td><td>✓</td><td>✗</td><td>✗</td><td>✓</td></tr>
<tr><td><code>--context</code></td><td>✓</td><td>✓</td><td>✗</td><td>✗</td><td>✗</td></tr>
<tr><td><code>--threshold-*</code></td><td>✓</td><td>✗</td><td>✗</td><td>✗</td><td>✗</td></tr>
<tr><td><code>--top / --tail</code></td><td>✓</td><td>✓</td><td>✗</td><td>✗</td><td>✗</td></tr>
<tr><td><code>--jobs</code></td><td>✓</td><td>✓</td><td>✗</td><td>✗</td><td>✗</td></tr>
<tr><td><code>--no-parallel</code></td><td>✓</td><td>✓</td><td>✗</td><td>✗</td><td>✗</td></tr>
<tr><td><code>--verbose</code></td><td>✓</td><td>✓</td><td>✗</td><td>✗</td><td>✓</td></tr>
<tr><td><code>--explain-metrics</code></td><td>✓</td><td>✗</td><td>✗</td><td>✗</td><td>✗</td></tr>
<tr><td><code>--debug-call-graph</code></td><td>✓</td><td>✗</td><td>✗</td><td>✗</td><td>✗</td></tr>
<tr><td><code>--trace-function</code></td><td>✓</td><td>✗</td><td>✗</td><td>✗</td><td>✗</td></tr>
<tr><td><code>--call-graph-stats</code></td><td>✓</td><td>✗</td><td>✗</td><td>✗</td><td>✗</td></tr>
<tr><td><code>--validate-call-graph</code></td><td>✓</td><td>✗</td><td>✗</td><td>✗</td><td>✗</td></tr>
<tr><td><code>--debug-format</code></td><td>✓</td><td>✗</td><td>✗</td><td>✗</td><td>✗</td></tr>
<tr><td><code>--show-dependencies</code></td><td>✓</td><td>✗</td><td>✗</td><td>✗</td><td>✗</td></tr>
<tr><td><code>--no-dependencies</code></td><td>✓</td><td>✗</td><td>✗</td><td>✗</td><td>✗</td></tr>
<tr><td><code>--max-callers</code></td><td>✓</td><td>✗</td><td>✗</td><td>✗</td><td>✗</td></tr>
<tr><td><code>--max-callees</code></td><td>✓</td><td>✗</td><td>✗</td><td>✗</td><td>✗</td></tr>
<tr><td><code>--show-external-calls</code></td><td>✓</td><td>✗</td><td>✗</td><td>✗</td><td>✗</td></tr>
<tr><td><code>--show-std-lib-calls</code></td><td>✓</td><td>✗</td><td>✗</td><td>✗</td><td>✗</td></tr>
<tr><td><code>--ast-functional-analysis</code></td><td>✓</td><td>✗</td><td>✗</td><td>✗</td><td>✗</td></tr>
<tr><td><code>--functional-analysis-profile</code></td><td>✓</td><td>✗</td><td>✗</td><td>✗</td><td>✗</td></tr>
<tr><td><code>--function</code></td><td>✗</td><td>✗</td><td>✗</td><td>✗</td><td>✓</td></tr>
<tr><td><code>--file</code></td><td>✗</td><td>✗</td><td>✗</td><td>✗</td><td>✓</td></tr>
<tr><td><code>--config</code></td><td>✗</td><td>✓</td><td>✗</td><td>✗</td><td>✗</td></tr>
<tr><td><code>--before / --after</code></td><td>✗</td><td>✗</td><td>✓</td><td>✗</td><td>✗</td></tr>
<tr><td><code>--force</code></td><td>✗</td><td>✗</td><td>✗</td><td>✓</td><td>✗</td></tr>
</tbody>
</table>
</div>
<p><strong>Note:</strong> The <code>validate</code> command supports output control (<code>--format</code>, <code>--output</code>), coverage integration (<code>--coverage-file</code>), context-aware analysis (<code>--context</code>), display filtering (<code>--top</code>, <code>--tail</code>, <code>--summary</code>), performance control (<code>--jobs</code>, <code>--no-parallel</code>), and verbosity options (<code>--verbose</code>) from the <code>analyze</code> command. Analysis thresholds (<code>--threshold-complexity</code>, <code>--threshold-duplication</code>, <code>--threshold-preset</code>) are configured via the <code>--config</code> file rather than as command-line options. Debugging features like call graph debugging and functional analysis are specific to the <code>analyze</code> command. The <code>explain-coverage</code> command is a specialized debugging tool for diagnosing coverage detection issues and has its own unique options (<code>--function</code>, <code>--file</code>).</p>
<h2 id="troubleshooting-2"><a class="header" href="#troubleshooting-2">Troubleshooting</a></h2>
<h3 id="performance-issues"><a class="header" href="#performance-issues">Performance Issues</a></h3>
<p><strong>Problem:</strong> Analysis is slow on large codebases</p>
<p><strong>Solutions:</strong></p>
<pre><code class="language-bash"># Use more threads (if you have CPU cores available)
debtmap analyze . --jobs 16

# Limit analysis scope
debtmap analyze . --max-files 500 --languages rust
</code></pre>
<h3 id="memory-issues"><a class="header" href="#memory-issues">Memory Issues</a></h3>
<p><strong>Problem:</strong> Analysis runs out of memory</p>
<p><strong>Solutions:</strong></p>
<pre><code class="language-bash"># Disable parallelization
debtmap analyze . --no-parallel

# Limit file count
debtmap analyze . --max-files 100

# Analyze in batches by language
debtmap analyze . --languages rust
debtmap analyze . --languages python
</code></pre>
<h3 id="output-issues"><a class="header" href="#output-issues">Output Issues</a></h3>
<p><strong>Problem:</strong> Terminal output has garbled characters</p>
<p><strong>Solution:</strong></p>
<pre><code class="language-bash"># Use plain mode
debtmap analyze . --plain
</code></pre>
<p><strong>Problem:</strong> Want machine-readable output</p>
<p><strong>Solution:</strong></p>
<pre><code class="language-bash"># Use JSON with plain mode
debtmap analyze . --format json --plain --output report.json
</code></pre>
<h3 id="threshold-issues"><a class="header" href="#threshold-issues">Threshold Issues</a></h3>
<p><strong>Problem:</strong> Too many items flagged</p>
<p><strong>Solutions:</strong></p>
<pre><code class="language-bash"># Use lenient preset
debtmap analyze . --threshold-preset lenient

# Increase threshold
debtmap analyze . --threshold-complexity 20

# Filter to high priority only
debtmap analyze . --min-priority high
</code></pre>
<p><strong>Problem:</strong> Not enough items flagged</p>
<p><strong>Solutions:</strong></p>
<pre><code class="language-bash"># Use strict preset
debtmap analyze . --threshold-preset strict

# Lower threshold
debtmap analyze . --threshold-complexity 5

# Show all items
debtmap analyze . --min-priority low
</code></pre>
<h2 id="best-practices-1"><a class="header" href="#best-practices-1">Best Practices</a></h2>
<h3 id="regular-analysis"><a class="header" href="#regular-analysis">Regular Analysis</a></h3>
<p>Run analysis regularly to track code quality trends:</p>
<pre><code class="language-bash"># Daily in CI
debtmap validate . --config debtmap.toml

# Weekly deep analysis with coverage
debtmap analyze . --coverage-file coverage.lcov --format json --output weekly-report.json
</code></pre>
<h3 id="performance-optimization"><a class="header" href="#performance-optimization">Performance Optimization</a></h3>
<p>For large codebases:</p>
<pre><code class="language-bash"># Use maximum parallelization
debtmap analyze . --jobs 0  # 0 = all cores

# Focus on changed files in CI
# (implement via custom scripts to analyze git diff)
</code></pre>
<h3 id="integration-with-coverage"><a class="header" href="#integration-with-coverage">Integration with Coverage</a></h3>
<p>Always analyze with coverage when available:</p>
<pre><code class="language-bash"># Rust example
cargo tarpaulin --out lcov
debtmap analyze src/ --coverage-file lcov.info

# Python example
pytest --cov --cov-report=lcov
debtmap analyze . --coverage-file coverage.lcov
</code></pre>
<p>Coverage integration helps prioritize untested complex code.</p>
<h2 id="additional-tools"><a class="header" href="#additional-tools">Additional Tools</a></h2>
<h3 id="prodigy-validate-debtmap-improvement"><a class="header" href="#prodigy-validate-debtmap-improvement">prodigy-validate-debtmap-improvement</a></h3>
<p>Specialized validation tool for Prodigy workflow integration.</p>
<p><strong>Description:</strong>
This binary is part of the Prodigy workflow system and provides specialized validation for Debtmap improvement workflows.</p>
<p><strong>Usage:</strong>
See Prodigy documentation for detailed usage instructions.</p>
<h2 id="see-also"><a class="header" href="#see-also">See Also</a></h2>
<ul>
<li><a href="#configuration-2">Configuration Format</a> - Detailed configuration file format</li>
<li><a href="#output-formats-3">Output Formats</a> - Understanding JSON, Markdown, and Terminal output</li>
<li><a href="#coverage-integration-1">Coverage Integration</a> - Integrating test coverage data</li>
<li><a href="#context-providers">Context Providers</a> - Understanding context-aware analysis</li>
<li><a href="#examples-5">Examples</a> - More comprehensive usage examples</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="analysis-guide"><a class="header" href="#analysis-guide">Analysis Guide</a></h1>
<p>This guide explains Debtmap’s analysis capabilities, metrics, and methodologies in depth. Use this to understand what Debtmap measures, how it scores technical debt, and how to interpret analysis results for maximum impact.</p>
<h2 id="subsections"><a class="header" href="#subsections">Subsections</a></h2>
<p>This chapter is organized into the following subsections:</p>
<ul>
<li><a href="#overview-1">Overview</a> - Introduction to analysis capabilities and overall approach</li>
<li><a href="#complexity-metrics-1">Complexity Metrics</a> - Comprehensive coverage of cyclomatic, cognitive, and entropy-based complexity with extensive examples</li>
<li><a href="#debt-patterns-1">Debt Patterns</a> - Detailed documentation of all debt pattern types with detection criteria</li>
<li><a href="#risk-scoring-1">Risk Scoring</a> - Risk calculation methodology, scoring formulas, and prioritization strategies</li>
<li><a href="#interpreting-results">Interpreting Results</a> - Result interpretation, action strategies, and detailed examples</li>
<li><a href="#analyzer-types">Analyzer Types</a> - Documentation of different analyzer modes and their use cases</li>
<li><a href="#advanced-features">Advanced Features</a> - Advanced analysis capabilities and techniques</li>
<li><a href="#example-outputs">Example Outputs</a> - Real-world output examples and interpretation guidance</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="overview-1"><a class="header" href="#overview-1">Overview</a></h1>
<p>Debtmap analyzes code through multiple lenses to provide a comprehensive view of technical health. The goal is to move beyond simple problem identification to <strong>evidence-based prioritization</strong> - showing what to fix first based on risk scores, test coverage gaps, and ROI calculations, with actionable recommendations backed by impact metrics.</p>
<h2 id="supported-languages"><a class="header" href="#supported-languages">Supported Languages</a></h2>
<ul>
<li><strong>Rust</strong> - Full analysis support with AST-based parsing via <code>syn</code></li>
<li><strong>Python</strong> - Partial support for basic metrics</li>
</ul>
<p>Source: <code>src/organization/language.rs:4-7</code></p>
<h2 id="analysis-capabilities-1"><a class="header" href="#analysis-capabilities-1">Analysis Capabilities</a></h2>
<h3 id="complexity-metrics"><a class="header" href="#complexity-metrics">Complexity Metrics</a></h3>
<p>Calculates multiple dimensions of code complexity:</p>
<ul>
<li><strong>Cyclomatic Complexity</strong> - Measures linearly independent paths through code (control flow branching)</li>
<li><strong>Cognitive Complexity</strong> - Quantifies human comprehension difficulty beyond raw paths</li>
<li><strong>Nesting Depth</strong> - Tracks maximum indentation levels</li>
<li><strong>Function Length</strong> - Lines of code per function</li>
<li><strong>Parameter Count</strong> - Number of function parameters</li>
<li><strong>Entropy Score</strong> - Pattern-based complexity adjustment that reduces false positives by up to 70%</li>
<li><strong>Purity Level</strong> - Functional purity classification (StrictlyPure, LocallyPure, ReadOnly, Impure)</li>
</ul>
<p>Source: <code>src/core/mod.rs:62-92</code> (FunctionMetrics struct)</p>
<p>See <a href="#complexity-metrics-1">Complexity Metrics</a> for detailed explanations and examples.</p>
<h3 id="debt-patterns"><a class="header" href="#debt-patterns">Debt Patterns</a></h3>
<p>Identifies <strong>25+ types of technical debt</strong> across 4 major categories:</p>
<p><strong>Testing Issues</strong> (6 types):</p>
<ul>
<li>Testing gaps in complex code</li>
<li>Complex test code requiring refactoring</li>
<li>Test duplication and flaky patterns</li>
<li>Over-complex assertions</li>
</ul>
<p><strong>Architectural Problems</strong> (7 types):</p>
<ul>
<li>God objects and god modules</li>
<li>Feature envy and primitive obsession</li>
<li>Scattered type implementations</li>
<li>Orphaned functions and utilities sprawl</li>
</ul>
<p><strong>Performance Issues</strong> (8 types):</p>
<ul>
<li>Async/await misuse and blocking I/O</li>
<li>Collection inefficiencies and nested loops</li>
<li>Memory allocation problems</li>
<li>Suboptimal data structures</li>
</ul>
<p><strong>Code Quality Issues</strong> (6 types):</p>
<ul>
<li>Complexity hotspots without test coverage</li>
<li>Dead code and duplication</li>
<li>Error swallowing and magic values</li>
</ul>
<p>Source: <code>src/priority/mod.rs:158-288</code> (DebtType enum)</p>
<p>See <a href="#debt-patterns-1">Debt Patterns</a> for detailed detection rules and examples.</p>
<h3 id="risk-scoring"><a class="header" href="#risk-scoring">Risk Scoring</a></h3>
<p>Combines complexity, test coverage, coupling, and change frequency through a <strong>multi-factor risk model</strong>:</p>
<p><strong>Risk Categories</strong>:</p>
<ul>
<li><strong>Critical</strong> - High complexity (&gt;15) + low coverage (&lt;30%)</li>
<li><strong>High</strong> - High complexity (&gt;10) + moderate coverage (&lt;60%)</li>
<li><strong>Medium</strong> - Moderate complexity (&gt;5) + low coverage (&lt;50%)</li>
<li><strong>Low</strong> - Low complexity or high coverage</li>
<li><strong>WellTested</strong> - High complexity with high coverage (good examples to learn from)</li>
</ul>
<p><strong>Coverage Penalty Calculation</strong>:</p>
<ul>
<li>Untested code receives a <strong>2.0x multiplier</strong></li>
<li>Partially tested code receives a <strong>1.5x multiplier</strong></li>
<li>Coverage gaps are penalized exponentially</li>
</ul>
<p><strong>Risk Score Weights</strong> (configurable):</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>coverage:          0.5   // Coverage weight
complexity:        0.3   // Cyclomatic weight
cognitive:         0.45  // Cognitive weight
debt:              0.2   // Debt factor weight
untested_penalty:  2.0   // Multiplier for untested code
<span class="boring">}</span></code></pre>
<p>Source: <code>src/risk/mod.rs:36-42</code>, <code>src/risk/strategy.rs:8-28</code></p>
<p>See <a href="#risk-scoring-1">Risk Scoring</a> for detailed scoring algorithms.</p>
<h3 id="prioritization"><a class="header" href="#prioritization">Prioritization</a></h3>
<p>Uses a <strong>multi-stage pipeline</strong> to assign priority tiers and estimate test writing impact:</p>
<ol>
<li><strong>Evidence Collection</strong> - Gather complexity, coverage, and coupling metrics</li>
<li><strong>Context Enrichment</strong> - Add architectural context and change frequency</li>
<li><strong>Baseline Scoring</strong> - Calculate initial risk scores using multi-factor model</li>
<li><strong>ROI Calculation</strong> - Estimate return on investment for test writing</li>
<li><strong>Final Priority</strong> - Assign priority tiers with risk reduction impact estimates</li>
</ol>
<p><strong>Priority Tiers</strong>:</p>
<ul>
<li>P0 (Critical) - Immediate action required</li>
<li>P1 (High) - Address in current sprint</li>
<li>P2 (Medium) - Plan for next cycle</li>
<li>P3 (Low) - Monitor and review</li>
</ul>
<p>Source: Features documented in <code>.prodigy/book-analysis/features.json:risk_assessment.prioritization</code></p>
<p>See <a href="#interpreting-results">Interpreting Results</a> for guidance on using priority rankings.</p>
<h2 id="how-it-works"><a class="header" href="#how-it-works">How It Works</a></h2>
<p>Debtmap uses a <strong>functional, multi-layered architecture</strong> for accurate and performant analysis:</p>
<h3 id="three-phase-analysis-pipeline"><a class="header" href="#three-phase-analysis-pipeline">Three-Phase Analysis Pipeline</a></h3>
<p><strong>Phase 1: Parallel Parsing</strong></p>
<ul>
<li>Language-specific AST generation using tree-sitter (Rust via <code>syn</code>)</li>
<li>Pure functional transformation: source code → AST</li>
<li>Files parsed once, ASTs cached and cloned for reuse (44% faster)</li>
<li>Runs in parallel using Rayon for CPU-intensive parsing</li>
</ul>
<p><strong>Phase 2: Parallel Analysis with Batching</strong></p>
<ul>
<li>Data flow graph construction (O(1) lookups via multi-index)</li>
<li>Purity analysis tracking pure vs impure functions</li>
<li>Pattern detection with entropy analysis</li>
<li>Metrics computation through pure functions</li>
<li>Default batch size: 100 items (configurable via <code>--batch-size</code>)</li>
</ul>
<p><strong>Phase 3: Sequential Aggregation</strong></p>
<ul>
<li>Combine parallel results into unified analysis</li>
<li>Apply multi-dimensional scoring (complexity + coverage + coupling)</li>
<li>Priority ranking and tier classification</li>
<li>Generate actionable recommendations with impact metrics</li>
</ul>
<p>Source: <code>ARCHITECTURE.md</code>, <code>src/builders/parallel_unified_analysis.rs:21-87</code></p>
<h3 id="key-architectural-patterns"><a class="header" href="#key-architectural-patterns">Key Architectural Patterns</a></h3>
<p><strong>Functional Core, Imperative Shell</strong>:</p>
<ul>
<li>Pure functions for all metric calculations</li>
<li>I/O isolated to file reading and output formatting</li>
<li>Enables easy testing and parallelization</li>
</ul>
<p><strong>Multi-Index Call Graph</strong> (O(1) lookups):</p>
<ul>
<li>Primary index: exact function ID lookup</li>
<li>Fuzzy index: name + file matching for generics</li>
<li>Name index: cross-file function resolution</li>
<li>Memory overhead: ~7MB for 10,000 functions</li>
</ul>
<p><strong>Parallel Processing with Rayon</strong>:</p>
<ul>
<li>CPU-bound work runs in parallel</li>
<li>Sequential aggregation maintains consistency</li>
<li>Adaptive batching optimizes memory usage</li>
</ul>
<p>Source: <code>ARCHITECTURE.md:120-200</code></p>
<p>See <a href="#architecture">Architecture</a> for detailed design documentation.</p>
<h2 id="key-differentiators"><a class="header" href="#key-differentiators">Key Differentiators</a></h2>
<p>What makes debtmap effective:</p>
<ul>
<li><strong>Pattern-Based Complexity Adjustment</strong> - Entropy analysis reduces false positives by identifying boilerplate patterns</li>
<li><strong>Multi-Pass Analysis</strong> - Compares raw vs normalized complexity for accurate attribution</li>
<li><strong>Coverage-Risk Correlation</strong> - Finds genuinely risky code, not just complex code</li>
<li><strong>Functional Purity Tracking</strong> - Identifies side effects and pure functions for targeted refactoring</li>
<li><strong>Context-Aware Detection</strong> - Considers architectural context, not just isolated metrics</li>
<li><strong>Evidence-Based Prioritization</strong> - ROI-driven recommendations backed by multiple signals</li>
</ul>
<h2 id="performance-1"><a class="header" href="#performance-1">Performance</a></h2>
<p>Debtmap achieves high throughput through parallel processing:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Codebase Size</th><th>Target</th><th>Actual</th><th>Speedup</th></tr>
</thead>
<tbody>
<tr><td>50 files</td><td>&lt;0.5s</td><td>~0.3s</td><td>4x</td></tr>
<tr><td>250 files</td><td>&lt;1s</td><td>~0.8s</td><td>6.25x</td></tr>
<tr><td>1000 files</td><td>&lt;5s</td><td>~3.5s</td><td>5.7x</td></tr>
</tbody>
</table>
</div>
<p>Source: <code>ARCHITECTURE.md:100-106</code></p>
<p>See <a href="#parallel-processing">Parallel Processing</a> for optimization details.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="complexity-metrics-1"><a class="header" href="#complexity-metrics-1">Complexity Metrics</a></h1>
<p>Debtmap measures complexity using multiple complementary approaches. Each metric captures a different aspect of code difficulty.</p>
<h2 id="cyclomatic-complexity"><a class="header" href="#cyclomatic-complexity">Cyclomatic Complexity</a></h2>
<p>Measures the number of linearly independent paths through code - essentially counting decision points.</p>
<p><strong>How it works:</strong></p>
<ul>
<li>Start with a base complexity of 1</li>
<li>Add 1 for each: <code>if</code>, <code>else if</code>, <code>match</code> arm, <code>while</code>, <code>for</code>, <code>&amp;&amp;</code>, <code>||</code>, <code>?</code> operator</li>
<li>Does NOT increase for <code>else</code> (it’s the alternate path, not a new decision)</li>
</ul>
<p><strong>Thresholds:</strong></p>
<ul>
<li><strong>1-5</strong>: Simple, easy to test - typically needs 1-3 test cases</li>
<li><strong>6-10</strong>: Moderate complexity - needs 4-8 test cases</li>
<li><strong>11-20</strong>: Complex, consider refactoring - needs 9+ test cases</li>
<li><strong>20+</strong>: Very complex, high risk - difficult to test thoroughly</li>
</ul>
<p><strong>Example:</strong></p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn validate_user(age: u32, has_license: bool, country: &amp;str) -&gt; bool {
    // Complexity: 4
    // Base (1) + if (1) + &amp;&amp; (1) + match (1) = 4
    if age &gt;= 18 &amp;&amp; has_license {
        match country {
            "US" | "CA" =&gt; true,
            _ =&gt; false,
        }
    } else {
        false
    }
}
<span class="boring">}</span></code></pre>
<h2 id="cognitive-complexity"><a class="header" href="#cognitive-complexity">Cognitive Complexity</a></h2>
<p>Measures how difficult code is to understand by considering nesting depth and control flow interruptions.</p>
<p><strong>How it differs from cyclomatic:</strong></p>
<ul>
<li>Nesting increases weight (deeply nested code is harder to understand)</li>
<li>Linear sequences don’t increase complexity (easier to follow)</li>
<li>Breaks and continues add complexity (interrupt normal flow)</li>
</ul>
<p><strong>Calculation:</strong></p>
<ul>
<li>Each structure (if, loop, match) gets a base score</li>
<li><strong>Nesting increases weight linearly</strong>: Each nesting level adds to the complexity score
<ul>
<li>Base level (no nesting): weight = 1</li>
<li>First nesting level: weight = 2</li>
<li>Second nesting level: weight = 3</li>
<li>Formula: <code>complexity = 1 + nesting_level</code> (from src/complexity/cognitive.rs:151)</li>
</ul>
</li>
<li>Break/continue/return in middle of function adds cognitive load</li>
</ul>
<p><strong>Example:</strong></p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Cyclomatic: 5, Cognitive: 8
fn process_items(items: Vec&lt;Item&gt;) -&gt; Vec&lt;Result&gt; {
    let mut results = vec![];

    for item in items {                    // +1 cognitive
        if item.is_valid() {               // +2 (nested in loop)
            match item.type {              // +3 (nested 2 levels)
                Type::A =&gt; results.push(process_a(item)),
                Type::B =&gt; {
                    if item.priority &gt; 5 { // +4 (nested 3 levels)
                        results.push(process_b_priority(item));
                    }
                }
                _ =&gt; continue,             // +1 (control flow interruption)
            }
        }
    }

    results
}
<span class="boring">}</span></code></pre>
<p><strong>Thresholds:</strong></p>
<ul>
<li><strong>0-5</strong>: Trivial - anyone can understand</li>
<li><strong>6-10</strong>: Simple - straightforward logic</li>
<li><strong>11-20</strong>: Moderate - requires careful reading</li>
<li><strong>21-40</strong>: Complex - difficult to understand</li>
<li><strong>40+</strong>: Very complex - needs refactoring</li>
</ul>
<h2 id="entropy-based-complexity-analysis"><a class="header" href="#entropy-based-complexity-analysis">Entropy-Based Complexity Analysis</a></h2>
<p>Uses information theory to distinguish genuinely complex code from pattern-based repetitive code. This dramatically reduces false positives for validation functions, dispatchers, and configuration parsers.</p>
<p><strong>How it works:</strong></p>
<ol>
<li>
<p><strong>Token Entropy</strong> (0.0-1.0): Measures variety in code tokens</p>
<ul>
<li>High entropy (0.7+): Diverse logic, genuinely complex</li>
<li>Low entropy (0.0-0.4): Repetitive patterns, less complex than it appears</li>
</ul>
</li>
<li>
<p><strong>Pattern Repetition</strong> (0.0-1.0): Detects repetitive structures in AST</p>
<ul>
<li>High repetition (0.7+): Similar blocks repeated (validation checks, case handlers)</li>
<li>Low repetition: Unique logic throughout</li>
</ul>
</li>
<li>
<p><strong>Branch Similarity</strong> (0.0-1.0): Analyzes similarity between conditional branches</p>
<ul>
<li>High similarity (0.8+): Branches do similar things (consistent handling)</li>
<li>Low similarity: Each branch has unique logic</li>
</ul>
</li>
<li>
<p><strong>Token Classification</strong>: Categorizes tokens by type with weighted importance (src/complexity/entropy_core.rs:44-54)</p>
<ul>
<li><strong>Token categories and weights</strong> (from src/complexity/entropy_traits.rs:24-44):
<ul>
<li><code>ControlFlow</code> (1.2): if, match, for, while - highest weight for control structures</li>
<li><code>Keyword</code> (1.0): language keywords like fn, let, pub</li>
<li><code>FunctionCall</code> (0.9): method calls and API usage</li>
<li><code>Operator</code> (0.8): +, -, *, ==, etc.</li>
<li><code>Identifier</code> (0.5): variable and function names</li>
<li><code>Literal</code> (0.3): string, number, boolean literals - lowest weight</li>
</ul>
</li>
<li>Higher weights emphasize structural complexity over superficial differences</li>
<li>Focuses entropy calculation on control flow and logic rather than data values</li>
</ul>
</li>
</ol>
<p><strong>Dampening logic:</strong> Dampening is applied when multiple factors indicate repetitive patterns:</p>
<ul>
<li>Low token entropy (&lt; 0.4) indicates simple, repetitive patterns</li>
<li>High pattern repetition (&gt; 0.6) shows similar code blocks (measured via PatternMetrics)</li>
<li>High branch similarity (&gt; 0.7) indicates consistent branching logic</li>
</ul>
<p><strong>Pattern detection</strong> (src/complexity/entropy_core.rs:56-85):</p>
<ul>
<li><code>PatternMetrics</code> tracks intermediate calculations:
<ul>
<li><code>total_patterns</code>: Total number of code patterns detected</li>
<li><code>unique_patterns</code>: Count of distinct patterns</li>
<li><code>repetition_ratio</code>: Calculated as <code>1.0 - (unique_patterns / total_patterns)</code></li>
</ul>
</li>
<li>High repetition ratio indicates validation functions, dispatchers, and configuration parsers</li>
</ul>
<p>When these conditions are met:</p>
<pre><code>effective_complexity = entropy × pattern_factor × similarity_factor
</code></pre>
<p><strong>Note on metrics</strong> (src/complexity/entropy_core.rs:28-32):</p>
<ul>
<li><code>token_entropy</code>: Measures unpredictability of code tokens (0.0-1.0), used for pattern detection</li>
<li><code>effective_complexity</code>: Final composite score after applying dampening adjustments</li>
<li>These are distinct metrics - <code>effective_complexity</code> combines multiple factors, while <code>token_entropy</code> is a single entropy measurement</li>
</ul>
<p><strong>Dampening cap:</strong> The dampening factor has a minimum of 0.7, ensuring no more than 30% reduction in complexity scores. This prevents over-correction of pattern-based code and maintains a baseline complexity floor for functions that still require understanding and maintenance.</p>
<p><strong>Example:</strong></p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Without entropy: Cyclomatic = 15 (appears very complex)
// With entropy: Effective = 5 (pattern-based, dampened 67%)
fn validate_config(config: &amp;Config) -&gt; Result&lt;(), ValidationError&gt; {
    if config.name.is_empty() { return Err(ValidationError::EmptyName); }
    if config.port == 0 { return Err(ValidationError::InvalidPort); }
    if config.host.is_empty() { return Err(ValidationError::EmptyHost); }
    if config.timeout == 0 { return Err(ValidationError::InvalidTimeout); }
    // ... 11 more similar checks
    Ok(())
}
<span class="boring">}</span></code></pre>
<p><strong>Enable in <code>.debtmap.toml</code>:</strong></p>
<pre><code class="language-toml">[entropy]
enabled = true                 # Enable entropy analysis (default: true)
weight = 0.5                  # Weight in adjustment (0.0-1.0)
use_classification = true     # Advanced token classification
pattern_threshold = 0.7       # Pattern detection threshold
entropy_threshold = 0.4       # Entropy below this triggers dampening
branch_threshold = 0.8        # Branch similarity threshold
max_combined_reduction = 0.3  # Maximum 30% reduction
</code></pre>
<p><strong>Output fields in EntropyScore:</strong></p>
<ul>
<li><code>unique_variables</code>: Count of distinct variables in the function (measures variable diversity)</li>
<li><code>max_nesting</code>: Maximum nesting depth detected (contributes to dampening calculation)</li>
<li><code>dampening_applied</code>: Actual dampening factor applied to the complexity score</li>
</ul>
<h2 id="nesting-depth"><a class="header" href="#nesting-depth">Nesting Depth</a></h2>
<p>Maximum level of indentation in a function. Deep nesting makes code hard to follow.</p>
<p><strong>Thresholds:</strong></p>
<ul>
<li><strong>1-2</strong>: Flat, easy to read</li>
<li><strong>3-4</strong>: Moderate nesting</li>
<li><strong>5+</strong>: Deep nesting, consider extracting functions</li>
</ul>
<p><strong>Example:</strong></p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Nesting depth: 4 (difficult to follow)
fn process(data: Data) -&gt; Result&lt;Output&gt; {
    if data.is_valid() {                    // Level 1
        for item in data.items {            // Level 2
            if item.active {                // Level 3
                match item.type {           // Level 4
                    Type::A =&gt; { /* ... */ }
                    Type::B =&gt; { /* ... */ }
                }
            }
        }
    }
}
<span class="boring">}</span></code></pre>
<p><strong>Refactored:</strong></p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Nesting depth: 2 (much clearer)
fn process(data: Data) -&gt; Result&lt;Output&gt; {
    if !data.is_valid() {
        return Err(Error::Invalid);
    }

    data.items
        .iter()
        .filter(|item| item.active)
        .map(|item| process_item(item))     // Extract to separate function
        .collect()
}
<span class="boring">}</span></code></pre>
<h2 id="function-length"><a class="header" href="#function-length">Function Length</a></h2>
<p>Number of lines in a function. Long functions often violate single responsibility principle.</p>
<p><strong>Thresholds:</strong></p>
<ul>
<li><strong>1-20 lines</strong>: Good - focused, single purpose</li>
<li><strong>21-50 lines</strong>: Acceptable - may have multiple steps</li>
<li><strong>51-100 lines</strong>: Long - consider breaking up</li>
<li><strong>100+ lines</strong>: Very long - definitely needs refactoring</li>
</ul>
<p><strong>Why length matters:</strong></p>
<ul>
<li>Harder to understand and remember</li>
<li>Harder to test thoroughly</li>
<li>Often violates single responsibility</li>
<li>Difficult to reuse</li>
</ul>
<h2 id="constructor-detection"><a class="header" href="#constructor-detection">Constructor Detection</a></h2>
<p>Debtmap identifies constructor functions using AST-based analysis (Spec 122), which goes beyond simple name-based detection to catch non-standard constructor patterns.</p>
<p><strong>Detection Strategy:</strong></p>
<ol>
<li><strong>Return Type Analysis</strong>: Functions returning <code>Self</code>, <code>Result&lt;Self&gt;</code>, or <code>Option&lt;Self&gt;</code></li>
<li><strong>Body Pattern Analysis</strong>: Struct initialization or simple field assignments</li>
<li><strong>Complexity Check</strong>: Low cyclomatic complexity (≤5), no loops, minimal branching</li>
</ol>
<p><strong>Why AST-based detection?</strong></p>
<p>Name-based detection (looking for <code>new</code>, <code>new_*</code>, <code>from_*</code>) misses non-standard constructors:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Caught by name-based detection
fn new() -&gt; Self {
    Self { timeout: 30 }
}

// Missed by name-based, caught by AST detection
pub fn create_default_client() -&gt; Self {
    Self { timeout: Duration::from_secs(30) }
}

pub fn initialized() -&gt; Self {
    Self::new()
}
<span class="boring">}</span></code></pre>
<p><strong>Builder vs Constructor:</strong></p>
<p>AST analysis distinguishes between constructors and builder methods:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Constructor: creates new instance
pub fn new(timeout: u32) -&gt; Self {
    Self { timeout }
}

// Builder method: modifies existing instance (NOT a constructor)
pub fn set_timeout(mut self, timeout: Duration) -&gt; Self {
    self.timeout = timeout;
    self  // Returns modified self, not new instance
}
<span class="boring">}</span></code></pre>
<p><strong>Detection Criteria:</strong></p>
<p>A function is classified as a constructor if:</p>
<ul>
<li>Returns <code>Self</code>, <code>Result&lt;Self&gt;</code>, or <code>Option&lt;Self&gt;</code></li>
<li>Contains struct initialization (<code>Self { ... }</code>) without loops</li>
<li>OR delegates to another constructor (<code>Self::new()</code>) with minimal logic</li>
</ul>
<p><strong>Fallback Behavior:</strong></p>
<p>If AST parsing fails (syntax errors, unsupported language), Debtmap gracefully falls back to name-based detection (Spec 117):</p>
<ul>
<li><code>new</code>, <code>new_*</code></li>
<li><code>try_new*</code></li>
<li><code>from_*</code></li>
</ul>
<p>This ensures analysis always completes, even on partially broken code.</p>
<p><strong>Performance:</strong></p>
<p>AST-based detection adds &lt; 5% overhead compared to name-only detection. See benchmarks:</p>
<pre><code class="language-bash">cargo bench --bench constructor_detection_bench
</code></pre>
<p><strong>Why it matters:</strong></p>
<p>Accurately identifying constructors helps:</p>
<ul>
<li>Exclude them from complexity thresholds (constructors naturally have high complexity)</li>
<li>Focus refactoring on business logic, not initialization code</li>
<li>Understand initialization patterns across the codebase</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="debt-patterns-1"><a class="header" href="#debt-patterns-1">Debt Patterns</a></h1>
<h2 id="debt-patterns-1-1"><a class="header" href="#debt-patterns-1-1">Debt Patterns</a></h2>
<p>Debtmap detects 27 types of technical debt, organized into 4 strategic categories. Each debt type is mapped to a category that guides prioritization and remediation strategies.</p>
<p><strong>Source</strong>: All debt types and category mappings verified from src/priority/mod.rs:158-347</p>
<h3 id="debt-type-enum"><a class="header" href="#debt-type-enum">Debt Type Enum</a></h3>
<p>The <code>DebtType</code> enum defines all specific debt patterns that Debtmap can detect.</p>
<p><strong>Note</strong>: Each <code>DebtType</code> variant carries structured diagnostic data specific to that pattern. For example, <code>ComplexityHotspot</code> includes <code>cyclomatic</code>, <code>cognitive</code>, and <code>adjusted_cyclomatic</code> (entropy-adjusted) fields that provide detailed metrics for the detected issue. This structured data enables precise prioritization and actionable recommendations.</p>
<p><strong>Source</strong>: DebtType structure defined in src/priority/mod.rs:158-288</p>
<p><strong>Testing Debt:</strong></p>
<ul>
<li><code>TestingGap</code> - Functions with insufficient test coverage</li>
<li><code>TestTodo</code> - TODO comments in test code</li>
<li><code>TestComplexity</code> - Test functions exceeding complexity thresholds</li>
<li><code>TestDuplication</code> - Duplicated code in test files</li>
<li><code>TestComplexityHotspot</code> - Complex test logic that’s hard to maintain</li>
<li><code>AssertionComplexity</code> - Complex test assertions</li>
<li><code>FlakyTestPattern</code> - Non-deterministic test behavior</li>
</ul>
<p><strong>Architecture Debt:</strong></p>
<ul>
<li><code>GodObject</code> - Classes with too many responsibilities</li>
<li><code>GodModule</code> - Modules with too many responsibilities</li>
<li><code>FeatureEnvy</code> - Using more data from other objects than own</li>
<li><code>PrimitiveObsession</code> - Overusing basic types instead of domain objects</li>
<li><code>ScatteredType</code> - Types with methods scattered across multiple files</li>
<li><code>OrphanedFunctions</code> - Functions that should be methods on a type</li>
<li><code>UtilitiesSprawl</code> - Utility modules with poor cohesion</li>
</ul>
<p><strong>Performance Debt:</strong></p>
<ul>
<li><code>AllocationInefficiency</code> - Inefficient memory allocations</li>
<li><code>StringConcatenation</code> - Inefficient string building in loops</li>
<li><code>NestedLoops</code> - Multiple nested iterations (O(n²) or worse)</li>
<li><code>BlockingIO</code> - Blocking I/O in async contexts</li>
<li><code>SuboptimalDataStructure</code> - Wrong data structure for access pattern</li>
<li><code>AsyncMisuse</code> - Improper async/await usage</li>
<li><code>ResourceLeak</code> - Resources not properly released</li>
<li><code>CollectionInefficiency</code> - Inefficient collection operations</li>
</ul>
<p><strong>Code Quality Debt:</strong></p>
<ul>
<li><code>ComplexityHotspot</code> - Functions exceeding complexity thresholds</li>
<li><code>DeadCode</code> - Unreachable or unused code</li>
<li><code>MagicValues</code> - Unexplained literal values</li>
<li><code>Risk</code> - High-risk code (complex + poorly tested)</li>
<li><code>Duplication</code> - Duplicated code blocks</li>
<li><code>ErrorSwallowing</code> - Errors caught but ignored</li>
</ul>
<h3 id="debt-categories"><a class="header" href="#debt-categories">Debt Categories</a></h3>
<p>The <code>DebtCategory</code> enum groups debt types into strategic categories:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub enum DebtCategory {
    Architecture,  // Structure, design, complexity
    Testing,       // Coverage, test quality
    Performance,   // Speed, memory, efficiency
    CodeQuality,   // Maintainability, readability
}
<span class="boring">}</span></code></pre>
<p><strong>Category Mapping:</strong></p>
<p><strong>Source</strong>: Verified from src/priority/mod.rs:309-347</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Debt Type</th><th>Category</th><th>Strategic Focus</th></tr>
</thead>
<tbody>
<tr><td>GodObject, GodModule, FeatureEnvy, PrimitiveObsession, ScatteredType, OrphanedFunctions, UtilitiesSprawl</td><td>Architecture</td><td>Structural improvements, design patterns, type organization</td></tr>
<tr><td>TestingGap, TestTodo, TestComplexity, TestDuplication, TestComplexityHotspot, AssertionComplexity, FlakyTestPattern</td><td>Testing</td><td>Test coverage, test quality</td></tr>
<tr><td>AllocationInefficiency, StringConcatenation, NestedLoops, BlockingIO, SuboptimalDataStructure, AsyncMisuse, ResourceLeak, CollectionInefficiency</td><td>Performance</td><td>Runtime efficiency, resource usage</td></tr>
<tr><td>ComplexityHotspot, DeadCode, MagicValues, Risk, Duplication, ErrorSwallowing</td><td>CodeQuality</td><td>Maintainability, reliability, code clarity</td></tr>
</tbody>
</table>
</div>
<p><strong>Language-Specific Debt Patterns:</strong></p>
<p>Some debt patterns only apply to languages with specific features:</p>
<ul>
<li><strong>BlockingIO, AsyncMisuse</strong>: Async-capable languages (Rust)</li>
<li><strong>AllocationInefficiency, ResourceLeak</strong>: Languages with manual memory management (Rust)</li>
<li><strong>Error handling patterns</strong>: Vary by language error model (Result in Rust)</li>
</ul>
<p>Debtmap automatically applies only the relevant debt patterns during analysis.</p>
<h3 id="examples-by-category"><a class="header" href="#examples-by-category">Examples by Category</a></h3>
<h4 id="architecture-debt"><a class="header" href="#architecture-debt">Architecture Debt</a></h4>
<p><strong>GodObject / GodModule</strong>: Too many responsibilities</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// God module: handles parsing, validation, storage, notifications
mod user_service {
    fn parse_user() { /* ... */ }
    fn validate_user() { /* ... */ }
    fn save_user() { /* ... */ }
    fn send_email() { /* ... */ }
    fn log_activity() { /* ... */ }
    // ... 20+ more functions
}
<span class="boring">}</span></code></pre>
<p><strong>When detected</strong>: Complexity-weighted scoring system (see detailed explanation below)
<strong>Action</strong>: Split into focused modules (parser, validator, repository, notifier)</p>
<h4 id="complexity-weighted-god-object-detection"><a class="header" href="#complexity-weighted-god-object-detection">Complexity-Weighted God Object Detection</a></h4>
<p>Debtmap uses <strong>complexity-weighted scoring</strong> for god object detection to reduce false positives on well-refactored code. This ensures that a file with 100 simple helper functions doesn’t rank higher than a file with 10 complex functions.</p>
<p><strong>The Problem:</strong></p>
<p>Traditional god object detection counts methods:</p>
<ul>
<li>File A: 100 methods (average complexity: 1.5) → Flagged as god object</li>
<li>File B: 10 methods (average complexity: 17.0) → Not flagged</li>
</ul>
<p>But File A might be a well-organized utility module with many small helpers, while File B is truly problematic with highly complex functions that need refactoring.</p>
<p><strong>The Solution:</strong></p>
<p>Debtmap weights each function by its cyclomatic complexity using this formula:</p>
<pre><code>weight = (max(1, complexity) / 3)^1.5
</code></pre>
<p><strong>Weight Examples:</strong></p>
<ul>
<li>Simple helper (complexity 1): weight ≈ 0.19</li>
<li>Baseline function (complexity 3): weight = 1.0</li>
<li>Moderate function (complexity 9): weight ≈ 5.2</li>
<li>Complex function (complexity 17): weight ≈ 13.5</li>
<li>Critical function (complexity 33): weight ≈ 36.5</li>
</ul>
<p><strong>God Object Score Calculation:</strong></p>
<pre><code>weighted_method_count = sum(weight for each function)
complexity_penalty = 0.7 if avg_complexity &lt; 3, 1.0 if 3-10, 1.5 if &gt; 10

god_object_score = (
    (weighted_method_count / threshold) * 40% +
    (field_count / threshold) * 20% +
    (responsibility_count / threshold) * 15% +
    (lines_of_code / 500) * 25%
) * complexity_penalty
</code></pre>
<p><strong>Threshold</strong>: God object detected if <code>score &gt;= 70.0</code></p>
<p><strong>Real-World Example:</strong></p>
<pre><code>shared_cache.rs:
  - 100 functions, average complexity: 1.5
  - Weighted score: ~19.0 (100 * 0.19)
  - God object score: 45.2
  - Result: Not a god object ✓

legacy_parser.rs:
  - 10 functions, average complexity: 17.0
  - Weighted score: ~135.0 (10 * 13.5)
  - God object score: 87.3
  - Result: God object detected ✓
</code></pre>
<p><strong>Benefits:</strong></p>
<ul>
<li><strong>Reduces false positives</strong> on utility modules with many simple functions</li>
<li><strong>Focuses attention</strong> on truly problematic complex modules</li>
<li><strong>Rewards good refactoring</strong> - breaking large functions into small helpers improves score</li>
<li><strong>Aligns with reality</strong> - complexity matters more than count for maintainability</li>
</ul>
<p><strong>How to View:</strong></p>
<p>When Debtmap detects a god object, the output includes:</p>
<ul>
<li>Raw method count</li>
<li>Weighted method count</li>
<li>Average complexity</li>
<li>God object score</li>
<li>Recommended module splits based on responsibility clustering</li>
</ul>
<h4 id="type-organization-debt"><a class="header" href="#type-organization-debt">Type Organization Debt</a></h4>
<p><strong>Source</strong>: Type organization patterns defined in src/priority/mod.rs:273-288, detection logic in src/organization/codebase_type_analyzer.rs</p>
<p>These patterns detect issues with how types and their associated functions are organized across the codebase.</p>
<p><strong>ScatteredType</strong>: Type with methods scattered across multiple files</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Type definition in types/user.rs
pub struct User {
    id: UserId,
    name: String,
}

// Methods scattered across files:
// In modules/auth.rs:
impl User {
    fn authenticate(&amp;self) -&gt; Result&lt;Session&gt; { /* ... */ }
}

// In modules/validation.rs:
impl User {
    fn validate_email(&amp;self) -&gt; Result&lt;()&gt; { /* ... */ }
}

// In modules/persistence.rs:
impl User {
    fn save(&amp;self) -&gt; Result&lt;()&gt; { /* ... */ }
}

// Problem: User methods spread across 4+ files!
<span class="boring">}</span></code></pre>
<p><strong>When detected</strong>: Type has methods in 2+ files (severity: Low=2 files, Medium=3-5 files, High=6+ files)
<strong>Action</strong>: Consolidate methods into primary file or create focused trait implementations
<strong>Source</strong>: Detection criteria from src/organization/codebase_type_analyzer.rs:46-47</p>
<p><strong>OrphanedFunctions</strong>: Functions that should be methods on a type</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Bad: Orphaned functions operating on User
fn validate_user_email(user: &amp;User) -&gt; Result&lt;()&gt; {
    // Email validation logic
}

fn calculate_user_age(user: &amp;User) -&gt; u32 {
    // Age calculation from birthdate
}

fn format_user_display(user: &amp;User) -&gt; String {
    // Display formatting
}

// Good: Functions converted to methods
impl User {
    fn validate_email(&amp;self) -&gt; Result&lt;()&gt; { /* ... */ }
    fn age(&amp;self) -&gt; u32 { /* ... */ }
    fn display(&amp;self) -&gt; String { /* ... */ }
}
<span class="boring">}</span></code></pre>
<p><strong>When detected</strong>: Multiple functions with shared type parameter pattern (e.g., all take <code>&amp;User</code>)
<strong>Action</strong>: Convert functions to methods on the target type
<strong>Source</strong>: Detection in src/organization/codebase_type_analyzer.rs:58-71</p>
<p><strong>UtilitiesSprawl</strong>: Utility module with poor cohesion</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Bad: utils.rs with mixed responsibilities
mod utils {
    fn parse_date(s: &amp;str) -&gt; Date { /* ... */ }
    fn validate_email(s: &amp;str) -&gt; bool { /* ... */ }
    fn calculate_hash(data: &amp;[u8]) -&gt; Hash { /* ... */ }
    fn format_currency(amount: f64) -&gt; String { /* ... */ }
    fn send_notification(msg: &amp;str) { /* ... */ }
    // ... 20+ more unrelated functions
}

// Good: Focused modules
mod date_utils { fn parse(s: &amp;str) -&gt; Date { /* ... */ } }
mod validators { fn email(s: &amp;str) -&gt; bool { /* ... */ } }
mod crypto { fn hash(data: &amp;[u8]) -&gt; Hash { /* ... */ } }
mod formatters { fn currency(amount: f64) -&gt; String { /* ... */ } }
mod notifications { fn send(msg: &amp;str) { /* ... */ } }
<span class="boring">}</span></code></pre>
<p><strong>When detected</strong>: Utility module has many functions operating on diverse types with low cohesion
<strong>Action</strong>: Split into focused modules based on domain or responsibility
<strong>Source</strong>: Detection in src/organization/codebase_type_analyzer.rs:74-80</p>
<h4 id="testing-debt"><a class="header" href="#testing-debt">Testing Debt</a></h4>
<p><strong>TestingGap</strong>: Functions with insufficient test coverage</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// 0% coverage - critical business logic untested
fn calculate_tax(amount: f64, region: &amp;str) -&gt; f64 {
    // Complex tax calculation logic
    // No tests exist for this function!
}
<span class="boring">}</span></code></pre>
<p><strong>When detected</strong>: Coverage data shows function has &lt; 80% line coverage
<strong>Action</strong>: Add unit tests to cover all branches and edge cases</p>
<p><strong>TestComplexity</strong>: Test functions too complex</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[test]
fn complex_test() {
    // Cyclomatic: 12 (too complex for a test)
    for input in test_cases {
        if input.is_special() {
            match input.type {
                /* complex test logic */
            }
        }
    }
}
<span class="boring">}</span></code></pre>
<p><strong>When detected</strong>: Test functions with cyclomatic &gt; 10 or cognitive &gt; 15
<strong>Action</strong>: Split into multiple focused tests, use test fixtures</p>
<p><strong>FlakyTestPattern</strong>: Non-deterministic tests</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[test]
fn flaky_test() {
    let result = async_operation().await;  // Timing-dependent
    thread::sleep(Duration::from_millis(100));  // Race condition!
    assert_eq!(result.status, "complete");
}
<span class="boring">}</span></code></pre>
<p><strong>When detected</strong>: Pattern analysis for timing dependencies, random values
<strong>Action</strong>: Use mocks, deterministic test data, proper async test utilities</p>
<h4 id="performance-debt"><a class="header" href="#performance-debt">Performance Debt</a></h4>
<p><strong>AllocationInefficiency</strong>: Excessive allocations</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Bad: Allocates on every iteration
fn process_items(items: &amp;[Item]) -&gt; Vec&lt;String&gt; {
    let mut results = Vec::new();
    for item in items {
        results.push(item.name.clone());  // Unnecessary clone
    }
    results
}

// Good: Pre-allocate, avoid clones
fn process_items(items: &amp;[Item]) -&gt; Vec&lt;&amp;str&gt; {
    items.iter().map(|item| item.name.as_str()).collect()
}
<span class="boring">}</span></code></pre>
<p><strong>BlockingIO</strong>: Blocking operations in async contexts</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Bad: Blocks async runtime
async fn load_data() -&gt; Result&lt;Data&gt; {
    let file = std::fs::read_to_string("data.json")?;  // Blocking!
    parse_json(&amp;file)
}

// Good: Async I/O
async fn load_data() -&gt; Result&lt;Data&gt; {
    let file = tokio::fs::read_to_string("data.json").await?;
    parse_json(&amp;file)
}
<span class="boring">}</span></code></pre>
<p><strong>NestedLoops</strong>: O(n²) or worse complexity</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Bad: O(n²) nested loops
fn find_duplicates(items: &amp;[Item]) -&gt; Vec&lt;(Item, Item)&gt; {
    let mut dupes = vec![];
    for i in 0..items.len() {
        for j in i+1..items.len() {
            if items[i] == items[j] {
                dupes.push((items[i].clone(), items[j].clone()));
            }
        }
    }
    dupes
}

// Good: O(n) with HashSet
fn find_duplicates(items: &amp;[Item]) -&gt; Vec&lt;Item&gt; {
    let mut seen = HashSet::new();
    items.iter().filter(|item| !seen.insert(item)).cloned().collect()
}
<span class="boring">}</span></code></pre>
<h4 id="code-quality-debt"><a class="header" href="#code-quality-debt">Code Quality Debt</a></h4>
<p><strong>ComplexityHotspot</strong>: Functions exceeding complexity thresholds</p>
<p><strong>Source</strong>: Categorized as CodeQuality in src/priority/mod.rs:340</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Cyclomatic: 22, Cognitive: 35
fn process_transaction(tx: Transaction, account: &amp;mut Account) -&gt; Result&lt;Receipt&gt; {
    if tx.amount &lt;= 0 {
        return Err(Error::InvalidAmount);
    }
    if account.balance &lt; tx.amount {
        if account.overdraft_enabled {
            if account.overdraft_limit &gt;= tx.amount {
                // More nested branches...
            }
        } else {
            return Err(Error::InsufficientFunds);
        }
    }
    // ... deeply nested logic with many branches
    Ok(receipt)
}
<span class="boring">}</span></code></pre>
<p><strong>When detected</strong>: Cyclomatic &gt; 10 OR Cognitive &gt; 15 (configurable)
<strong>Action</strong>: Break into smaller functions, extract validation, simplify control flow
<strong>Note</strong>: Includes entropy-adjusted complexity (adjusted_cyclomatic) when available</p>
<p><strong>DeadCode</strong>: Unreachable or unused code</p>
<p><strong>Source</strong>: Categorized as CodeQuality in src/priority/mod.rs:341</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Private function never called within module
fn obsolete_calculation(x: i32) -&gt; i32 {
    x * 2 + 5  // Dead code - no callers
}

// Public function but no external usage
pub fn deprecated_api(data: &amp;str) -&gt; Result&lt;()&gt; {
    // Unreachable in practice
    Ok(())
}
<span class="boring">}</span></code></pre>
<p><strong>When detected</strong>: Function visibility analysis + call graph shows no callers
<strong>Action</strong>: Remove dead code or document if intentionally kept for future use</p>
<p><strong>MagicValues</strong>: Unexplained literal values</p>
<p><strong>Source</strong>: Categorized as CodeQuality in src/priority/mod.rs:345</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Bad: Magic numbers
fn calculate_price(quantity: u32) -&gt; f64 {
    quantity as f64 * 19.99 + 5.0  // What are these numbers?
}

// Good: Named constants
const UNIT_PRICE: f64 = 19.99;
const SHIPPING_COST: f64 = 5.0;
fn calculate_price(quantity: u32) -&gt; f64 {
    quantity as f64 * UNIT_PRICE + SHIPPING_COST
}
<span class="boring">}</span></code></pre>
<p><strong>When detected</strong>: Numeric or string literals without clear context (excludes 0, 1, common patterns)
<strong>Action</strong>: Extract to named constants or configuration</p>
<p><strong>Duplication</strong>: Duplicated code blocks</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// File A:
fn process_user(user: User) -&gt; Result&lt;()&gt; {
    validate_email(&amp;user.email)?;
    validate_age(user.age)?;
    save_to_database(&amp;user)?;
    send_welcome_email(&amp;user.email)?;
    Ok(())
}

// File B: Duplicated validation
fn process_admin(admin: Admin) -&gt; Result&lt;()&gt; {
    validate_email(&amp;admin.email)?;  // Duplicated
    validate_age(admin.age)?;       // Duplicated
    save_to_database(&amp;admin)?;
    grant_admin_privileges(&amp;admin)?;
    Ok(())
}
<span class="boring">}</span></code></pre>
<p><strong>When detected</strong>: Similar code blocks &gt; 50 lines (configurable)
<strong>Action</strong>: Extract shared code into reusable functions</p>
<p><strong>ErrorSwallowing</strong>: Errors caught but ignored</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Bad: Error swallowed, no context
match risky_operation() {
    Ok(result) =&gt; process(result),
    Err(_) =&gt; {}, // Silent failure!
}

// Good: Error handled with context
match risky_operation() {
    Ok(result) =&gt; process(result),
    Err(e) =&gt; {
        log::error!("Risky operation failed: {}", e);
        return Err(e.into());
    }
}
<span class="boring">}</span></code></pre>
<p><strong>When detected</strong>: Empty catch blocks, ignored Results
<strong>Action</strong>: Add proper error logging and propagation</p>
<p><strong>Risk</strong>: High-risk code (complex + poorly tested)</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Cyclomatic: 18, Coverage: 20%, Risk Score: 47.6 (HIGH)
fn process_payment(tx: Transaction) -&gt; Result&lt;Receipt&gt; {
    // Complex payment logic with minimal testing
    // High risk of bugs in production
}
<span class="boring">}</span></code></pre>
<p><strong>When detected</strong>: Combines complexity metrics with coverage data
<strong>Action</strong>: Either add comprehensive tests OR refactor to reduce complexity</p>
<h3 id="debt-scoring-formula"><a class="header" href="#debt-scoring-formula">Debt Scoring Formula</a></h3>
<p>Each debt item gets a score based on priority and type:</p>
<pre><code>debt_score = priority_weight × type_weight
</code></pre>
<p><strong>Priority weights:</strong></p>
<ul>
<li>Low = 1</li>
<li>Medium = 3</li>
<li>High = 5</li>
<li>Critical = 10</li>
</ul>
<p><strong>Combined examples:</strong></p>
<ul>
<li>Low Todo = 1 × 1 = 1</li>
<li>Medium Fixme = 3 × 2 = 6</li>
<li>High Complexity = 5 × 5 = 25</li>
<li>Critical Complexity = 10 × 5 = 50</li>
</ul>
<p><strong>Total debt score</strong> = Sum of all debt item scores</p>
<p>Lower is better. Track over time to measure improvement.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="risk-scoring-1"><a class="header" href="#risk-scoring-1">Risk Scoring</a></h1>
<p>Debtmap’s risk scoring identifies code that is both complex AND poorly tested - the true risk hotspots.</p>
<h2 id="unified-scoring-system"><a class="header" href="#unified-scoring-system">Unified Scoring System</a></h2>
<p>Debtmap uses a <strong>unified scoring system</strong> (0-10 scale) as the primary prioritization mechanism. This multi-factor approach balances complexity, test coverage, and dependency impact, adjusted by function role.</p>
<p><strong>Source</strong>: <a href="../src/priority/unified_scorer.rs">src/priority/unified_scorer.rs:22-291</a></p>
<h3 id="score-scale-and-priority-classifications"><a class="header" href="#score-scale-and-priority-classifications">Score Scale and Priority Classifications</a></h3>
<p>Functions receive scores from 0 (minimal risk) to 10 (critical risk):</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Score Range</th><th>Priority</th><th>Description</th><th>Action</th></tr>
</thead>
<tbody>
<tr><td><strong>9.0-10.0</strong></td><td>Critical</td><td>Severe risk requiring immediate attention</td><td>Address immediately</td></tr>
<tr><td><strong>7.0-8.9</strong></td><td>High</td><td>Significant risk, should be addressed soon</td><td>Plan for this sprint</td></tr>
<tr><td><strong>5.0-6.9</strong></td><td>Medium</td><td>Moderate risk, plan for future work</td><td>Schedule for next sprint</td></tr>
<tr><td><strong>3.0-4.9</strong></td><td>Low</td><td>Minor risk, lower priority</td><td>Monitor and address as time permits</td></tr>
<tr><td><strong>0.0-2.9</strong></td><td>Minimal</td><td>Well-managed code</td><td>Continue monitoring</td></tr>
</tbody>
</table>
</div>
<h3 id="scoring-formula"><a class="header" href="#scoring-formula">Scoring Formula</a></h3>
<p>The unified score combines three weighted factors:</p>
<pre><code>Base Score = (Complexity Factor × Weight) + (Coverage Factor × Weight) + (Dependency Factor × Weight)

Final Score = Base Score × Role Multiplier × Purity Adjustment
</code></pre>
<p><strong>Source</strong>: <a href="../src/priority/unified_scorer.rs">src/priority/unified_scorer.rs:170-291</a> (calculate_unified_priority_with_debt)</p>
<h4 id="dynamic-weight-adjustment"><a class="header" href="#dynamic-weight-adjustment">Dynamic Weight Adjustment</a></h4>
<p><strong>IMPORTANT</strong>: Weights are dynamically adjusted based on coverage data availability.</p>
<p><strong>When coverage data is available</strong> (default):</p>
<ul>
<li><strong>Complexity</strong>: ~35-40% (via complexity_factor)</li>
<li><strong>Coverage</strong>: ~35-40% (via coverage multiplier dampening)</li>
<li><strong>Dependency</strong>: ~20-25%</li>
</ul>
<p><strong>When coverage data is NOT available</strong>:</p>
<ul>
<li><strong>Complexity</strong>: 50%</li>
<li><strong>Dependency</strong>: 25%</li>
<li><strong>Debt patterns</strong>: 25% (reserved for additive adjustments)</li>
</ul>
<p><strong>Source</strong>:</p>
<ul>
<li>With coverage: <a href="../src/priority/scoring/calculation.rs">src/priority/scoring/calculation.rs:68-82</a> (calculate_base_score_with_coverage_multiplier)</li>
<li>Without coverage: <a href="../src/priority/scoring/calculation.rs">src/priority/scoring/calculation.rs:119-129</a> (calculate_base_score_no_coverage)</li>
</ul>
<p>These weights can be adjusted in <code>.debtmap.toml</code> to match your team’s priorities.</p>
<h4 id="factor-calculations"><a class="header" href="#factor-calculations">Factor Calculations</a></h4>
<p><strong>Complexity Factor</strong> (0-10 scale):</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Source: src/priority/scoring/calculation.rs:54-59
Complexity Factor = (raw_complexity / 2.0).clamp(0.0, 10.0)

// Where raw_complexity is weighted combination:
// Default: 30% cyclomatic + 70% cognitive
// For orchestrators: 25% cyclomatic + 75% cognitive
<span class="boring">}</span></code></pre>
<p>Maps normalized complexity (0-20 range) to 0-10 scale. Uses configurable weights that prioritize cognitive complexity (70%) over cyclomatic complexity (30%) as it correlates better with defect density.</p>
<p><strong>Source</strong>: <a href="../src/config/scoring.rs">src/config/scoring.rs:221-267</a> (ComplexityWeightsConfig)</p>
<p><strong>Coverage Factor</strong> (0-10 scale):</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Source: src/priority/scoring/calculation.rs:8-21
Coverage Multiplier = 1.0 - coverage_percentage

// Applied as dampening:
Base Score × Coverage Multiplier
<span class="boring">}</span></code></pre>
<p>Coverage acts as a <strong>dampening multiplier</strong>:</p>
<ul>
<li>0% coverage → multiplier = 1.0 (no dampening)</li>
<li>50% coverage → multiplier = 0.5 (50% reduction)</li>
<li>100% coverage → multiplier = 0.0 (maximum dampening)</li>
</ul>
<p>Uncovered complex code scores higher than uncovered simple code. Well-tested code gets lower scores.</p>
<p><strong>Dependency Factor</strong> (0-10 scale):</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Source: src/priority/scoring/calculation.rs:61-66
Dependency Factor = (upstream_caller_count / 2.0).min(10.0)
<span class="boring">}</span></code></pre>
<p>Based on call graph analysis with linear scaling:</p>
<ul>
<li>0-1 upstream callers → score 0-0.5 (low impact)</li>
<li>2-4 upstream callers → score 1.0-2.0 (moderate impact)</li>
<li>5+ upstream callers → score 2.5-10.0 (high impact, capped at 10.0)</li>
</ul>
<p><strong>Critical path bonus</strong>: Functions on critical paths from entry points receive additional dependency weight.</p>
<h3 id="role-based-prioritization"><a class="header" href="#role-based-prioritization">Role-Based Prioritization</a></h3>
<p>The unified score is multiplied by a <strong>role multiplier</strong> based on the function’s semantic classification.</p>
<p><strong>Source</strong>: <a href="../src/priority/semantic_classifier/mod.rs">src/priority/semantic_classifier/mod.rs:24-33</a> (FunctionRole enum)</p>
<h4 id="role-multipliers"><a class="header" href="#role-multipliers">Role Multipliers</a></h4>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Role</th><th>Multiplier</th><th>Description</th><th>When Applied</th></tr>
</thead>
<tbody>
<tr><td><strong>EntryPoint</strong></td><td>1.5×</td><td>main(), HTTP handlers, API endpoints</td><td>User-facing code where bugs have immediate impact</td></tr>
<tr><td><strong>PureLogic</strong> (complex)</td><td>1.3×</td><td>Business logic with complexity &gt; 5.0</td><td>Critical domain functions</td></tr>
<tr><td><strong>PureLogic</strong> (simple)</td><td>1.0×</td><td>Business logic with complexity ≤ 5.0</td><td>Baseline importance for domain code</td></tr>
<tr><td><strong>Orchestrator</strong></td><td>0.8×</td><td>Coordinates 5+ other functions</td><td>Delegation-heavy code with low cognitive load</td></tr>
<tr><td><strong>PatternMatch</strong></td><td>0.6×</td><td>Simple pattern matching functions</td><td>Low complexity branching logic</td></tr>
<tr><td><strong>IOWrapper</strong></td><td>0.5×</td><td>Thin I/O layer (file, network, database)</td><td>Simple wrappers around external systems</td></tr>
<tr><td><strong>Debug</strong></td><td>0.3×</td><td>Debug/diagnostic functions</td><td>Lowest test priority</td></tr>
</tbody>
</table>
</div>
<p><strong>Source</strong>:</p>
<ul>
<li>Multiplier values: <a href="../src/priority/unified_scorer.rs">src/priority/unified_scorer.rs:385-399</a> (calculate_role_multiplier)</li>
<li>Configuration defaults: <a href="../src/config/scoring.rs">src/config/scoring.rs:147-220</a> (RoleMultipliers)</li>
</ul>
<p><strong>Note</strong>: PureLogic has a <strong>dynamic multiplier</strong> that adjusts based on complexity. Simple business logic (≤ 5.0 complexity) gets baseline priority, while complex business logic (&gt; 5.0) receives elevated priority (1.3×).</p>
<h4 id="how-role-classification-works"><a class="header" href="#how-role-classification-works">How Role Classification Works</a></h4>
<p>Debtmap identifies function roles through a rule-based classifier with specific detection heuristics:</p>
<p><strong>Source</strong>: <a href="../src/priority/semantic_classifier/mod.rs">src/priority/semantic_classifier/mod.rs:46-114</a> (classify_by_rules)</p>
<p><strong>Detection Rules (in priority order):</strong></p>
<ol>
<li>
<p><strong>EntryPoint</strong> - Detected by:</p>
<ul>
<li>Name patterns: <code>main</code>, <code>handle_*</code>, <code>run_*</code></li>
<li>Call graph analysis: no upstream callers (entry point to call graph)</li>
<li><strong>Source</strong>: Line 54</li>
</ul>
</li>
<li>
<p><strong>Debug</strong> - Detected by:</p>
<ul>
<li>Name patterns: <code>debug_*</code>, <code>dump_*</code>, <code>log_*</code>, <code>print_*</code>, <code>display_*</code>, <code>trace_*</code>, <code>*_diagnostics</code>, <code>*_debug</code>, <code>*_stats</code></li>
<li>Complexity limit: cognitive ≤ 10</li>
<li><strong>Source</strong>: Line 59, <a href="../src/priority/semantic_classifier/classifiers.rs">src/priority/semantic_classifier/classifiers.rs:14-65</a></li>
</ul>
</li>
<li>
<p><strong>Constructors</strong> (classified as PureLogic) - Detected by:</p>
<ul>
<li>Name patterns: <code>new</code>, <code>with_*</code>, <code>from_*</code>, <code>default</code>, <code>create_*</code>, <code>make_*</code>, <code>build_*</code></li>
<li>Complexity thresholds: cyclomatic ≤ 2, cognitive ≤ 3, length &lt; 15, nesting ≤ 1</li>
<li><strong>Source</strong>: Line 64, <a href="../src/priority/semantic_classifier/classifiers.rs">src/priority/semantic_classifier/classifiers.rs:67-115</a></li>
</ul>
</li>
<li>
<p><strong>Accessors</strong> (classified as IOWrapper) - Detected by:</p>
<ul>
<li>Name patterns: <code>get_*</code>, <code>is_*</code>, <code>has_*</code>, <code>can_*</code>, <code>should_*</code>, <code>as_*</code>, <code>to_*</code>, single-word accessors (<code>id</code>, <code>name</code>, <code>value</code>, etc.)</li>
<li>Complexity thresholds: cyclomatic ≤ 2, cognitive ≤ 1, length &lt; 10, nesting ≤ 1</li>
<li><strong>Source</strong>: Line 77, <a href="../src/priority/semantic_classifier/mod.rs">src/priority/semantic_classifier/mod.rs:147-177</a> (is_accessor_method)</li>
</ul>
</li>
<li>
<p><strong>PatternMatch</strong> - Detected by:</p>
<ul>
<li>Simple match/if-else chains</li>
<li>Low complexity relative to branch count</li>
<li><strong>Source</strong>: Line 99</li>
</ul>
</li>
<li>
<p><strong>IOWrapper</strong> - Detected by:</p>
<ul>
<li>Simple file/network/database operations</li>
<li>Thin wrapper around I/O primitives</li>
<li><strong>Source</strong>: Line 104</li>
</ul>
</li>
<li>
<p><strong>Orchestrator</strong> - Detected by:</p>
<ul>
<li>High delegation ratio (calls 5+ functions)</li>
<li>Low cognitive complexity relative to cyclomatic complexity</li>
<li>Coordinates other functions without complex logic</li>
<li><strong>Source</strong>: Line 109</li>
</ul>
</li>
<li>
<p><strong>PureLogic</strong> (default) - Applied when:</p>
<ul>
<li>None of the above patterns match</li>
<li>Assumed to be core business logic</li>
</ul>
</li>
</ol>
<h4 id="example-same-complexity-different-priorities"><a class="header" href="#example-same-complexity-different-priorities">Example: Same Complexity, Different Priorities</a></h4>
<p>Consider a function with base score 8.0:</p>
<pre><code>If classified as EntryPoint:
  Final Score = 8.0 × 1.5 = 12.0 (capped at 10.0) → CRITICAL priority

If classified as PureLogic (complex):
  Final Score = 8.0 × 1.3 = 10.4 (capped at 10.0) → CRITICAL priority

If classified as PureLogic (simple):
  Final Score = 8.0 × 1.0 = 8.0 → HIGH priority

If classified as Orchestrator:
  Final Score = 8.0 × 0.8 = 6.4 → MEDIUM priority

If classified as IOWrapper:
  Final Score = 8.0 × 0.5 = 4.0 → LOW priority
</code></pre>
<p>This ensures that complex code in critical paths gets higher priority than equally complex utility code.</p>
<p><strong>Real Example from Codebase</strong>:</p>
<p>A payment processing function with cyclomatic complexity 18 and cognitive complexity 25:</p>
<ul>
<li>If it directly implements business logic → <strong>PureLogic (complex)</strong> → 1.3× multiplier</li>
<li>If it mainly delegates to other payment functions → <strong>Orchestrator</strong> → 0.8× multiplier</li>
<li>If it’s a thin wrapper around a payment API → <strong>IOWrapper</strong> → 0.5× multiplier</li>
</ul>
<h3 id="coverage-propagation"><a class="header" href="#coverage-propagation">Coverage Propagation</a></h3>
<p>Coverage impact flows through the call graph using <strong>transitive coverage</strong> and <strong>indirect coverage</strong> analysis.</p>
<p><strong>Source</strong>: <a href="../src/priority/coverage_propagation.rs">src/priority/coverage_propagation.rs:291-387</a></p>
<h4 id="how-it-works-1"><a class="header" href="#how-it-works-1">How It Works</a></h4>
<p>Transitive coverage is calculated via call graph traversal with distance-based dampening:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Source: src/priority/coverage_propagation.rs:342-364
Indirect Coverage = Σ(Caller Coverage × 0.7^distance)

Where:
- distance = hops from tested code (MAX_DEPTH = 3)
- DISTANCE_DISCOUNT = 0.7 (70% per hop)
- Well-tested threshold = 0.8 (80% coverage)
<span class="boring">}</span></code></pre>
<p><strong>Implementation Details</strong>:</p>
<ol>
<li><strong>Transitive coverage</strong> is calculated via recursive call graph traversal</li>
<li>Results are stored in <code>UnifiedDebtItem.transitive_coverage</code> field (<strong>Source</strong>: <a href="../src/priority/unified_scorer.rs">src/priority/unified_scorer.rs:50</a>)</li>
<li>Weights decay exponentially with call graph depth:
<ul>
<li>1 hop away: contribution × 0.7</li>
<li>2 hops away: contribution × 0.49 (0.7²)</li>
<li>3 hops away: contribution × 0.343 (0.7³)</li>
</ul>
</li>
<li>Used to adjust coverage factor in scoring, reducing false positives for utility functions</li>
</ol>
<h4 id="coverage-urgency-calculation"><a class="header" href="#coverage-urgency-calculation">Coverage Urgency Calculation</a></h4>
<p>The system calculates <strong>coverage urgency</strong> (0-10 scale) by blending direct and transitive coverage:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Source: src/priority/coverage_propagation.rs:237-270
Effective Coverage = (Direct Coverage × 0.7) + (Transitive Coverage × 0.3)

Coverage Urgency = (1.0 - Effective Coverage) × Complexity Weight × 10.0
<span class="boring">}</span></code></pre>
<p>Complexity weighting uses logarithmic scaling to prioritize complex functions.</p>
<h4 id="example-scenarios"><a class="header" href="#example-scenarios">Example Scenarios</a></h4>
<p><strong>Scenario 1: Untested function with well-tested callers</strong></p>
<pre><code>Function A: 0% direct coverage
  Called by (1 hop):
    - handle_request (95% coverage): contributes 95% × 0.7 = 66.5%
    - process_payment (90% coverage): contributes 90% × 0.7 = 63%
    - validate_order (88% coverage): contributes 88% × 0.7 = 61.6%

Indirect coverage: ~66% (highest contributor)
Effective coverage: (0% × 0.7) + (66% × 0.3) = ~20%
Final priority: Lower than isolated 0% coverage function
</code></pre>
<p><strong>Scenario 2: Untested function on critical path</strong></p>
<pre><code>Function B: 0% direct coverage
  Called by (1 hop):
    - main (0% coverage): contributes 0% × 0.7 = 0%
    - startup (10% coverage): contributes 10% × 0.7 = 7%

Indirect coverage: ~7% (minimal coverage benefit)
Effective coverage: (0% × 0.7) + (7% × 0.3) = ~2%
Final priority: Higher - on critical path with no safety net
</code></pre>
<p><strong>Scenario 3: Multi-hop propagation</strong></p>
<pre><code>Function C: 0% direct coverage
  Called by utility_helper (40% coverage, 1 hop):
    utility_helper is called by:
      - api_handler (95% coverage, 2 hops): contributes 95% × 0.7² = 46.6%

Indirect coverage via 2-hop path: ~46%
Effective coverage: ~14%
Final priority: Moderate - benefits from indirect testing
</code></pre>
<p>Coverage propagation prevents false alarms about utility functions called only by well-tested code, while highlighting genuinely risky untested code on critical paths.</p>
<h3 id="unified-score-example"><a class="header" href="#unified-score-example">Unified Score Example</a></h3>
<p>Updated example using actual implementation:</p>
<pre><code>Function: process_payment
  Location: src/payments.rs:145

Metrics:
  - Cyclomatic complexity: 18
  - Cognitive complexity: 25
  - Test coverage: 20%
  - Upstream callers: 3
  - Classified role: PureLogic (complex, since complexity &gt; 5.0)

Step 1: Calculate raw complexity
  Raw Complexity = (cyclomatic × 0.3) + (cognitive × 0.7)
                 = (18 × 0.3) + (25 × 0.7)
                 = 5.4 + 17.5
                 = 22.9

Step 2: Normalize to 0-10 scale
  Complexity Factor = (22.9 / 2.0).clamp(0.0, 10.0)
                    = 10.0 (capped)
  // Source: src/priority/scoring/calculation.rs:54-59

Step 3: Calculate coverage multiplier
  Coverage Multiplier = 1.0 - 0.20 = 0.80
  // Source: src/priority/scoring/calculation.rs:8-21

Step 4: Calculate dependency factor
  Dependency Factor = (3 / 2.0).min(10.0) = 1.5
  // Source: src/priority/scoring/calculation.rs:61-66

Step 5: Calculate base score (with dynamic weights)
  Base Score = (Complexity Factor × weight) + (Coverage dampening) + (Dependency Factor × weight)

  // Actual implementation uses coverage as dampening multiplier
  Base = ((10.0 × 0.35) + (1.5 × 0.20)) × 0.80
       = (3.5 + 0.3) × 0.80
       = 3.04
  // Source: src/priority/scoring/calculation.rs:68-82

Step 6: Apply role multiplier
  Role Multiplier = 1.3 (PureLogic with complexity &gt; 5.0)
  // Source: src/priority/unified_scorer.rs:385-399

  Final Score = 3.04 × 1.3 = 3.95 → LOW priority

Note: The 20% coverage dampening significantly reduces the final score.
If this function had 0% coverage:
  Coverage Multiplier = 1.0 (no dampening)
  Base Score = 3.8
  Final Score = 3.8 × 1.3 = 4.94 → LOW priority

If this function had 0% coverage AND higher dependency (8 callers):
  Dependency Factor = (8 / 2.0).min(10.0) = 4.0
  Base Score = ((10.0 × 0.35) + (4.0 × 0.20)) × 1.0 = 4.3
  Final Score = 4.3 × 1.3 = 5.59 → MEDIUM priority
</code></pre>
<p><strong>Key Insight</strong>: Coverage acts as a <strong>dampening multiplier</strong>, not an additive factor. The example in the original documentation overestimated risk by treating coverage as additive. The actual implementation properly dampens scores for tested code.</p>
<h3 id="legacy-risk-scoring-pre-02x"><a class="header" href="#legacy-risk-scoring-pre-02x">Legacy Risk Scoring (Pre-0.2.x)</a></h3>
<p>Prior to the unified scoring system, Debtmap used a simpler additive risk formula. This is still available for compatibility but unified scoring is now the default and provides better prioritization.</p>
<h3 id="risk-categories"><a class="header" href="#risk-categories">Risk Categories</a></h3>
<p><strong>Note:</strong> The <code>RiskLevel</code> enum (Low, Medium, High, Critical) is used for <strong>legacy risk scoring compatibility</strong>. When using <strong>unified scoring</strong> (0-10 scale), refer to the priority classifications shown in the Unified Scoring System section above.</p>
<h4 id="legacy-risklevel-enum"><a class="header" href="#legacy-risklevel-enum">Legacy RiskLevel Enum</a></h4>
<p>For legacy risk scoring, Debtmap classifies functions into four risk levels:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub enum RiskLevel {
    Low,       // Score &lt; 10
    Medium,    // Score 10-24
    High,      // Score 25-49
    Critical,  // Score ≥ 50
}
<span class="boring">}</span></code></pre>
<p><strong>Critical</strong> (legacy score ≥ 50)</p>
<ul>
<li>High complexity (cyclomatic &gt; 15) AND low coverage (&lt; 30%)</li>
<li>Untested code that’s likely to break and hard to fix</li>
<li><strong>Action</strong>: Immediate attention required - add tests or refactor</li>
</ul>
<p><strong>High</strong> (legacy score 25-49)</p>
<ul>
<li>High complexity (cyclomatic &gt; 10) AND moderate coverage (&lt; 60%)</li>
<li>Risky code with incomplete testing</li>
<li><strong>Action</strong>: Should be addressed soon</li>
</ul>
<p><strong>Medium</strong> (legacy score 10-24)</p>
<ul>
<li>Moderate complexity (cyclomatic &gt; 5) AND low coverage (&lt; 50%)</li>
<li>OR: High complexity with good coverage</li>
<li><strong>Action</strong>: Plan for next sprint</li>
</ul>
<p><strong>Low</strong> (legacy score &lt; 10)</p>
<ul>
<li>Low complexity OR high coverage</li>
<li>Well-managed code</li>
<li><strong>Action</strong>: Monitor, low priority</li>
</ul>
<h4 id="unified-scoring-priority-levels"><a class="header" href="#unified-scoring-priority-levels">Unified Scoring Priority Levels</a></h4>
<p>When using unified scoring (default), functions are classified using the 0-10 scale:</p>
<ul>
<li><strong>Critical</strong> (9.0-10.0): Immediate attention</li>
<li><strong>High</strong> (7.0-8.9): Address this sprint</li>
<li><strong>Medium</strong> (5.0-6.9): Plan for next sprint</li>
<li><strong>Low</strong> (3.0-4.9): Monitor and address as time permits</li>
<li><strong>Minimal</strong> (0.0-2.9): Well-managed code</li>
</ul>
<p><strong>Well-tested complex code</strong> is an <strong>outcome</strong> in both systems, not a separate category:</p>
<ul>
<li>Complex function (cyclomatic 18, cognitive 25) with 95% coverage</li>
<li>Unified score: ~2.5 (Minimal priority due to coverage dampening)</li>
<li>Legacy risk score: ~8 (Low risk)</li>
<li>Falls into low-priority categories because good testing mitigates complexity</li>
<li>This is the desired state for inherently complex business logic</li>
</ul>
<h3 id="legacy-risk-calculation"><a class="header" href="#legacy-risk-calculation">Legacy Risk Calculation</a></h3>
<p><strong>Note:</strong> The legacy risk calculation is still supported for compatibility but has been superseded by the unified scoring system (see above). Unified scoring provides better prioritization through its multi-factor, weighted approach with role-based adjustments.</p>
<p>The legacy risk score uses a simpler additive formula:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>risk_score = complexity_factor + coverage_factor + debt_factor

where:
  complexity_factor = (cyclomatic / 5) + (cognitive / 10)
  coverage_factor = (1 - coverage_percentage) × 50
  debt_factor = debt_score / 10  // If debt data available
<span class="boring">}</span></code></pre>
<p><strong>Note on debt_score</strong>: The <code>debt_score</code> comes from <strong>DebtAggregator</strong> which combines multiple debt dimensions:</p>
<ul>
<li>Testing debt (unwrap calls, untested error paths)</li>
<li>Resource debt (unclosed files, memory leaks)</li>
<li>Duplication debt (code clones)</li>
</ul>
<p><strong>Source</strong>: <a href="../src/priority/debt_aggregator">src/priority/debt_aggregator/</a></p>
<p><strong>Example (legacy scoring):</strong></p>
<pre><code>Function: process_payment
  - Cyclomatic complexity: 18
  - Cognitive complexity: 25
  - Coverage: 20%
  - Debt score: 15 (from DebtAggregator)

Calculation:
  complexity_factor = (18 / 5) + (25 / 10) = 3.6 + 2.5 = 6.1
  coverage_factor = (1 - 0.20) × 50 = 40
  debt_factor = 15 / 10 = 1.5

  risk_score = 6.1 + 40 + 1.5 = 47.6 (HIGH RISK)
</code></pre>
<p><strong>When to use legacy scoring:</strong></p>
<ul>
<li>Comparing with historical data from older Debtmap versions</li>
<li>Teams with existing workflows built around the old scale</li>
<li>Gradual migration to unified scoring</li>
</ul>
<p><strong>Why unified scoring is better:</strong></p>
<ul>
<li>Normalized 0-10 scale is more intuitive</li>
<li>Dynamic weights adjust based on coverage data availability</li>
<li>Role multipliers adjust priority based on function importance</li>
<li>Coverage propagation reduces false positives for utility functions</li>
<li>Purity adjustments reward functional programming patterns</li>
</ul>
<h3 id="test-effort-assessment"><a class="header" href="#test-effort-assessment">Test Effort Assessment</a></h3>
<p>Debtmap estimates testing difficulty based on complexity metrics using an advanced effort model.</p>
<p><strong>Source</strong>: <a href="../src/risk/roi/effort.rs">src/risk/roi/effort.rs</a> (AdvancedEffortModel)</p>
<h4 id="how-effort-is-calculated"><a class="header" href="#how-effort-is-calculated">How Effort is Calculated</a></h4>
<p>Test effort estimation involves two components:</p>
<ol>
<li>
<p><strong>Test case count</strong>: Estimated from <strong>cyclomatic complexity</strong> (branch coverage)</p>
<ul>
<li>Each branch represents a code path that needs testing</li>
<li>Formula approximates test cases needed for comprehensive branch coverage</li>
</ul>
</li>
<li>
<p><strong>Time estimate</strong>: Calculated from <strong>cognitive complexity</strong> (comprehension difficulty)</p>
<ul>
<li>Higher cognitive complexity means more time to understand and write tests</li>
<li>Includes setup cost, assertion cost, and complexity multipliers</li>
<li>Optional learning system can adjust estimates based on historical data</li>
</ul>
</li>
</ol>
<p><strong>Difficulty Levels:</strong></p>
<ul>
<li><strong>Trivial</strong> (cognitive &lt; 5): 1-2 test cases, &lt; 1 hour</li>
<li><strong>Simple</strong> (cognitive 5-10): 3-5 test cases, 1-2 hours</li>
<li><strong>Moderate</strong> (cognitive 10-20): 6-10 test cases, 2-4 hours</li>
<li><strong>Complex</strong> (cognitive 20-40): 11-20 test cases, 4-8 hours</li>
<li><strong>VeryComplex</strong> (cognitive &gt; 40): 20+ test cases, 8+ hours</li>
</ul>
<p><strong>Test Effort includes:</strong></p>
<ul>
<li><strong>Cognitive load</strong>: How hard to understand the function</li>
<li><strong>Branch count</strong> (cyclomatic): Number of paths to test</li>
<li><strong>Recommended test cases</strong>: Estimated from cyclomatic complexity</li>
<li><strong>Estimated hours</strong>: Derived from cognitive complexity with setup overhead</li>
</ul>
<h3 id="risk-distribution"><a class="header" href="#risk-distribution">Risk Distribution</a></h3>
<p>Debtmap provides codebase-wide risk metrics:</p>
<pre><code class="language-json">{
  "risk_distribution": {
    "critical_count": 12,
    "high_count": 45,
    "medium_count": 123,
    "low_count": 456,
    "minimal_count": 234,
    "total_functions": 870
  },
  "codebase_risk_score": 1247.5
}
</code></pre>
<p><strong>Interpreting distribution:</strong></p>
<ul>
<li><strong>Healthy codebase</strong>: Most functions in Low/Minimal priority (unified scoring) or Low/WellTested (legacy)</li>
<li><strong>Needs attention</strong>: Many Critical/High priority functions</li>
<li><strong>Technical debt</strong>: High codebase risk score</li>
</ul>
<h4 id="legacy-vs-unified-risk-distribution-fields"><a class="header" href="#legacy-vs-unified-risk-distribution-fields">Legacy vs Unified Risk Distribution Fields</a></h4>
<p><strong>IMPORTANT</strong>: The field names differ between legacy and unified scoring systems:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Unified Scoring (0-10 scale)</th><th>Legacy Scoring (RiskCategory enum)</th></tr>
</thead>
<tbody>
<tr><td><code>minimal_count</code> (0-2.9)</td><td>Not present</td></tr>
<tr><td><code>low_count</code> (3.0-4.9)</td><td><code>low_count</code></td></tr>
<tr><td><code>medium_count</code> (5.0-6.9)</td><td><code>medium_count</code></td></tr>
<tr><td><code>high_count</code> (7.0-8.9)</td><td><code>high_count</code></td></tr>
<tr><td><code>critical_count</code> (9.0-10.0)</td><td><code>critical_count</code></td></tr>
<tr><td>Not present</td><td><code>well_tested_count</code> (legacy outcome)</td></tr>
</tbody>
</table>
</div>
<p><strong>Sources</strong>:</p>
<ul>
<li>Unified priority tiers: <a href="../src/priority/tiers.rs">src/priority/tiers.rs</a></li>
<li>Legacy RiskCategory enum: <a href="../src/risk/mod.rs">src/risk/mod.rs:36-42</a></li>
</ul>
<p><strong>Note on minimal_count:</strong></p>
<p>In unified scoring (0-10 scale), <code>minimal_count</code> represents functions scoring 0-2.9, which includes:</p>
<ul>
<li>Simple utility functions with low complexity</li>
<li>Helper functions with minimal risk</li>
<li>Well-tested complex code that scores low due to coverage dampening</li>
</ul>
<p>This is not a separate risk category but an <strong>outcome</strong> of the unified scoring system. Complex business logic with 95% test coverage appropriately receives a minimal score (0-2.9), reflecting that good testing mitigates complexity risk.</p>
<p><strong>When using legacy scoring</strong>, there is <strong>NO</strong> <code>minimal_count</code> field. Instead, you’ll see <code>well_tested_count</code> which represents functions that are both complex and well-tested (the desired outcome).</p>
<h3 id="testing-recommendations"><a class="header" href="#testing-recommendations">Testing Recommendations</a></h3>
<p>When coverage data is provided, Debtmap generates prioritized testing recommendations with ROI analysis.</p>
<p><strong>Source</strong>: <a href="../src/risk/roi/mod.rs">src/risk/roi/mod.rs:66-113</a></p>
<h4 id="roi-calculation"><a class="header" href="#roi-calculation">ROI Calculation</a></h4>
<p>The ROI calculation is much richer than a simple risk/effort ratio. It includes cascade impacts, module multipliers, and complexity weighting:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Source: src/risk/roi/mod.rs:66-113
ROI = ((Direct_Impact × Module_Multiplier) + (Cascade_Impact × Cascade_Weight))
      × Dependency_Factor × Complexity_Weight / Adjusted_Effort
<span class="boring">}</span></code></pre>
<p><strong>Formula Components:</strong></p>
<ol>
<li>
<p><strong>Direct Impact</strong>: Risk reduction from testing this function directly</p>
</li>
<li>
<p><strong>Module Multiplier</strong> (based on module type):</p>
<ul>
<li>EntryPoint = 2.0 (highest priority for user-facing code)</li>
<li>Core = 1.5 (domain logic)</li>
<li>Api = 1.2 (API endpoints)</li>
<li>Model = 1.1 (data models)</li>
<li>IO = 1.0 (baseline for I/O operations)</li>
</ul>
</li>
<li>
<p><strong>Cascade Impact</strong>: Risk reduction in dependent functions</p>
<ul>
<li>Calculated using cascade analyzer</li>
<li><strong>Cascade Weight</strong>: Configurable (default 0.5)</li>
<li><strong>Max Cascade Depth</strong>: 3 hops (configurable)</li>
</ul>
</li>
<li>
<p><strong>Dependency Factor</strong>: Amplifies ROI based on number of dependents</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>Dependency_Factor = 1.0 + min(dependent_count × 0.1, 1.0)
<span class="boring">}</span></code></pre>
<ul>
<li>Capped at 2.0× multiplier</li>
<li>Rewards testing functions with many dependents</li>
</ul>
</li>
<li>
<p><strong>Complexity Weight</strong>: Penalizes trivial delegation functions</p>
<ul>
<li>(cyclomatic=1, cognitive=0-1): 0.1 (trivial delegation)</li>
<li>(cyclomatic=1, cognitive=2-3): 0.3 (very simple)</li>
<li>(cyclomatic=2-3, any): 0.5 (simple)</li>
<li>(cyclomatic=4-5, any): 0.7 (moderate)</li>
<li>Other: 1.0 (complex, full weight)</li>
</ul>
</li>
<li>
<p><strong>Adjusted Effort</strong>: Base effort adjusted by learning system (if enabled)</p>
<ul>
<li>Learning system tracks historical test writing effort</li>
<li>Adjusts estimates based on actual time spent</li>
</ul>
</li>
</ol>
<p><strong>ROI Scaling</strong> (for intuitive 0-10 scale):</p>
<ul>
<li>raw_roi &gt; 20.0: <code>10.0 + ln(raw_roi - 20.0)</code> (logarithmic dampening)</li>
<li>10.0 &lt; raw_roi ≤ 20.0: <code>5.0 + (raw_roi - 20.0) × 0.5</code> (linear dampening)</li>
<li>Otherwise: raw_roi (no scaling)</li>
</ul>
<p><strong>Sources</strong>:</p>
<ul>
<li>ROI model: <a href="../src/risk/roi/models.rs">src/risk/roi/models.rs:4-11</a></li>
<li>Effort estimation: <a href="../src/risk/roi/effort.rs">src/risk/roi/effort.rs</a></li>
<li>Cascade impact: <a href="../src/risk/roi/cascade.rs">src/risk/roi/cascade.rs</a></li>
</ul>
<h4 id="example-roi-output"><a class="header" href="#example-roi-output">Example ROI Output</a></h4>
<pre><code class="language-json">{
  "function": "process_transaction",
  "file": "src/payments.rs",
  "line": 145,
  "current_risk": 47.6,
  "potential_risk_reduction": 35.2,
  "test_effort_estimate": {
    "estimated_difficulty": "Complex",
    "cognitive_load": 25,
    "branch_count": 18,
    "recommended_test_cases": 12,
    "estimated_hours": 6.5
  },
  "roi": 8.2,
  "roi_breakdown": {
    "direct_impact": 35.2,
    "module_multiplier": 1.5,
    "cascade_impact": 12.4,
    "cascade_weight": 0.5,
    "dependency_factor": 1.3,
    "complexity_weight": 1.0,
    "adjusted_effort": 6.5
  },
  "rationale": "High complexity with low coverage (20%) and 3 downstream dependencies. Testing will reduce risk by 74%. Cascade effect improves 8 dependent functions.",
  "dependencies": {
    "upstream_callers": ["handle_payment_request"],
    "downstream_callees": ["validate_amount", "check_balance", "record_transaction"],
    "dependent_count": 13
  },
  "confidence": 0.85
}
</code></pre>
<p><strong>Interpreting ROI:</strong></p>
<ul>
<li><strong>ROI &gt; 5.0</strong>: Excellent return on investment, prioritize highly</li>
<li><strong>ROI 3.0-5.0</strong>: Good return, address soon</li>
<li><strong>ROI 1.0-3.0</strong>: Moderate return, plan for future work</li>
<li><strong>ROI &lt; 1.0</strong>: Low return, consider other priorities</li>
</ul>
<p><strong>Key Insight</strong>: The cascade impact calculation means that testing a critical utility function with many dependents can have higher ROI than testing a complex but isolated function. This helps identify “force multiplier” tests that improve coverage across multiple modules.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="interpreting-results"><a class="header" href="#interpreting-results">Interpreting Results</a></h1>
<h2 id="interpreting-results-1"><a class="header" href="#interpreting-results-1">Interpreting Results</a></h2>
<h3 id="understanding-output-formats"><a class="header" href="#understanding-output-formats">Understanding Output Formats</a></h3>
<p>Debtmap provides three output formats:</p>
<p><strong>Terminal</strong> (default): Human-readable with colors and tables</p>
<pre><code class="language-bash">debtmap analyze .
</code></pre>
<p><strong>JSON</strong>: Machine-readable for CI/CD integration</p>
<pre><code class="language-bash">debtmap analyze . --format json --output report.json
</code></pre>
<p><strong>Markdown</strong>: Documentation-friendly</p>
<pre><code class="language-bash">debtmap analyze . --format markdown --output report.md
</code></pre>
<h3 id="json-structure"><a class="header" href="#json-structure">JSON Structure</a></h3>
<p>Debtmap uses a unified JSON format (spec 108) that provides consistent structure for all debt items. The output is generated by converting the internal <code>UnifiedAnalysis</code> to <code>UnifiedOutput</code> format (src/output/unified.rs).</p>
<p><strong>Top-level JSON structure:</strong></p>
<pre><code class="language-json">{
  "format_version": "1.0",
  "metadata": {
    "debtmap_version": "0.2.0",
    "generated_at": "2025-12-04T12:00:00Z",
    "project_root": "/path/to/project",
    "analysis_type": "full"
  },
  "summary": {
    "total_items": 45,
    "total_debt_score": 2847.3,
    "debt_density": 113.89,
    "total_loc": 25000,
    "by_type": {
      "File": 3,
      "Function": 42
    },
    "by_category": {
      "Architecture": 5,
      "Testing": 23,
      "Performance": 7,
      "CodeQuality": 10
    },
    "score_distribution": {
      "critical": 2,
      "high": 8,
      "medium": 18,
      "low": 17
    }
  },
  "items": [ /* array of UnifiedDebtItemOutput */ ]
}
</code></pre>
<p><strong>Individual debt item structure (function-level):</strong></p>
<pre><code class="language-json">{
  "type": "Function",
  "score": 87.3,
  "category": "Testing",
  "priority": "high",
  "location": {
    "file": "src/main.rs",
    "line": 42,
    "function": "process_data"
  },
  "metrics": {
    "cyclomatic_complexity": 15,
    "cognitive_complexity": 22,
    "length": 68,
    "nesting_depth": 4,
    "coverage": 0.0,
    "uncovered_lines": [42, 43, 44, 45, 46],
    "entropy_score": 0.65
  },
  "debt_type": {
    "ComplexityHotspot": {
      "cyclomatic": 15,
      "cognitive": 22,
      "adjusted_cyclomatic": null
    }
  },
  "function_role": "BusinessLogic",
  "purity_analysis": {
    "is_pure": false,
    "confidence": 0.8,
    "side_effects": ["file_io", "network"]
  },
  "dependencies": {
    "upstream_count": 2,
    "downstream_count": 3,
    "upstream_callers": ["main", "process_request"],
    "downstream_callees": ["validate", "save", "notify"]
  },
  "recommendation": {
    "action": "Add test coverage for complex function",
    "priority": "High",
    "implementation_steps": [
      "Write unit tests covering all 15 branches",
      "Consider extracting validation logic to reduce complexity"
    ]
  },
  "impact": {
    "coverage_improvement": 0.85,
    "complexity_reduction": 12.0,
    "risk_reduction": 3.7
  },
  "adjusted_complexity": {
    "dampened_cyclomatic": 12.5,
    "dampening_factor": 0.83
  },
  "complexity_pattern": "validation",
  "pattern_type": "state_machine",
  "pattern_confidence": 0.72
}
</code></pre>
<p><strong>Source:</strong> JSON structure defined in src/output/unified.rs:18-24 (UnifiedOutput), lines 66-71 (UnifiedDebtItemOutput enum), lines 156-183 (FunctionDebtItemOutput)</p>
<p><strong>Note:</strong> The <code>"type"</code> field uses a tagged enum format where the value is either <code>"Function"</code> or <code>"File"</code>. All items have consistent top-level fields (<code>score</code>, <code>category</code>, <code>priority</code>, <code>location</code>) regardless of type, simplifying filtering and sorting across mixed results.</p>
<h3 id="reading-function-metrics"><a class="header" href="#reading-function-metrics">Reading Function Metrics</a></h3>
<p><strong>Key fields in <code>metrics</code> object:</strong></p>
<ul>
<li><code>cyclomatic_complexity</code>: Decision points - guides test case count (src/output/unified.rs:195)</li>
<li><code>cognitive_complexity</code>: Understanding difficulty - guides refactoring priority</li>
<li><code>length</code>: Lines of code - signals SRP violations</li>
<li><code>nesting_depth</code>: Indentation depth - signals need for extraction</li>
<li><code>coverage</code>: Test coverage percentage (0.0-1.0, optional if no coverage data)</li>
<li><code>uncovered_lines</code>: Array of line numbers not covered by tests (optional)</li>
<li><code>entropy_score</code>: Pattern analysis score for false positive reduction (0.0-1.0, optional)</li>
</ul>
<p><strong>Fields at item level:</strong></p>
<ul>
<li><code>function_role</code>: Function importance (<code>"EntryPoint"</code>, <code>"BusinessLogic"</code>, <code>"Utility"</code>, <code>"TestHelper"</code>) - affects score multiplier (src/priority/mod.rs:231)</li>
<li><code>purity_analysis</code>: Whether function has side effects (optional) - affects testability assessment
<ul>
<li><code>is_pure</code>: Boolean indicating no side effects</li>
<li><code>confidence</code>: How certain (0.0-1.0)</li>
<li><code>side_effects</code>: Array of detected side effect types (e.g., “file_io”, “network”, “mutation”)</li>
</ul>
</li>
<li><code>dependencies</code>: Call graph relationships
<ul>
<li><code>upstream_count</code>: Number of functions that call this one (impact radius)</li>
<li><code>downstream_count</code>: Number of functions this calls (complexity)</li>
<li><code>upstream_callers</code>: Names of calling functions (optional, included if count &gt; 0)</li>
<li><code>downstream_callees</code>: Names of called functions (optional, included if count &gt; 0)</li>
</ul>
</li>
</ul>
<p><strong>Complexity adjustment fields:</strong></p>
<ul>
<li><code>adjusted_complexity</code>: Entropy-based dampening applied (optional, included when dampening factor &lt; 1.0)
<ul>
<li><code>dampened_cyclomatic</code>: Reduced complexity after pattern analysis</li>
<li><code>dampening_factor</code>: Multiplier applied (e.g., 0.83 = 17% reduction)</li>
</ul>
</li>
<li><code>complexity_pattern</code>: Detected pattern name (e.g., “validation”, “dispatch”, “error_handling”)</li>
<li><code>pattern_type</code>: Pattern category (e.g., “state_machine”, “coordinator”) - high-level classification</li>
<li><code>pattern_confidence</code>: Confidence in pattern detection (0.0-1.0, shown if ≥ 0.5)</li>
</ul>
<p><strong>Pattern detection and complexity adjustment:</strong></p>
<p>Debtmap detects common code patterns that appear complex but are actually maintainable (src/complexity/entropy_analysis.rs). When detected with high confidence (≥ 0.5), complexity metrics are dampened:</p>
<p><strong>Validation patterns</strong> - Repetitive input checking:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Appears complex (cyclomatic = 15) but is repetitive
if field1.is_empty() { return Err(...) }
if field2.is_empty() { return Err(...) }
// ... repeated for many fields
<span class="boring">}</span></code></pre>
<p>Dampening: ~20-40% reduction if similarity is high</p>
<p><strong>State machine patterns</strong> - Structured state transitions:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>match state {
    State::Init =&gt; { /* transition logic */ }
    State::Running =&gt; { /* transition logic */ }
    // ... many similar cases
}
<span class="boring">}</span></code></pre>
<p>Dampening: ~30-50% reduction for consistent transition logic</p>
<p><strong>Error handling patterns</strong> - Systematic error checking:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let x = operation1().map_err(|e| /* wrap error */)?;
let y = operation2().map_err(|e| /* wrap error */)?;
// ... repeated error wrapping
<span class="boring">}</span></code></pre>
<p>Dampening: ~15-30% reduction for consistent error propagation</p>
<p><strong>When to trust adjusted_complexity:</strong></p>
<ul>
<li><code>pattern_confidence ≥ 0.7</code>: High confidence, use <code>dampened_cyclomatic</code> for priority decisions</li>
<li><code>pattern_confidence 0.5-0.7</code>: Moderate confidence, consider both original and dampened values</li>
<li><code>pattern_confidence &lt; 0.5</code> or missing: Use original <code>cyclomatic_complexity</code></li>
</ul>
<p><strong>Entropy score interpretation:</strong></p>
<ul>
<li><code>entropy_score ≥ 0.7</code>: High entropy, genuinely complex code - prioritize refactoring</li>
<li><code>entropy_score 0.4-0.7</code>: Moderate entropy, some repetition - review manually</li>
<li><code>entropy_score &lt; 0.4</code>: Low entropy, highly repetitive pattern - likely false positive if flagged complex</li>
</ul>
<h3 id="prioritizing-work"><a class="header" href="#prioritizing-work">Prioritizing Work</a></h3>
<p>Debtmap provides multiple prioritization strategies, with <strong>unified scoring (0-10 scale)</strong> as the recommended default for most workflows:</p>
<p><strong>1. By Unified Score (default - recommended)</strong></p>
<pre><code class="language-bash">debtmap analyze . --top 10
</code></pre>
<p>Shows top 10 items by <strong>combined complexity, coverage, and dependency factors</strong>, weighted and adjusted by function role.</p>
<p><strong>Why use unified scoring:</strong></p>
<ul>
<li>Balances complexity (40%), coverage (40%), and dependency impact (20%)</li>
<li>Adjusts for function importance (entry points prioritized over utilities)</li>
<li>Normalized 0-10 scale is intuitive and consistent</li>
<li>Reduces false positives through coverage propagation</li>
<li>Best for <strong>sprint planning</strong> and <strong>function-level refactoring decisions</strong></li>
</ul>
<p><strong>Example:</strong></p>
<pre><code class="language-bash"># Show top 20 critical items
debtmap analyze . --min-priority 7.0 --top 20

# Focus on high-impact functions (score &gt;= 7.0)
debtmap analyze . --format json | jq '.functions[] | select(.unified_score &gt;= 7.0)'
</code></pre>
<p><strong>2. By Risk Category (legacy compatibility)</strong></p>
<pre><code class="language-bash">debtmap analyze . --min-priority high
</code></pre>
<p>Shows only HIGH and CRITICAL priority items using legacy risk scoring.</p>
<p><strong>Note:</strong> Legacy risk scoring uses additive formulas and unbounded scales. Prefer unified scoring for new workflows.</p>
<p><strong>3. By Debt Type</strong></p>
<pre><code class="language-bash">debtmap analyze . --filter Architecture,Testing
</code></pre>
<p>Focuses on specific categories:</p>
<ul>
<li><code>Architecture</code>: God objects, complexity, dead code</li>
<li><code>Testing</code>: Coverage gaps, test quality</li>
<li><code>Performance</code>: Resource leaks, inefficiencies</li>
<li><code>CodeQuality</code>: Code smells, maintainability</li>
</ul>
<p><strong>4. By ROI (with coverage)</strong></p>
<pre><code class="language-bash">debtmap analyze . --lcov coverage.lcov --top 20
</code></pre>
<p>Prioritizes by return on investment for testing/refactoring. Combines unified scoring with test effort estimates to identify high-value work.</p>
<p><strong>Choosing the right strategy:</strong></p>
<ul>
<li><strong>Sprint planning for developers</strong>: Use unified scoring (<code>--top N</code>)</li>
<li><strong>Architectural review</strong>: Use tiered prioritization (<code>--summary</code>)</li>
<li><strong>Category-focused work</strong>: Use debt type filtering (<code>--filter</code>)</li>
<li><strong>Testing priorities</strong>: Use ROI analysis with coverage data (<code>--lcov</code>)</li>
<li><strong>Historical comparisons</strong>: Use legacy risk scoring (for consistency with old reports)</li>
</ul>
<h3 id="tiered-prioritization"><a class="header" href="#tiered-prioritization">Tiered Prioritization</a></h3>
<p><strong>Note:</strong> Tiered prioritization uses <strong>traditional debt scoring</strong> (additive, higher = worse) and is complementary to the unified scoring system (0-10 scale). Both systems can be used together:</p>
<ul>
<li><strong>Unified scoring</strong> (0-10 scale): Best for <strong>function-level prioritization</strong> and sprint planning</li>
<li><strong>Tiered prioritization</strong> (debt tiers): Best for <strong>architectural focus</strong> and strategic debt planning</li>
</ul>
<p>Use <code>--summary</code> for tiered view focusing on architectural issues, or default output for function-level unified scores.</p>
<p>Debtmap uses a tier-based system to map debt scores to actionable priority levels. Each tier includes effort estimates and strategic guidance for efficient debt remediation.</p>
<h4 id="tier-levels"><a class="header" href="#tier-levels">Tier Levels</a></h4>
<p>The <code>Tier</code> enum defines four priority levels based on score thresholds:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub enum Tier {
    Critical,  // Score ≥ 90
    High,      // Score 70-89.9
    Moderate,  // Score 50-69.9
    Low,       // Score &lt; 50
}
<span class="boring">}</span></code></pre>
<p><strong>Score-to-Tier Mapping:</strong></p>
<ul>
<li><strong>Critical</strong> (≥ 90): Immediate action required - blocks progress</li>
<li><strong>High</strong> (70-89.9): Should be addressed this sprint</li>
<li><strong>Moderate</strong> (50-69.9): Plan for next sprint</li>
<li><strong>Low</strong> (&lt; 50): Background maintenance work</li>
</ul>
<h4 id="effort-estimates-per-tier"><a class="header" href="#effort-estimates-per-tier">Effort Estimates Per Tier</a></h4>
<p>Each tier includes estimated effort based on typical remediation patterns:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Tier</th><th>Estimated Effort</th><th>Typical Work</th></tr>
</thead>
<tbody>
<tr><td><strong>Critical</strong></td><td>1-2 days</td><td>Major refactoring, comprehensive testing, architectural changes</td></tr>
<tr><td><strong>High</strong></td><td>2-4 hours</td><td>Extract functions, add test coverage, fix resource leaks</td></tr>
<tr><td><strong>Moderate</strong></td><td>1-2 hours</td><td>Simplify logic, reduce duplication, improve error handling</td></tr>
<tr><td><strong>Low</strong></td><td>30 minutes</td><td>Address TODOs, minor cleanup, documentation</td></tr>
</tbody>
</table>
</div>
<p><strong>Effort calculation considers:</strong></p>
<ul>
<li>Complexity metrics (cyclomatic, cognitive)</li>
<li>Test coverage gaps</li>
<li>Number of dependencies (upstream/downstream)</li>
<li>Debt category (Architecture debt takes longer than CodeQuality)</li>
</ul>
<h4 id="tiered-display-grouping"><a class="header" href="#tiered-display-grouping">Tiered Display Grouping</a></h4>
<p><strong>Note:</strong> <code>TieredDisplay</code> is an internal structure (src/priority/mod.rs:515) used for terminal output formatting and is <strong>not serialized to JSON</strong>. JSON output includes individual items with their <code>priority</code> field (<code>critical</code>, <code>high</code>, <code>medium</code>, <code>low</code>) based on score thresholds (src/output/unified.rs:74-81).</p>
<p>The internal <code>TieredDisplay</code> structure groups similar debt items for batch action recommendations in terminal output:</p>
<p><strong>Grouping strategy:</strong></p>
<ul>
<li>Groups items by tier (Critical/High/Moderate/Low) and similarity pattern</li>
<li>Prevents grouping of god objects (always show individually)</li>
<li>Prevents grouping of Critical items (each needs individual attention)</li>
<li>Suggests batch actions for similar Low/Moderate items in terminal output</li>
</ul>
<p><strong>To view tiered grouping in JSON, use priority filtering:</strong></p>
<pre><code class="language-bash"># Get all Critical items (priority: "critical")
debtmap analyze . --format json | jq '.items[] | select(.priority == "critical")'

# Get High priority items
debtmap analyze . --format json | jq '.items[] | select(.priority == "high")'

# Count items by priority
debtmap analyze . --format json | jq '.summary.score_distribution'
</code></pre>
<p>The <code>score_distribution</code> in the summary provides counts for each priority tier.</p>
<h4 id="using-tiered-prioritization"><a class="header" href="#using-tiered-prioritization">Using Tiered Prioritization</a></h4>
<p><strong>1. Start with Critical tier:</strong></p>
<pre><code class="language-bash">debtmap analyze . --min-priority critical
</code></pre>
<p>Focus on items with score ≥ 90. These typically represent:</p>
<ul>
<li>Complex functions with 0% coverage</li>
<li>God objects blocking feature development</li>
<li>Critical resource leaks or security issues</li>
</ul>
<p><strong>2. Plan High tier work:</strong></p>
<pre><code class="language-bash">debtmap analyze . --min-priority high --format json &gt; sprint-plan.json
</code></pre>
<p>Schedule 2-4 hours per item for this sprint. Look for:</p>
<ul>
<li>Functions approaching complexity thresholds</li>
<li>Moderate coverage gaps on important code paths</li>
<li>Performance bottlenecks with clear solutions</li>
</ul>
<p><strong>3. Batch Moderate tier items:</strong></p>
<pre><code class="language-bash">debtmap analyze . --min-priority moderate
</code></pre>
<p>Review batch recommendations. Examples:</p>
<ul>
<li>“10 validation functions detected - extract common pattern”</li>
<li>“5 similar test files with duplication - create shared fixtures”</li>
<li>“8 functions with magic values - create constants module”</li>
</ul>
<p><strong>4. Schedule Low tier background work:</strong>
Address during slack time or as warm-up tasks for new contributors.</p>
<h4 id="strategic-guidance-by-tier"><a class="header" href="#strategic-guidance-by-tier">Strategic Guidance by Tier</a></h4>
<p><strong>Critical Tier Strategy:</strong></p>
<ul>
<li><strong>Block new features</strong> until addressed</li>
<li><strong>Pair programming</strong> recommended for complex items</li>
<li><strong>Architectural review</strong> before major refactoring</li>
<li><strong>Comprehensive testing</strong> after changes</li>
</ul>
<p><strong>High Tier Strategy:</strong></p>
<ul>
<li><strong>Sprint planning priority</strong></li>
<li><strong>Impact analysis</strong> before changes</li>
<li><strong>Code review</strong> from senior developers</li>
<li><strong>Integration testing</strong> after changes</li>
</ul>
<p><strong>Moderate Tier Strategy:</strong></p>
<ul>
<li><strong>Batch similar items</strong> for efficiency</li>
<li><strong>Extract patterns</strong> across multiple files</li>
<li><strong>Incremental improvement</strong> over multiple PRs</li>
<li><strong>Regression testing</strong> for affected areas</li>
</ul>
<p><strong>Low Tier Strategy:</strong></p>
<ul>
<li><strong>Good first issues</strong> for new contributors</li>
<li><strong>Documentation improvements</strong></li>
<li><strong>Code cleanup</strong> during refactoring nearby code</li>
<li><strong>Technical debt gardening</strong> sessions</li>
</ul>
<h3 id="categorized-debt-analysis"><a class="header" href="#categorized-debt-analysis">Categorized Debt Analysis</a></h3>
<p><strong>Note:</strong> <code>CategorizedDebt</code> is an internal analysis structure (src/priority/mod.rs:419) used for query operations and markdown formatting. It is <strong>not serialized to JSON output</strong>. The JSON output uses <code>by_category</code> in the summary section instead (see JSON Structure above).</p>
<p>Debtmap’s internal <code>CategorizedDebt</code> analysis groups debt items by category and identifies cross-category dependencies. This analysis powers the markdown output and internal query methods but is not directly exposed in JSON format.</p>
<h4 id="categorysummary"><a class="header" href="#categorysummary">CategorySummary</a></h4>
<p>Each category gets a summary with metrics for planning:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct CategorySummary {
    pub category: DebtCategory,
    pub total_score: f64,
    pub item_count: usize,
    pub estimated_effort_hours: f64,
    pub average_severity: f64,
    pub top_items: Vec&lt;DebtItem&gt;,  // Up to 5 highest priority
}
<span class="boring">}</span></code></pre>
<p><strong>Effort estimation formulas:</strong></p>
<ul>
<li><strong>Architecture debt</strong>: <code>complexity_score / 10 × 2</code> hours (structural changes take longer)</li>
<li><strong>Testing debt</strong>: <code>complexity_score / 10 × 1.5</code> hours (writing tests)</li>
<li><strong>Performance debt</strong>: <code>complexity_score / 10 × 1.8</code> hours (profiling + optimization)</li>
<li><strong>CodeQuality debt</strong>: <code>complexity_score / 10 × 1.2</code> hours (refactoring)</li>
</ul>
<p><strong>Example category summary:</strong></p>
<pre><code class="language-json">{
  "category": "Architecture",
  "total_score": 487.5,
  "item_count": 15,
  "estimated_effort_hours": 97.5,
  "average_severity": 32.5,
  "top_items": [
    {
      "debt_type": "GodObject",
      "file": "src/services/user_service.rs",
      "score": 95.0,
      "estimated_effort_hours": 16.0
    },
    {
      "debt_type": "ComplexityHotspot",
      "file": "src/payments/processor.rs",
      "score": 87.3,
      "estimated_effort_hours": 14.0
    }
  ]
}
</code></pre>
<h4 id="cross-category-dependencies"><a class="header" href="#cross-category-dependencies">Cross-Category Dependencies</a></h4>
<p><code>CrossCategoryDependency</code> identifies blocking relationships between different debt categories:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct CrossCategoryDependency {
    pub from_category: DebtCategory,
    pub to_category: DebtCategory,
    pub blocking_items: Vec&lt;(DebtItem, DebtItem)&gt;,
    pub impact_level: ImpactLevel,  // Critical, High, Medium, Low
    pub recommendation: String,
}
<span class="boring">}</span></code></pre>
<p><strong>Common dependency patterns:</strong></p>
<p><strong>1. Architecture blocks Testing:</strong></p>
<ul>
<li><strong>Pattern</strong>: God objects are too complex to test effectively</li>
<li><strong>Example</strong>: <code>UserService</code> has 50+ functions, making comprehensive testing impractical</li>
<li><strong>Impact</strong>: Critical - cannot improve test coverage without refactoring</li>
<li><strong>Recommendation</strong>: “Split god object into 4-5 focused modules before adding tests”</li>
</ul>
<p><strong>2. Async issues require Architecture changes:</strong></p>
<ul>
<li><strong>Pattern</strong>: Blocking I/O in async contexts requires architectural redesign</li>
<li><strong>Example</strong>: Sync database calls in async handlers</li>
<li><strong>Impact</strong>: High - performance problems require design changes</li>
<li><strong>Recommendation</strong>: “Introduce async database layer before optimizing handlers”</li>
</ul>
<p><strong>3. Complexity affects Testability:</strong></p>
<ul>
<li><strong>Pattern</strong>: High cyclomatic complexity makes thorough testing difficult</li>
<li><strong>Example</strong>: Function with 22 branches needs 22+ test cases</li>
<li><strong>Impact</strong>: High - testing effort grows exponentially with complexity</li>
<li><strong>Recommendation</strong>: “Reduce complexity to &lt; 10 before writing comprehensive tests”</li>
</ul>
<p><strong>4. Performance requires Architecture:</strong></p>
<ul>
<li><strong>Pattern</strong>: O(n²) nested loops need different data structures</li>
<li><strong>Example</strong>: Linear search in loops should use HashMap</li>
<li><strong>Impact</strong>: Medium - optimization requires structural changes</li>
<li><strong>Recommendation</strong>: “Refactor data structure before micro-optimizations”</li>
</ul>
<p><strong>Example cross-category dependency:</strong></p>
<pre><code class="language-json">{
  "from_category": "Architecture",
  "to_category": "Testing",
  "impact_level": "Critical",
  "blocking_items": [
    {
      "blocker": {
        "debt_type": "GodObject",
        "file": "src/services/user_service.rs",
        "functions": 52,
        "score": 95.0
      },
      "blocked": {
        "debt_type": "TestingGap",
        "file": "src/services/user_service.rs",
        "coverage": 15,
        "score": 78.0
      }
    }
  ],
  "recommendation": "Split UserService into focused modules (auth, profile, settings, notifications) before attempting to improve test coverage. Current structure makes comprehensive testing impractical.",
  "estimated_unblock_effort_hours": 16.0
}
</code></pre>
<h4 id="using-category-based-analysis"><a class="header" href="#using-category-based-analysis">Using Category-Based Analysis</a></h4>
<p><strong>View category distribution in JSON:</strong></p>
<pre><code class="language-bash">debtmap analyze . --format json | jq '.summary.by_category'
</code></pre>
<p>This shows item counts per category (Architecture, Testing, Performance, CodeQuality).</p>
<p><strong>Filter items by category:</strong></p>
<pre><code class="language-bash">debtmap analyze . --format json | jq '.items[] | select(.category == "Architecture")'
</code></pre>
<p><strong>Focus on specific category with CLI:</strong></p>
<pre><code class="language-bash">debtmap analyze . --filter Architecture --top 10
</code></pre>
<p><strong>Generate markdown for detailed category analysis:</strong></p>
<pre><code class="language-bash">debtmap analyze . --format markdown --output report.md
</code></pre>
<p>The markdown format includes full CategorySummary details with effort estimates and cross-category dependency analysis.</p>
<p><strong>Strategic planning workflow:</strong></p>
<ol>
<li>
<p><strong>Review category summaries:</strong></p>
<ul>
<li>Identify which category has highest total score</li>
<li>Check estimated effort hours per category</li>
<li>Note average severity to gauge urgency</li>
</ul>
</li>
<li>
<p><strong>Check cross-category dependencies:</strong></p>
<ul>
<li>Find Critical and High impact blockers</li>
<li>Prioritize blockers before blocked items</li>
<li>Plan architectural changes before optimization</li>
</ul>
</li>
<li>
<p><strong>Plan remediation order:</strong></p>
<pre><code>Example decision tree:
- Architecture score &gt; 400? → Address god objects first
- Testing gap with low complexity? → Quick wins, add tests
- Performance issues + architecture debt? → Refactor structure first
- High code quality debt but good architecture? → Incremental cleanup
</code></pre>
</li>
<li>
<p><strong>Use category-specific strategies:</strong></p>
<ul>
<li><strong>Architecture</strong>: Pair programming, design reviews, incremental refactoring</li>
<li><strong>Testing</strong>: TDD for new code, characterization tests for legacy</li>
<li><strong>Performance</strong>: Profiling first, optimize hot paths, avoid premature optimization</li>
<li><strong>CodeQuality</strong>: Code review focus, linting rules, consistent patterns</li>
</ul>
</li>
</ol>
<p><strong>Note on output formats:</strong> CategorySummary and CrossCategoryDependency details are available in markdown format only. The JSON output provides category counts in <code>summary.by_category</code> and you can filter items by category using the <code>category</code> field on each item.</p>
<h3 id="debt-density-metric"><a class="header" href="#debt-density-metric">Debt Density Metric</a></h3>
<p>Debt density normalizes technical debt scores across projects of different sizes, providing a per-1000-lines-of-code metric for fair comparison.</p>
<h4 id="formula"><a class="header" href="#formula">Formula</a></h4>
<pre><code>debt_density = (total_debt_score / total_lines_of_code) × 1000
</code></pre>
<p><strong>Example calculation:</strong></p>
<pre><code>Project A:
  - Total debt score: 1,250
  - Total lines of code: 25,000
  - Debt density: (1,250 / 25,000) × 1000 = 50

Project B:
  - Total debt score: 2,500
  - Total lines of code: 50,000
  - Debt density: (2,500 / 50,000) × 1000 = 50
</code></pre>
<p>Projects A and B have <strong>equal debt density</strong> (50) despite B having twice the absolute debt, because B is also twice as large. They have proportionally similar technical debt.</p>
<h4 id="interpretation-guidelines"><a class="header" href="#interpretation-guidelines">Interpretation Guidelines</a></h4>
<p>Use these thresholds to assess codebase health:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Debt Density</th><th>Assessment</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><strong>0-50</strong></td><td>Clean</td><td>Well-maintained codebase, minimal debt</td></tr>
<tr><td><strong>51-100</strong></td><td>Moderate</td><td>Typical technical debt, manageable</td></tr>
<tr><td><strong>101-150</strong></td><td>High</td><td>Significant debt, prioritize remediation</td></tr>
<tr><td><strong>150+</strong></td><td>Critical</td><td>Severe debt burden, may impede development</td></tr>
</tbody>
</table>
</div>
<p><strong>Context matters:</strong></p>
<ul>
<li><strong>Early-stage projects</strong>: Often have higher density (rapid iteration)</li>
<li><strong>Mature projects</strong>: Should trend toward lower density over time</li>
<li><strong>Legacy systems</strong>: May have high density, track trend over time</li>
<li><strong>Greenfield rewrites</strong>: Aim for density &lt; 50</li>
</ul>
<h4 id="using-debt-density"><a class="header" href="#using-debt-density">Using Debt Density</a></h4>
<p><strong>1. Compare projects fairly:</strong></p>
<pre><code class="language-bash"># Small microservice (5,000 LOC, debt = 250)
# Debt density: 50

# Large monolith (100,000 LOC, debt = 5,000)
# Debt density: 50

# Equal health despite size difference
</code></pre>
<p><strong>2. Track improvement over time:</strong></p>
<pre><code>Sprint 1: 50,000 LOC, debt = 7,500, density = 150 (High)
Sprint 5: 52,000 LOC, debt = 6,500, density = 125 (Improving)
Sprint 10: 54,000 LOC, debt = 4,860, density = 90 (Moderate)
</code></pre>
<p><strong>3. Set team goals:</strong></p>
<pre><code>Current density: 120
Target density: &lt; 80 (by Q4)
Reduction needed: 40 points

Strategy:
- Fix 2-3 Critical items per sprint
- Prevent new debt (enforce thresholds)
- Refactor before adding features in high-debt modules
</code></pre>
<p><strong>4. Benchmark across teams/projects:</strong></p>
<pre><code class="language-json">{
  "team_metrics": [
    {
      "project": "auth-service",
      "debt_density": 45,
      "assessment": "Clean",
      "trend": "stable"
    },
    {
      "project": "billing-service",
      "debt_density": 95,
      "assessment": "Moderate",
      "trend": "improving"
    },
    {
      "project": "legacy-api",
      "debt_density": 165,
      "assessment": "Critical",
      "trend": "worsening"
    }
  ]
}
</code></pre>
<h4 id="limitations"><a class="header" href="#limitations">Limitations</a></h4>
<p><strong>Debt density doesn’t account for:</strong></p>
<ul>
<li><strong>Code importance</strong>: 100 LOC in payment logic ≠ 100 LOC in logging utils</li>
<li><strong>Complexity distribution</strong>: One 1000-line god object vs. 1000 simple functions</li>
<li><strong>Test coverage</strong>: 50% coverage on critical paths vs. low-priority features</li>
<li><strong>Team familiarity</strong>: New codebase vs. well-understood legacy system</li>
</ul>
<p><strong>Best practices:</strong></p>
<ul>
<li>Use density as <strong>one metric among many</strong></li>
<li>Combine with category analysis and tiered prioritization</li>
<li>Focus on <strong>trend</strong> (improving/stable/worsening) over absolute number</li>
<li>Consider <strong>debt per module</strong> for more granular insights</li>
</ul>
<h4 id="debt-density-in-cicd"><a class="header" href="#debt-density-in-cicd">Debt Density in CI/CD</a></h4>
<p><strong>Track density over time:</strong></p>
<pre><code class="language-bash"># Generate report with density
debtmap analyze . --format json --output debt-report.json

# Extract density for trending
DENSITY=$(jq '.debt_density' debt-report.json)

# Store in metrics database
echo "debtmap.density:${DENSITY}|g" | nc -u -w0 statsd 8125
</code></pre>
<p><strong>Set threshold gates:</strong></p>
<pre><code class="language-yaml"># .github/workflows/debt-check.yml
- name: Check debt density
  run: |
    DENSITY=$(debtmap analyze . --format json | jq '.debt_density')
    if (( $(echo "$DENSITY &gt; 150" | bc -l) )); then
      echo "❌ Debt density too high: $DENSITY (limit: 150)"
      exit 1
    fi
    echo "✅ Debt density acceptable: $DENSITY"
</code></pre>
<h3 id="actionable-insights"><a class="header" href="#actionable-insights">Actionable Insights</a></h3>
<p>Each recommendation includes:</p>
<p><strong>ACTION</strong>: What to do</p>
<ul>
<li>“Add 6 unit tests for full coverage”</li>
<li>“Refactor into 3 smaller functions”</li>
<li>“Extract validation to separate function”</li>
</ul>
<p><strong>IMPACT</strong>: Expected improvement</p>
<ul>
<li>“Full test coverage, -3.7 risk”</li>
<li>“Reduce complexity from 22 to 8”</li>
<li>“Eliminate 120 lines of duplication”</li>
</ul>
<p><strong>WHY</strong>: Rationale</p>
<ul>
<li>“Business logic with 0% coverage, manageable complexity”</li>
<li>“High complexity with low coverage threatens stability”</li>
<li>“Repeated validation pattern across 5 files”</li>
</ul>
<p><strong>Example workflow:</strong></p>
<ol>
<li>Run analysis with coverage: <code>debtmap analyze . --lcov coverage.lcov</code></li>
<li>Filter to CRITICAL items: <code>--min-priority critical</code></li>
<li>Review top 5 recommendations</li>
<li>Start with highest ROI items</li>
<li>Rerun analysis to track progress</li>
</ol>
<h3 id="common-patterns-to-recognize"><a class="header" href="#common-patterns-to-recognize">Common Patterns to Recognize</a></h3>
<p><strong>Pattern 1: High Complexity, Well Tested</strong></p>
<pre><code>Complexity: 25, Coverage: 95%, Risk: LOW
</code></pre>
<p>This is actually good! Complex but thoroughly tested code. Learn from this approach.</p>
<p><strong>Pattern 2: Moderate Complexity, No Tests</strong></p>
<pre><code>Complexity: 12, Coverage: 0%, Risk: CRITICAL
</code></pre>
<p>Highest priority - manageable complexity, should be easy to test.</p>
<p><strong>Pattern 3: Low Complexity, No Tests</strong></p>
<pre><code>Complexity: 3, Coverage: 0%, Risk: LOW
</code></pre>
<p>Low priority - simple code, less risky without tests.</p>
<p><strong>Pattern 4: Repetitive High Complexity (Dampened)</strong></p>
<pre><code>Cyclomatic: 20, Effective: 7 (65% dampened), Risk: LOW
</code></pre>
<p>Validation or dispatch pattern - looks complex but is repetitive. Lower priority.</p>
<p><strong>Pattern 5: God Object</strong></p>
<pre><code>File: services.rs, Functions: 50+, Responsibilities: 15+
</code></pre>
<p>Architectural issue - split before adding features.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="analyzer-types"><a class="header" href="#analyzer-types">Analyzer Types</a></h1>
<h2 id="overview-2"><a class="header" href="#overview-2">Overview</a></h2>
<p><strong>Debtmap is a Rust-only code analysis tool.</strong> As of specification 191, debtmap focuses exclusively on Rust codebases to provide deep, language-specific insights into code complexity, technical debt, and architectural patterns.</p>
<p>While the architecture supports extensibility through the <code>Analyzer</code> trait, only Rust is actively supported and maintained. Files in other programming languages are automatically filtered during discovery and never reach the analysis phase.</p>
<p><strong>Source</strong>: As documented in src/core/mod.rs:376-377 and src/core/injection.rs:198-200</p>
<h2 id="rust-analyzer"><a class="header" href="#rust-analyzer">Rust Analyzer</a></h2>
<p>Debtmap provides comprehensive analysis for Rust codebases using the <code>syn</code> crate for native AST parsing.</p>
<h3 id="core-capabilities"><a class="header" href="#core-capabilities">Core Capabilities</a></h3>
<p>The Rust analyzer (<code>src/analyzers/rust.rs</code>) provides:</p>
<ul>
<li><strong>Complexity Metrics</strong>: Cyclomatic complexity, cognitive complexity, and entropy analysis</li>
<li><strong>Purity Detection</strong>: Identifies pure functions with confidence scoring</li>
<li><strong>Call Graph Analysis</strong>: Tracks upstream callers and downstream callees with transitive relationships</li>
<li><strong>Trait Implementation Tracking</strong>: Monitors trait implementations across the codebase</li>
<li><strong>Macro Expansion Support</strong>: Analyzes complexity within macros accurately</li>
<li><strong>Pattern-Based Adjustments</strong>: Recognizes and adjusts for code generation patterns</li>
<li><strong>Visibility Tracking</strong>: Distinguishes <code>pub</code>, <code>pub(crate)</code>, and private functions</li>
<li><strong>Test Module Detection</strong>: Identifies <code>#[cfg(test)]</code> modules and <code>#[test]</code> functions</li>
</ul>
<p><strong>Source</strong>: Capabilities verified in src/analyzers/rust.rs:1-100</p>
<h3 id="semantic-function-classification"><a class="header" href="#semantic-function-classification">Semantic Function Classification</a></h3>
<p>The Rust analyzer automatically classifies functions by their role in the system. This classification feeds into the unified scoring system’s role multiplier for accurate technical debt assessment.</p>
<p><strong>Classification Categories</strong> (src/analyzers/rust.rs):</p>
<ul>
<li><strong>Entry Points</strong>: Functions named <code>main</code>, <code>start</code>, or public functions in <code>bin/</code> modules</li>
<li><strong>Business Logic</strong>: Core domain functions containing complex algorithms and business rules</li>
<li><strong>Data Access</strong>: Functions performing database queries, file I/O, or network operations</li>
<li><strong>Infrastructure</strong>: Logging, configuration, monitoring, and error handling utilities</li>
<li><strong>Utilities</strong>: Helper functions, formatters, type converters, and validation functions</li>
<li><strong>Test Code</strong>: Functions in <code>#[cfg(test)]</code> modules or marked with <code>#[test]</code> attribute</li>
</ul>
<p>These classifications are used to calculate role-based priority multipliers in the risk scoring system. See <a href="#risk-scoring-1">Risk Scoring</a> for details on how semantic classification affects debt prioritization.</p>
<h2 id="language-support"><a class="header" href="#language-support">Language Support</a></h2>
<h3 id="supported-rust-only"><a class="header" href="#supported-rust-only">Supported: Rust Only</a></h3>
<p>Debtmap exclusively analyzes Rust source files (<code>.rs</code> extension). All analysis features, metrics, and debt detection patterns are designed specifically for Rust’s syntax and semantics.</p>
<p><strong>Language Detection</strong> (src/core/mod.rs:386-391):</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub fn from_path(path: &amp;std::path::Path) -&gt; Self {
    path.extension()
        .and_then(|ext| ext.to_str())
        .map(Self::from_extension)
        .unwrap_or(Language::Unknown)
}
<span class="boring">}</span></code></pre>
<p>The <code>Language</code> enum (src/core/mod.rs:368-372) includes <code>Rust</code>, <code>Python</code>, and <code>Unknown</code> variants, but only <code>Rust</code> is actively processed:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub enum Language {
    Rust,
    Python,  // Architectural placeholder, not supported
    Unknown,
}
<span class="boring">}</span></code></pre>
<h3 id="file-filtering-behavior"><a class="header" href="#file-filtering-behavior">File Filtering Behavior</a></h3>
<p>During file discovery, debtmap filters files by extension:</p>
<ol>
<li><strong>Rust files (<code>.rs</code>)</strong>: Parsed and analyzed</li>
<li><strong>All other files</strong>: Silently filtered out—no warnings or errors generated</li>
<li><strong>Unknown extensions</strong>: Mapped to <code>Language::Unknown</code> and filtered during discovery</li>
</ol>
<p><strong>Source</strong>: Language detection implemented in src/core/mod.rs:375-391</p>
<p><strong>Example Usage</strong>:</p>
<pre><code class="language-bash"># Analyze all Rust files in current directory
debtmap analyze .

# Analyze specific Rust file
debtmap analyze src/main.rs

# Python, JavaScript, and other files are ignored
# (no error messages, just skipped)
</code></pre>
<h2 id="extensibility"><a class="header" href="#extensibility">Extensibility</a></h2>
<p>While debtmap currently focuses on Rust-only analysis, the architecture is designed to support additional languages in the future through the <code>Analyzer</code> trait.</p>
<h3 id="analyzer-trait"><a class="header" href="#analyzer-trait">Analyzer Trait</a></h3>
<p>The core <code>Analyzer</code> trait defines the interface for language-specific analyzers (src/analyzers/mod.rs:39-43):</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub trait Analyzer: Send + Sync {
    fn parse(&amp;self, content: &amp;str, path: std::path::PathBuf) -&gt; Result&lt;Ast&gt;;
    fn analyze(&amp;self, ast: &amp;Ast) -&gt; FileMetrics;
    fn language(&amp;self) -&gt; crate::core::Language;
}
<span class="boring">}</span></code></pre>
<p><strong>Note</strong>: There is also a generic <code>Analyzer</code> trait with associated types in src/core/traits.rs:11-16, used for internal abstractions. The trait shown above is the public extension point for language analyzers.</p>
<h3 id="current-implementation"><a class="header" href="#current-implementation">Current Implementation</a></h3>
<p>The <code>AnalyzerFactory</code> (src/core/injection.rs:190-203) creates language-specific analyzers:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>impl AnalyzerFactory {
    pub fn create_analyzer(&amp;self, language: Language) -&gt; Box&lt;dyn Analyzer&lt;...&gt;&gt; {
        match language {
            Language::Rust =&gt; Box::new(RustAnalyzerAdapter::new()),
            Language::Python =&gt; {
                panic!("Python analysis is not currently supported.
                       Debtmap is focusing exclusively on Rust analysis.")
            }
        }
    }
}
<span class="boring">}</span></code></pre>
<h3 id="adding-language-support-future"><a class="header" href="#adding-language-support-future">Adding Language Support (Future)</a></h3>
<p>To add support for a new language:</p>
<ol>
<li><strong>Implement the <code>Analyzer</code> trait</strong> with language-specific parsing and analysis</li>
<li><strong>Add the language variant</strong> to the <code>Language</code> enum (src/core/mod.rs:368-372)</li>
<li><strong>Update <code>from_extension()</code></strong> to recognize the file extension (src/core/mod.rs:375-384)</li>
<li><strong>Register in <code>AnalyzerFactory</code></strong> to instantiate your analyzer (src/core/injection.rs:196-201)</li>
</ol>
<p><strong>Reference Implementation</strong>: See <code>src/analyzers/rust.rs</code> for a complete example of implementing the <code>Analyzer</code> trait with full complexity analysis, purity detection, and call graph support.</p>
<h2 id="see-also-1"><a class="header" href="#see-also-1">See Also</a></h2>
<ul>
<li><a href="#overview-1">Overview</a> - Analysis pipeline and workflow</li>
<li><a href="#complexity-metrics-1">Complexity Metrics</a> - Detailed metric calculations</li>
<li><a href="#risk-scoring-1">Risk Scoring</a> - How semantic classification affects prioritization</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="advanced-features"><a class="header" href="#advanced-features">Advanced Features</a></h1>
<h2 id="advanced-features-1"><a class="header" href="#advanced-features-1">Advanced Features</a></h2>
<h3 id="purity-detection"><a class="header" href="#purity-detection">Purity Detection</a></h3>
<p>Debtmap detects pure functions - those without side effects that always return the same output for the same input.</p>
<p><strong>What makes a function pure:</strong></p>
<ul>
<li>No I/O operations (file, network, database)</li>
<li>No mutable global state</li>
<li>No random number generation</li>
<li>No system calls</li>
<li>Deterministic output</li>
</ul>
<p><strong>Purity detection is optional:</strong></p>
<ul>
<li>Both <code>is_pure</code> and <code>purity_confidence</code> are <code>Option</code> types</li>
<li>May be <code>None</code> for some functions or languages where detection is not available</li>
<li>Rust has the most comprehensive purity detection support</li>
</ul>
<p><strong>Three-level purity classification:</strong>
The <code>purity_level</code> field provides a more nuanced classification than the binary <code>is_pure</code>:</p>
<ul>
<li><strong>Pure</strong>: No side effects detected, deterministic output</li>
<li><strong>ProbablyPure</strong>: Mostly pure with minor suspicious patterns</li>
<li><strong>Impure</strong>: Clear side effects or non-deterministic behavior</li>
</ul>
<p>This three-level classification (available in <code>src/core/mod.rs:91</code>) complements the confidence scoring for more precise analysis.</p>
<p><strong>Confidence scoring (when available):</strong></p>
<ul>
<li><strong>0.9-1.0</strong>: Very confident (no side effects detected)</li>
<li><strong>0.7-0.8</strong>: Likely pure (minimal suspicious patterns)</li>
<li><strong>0.5-0.6</strong>: Uncertain (some suspicious patterns)</li>
<li><strong>0.0-0.4</strong>: Likely impure (side effects detected)</li>
</ul>
<p><strong>Example:</strong></p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Pure: confidence = 0.95
fn calculate_total(items: &amp;[Item]) -&gt; f64 {
    items.iter().map(|i| i.price).sum()
}

// Impure: confidence = 0.1 (I/O detected)
fn save_total(items: &amp;[Item]) -&gt; Result&lt;()&gt; {
    let total = items.iter().map(|i| i.price).sum();
    write_to_file(total)  // Side effect!
}
<span class="boring">}</span></code></pre>
<p><strong>Benefits:</strong></p>
<ul>
<li>Pure functions are easier to test</li>
<li>Can be safely cached or memoized</li>
<li>Safe to parallelize</li>
<li>Easier to reason about</li>
</ul>
<h3 id="data-flow-analysis"><a class="header" href="#data-flow-analysis">Data Flow Analysis</a></h3>
<p>Debtmap builds a comprehensive <code>DataFlowGraph</code> that extends basic call graph analysis with variable dependencies, data transformations, I/O operations, and purity tracking.</p>
<h4 id="call-graph-foundation"><a class="header" href="#call-graph-foundation">Call Graph Foundation</a></h4>
<p><strong>Upstream callers</strong> - Who calls this function</p>
<ul>
<li>Indicates impact radius</li>
<li>More callers = higher impact if it breaks</li>
</ul>
<p><strong>Downstream callees</strong> - What this function calls</p>
<ul>
<li>Indicates dependencies</li>
<li>More callees = more integration testing needed</li>
</ul>
<p><strong>Example:</strong></p>
<pre><code class="language-json">{
  "name": "process_payment",
  "upstream_callers": [
    "handle_checkout",
    "process_subscription",
    "handle_refund"
  ],
  "downstream_callees": [
    "validate_payment_method",
    "calculate_fees",
    "record_transaction",
    "send_receipt"
  ]
}
</code></pre>
<h4 id="variable-dependency-tracking"><a class="header" href="#variable-dependency-tracking">Variable Dependency Tracking</a></h4>
<p><code>DataFlowGraph</code> tracks which variables each function depends on:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct DataFlowGraph {
    // Maps function_id -&gt; set of variable names used
    variable_dependencies: HashMap&lt;String, HashSet&lt;String&gt;&gt;,
    // ...
}
<span class="boring">}</span></code></pre>
<p><strong>What it tracks:</strong></p>
<ul>
<li>Local variables accessed in function body</li>
<li>Function parameters</li>
<li>Captured variables (closures)</li>
</ul>
<p><strong>Benefits:</strong></p>
<ul>
<li>Identify functions coupled through shared state</li>
<li>Detect potential side effect chains</li>
<li>Guide refactoring to reduce coupling</li>
</ul>
<p><strong>Example output:</strong></p>
<pre><code class="language-json">{
  "function": "calculate_total",
  "variable_dependencies": ["items", "tax_rate", "discount", "total"],
  "parameter_count": 3,
  "local_var_count": 1
}
</code></pre>
<h4 id="data-transformation-patterns"><a class="header" href="#data-transformation-patterns">Data Transformation Patterns</a></h4>
<p><code>DataFlowGraph</code> identifies common functional programming patterns:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub enum TransformationType {
    Map,        // Transform each element
    Filter,     // Select subset of elements
    Reduce,     // Aggregate to single value
    FlatMap,    // Transform and flatten
    Unknown,    // Other transformations
}
<span class="boring">}</span></code></pre>
<p><strong>Pattern detection:</strong></p>
<ul>
<li>Recognizes iterator chains (<code>.map()</code>, <code>.filter()</code>, <code>.fold()</code>)</li>
<li>Identifies functional vs imperative data flow</li>
<li>Tracks input/output variable relationships</li>
</ul>
<p><strong>Example:</strong></p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Detected as: Filter → Map → Reduce pattern
fn total_active_users(users: &amp;[User]) -&gt; f64 {
    users.iter()
        .filter(|u| u.active)      // Filter transformation
        .map(|u| u.balance)        // Map transformation
        .sum()                      // Reduce transformation
}
<span class="boring">}</span></code></pre>
<p><strong>Transformation metadata:</strong></p>
<pre><code class="language-json">{
  "function": "total_active_users",
  "input_vars": ["users"],
  "output_vars": ["sum_result"],
  "transformation_type": "Reduce",
  "is_functional_style": true,
  "pipeline_length": 3
}
</code></pre>
<h4 id="io-operation-detection"><a class="header" href="#io-operation-detection">I/O Operation Detection</a></h4>
<p>Tracks functions performing I/O operations for purity and performance analysis:</p>
<p><strong>I/O categories tracked:</strong></p>
<ul>
<li><strong>File I/O</strong>: <code>std::fs</code>, <code>File::open</code>, <code>read_to_string</code></li>
<li><strong>Network I/O</strong>: HTTP requests, socket operations</li>
<li><strong>Database I/O</strong>: SQL queries, ORM operations</li>
<li><strong>System calls</strong>: Process spawning, environment access</li>
<li><strong>Blocking operations</strong>: <code>thread::sleep</code>, synchronous I/O in async</li>
</ul>
<p><strong>Example detection:</strong></p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Detected I/O operations: FileRead, FileWrite
fn save_config(config: &amp;Config, path: &amp;Path) -&gt; Result&lt;()&gt; {
    let json = serde_json::to_string(config)?;  // No I/O
    std::fs::write(path, json)?;                 // FileWrite detected
    Ok(())
}
<span class="boring">}</span></code></pre>
<p><strong>I/O metadata:</strong></p>
<pre><code class="language-json">{
  "function": "save_config",
  "io_operations": ["FileWrite"],
  "is_blocking": true,
  "affects_purity": true,
  "async_safe": false
}
</code></pre>
<h4 id="purity-analysis-integration"><a class="header" href="#purity-analysis-integration">Purity Analysis Integration</a></h4>
<p><code>DataFlowGraph</code> integrates with purity detection to provide comprehensive side effect analysis:</p>
<p><strong>Side effect tracking:</strong></p>
<ul>
<li>I/O operations (file, network, console)</li>
<li>Global state mutations</li>
<li>Random number generation</li>
<li>System time access</li>
<li>Non-deterministic behavior</li>
</ul>
<p><strong>Purity confidence factors:</strong></p>
<ul>
<li><strong>1.0</strong>: Pure mathematical function, no side effects</li>
<li><strong>0.8</strong>: Pure with deterministic data transformations</li>
<li><strong>0.5</strong>: Mixed - some suspicious patterns</li>
<li><strong>0.2</strong>: Likely impure - I/O detected</li>
<li><strong>0.0</strong>: Definitely impure - multiple side effects</li>
</ul>
<p><strong>Example analysis:</strong></p>
<pre><code class="language-json">{
  "function": "calculate_discount",
  "is_pure": true,
  "purity_confidence": 0.95,
  "side_effects": [],
  "deterministic": true,
  "safe_to_parallelize": true,
  "safe_to_cache": true
}
</code></pre>
<h4 id="modification-impact-analysis"><a class="header" href="#modification-impact-analysis">Modification Impact Analysis</a></h4>
<p><code>DataFlowGraph</code> calculates the impact of modifying a function:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct ModificationImpact {
    pub function_name: String,
    pub affected_functions: Vec&lt;String&gt;,  // Upstream callers
    pub dependency_count: usize,          // Downstream callees
    pub has_side_effects: bool,
    pub risk_level: RiskLevel,
}
<span class="boring">}</span></code></pre>
<p><strong>Risk level calculation:</strong></p>
<ul>
<li><strong>Critical</strong>: Many upstream callers + side effects + low test coverage</li>
<li><strong>High</strong>: Many callers OR side effects with moderate coverage</li>
<li><strong>Medium</strong>: Few callers with side effects OR many callers with good coverage</li>
<li><strong>Low</strong>: Few callers, no side effects, or well-tested</li>
</ul>
<p><strong>Example impact analysis:</strong></p>
<pre><code class="language-json">{
  "function": "validate_payment_method",
  "modification_impact": {
    "affected_functions": 4,
    "dependency_count": 8,
    "has_side_effects": true,
    "risk_level": "High"
  }
}
</code></pre>
<p><strong>Note</strong>: The <code>affected_functions</code> field contains the count of upstream callers. The actual function names can be obtained from the <code>upstream_callers</code> field in the function metadata.</p>
<p><strong>Using modification impact:</strong></p>
<pre><code class="language-bash"># Analyze impact before refactoring
debtmap analyze . --format json | jq '.functions[] | select(.name == "validate_payment_method") | .modification_impact'
</code></pre>
<p><strong>Impact analysis uses:</strong></p>
<ul>
<li><strong>Refactoring planning</strong>: Understand blast radius before changes</li>
<li><strong>Test prioritization</strong>: Focus tests on high-impact functions</li>
<li><strong>Code review</strong>: Flag high-risk changes for extra scrutiny</li>
<li><strong>Dependency management</strong>: Identify tightly coupled components</li>
</ul>
<h4 id="dataflowgraph-methods"><a class="header" href="#dataflowgraph-methods">DataFlowGraph Methods</a></h4>
<p>Key methods for data flow analysis:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Add function with its dependencies
pub fn add_function(&amp;mut self, function_id: String, callees: Vec&lt;String&gt;)

// Track variable dependencies
pub fn add_variable_dependency(&amp;mut self, function_id: String, var_name: String)

// Record I/O operations
pub fn add_io_operation(&amp;mut self, function_id: String, io_type: IoType)

// Calculate modification impact
pub fn calculate_modification_impact(&amp;self, function_id: &amp;str) -&gt; ModificationImpact

// Get all functions affected by a change
pub fn get_affected_functions(&amp;self, function_id: &amp;str) -&gt; Vec&lt;String&gt;

// Find functions with side effects
pub fn find_functions_with_side_effects(&amp;self) -&gt; Vec&lt;String&gt;
<span class="boring">}</span></code></pre>
<p><strong>Integration in analysis pipeline:</strong></p>
<ol>
<li>Parser builds initial call graph</li>
<li>DataFlowGraph extends with variable/I/O tracking</li>
<li>Purity analyzer adds side effect information</li>
<li>Modification impact calculated for each function</li>
<li>Results used in prioritization and risk scoring</li>
</ol>
<p><strong>Connection to Unified Scoring:</strong></p>
<p>The dependency analysis from DataFlowGraph directly feeds into the <strong>unified scoring system’s dependency factor</strong> (20% weight):</p>
<ul>
<li><strong>Dependency Factor Calculation</strong>: Functions with high upstream caller count or on critical paths from entry points receive higher dependency scores (8-10)</li>
<li><strong>Isolated Utilities</strong>: Functions with few or no callers score lower (1-3) on dependency factor</li>
<li><strong>Impact Prioritization</strong>: This helps prioritize functions where bugs have wider impact across the codebase</li>
<li><strong>Modification Risk</strong>: The modification impact analysis uses dependency data to calculate blast radius when changes are made</li>
</ul>
<p><strong>Example:</strong></p>
<pre><code>Function: validate_payment_method
  Upstream callers: 4 (high impact)
  → Dependency Factor: 8.0

Function: format_currency_string
  Upstream callers: 0 (utility)
  → Dependency Factor: 1.5

Both have same complexity, but validate_payment_method gets higher unified score
due to its critical role in the call graph.
</code></pre>
<p>This integration ensures that the unified scoring system considers not just internal function complexity and test coverage, but also the function’s importance in the broader codebase architecture.</p>
<h3 id="entropy-based-complexity"><a class="header" href="#entropy-based-complexity">Entropy-Based Complexity</a></h3>
<p>Advanced pattern detection to reduce false positives.</p>
<p><strong>Token Classification:</strong></p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>enum TokenType {
    Variable,     // Weight: 1.0
    Method,       // Weight: 1.5 (more important)
    Literal,      // Weight: 0.5 (less important)
    Keyword,      // Weight: 0.8
    Operator,     // Weight: 0.6
}
<span class="boring">}</span></code></pre>
<p><strong>Shannon Entropy Calculation:</strong></p>
<pre><code>H(X) = -Σ p(x) × log₂(p(x))
</code></pre>
<p>where p(x) is the probability of each token type.</p>
<p><strong>Dampening Decision:</strong></p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>if entropy_score.token_entropy &lt; 0.4
   &amp;&amp; entropy_score.pattern_repetition &gt; 0.6
   &amp;&amp; entropy_score.branch_similarity &gt; 0.7
{
    // Apply dampening
    effective_complexity = base_complexity × (1 - dampening_factor);
}
<span class="boring">}</span></code></pre>
<p><strong>Output explanation:</strong></p>
<pre><code>Function: validate_input
  Cyclomatic: 15 → Effective: 5
  Reasoning:
    - High pattern repetition detected (85%)
    - Low token entropy indicates simple patterns (0.32)
    - Similar branch structures found (92% similarity)
    - Complexity reduced by 67% due to pattern-based code
</code></pre>
<h3 id="entropy-analysis-caching"><a class="header" href="#entropy-analysis-caching">Entropy Analysis Caching</a></h3>
<p><code>EntropyAnalyzer</code> includes an LRU-style cache for performance optimization when analyzing large codebases or performing repeated analysis.</p>
<h4 id="cache-structure"><a class="header" href="#cache-structure">Cache Structure</a></h4>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>struct CacheEntry {
    score: EntropyScore,
    timestamp: Instant,
    hit_count: usize,
}
<span class="boring">}</span></code></pre>
<p><strong>Cache configuration:</strong></p>
<ul>
<li><strong>Default size</strong>: 1000 entries</li>
<li><strong>Eviction policy</strong>: LRU (Least Recently Used)</li>
<li><strong>Memory per entry</strong>: ~128 bytes</li>
<li><strong>Total memory overhead</strong>: ~128 KB for default size</li>
</ul>
<h4 id="cache-statistics"><a class="header" href="#cache-statistics">Cache Statistics</a></h4>
<p>The analyzer tracks cache performance:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct CacheStats {
    pub hits: usize,
    pub misses: usize,
    pub evictions: usize,
    pub hit_rate: f64,
    pub memory_usage: usize,
}
<span class="boring">}</span></code></pre>
<p><strong>Example stats output:</strong></p>
<pre><code class="language-json">{
  "entropy_cache_stats": {
    "hits": 3427,
    "misses": 1573,
    "evictions": 573,
    "hit_rate": 0.685,
    "memory_usage": 128000
  }
}
</code></pre>
<p><strong>Hit rate interpretation:</strong></p>
<ul>
<li><strong>&gt; 0.7</strong>: Excellent - many repeated analyses, cache is effective</li>
<li><strong>0.4-0.7</strong>: Good - moderate reuse, typical for incremental analysis</li>
<li><strong>&lt; 0.4</strong>: Low - mostly unique functions, cache less helpful</li>
</ul>
<h4 id="performance-benefits"><a class="header" href="#performance-benefits">Performance Benefits</a></h4>
<p><strong>Typical performance gains:</strong></p>
<ul>
<li><strong>Cold analysis</strong>: 100ms baseline (no cache benefit)</li>
<li><strong>Incremental analysis</strong>: 30-40ms (~60-70% faster) for unchanged functions</li>
<li><strong>Re-analysis</strong>: 15-20ms (~80-85% faster) for recently analyzed functions</li>
</ul>
<p><strong>Best for:</strong></p>
<ul>
<li><strong>Watch mode</strong>: Analyzing on file save (repeated analysis of same files)</li>
<li><strong>CI/CD</strong>: Comparing feature branch to main (overlap in functions)</li>
<li><strong>Large codebases</strong>: Many similar functions benefit from pattern caching</li>
</ul>
<p><strong>Memory estimation:</strong></p>
<pre><code>Total cache memory = entry_count × 128 bytes

Examples:
- 1,000 entries: ~128 KB (default)
- 5,000 entries: ~640 KB (large projects)
- 10,000 entries: ~1.25 MB (very large)
</code></pre>
<h4 id="cache-management"><a class="header" href="#cache-management">Cache Management</a></h4>
<p><strong>Automatic eviction:</strong></p>
<ul>
<li>When cache reaches size limit, oldest entries evicted</li>
<li>Hit count influences retention (frequently accessed stay longer)</li>
<li>Timestamp used for LRU ordering</li>
</ul>
<p><strong>Cache invalidation:</strong></p>
<ul>
<li>Function source changes invalidate entry</li>
<li>Cache cleared between major analysis runs</li>
<li>No manual invalidation needed</li>
</ul>
<p><strong>Configuration (if exposed in future):</strong></p>
<pre><code class="language-toml">[entropy.cache]
enabled = true
size = 1000           # Number of entries
ttl_seconds = 3600    # Optional: expire after 1 hour
</code></pre>
<h3 id="context-aware-analysis-2"><a class="header" href="#context-aware-analysis-2">Context-Aware Analysis</a></h3>
<p>Debtmap adjusts analysis based on code context:</p>
<p><strong>Pattern Recognition:</strong></p>
<ul>
<li>Validation patterns (repetitive checks)</li>
<li>Dispatcher patterns (routing logic)</li>
<li>Builder patterns (fluent APIs)</li>
<li>Configuration parsers (key-value processing)</li>
</ul>
<p><strong>Adjustment Strategies:</strong></p>
<ul>
<li>Reduce false positives for recognized patterns</li>
<li>Apply appropriate thresholds by pattern type</li>
<li>Consider pattern confidence in scoring</li>
</ul>
<p><strong>Example:</strong></p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Recognized as "validation_pattern"
// Complexity dampening applied
fn validate_user_input(input: &amp;UserInput) -&gt; Result&lt;()&gt; {
    if input.name.is_empty() { return Err(Error::EmptyName); }
    if input.email.is_empty() { return Err(Error::EmptyEmail); }
    if input.age &lt; 13 { return Err(Error::TooYoung); }
    // ... more similar validations
    Ok(())
}
<span class="boring">}</span></code></pre>
<h3 id="coverage-integration"><a class="header" href="#coverage-integration">Coverage Integration</a></h3>
<p>Debtmap parses LCOV coverage data for risk analysis:</p>
<p><strong>LCOV Support:</strong></p>
<ul>
<li>Standard format from most coverage tools</li>
<li>Line-level coverage tracking</li>
<li>Function-level aggregation</li>
</ul>
<p><strong>Coverage Index:</strong></p>
<ul>
<li>O(1) exact name lookups (~0.5μs)</li>
<li>O(log n) line-based fallback (~5-8μs)</li>
<li>~200 bytes per function</li>
<li>Thread-safe (<code>Arc&lt;CoverageIndex&gt;</code>)</li>
</ul>
<h4 id="performance-characteristics"><a class="header" href="#performance-characteristics">Performance Characteristics</a></h4>
<p><strong>Index Build Performance:</strong></p>
<ul>
<li>Index construction: O(n), approximately 20-30ms for 5,000 functions</li>
<li>Memory usage: ~200 bytes per record (~2MB for 5,000 functions)</li>
<li>Scales linearly with function count</li>
</ul>
<p><strong>Lookup Performance:</strong></p>
<ul>
<li>Exact match (function name): O(1) average, ~0.5μs per lookup</li>
<li>Line-based fallback: O(log n), ~5-8μs per lookup</li>
<li>Cache-friendly data structure for hot paths</li>
</ul>
<p><strong>Analysis Overhead:</strong></p>
<ul>
<li>Coverage integration overhead: ~2.5x baseline analysis time</li>
<li>Target overhead: ≤3x (maintained through optimizations)</li>
<li>Example timing: 53ms baseline → 130ms with coverage (2.45x overhead)</li>
<li>Overhead includes index build + lookups + coverage propagation</li>
</ul>
<p><strong>When to use coverage integration:</strong></p>
<ul>
<li><strong>Skip coverage</strong> (faster iteration): For rapid development iteration or quick local checks, omit <code>--lcov</code> to get baseline results 2.5x faster</li>
<li><strong>Include coverage</strong> (comprehensive analysis): Use coverage integration for final validation, sprint planning, and CI/CD gates where comprehensive risk analysis is needed</li>
</ul>
<p><strong>Thread Safety:</strong></p>
<ul>
<li>Coverage index wrapped in <code>Arc&amp;lt;CoverageIndex&amp;gt;</code> for lock-free parallel access</li>
<li>Multiple analyzer threads can query coverage simultaneously</li>
<li>No contention on reads, suitable for parallel analysis pipelines</li>
</ul>
<p><strong>Memory Footprint:</strong></p>
<pre><code>Total memory = (function_count × 200 bytes) + index overhead

Examples:
- 1,000 functions: ~200 KB
- 5,000 functions: ~2 MB
- 10,000 functions: ~4 MB
</code></pre>
<p><strong>Scalability:</strong></p>
<ul>
<li>Tested with codebases up to 10,000 functions</li>
<li>Performance remains predictable and acceptable</li>
<li>Memory usage stays bounded and reasonable</li>
</ul>
<p><strong>Generating coverage:</strong></p>
<pre><code class="language-bash"># Rust (using cargo-tarpaulin)
cargo tarpaulin --out lcov --output-dir target/coverage

# Or using cargo-llvm-cov
cargo llvm-cov --lcov --output-path target/coverage/lcov.info
</code></pre>
<p><strong>Using with Debtmap:</strong></p>
<pre><code class="language-bash">debtmap analyze . --lcov target/coverage/lcov.info
</code></pre>
<p><strong>Coverage dampening:</strong>
When coverage data is provided, debt scores are dampened for well-tested code:</p>
<pre><code>final_score = base_score × (1 - coverage_percentage)
</code></pre>
<p>This ensures well-tested complex code gets lower priority than untested simple code.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="example-outputs"><a class="header" href="#example-outputs">Example Outputs</a></h1>
<p>This page demonstrates realistic examples of debtmap’s terminal and JSON output formats using the unified format (spec 108).</p>
<h2 id="high-complexity-function-needs-refactoring"><a class="header" href="#high-complexity-function-needs-refactoring">High Complexity Function (Needs Refactoring)</a></h2>
<p><strong>Terminal Output:</strong></p>
<pre><code>#1 SCORE: 9.2 [CRITICAL]
├─ COMPLEXITY: ./src/payments/processor.rs:145 process_transaction()
├─ ACTION: Refactor into 4 smaller functions
├─ IMPACT: Reduce complexity from 25 to 8, improve testability
├─ COMPLEXITY: cyclomatic=25, branches=25, cognitive=38, nesting=5, lines=120
├─ DEPENDENCIES: 3 upstream, 8 downstream
└─ WHY: Exceeds all complexity thresholds, difficult to test and maintain
</code></pre>
<p><strong>JSON Output (Unified Format):</strong></p>
<pre><code class="language-json">{
  "type": "Function",
  "score": 92.5,
  "category": "CodeQuality",
  "priority": "critical",
  "location": {
    "file": "src/payments/processor.rs",
    "line": 145,
    "function": "process_transaction"
  },
  "metrics": {
    "cyclomatic_complexity": 25,
    "cognitive_complexity": 38,
    "length": 120,
    "nesting_depth": 5,
    "coverage": 0.15,
    "uncovered_lines": [145, 147, 152, 158, 165, 172, 180, 185]
  },
  "debt_type": {
    "ComplexityHotspot": {
      "cyclomatic": 25,
      "cognitive": 38,
      "adjusted_cyclomatic": null
    }
  },
  "function_role": "Orchestrator",
  "purity_analysis": {
    "is_pure": false,
    "confidence": 0.15,
    "side_effects": ["mutates_state", "io_operations", "database_access"]
  },
  "dependencies": {
    "upstream_count": 3,
    "downstream_count": 8,
    "upstream_callers": [
      "handle_payment",
      "handle_subscription",
      "handle_refund"
    ],
    "downstream_callees": [
      "validate",
      "calculate_fees",
      "record_transaction",
      "send_receipt",
      "update_balance",
      "log_transaction",
      "check_fraud",
      "notify_user"
    ]
  },
  "recommendation": {
    "action": "Refactor into 4 smaller, focused functions",
    "implementation_steps": [
      "Extract validation logic into validate_payment_request",
      "Create calculate_payment_totals for fee calculation",
      "Move side effects to separate transaction recorder",
      "Keep process_transaction as thin orchestrator"
    ]
  },
  "impact": {
    "coverage_improvement": 0.55,
    "complexity_reduction": 68.0,
    "risk_reduction": 7.8
  },
  "scoring_details": {
    "coverage_score": 45.0,
    "complexity_score": 38.5,
    "dependency_score": 9.0,
    "base_score": 92.5,
    "role_multiplier": 1.0,
    "final_score": 92.5
  }
}
</code></pre>
<p><strong>Source</strong>: Structure from <code>src/output/unified.rs:FunctionDebtItemOutput</code> (lines 158-183)</p>
<p><strong>Key Fields Explained:</strong></p>
<ul>
<li><code>type</code>: Always <code>"Function"</code> for function-level debt items</li>
<li><code>score</code>: Unified debt score (same path for File and Function items)</li>
<li><code>category</code>: One of <code>CodeQuality</code>, <code>Architecture</code>, <code>Testing</code>, <code>Performance</code></li>
<li><code>priority</code>: Derived from score (<code>critical</code> &gt;= 100, <code>high</code> &gt;= 50, <code>medium</code> &gt;= 20, <code>low</code> &lt; 20)</li>
<li><code>location</code>: Unified location structure with file, line, and function name</li>
<li><code>function_role</code>: Classification from <code>FunctionRole</code> enum (see below)</li>
<li><code>debt_type</code>: Tagged enum with variant-specific data</li>
</ul>
<h2 id="function-role-classification"><a class="header" href="#function-role-classification">Function Role Classification</a></h2>
<p>The <code>function_role</code> field helps prioritize testing and refactoring efforts based on the function’s architectural purpose.</p>
<p><strong>Source</strong>: <code>src/priority/semantic_classifier/mod.rs:25-33</code></p>
<pre><code class="language-json">{
  "function_role": "PureLogic"
}
</code></pre>
<p><strong>Available Roles:</strong></p>
<ul>
<li><code>PureLogic</code> - Business logic, high test priority</li>
<li><code>Orchestrator</code> - Coordinates other functions (like the example above)</li>
<li><code>IOWrapper</code> - Thin I/O layer, lower test priority</li>
<li><code>EntryPoint</code> - Main entry points (main, CLI handlers)</li>
<li><code>PatternMatch</code> - Pattern matching function (often low complexity)</li>
<li><code>Debug</code> - Debug/diagnostic functions (low test priority)</li>
<li><code>Unknown</code> - Cannot classify automatically</li>
</ul>
<h2 id="file-level-debt-god-object"><a class="header" href="#file-level-debt-god-object">File-Level Debt (God Object)</a></h2>
<p><strong>Terminal Output:</strong></p>
<pre><code>#2 SCORE: 8.7 [HIGH]
├─ GOD OBJECT: ./src/services/user_manager.rs
├─ ACTION: Split into 4 focused modules
├─ METRICS: 1250 lines, 45 functions, avg complexity 12.3
├─ INDICATORS: High responsibility count (8), excessive dependencies
└─ WHY: File has too many responsibilities, difficult to maintain
</code></pre>
<p><strong>JSON Output (Unified Format):</strong></p>
<pre><code class="language-json">{
  "type": "File",
  "score": 87.0,
  "category": "Architecture",
  "priority": "high",
  "location": {
    "file": "src/services/user_manager.rs"
  },
  "metrics": {
    "lines": 1250,
    "functions": 45,
    "classes": 3,
    "avg_complexity": 12.3,
    "max_complexity": 28,
    "total_complexity": 554,
    "coverage": 0.62,
    "uncovered_lines": 125
  },
  "god_object_indicators": {
    "responsibility_count": 8,
    "data_class_count": 12,
    "method_groups": [
      "authentication",
      "authorization",
      "profile_management",
      "session_handling",
      "notification_preferences",
      "audit_logging",
      "password_management",
      "role_management"
    ],
    "coupling_score": 0.78,
    "cohesion_score": 0.34
  },
  "recommendation": {
    "action": "Split into focused modules by responsibility",
    "implementation_steps": [
      "Extract authentication into auth_service.rs",
      "Move authorization to permission_service.rs",
      "Create profile_service.rs for user data management",
      "Separate audit concerns into audit_logger.rs"
    ]
  },
  "impact": {
    "complexity_reduction": 45.0,
    "maintainability_improvement": 0.68,
    "test_effort": 8.5
  }
}
</code></pre>
<p><strong>Source</strong>: Structure from <code>src/output/unified.rs:FileDebtItemOutput</code> (lines 110-123) and <code>src/priority/file_metrics.rs:GodObjectIndicators</code></p>
<h2 id="test-gap-needs-testing"><a class="header" href="#test-gap-needs-testing">Test Gap (Needs Testing)</a></h2>
<p><strong>Terminal Output:</strong></p>
<pre><code>#3 SCORE: 8.9 [CRITICAL]
├─ TEST GAP: ./src/analyzers/rust_call_graph.rs:38 add_function_to_graph()
├─ ACTION: Add 6 unit tests for full coverage
├─ IMPACT: Full test coverage, -3.7 risk reduction
├─ COMPLEXITY: cyclomatic=6, branches=6, cognitive=8, nesting=2, lines=32
├─ DEPENDENCIES: 0 upstream, 11 downstream
├─ TEST EFFORT: Simple (2-3 hours)
└─ WHY: Business logic with 0% coverage, manageable complexity (cyclo=6, cog=8)
    High impact - 11 functions depend on this
</code></pre>
<p><strong>JSON Output (Unified Format):</strong></p>
<pre><code class="language-json">{
  "type": "Function",
  "score": 89.0,
  "category": "Testing",
  "priority": "critical",
  "location": {
    "file": "src/analyzers/rust_call_graph.rs",
    "line": 38,
    "function": "add_function_to_graph"
  },
  "metrics": {
    "cyclomatic_complexity": 6,
    "cognitive_complexity": 8,
    "length": 32,
    "nesting_depth": 2,
    "coverage": 0.0,
    "uncovered_lines": [38, 39, 40, 42, 45, 48, 51, 54, 57, 60, 63, 66]
  },
  "debt_type": {
    "TestingGap": {
      "coverage": 0.0,
      "cyclomatic": 6,
      "cognitive": 8
    }
  },
  "function_role": "PureLogic",
  "purity_analysis": {
    "is_pure": false,
    "confidence": 0.65
  },
  "dependencies": {
    "upstream_count": 0,
    "downstream_count": 11,
    "downstream_callees": [
      "get_function_name",
      "extract_parameters",
      "parse_return_type",
      "add_to_registry",
      "update_call_sites",
      "resolve_types",
      "track_visibility",
      "record_location",
      "increment_counter",
      "validate_signature",
      "log_registration"
    ]
  },
  "recommendation": {
    "action": "Add unit tests for core business logic",
    "implementation_steps": [
      "Test happy path with valid function definition",
      "Test error cases: null input, invalid syntax",
      "Test edge cases: complex generics, lifetimes",
      "Test integration with registry updates",
      "Verify correct handling of visibility modifiers",
      "Test type resolution edge cases"
    ]
  },
  "impact": {
    "coverage_improvement": 1.0,
    "complexity_reduction": 0.0,
    "risk_reduction": 3.7
  }
}
</code></pre>
<p><strong>Source</strong>: Structure from <code>src/output/unified.rs:FunctionDebtItemOutput</code> with <code>debt_type</code> from <code>src/priority/mod.rs:158-171</code></p>
<h2 id="entropy-dampened-validation-function"><a class="header" href="#entropy-dampened-validation-function">Entropy-Dampened Validation Function</a></h2>
<p>This example shows how debtmap’s entropy analysis reduces false positives for repetitive code patterns.</p>
<p><strong>Terminal Output:</strong></p>
<pre><code>Function: validate_config
  File: src/config/validator.rs:23
  Cyclomatic: 20 → Effective: 7 (65% dampened)
  Risk: LOW

  Entropy Analysis:
    ├─ Token Entropy: 0.28 (low variety - repetitive patterns)
    ├─ Pattern Repetition: 0.88 (high similarity between checks)
    ├─ Branch Similarity: 0.91 (consistent validation structure)
    └─ Reasoning: Complexity reduced by 65% due to pattern-based code

  This appears complex but is actually a repetitive validation pattern.
  Lower priority for refactoring.
</code></pre>
<p><strong>JSON Output (Unified Format):</strong></p>
<pre><code class="language-json">{
  "type": "Function",
  "score": 15.2,
  "category": "CodeQuality",
  "priority": "low",
  "location": {
    "file": "src/config/validator.rs",
    "line": 23,
    "function": "validate_config"
  },
  "metrics": {
    "cyclomatic_complexity": 20,
    "cognitive_complexity": 18,
    "length": 85,
    "nesting_depth": 3,
    "coverage": 0.95,
    "entropy_score": 0.28
  },
  "debt_type": {
    "ComplexityHotspot": {
      "cyclomatic": 20,
      "cognitive": 18,
      "adjusted_cyclomatic": 7
    }
  },
  "adjusted_complexity": {
    "dampened_cyclomatic": 7.0,
    "dampening_factor": 0.65
  },
  "function_role": "PatternMatch",
  "recommendation": {
    "action": "Low priority - repetitive validation pattern"
  },
  "impact": {
    "coverage_improvement": 0.05,
    "complexity_reduction": 0.0,
    "risk_reduction": 0.8
  },
  "scoring_details": {
    "coverage_score": 2.5,
    "complexity_score": 7.0,
    "dependency_score": 0.0,
    "base_score": 43.5,
    "entropy_dampening": 0.65,
    "role_multiplier": 0.35,
    "final_score": 15.2
  }
}
</code></pre>
<p><strong>Source</strong>: <code>adjusted_complexity</code> from <code>src/output/unified.rs:186-190</code>, entropy dampening spec 182</p>
<p><strong>Key Points:</strong></p>
<ul>
<li><code>adjusted_cyclomatic</code>: Entropy-dampened complexity value (7 vs original 20)</li>
<li><code>dampening_factor</code>: Amount of reduction applied (0.65 = 65% reduction)</li>
<li><code>entropy_score</code>: Low value (0.28) indicates repetitive patterns</li>
<li>Score reduced from 43.5 to 15.2 due to entropy analysis</li>
</ul>
<h2 id="pattern-detection-example"><a class="header" href="#pattern-detection-example">Pattern Detection Example</a></h2>
<p>When debtmap detects a specific complexity pattern, it includes pattern metadata.</p>
<p><strong>JSON Output:</strong></p>
<pre><code class="language-json">{
  "type": "Function",
  "score": 65.0,
  "category": "CodeQuality",
  "priority": "high",
  "location": {
    "file": "src/state/workflow_executor.rs",
    "line": 78,
    "function": "execute_transition"
  },
  "pattern_type": "state_machine",
  "pattern_confidence": 0.87,
  "pattern_details": {
    "state_count": 12,
    "transition_count": 34,
    "branch_entropy": 0.82,
    "state_cohesion": 0.91
  },
  "complexity_pattern": "State machine with 12 states, high cohesion"
}
</code></pre>
<p><strong>Source</strong>: <code>pattern_type</code> and <code>pattern_confidence</code> from <code>src/output/unified.rs:178-182</code></p>
<p><strong>Available Pattern Types:</strong></p>
<ul>
<li><code>state_machine</code> - State transition logic</li>
<li><code>coordinator</code> - Function orchestrating multiple operations</li>
<li>Pattern detection threshold: 0.7 confidence (from <code>src/io/writers/pattern_display.rs:PATTERN_CONFIDENCE_THRESHOLD</code>)</li>
</ul>
<h2 id="test-file-detection"><a class="header" href="#test-file-detection">Test File Detection</a></h2>
<p>Debtmap automatically labels test files using the <code>file_context_label</code> field (spec 166).</p>
<p><strong>JSON Output:</strong></p>
<pre><code class="language-json">{
  "type": "Function",
  "location": {
    "file": "tests/integration/payment_test.rs",
    "line": 45,
    "function": "test_payment_processing",
    "file_context_label": "TEST FILE"
  }
}
</code></pre>
<p><strong>Labels:</strong></p>
<ul>
<li><code>"TEST FILE"</code> - File is definitively a test file</li>
<li><code>"PROBABLE TEST"</code> - File likely contains tests but not confirmed</li>
</ul>
<p><strong>Source</strong>: <code>file_context_label</code> from <code>src/output/unified.rs:106</code></p>
<h2 id="summary-statistics"><a class="header" href="#summary-statistics">Summary Statistics</a></h2>
<p>The unified format includes summary statistics at the top level.</p>
<p><strong>JSON Output:</strong></p>
<pre><code class="language-json">{
  "format_version": "1.0.0",
  "metadata": {
    "debtmap_version": "0.5.0",
    "generated_at": "2025-12-04T22:15:00Z",
    "project_root": "/home/user/myproject",
    "analysis_type": "full"
  },
  "summary": {
    "total_items": 127,
    "total_debt_score": 2845.6,
    "debt_density": 0.18,
    "total_loc": 15823,
    "by_type": {
      "File": 8,
      "Function": 119
    },
    "by_category": {
      "CodeQuality": 67,
      "Architecture": 12,
      "Testing": 42,
      "Performance": 6
    },
    "score_distribution": {
      "critical": 15,
      "high": 34,
      "medium": 58,
      "low": 20
    }
  },
  "items": []
}
</code></pre>
<p><strong>Source</strong>: <code>UnifiedOutput</code> structure from <code>src/output/unified.rs:18-24</code> and <code>DebtSummary</code> from lines 36-45</p>
<p><strong>Key Summary Fields:</strong></p>
<ul>
<li><code>debt_density</code>: Total debt score per 1000 lines of code</li>
<li><code>by_type</code>: Count of File vs Function debt items</li>
<li><code>by_category</code>: Count by debt category</li>
<li><code>score_distribution</code>: Count of items by priority level</li>
</ul>
<h2 id="beforeafter-refactoring-comparison"><a class="header" href="#beforeafter-refactoring-comparison">Before/After Refactoring Comparison</a></h2>
<p><strong>Before:</strong></p>
<pre><code>Function: process_order
  Cyclomatic: 22
  Cognitive: 35
  Coverage: 15%
  Risk Score: 52.3 (CRITICAL)
  Debt Score: 50 (Critical Complexity)
</code></pre>
<p><strong>After:</strong></p>
<pre><code>Function: process_order (refactored)
  Cyclomatic: 5
  Cognitive: 6
  Coverage: 92%
  Risk Score: 2.1 (LOW)
  Debt Score: 0 (no debt)

Extracted functions:
  - validate_order (Cyclomatic: 4, Coverage: 100%)
  - calculate_totals (Cyclomatic: 3, Coverage: 95%)
  - apply_discounts (Cyclomatic: 6, Coverage: 88%)
  - finalize_order (Cyclomatic: 4, Coverage: 90%)

Impact:
  ✓ Complexity reduced by 77%
  ✓ Coverage improved by 513%
  ✓ Risk reduced by 96%
  ✓ Created 4 focused, testable functions
</code></pre>
<h2 id="well-tested-complex-function-good-example"><a class="header" href="#well-tested-complex-function-good-example">Well-Tested Complex Function (Good Example)</a></h2>
<p>Not all complexity is bad. This example shows a legitimately complex function with excellent test coverage.</p>
<p><strong>Terminal Output:</strong></p>
<pre><code>Function: calculate_tax (WELL TESTED - Good Example!)
  File: src/tax/calculator.rs:78
  Complexity: Cyclomatic=18, Cognitive=22
  Coverage: 98%
  Risk: LOW

  Why this is good:
  - High complexity is necessary (tax rules are complex)
  - Thoroughly tested with 45 test cases
  - Clear documentation of edge cases
  - Good example to follow for other complex logic
</code></pre>
<h2 id="next-steps-2"><a class="header" href="#next-steps-2">Next Steps</a></h2>
<ul>
<li><strong><a href="#output-formats-3">Output Formats</a></strong> - Complete JSON schema and format documentation</li>
<li><strong><a href="#configuration-2">Configuration</a></strong> - Customize thresholds and analysis behavior</li>
<li><strong><a href="#advanced-features">Advanced Features</a></strong> - Purity analysis, entropy dampening, pattern detection</li>
</ul>
<p>For questions or issues, visit <a href="https://github.com/iepathos/debtmap/issues">GitHub Issues</a>.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="compare-analysis"><a class="header" href="#compare-analysis">Compare Analysis</a></h1>
<p>The <code>compare</code> command enables you to track technical debt changes over time by comparing two analysis results. This is essential for validating refactoring efforts, detecting regressions in pull requests, and monitoring project health trends.</p>
<h2 id="implementation-status"><a class="header" href="#implementation-status">Implementation Status</a></h2>
<p><strong>All Features Available Now</strong>:</p>
<ul>
<li>✅ Target location tracking with intelligent fuzzy matching</li>
<li>✅ Detailed improvement percentage calculations (per-item)</li>
<li>✅ Multiple output formats (JSON, Markdown, Terminal, HTML)</li>
<li>✅ Implementation plan parsing for target extraction</li>
<li>✅ Four match strategies (Exact, FunctionLevel, ApproximateName, FileLevel)</li>
<li>✅ Resolved items tracking (debt eliminated)</li>
<li>✅ Improved items detection (score reduction ≥ 30%)</li>
<li>✅ New critical items detection (regressions)</li>
<li>✅ Project health metrics and trends</li>
<li>✅ CI/CD integration support</li>
</ul>
<h2 id="overview-3"><a class="header" href="#overview-3">Overview</a></h2>
<p>The compare command analyzes differences between “before” and “after” debtmap analyses, providing:</p>
<ul>
<li><strong>Target location tracking</strong> - Monitor specific code locations through refactoring with fuzzy matching</li>
<li><strong>Validation tracking</strong> - Verify debt items are resolved or improved</li>
<li><strong>Project health metrics</strong> - Track overall debt trends across your codebase</li>
<li><strong>Regression detection</strong> - Identify new critical debt items introduced (score ≥ 60.0)</li>
<li><strong>Improvement tracking</strong> - Measure and celebrate debt reduction with detailed per-item metrics</li>
<li><strong>CI/CD integration</strong> - Automate quality gates in your pipeline</li>
</ul>
<h2 id="basic-usage"><a class="header" href="#basic-usage">Basic Usage</a></h2>
<h3 id="command-syntax"><a class="header" href="#command-syntax">Command Syntax</a></h3>
<pre><code class="language-bash">debtmap compare \
  --before path/to/before.json \
  --after path/to/after.json \
  --output validation.json
</code></pre>
<h3 id="command-line-options"><a class="header" href="#command-line-options">Command-Line Options</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Option</th><th>Required</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>--before FILE</code></td><td>Yes</td><td>Path to “before” analysis JSON</td></tr>
<tr><td><code>--after FILE</code></td><td>Yes</td><td>Path to “after” analysis JSON</td></tr>
<tr><td><code>--output FILE</code></td><td>No</td><td>Output file path (default: stdout)</td></tr>
<tr><td><code>--plan FILE</code></td><td>No</td><td>Implementation plan to extract target location</td></tr>
<tr><td><code>--target-location LOCATION</code></td><td>No</td><td>Manual target location (format: <code>file:function:line</code>)</td></tr>
<tr><td><code>--format FORMAT</code></td><td>No</td><td>Output format: <code>json</code>, <code>markdown</code>, <code>terminal</code>, or <code>html</code> (default: json)</td></tr>
</tbody>
</table>
</div>
<p>All comparison features are available now, including target location tracking, fuzzy matching, and multiple output formats.</p>
<h2 id="target-location-tracking"><a class="header" href="#target-location-tracking">Target Location Tracking</a></h2>
<p>Target location tracking allows you to monitor specific code locations through refactoring changes. The compare command uses intelligent fuzzy matching to find your target even when code is moved or renamed.</p>
<h3 id="location-format"><a class="header" href="#location-format">Location Format</a></h3>
<p>Target locations use the format: <code>file:function:line</code></p>
<p>Examples:</p>
<ul>
<li><code>src/main.rs:complex_function:42</code></li>
<li><code>lib/parser.rs:parse_expression:156</code></li>
<li><code>api/handler.rs:process_request:89</code></li>
</ul>
<h3 id="specifying-target-locations"><a class="header" href="#specifying-target-locations">Specifying Target Locations</a></h3>
<h4 id="option-1-via-implementation-plan"><a class="header" href="#option-1-via-implementation-plan">Option 1: Via Implementation Plan</a></h4>
<p>Create an <code>IMPLEMENTATION_PLAN.md</code> file with a target location section:</p>
<pre><code class="language-markdown"># Implementation Plan

## Target Item
**Location**: ./src/example.rs:complex_function:45
**Current Debt Score**: 85.5
**Severity**: critical

## Problem Analysis
The `complex_function` has high cognitive complexity...

## Proposed Solution
1. Extract nested conditionals into separate functions
2. Use early returns to reduce nesting depth
3. Add comprehensive unit tests
</code></pre>
<p>Then run compare with the plan:</p>
<pre><code class="language-bash">debtmap compare --before before.json --after after.json --plan IMPLEMENTATION_PLAN.md
</code></pre>
<h4 id="option-2-manual-target-location"><a class="header" href="#option-2-manual-target-location">Option 2: Manual Target Location</a></h4>
<p>Specify the target directly via command-line:</p>
<pre><code class="language-bash">debtmap compare \
  --before before.json \
  --after after.json \
  --target-location "src/example.rs:complex_function:45"
</code></pre>
<h3 id="matching-strategies"><a class="header" href="#matching-strategies">Matching Strategies</a></h3>
<p>Debtmap uses intelligent matching to find your target item even when code changes. The matcher tries multiple strategies in order, using the most precise match available:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Strategy</th><th>When Used</th><th>Confidence</th></tr>
</thead>
<tbody>
<tr><td><strong>Exact</strong></td><td><code>file:function:line</code> matches exactly</td><td>1.0</td></tr>
<tr><td><strong>FunctionLevel</strong></td><td><code>file:function</code> matches (any line)</td><td>0.8</td></tr>
<tr><td><strong>ApproximateName</strong></td><td>Fuzzy name matching finds similar function</td><td>0.6</td></tr>
<tr><td><strong>FileLevel</strong></td><td>All items in file match</td><td>0.4</td></tr>
</tbody>
</table>
</div>
<p>The comparison result includes the match strategy and confidence score used, along with the count of matched items (useful when fuzzy matching finds multiple candidates).</p>
<h3 id="target-status-values"><a class="header" href="#target-status-values">Target Status Values</a></h3>
<p>After comparing, the target item will have one of these statuses:</p>
<ul>
<li><strong>Resolved</strong> - Item no longer exists in after analysis (debt eliminated!)</li>
<li><strong>Improved</strong> - Item exists but with lower debt score</li>
<li><strong>Unchanged</strong> - Item exists with similar metrics (within 5%)</li>
<li><strong>Regressed</strong> - Item exists but got worse</li>
<li><strong>NotFoundBefore</strong> - Item didn’t exist in before analysis</li>
<li><strong>NotFound</strong> - Item not found in either analysis</li>
</ul>
<h2 id="project-health-metrics"><a class="header" href="#project-health-metrics">Project Health Metrics</a></h2>
<p>The compare command tracks project-wide health metrics to show overall trends.</p>
<h3 id="tracked-metrics"><a class="header" href="#tracked-metrics">Tracked Metrics</a></h3>
<pre><code class="language-json">{
  "project_health": {
    "before": {
      "total_debt_score": 450.5,
      "total_items": 25,
      "critical_items": 5,
      "high_priority_items": 12,
      "average_score": 18.02
    },
    "after": {
      "total_debt_score": 380.2,
      "total_items": 22,
      "critical_items": 3,
      "high_priority_items": 10,
      "average_score": 17.28
    },
    "changes": {
      "debt_score_change": -70.3,
      "debt_score_change_pct": -15.6,
      "items_change": -3,
      "critical_items_change": -2
    }
  }
}
</code></pre>
<h3 id="understanding-metrics"><a class="header" href="#understanding-metrics">Understanding Metrics</a></h3>
<ul>
<li><strong>total_debt_score</strong> - Sum of all debt item scores</li>
<li><strong>total_items</strong> - Total number of debt items detected</li>
<li><strong>critical_items</strong> - Items with score ≥ 60.0 (critical threshold)</li>
<li><strong>high_priority_items</strong> - Items with score ≥ 40.0 (high priority threshold)</li>
<li><strong>average_score</strong> - Mean debt score across all items</li>
<li><strong>debt_score_change</strong> - Absolute change in total debt</li>
<li><strong>debt_score_change_pct</strong> - Percentage change in total debt</li>
</ul>
<h3 id="debt-trends"><a class="header" href="#debt-trends">Debt Trends</a></h3>
<p>The comparison calculates an overall debt trend based on the percentage change:</p>
<ul>
<li><strong>Improving</strong> - Debt decreased by more than 5%</li>
<li><strong>Stable</strong> - Debt changed by less than 5% (within normal variance)</li>
<li><strong>Regressing</strong> - Debt increased by more than 5%</li>
</ul>
<h2 id="regression-detection"><a class="header" href="#regression-detection">Regression Detection</a></h2>
<p>Regressions are new critical debt items (score ≥ 60.0) that appear in the after analysis.</p>
<h3 id="what-counts-as-a-regression"><a class="header" href="#what-counts-as-a-regression">What Counts as a Regression</a></h3>
<p>A regression is detected when:</p>
<ol>
<li>An item exists in the after analysis</li>
<li>The item does NOT exist in the before analysis</li>
<li>The item has a debt score ≥ 60.0 (critical severity threshold)</li>
</ol>
<h3 id="regression-output"><a class="header" href="#regression-output">Regression Output</a></h3>
<p>The compare command returns a <code>ComparisonResult</code> with detailed regression information:</p>
<pre><code class="language-json">{
  "regressions": [
    {
      "location": "src/new_feature.rs:process_data:23",
      "score": 65.5,
      "debt_type": "high_complexity",
      "description": "Function has cyclomatic complexity of 12 and cognitive complexity of 15"
    }
  ]
}
</code></pre>
<p>Each regression item includes:</p>
<ul>
<li><strong>location</strong> - Full path with function and line number</li>
<li><strong>score</strong> - Debt score (≥ 60.0 for regressions)</li>
<li><strong>debt_type</strong> - Type of debt detected (e.g., “high_complexity”, “god_object”)</li>
<li><strong>description</strong> - Human-readable explanation of the issue</li>
</ul>
<h3 id="using-regressions-in-cicd"><a class="header" href="#using-regressions-in-cicd">Using Regressions in CI/CD</a></h3>
<p>Fail your CI build if regressions are detected:</p>
<pre><code class="language-bash"># Run comparison
debtmap compare --before before.json --after after.json --output result.json

# Check for regressions
REGRESSION_COUNT=$(jq '.regressions | length' result.json)

if [ "$REGRESSION_COUNT" -gt 0 ]; then
  echo "❌ Regression detected - $REGRESSION_COUNT new critical debt items found"
  jq '.regressions[]' result.json
  exit 1
fi

# Check overall debt trend
TREND=$(jq -r '.summary.overall_debt_trend' result.json)
if [ "$TREND" = "Regressing" ]; then
  echo "⚠️ Warning: Overall debt is increasing"
fi
</code></pre>
<h2 id="improvement-tracking"><a class="header" href="#improvement-tracking">Improvement Tracking</a></h2>
<p>The compare command tracks improvements as a list of <code>ImprovementItem</code> objects with detailed before/after metrics.</p>
<h3 id="improvement-types"><a class="header" href="#improvement-types">Improvement Types</a></h3>
<p>The <code>ImprovementType</code> enum (src/comparison/types.rs:120-126) defines four improvement categories:</p>
<ul>
<li><strong>Resolved</strong> - Debt item completely eliminated (no longer in after analysis) ✓ <em>Auto-detected</em></li>
<li><strong>ScoreReduced</strong> - Overall debt score reduced significantly (≥ 30% reduction) ✓ <em>Auto-detected</em></li>
<li><strong>ComplexityReduced</strong> - Cyclomatic or cognitive complexity decreased (defined but not auto-classified)</li>
<li><strong>CoverageImproved</strong> - Test coverage increased (defined but not auto-classified)</li>
</ul>
<p><strong>Note</strong>: Currently, the comparator automatically detects and classifies <code>Resolved</code> and <code>ScoreReduced</code> improvements (src/comparison/comparator.rs:134-191). The <code>ComplexityReduced</code> and <code>CoverageImproved</code> types are defined in the type system but not yet auto-assigned during comparison. Future versions may extend auto-detection to classify complexity and coverage improvements.</p>
<h3 id="improvement-items-structure"><a class="header" href="#improvement-items-structure">Improvement Items Structure</a></h3>
<pre><code class="language-json">{
  "improvements": [
    {
      "location": "src/example.rs:complex_function:45",
      "before_score": 68.5,
      "after_score": 35.2,
      "improvement_type": "ScoreReduced"
    },
    {
      "location": "src/legacy.rs:old_code:120",
      "before_score": 72.0,
      "after_score": null,
      "improvement_type": "Resolved"
    },
    {
      "location": "src/utils.rs:helper_function:88",
      "before_score": 45.0,
      "after_score": 28.0,
      "improvement_type": "ComplexityReduced"
    }
  ]
}
</code></pre>
<p>Each improvement item includes:</p>
<ul>
<li><strong>location</strong> - Full path with function and line number</li>
<li><strong>before_score</strong> - Original debt score</li>
<li><strong>after_score</strong> - New debt score (null if resolved)</li>
<li><strong>improvement_type</strong> - Type of improvement achieved</li>
</ul>
<h2 id="beforeafter-metrics"><a class="header" href="#beforeafter-metrics">Before/After Metrics</a></h2>
<p>When you specify a target location (via <code>--plan</code> or <code>--target-location</code>), the compare command provides detailed before/after metrics for that specific code location.</p>
<h3 id="target-item-comparison"><a class="header" href="#target-item-comparison">Target Item Comparison</a></h3>
<pre><code class="language-json">{
  "target_item": {
    "location": "src/example.rs:complex_function:45",
    "match_strategy": "Exact",
    "match_confidence": 1.0,
    "matched_items_count": 1,
    "before": {
      "score": 68.5,
      "cyclomatic_complexity": 8,
      "cognitive_complexity": 15,
      "coverage": 45.0,
      "function_length": 120,
      "nesting_depth": 4
    },
    "after": {
      "score": 35.1,
      "cyclomatic_complexity": 3,
      "cognitive_complexity": 5,
      "coverage": 85.0,
      "function_length": 45,
      "nesting_depth": 2
    },
    "improvements": {
      "score_reduction_pct": 48.8,
      "complexity_reduction_pct": 66.7,
      "coverage_improvement_pct": 88.9
    },
    "status": "Improved"
  }
}
</code></pre>
<h3 id="target-metrics-fields"><a class="header" href="#target-metrics-fields">Target Metrics Fields</a></h3>
<p>Each <code>TargetMetrics</code> object (before/after) includes:</p>
<ul>
<li><strong>score</strong> - Unified debt score</li>
<li><strong>cyclomatic_complexity</strong> - Cyclomatic complexity metric</li>
<li><strong>cognitive_complexity</strong> - Cognitive complexity metric</li>
<li><strong>coverage</strong> - Test coverage percentage</li>
<li><strong>function_length</strong> - Lines of code in function</li>
<li><strong>nesting_depth</strong> - Maximum nesting depth</li>
</ul>
<h3 id="improvement-percentages"><a class="header" href="#improvement-percentages">Improvement Percentages</a></h3>
<p>The <code>improvements</code> object provides percentage improvements:</p>
<ul>
<li><strong>score_reduction_pct</strong> - Percentage reduction in overall debt score</li>
<li><strong>complexity_reduction_pct</strong> - Reduction in cyclomatic/cognitive complexity</li>
<li><strong>coverage_improvement_pct</strong> - Increase in test coverage</li>
</ul>
<h3 id="metric-aggregation"><a class="header" href="#metric-aggregation">Metric Aggregation</a></h3>
<p>When multiple items match the target location (due to fuzzy matching), metrics are aggregated:</p>
<ul>
<li><strong>score</strong> - Average across matched items</li>
<li><strong>cyclomatic_complexity</strong> - Average</li>
<li><strong>cognitive_complexity</strong> - Average</li>
<li><strong>coverage</strong> - Average</li>
<li><strong>function_length</strong> - Average</li>
<li><strong>nesting_depth</strong> - Maximum (worst case)</li>
</ul>
<p>The <code>matched_items_count</code> field tells you how many items were aggregated.</p>
<h3 id="validating-refactoring-success"><a class="header" href="#validating-refactoring-success">Validating Refactoring Success</a></h3>
<p>Use the comparison output to verify your refactoring:</p>
<pre><code class="language-bash"># Check target status
STATUS=$(jq -r '.target_item.status' result.json)
SCORE_REDUCTION=$(jq '.target_item.improvements.score_reduction_pct' result.json)

echo "Target Status: $STATUS"
echo "Score Reduction: ${SCORE_REDUCTION}%"

# Check for improvements
IMPROVEMENT_COUNT=$(jq '.improvements | length' result.json)
echo "Improvements: $IMPROVEMENT_COUNT items"

# Verify no regressions
REGRESSION_COUNT=$(jq '.regressions | length' result.json)
if [ "$REGRESSION_COUNT" -eq 0 ]; then
  echo "✅ No regressions detected!"
else
  echo "⚠️ $REGRESSION_COUNT new critical items"
fi
</code></pre>
<h2 id="output-formats-2"><a class="header" href="#output-formats-2">Output Formats</a></h2>
<h3 id="json-format"><a class="header" href="#json-format">JSON Format</a></h3>
<p>The default JSON format provides complete comparison results:</p>
<pre><code class="language-bash">debtmap compare --before before.json --after after.json --output result.json
</code></pre>
<p>The <code>ComparisonResult</code> JSON output (src/comparison/types.rs:3-22) includes:</p>
<ul>
<li><code>metadata</code> - Comparison metadata (timestamp, file paths, target location)</li>
<li><code>target_item</code> - Target item comparison with before/after metrics (if specified)</li>
<li><code>project_health</code> - Project-wide health metrics comparison</li>
<li><code>regressions</code> - List of new critical items (score ≥ 60.0)</li>
<li><code>improvements</code> - List of improved/resolved items (≥30% score reduction or resolved)</li>
<li><code>summary</code> - Summary statistics and overall debt trend</li>
</ul>
<p>Example output:</p>
<pre><code class="language-json">{
  "metadata": {
    "comparison_date": "2024-01-15T10:30:00Z",
    "before_file": "before.json",
    "after_file": "after.json",
    "target_location": "src/example.rs:complex_function:45"
  },
  "target_item": {
    "status": "Improved",
    "improvements": {
      "score_reduction_pct": 48.8
    }
  },
  "summary": {
    "target_improved": true,
    "new_critical_count": 0,
    "resolved_count": 3,
    "overall_debt_trend": "Improving"
  }
}
</code></pre>
<h3 id="markdown-format"><a class="header" href="#markdown-format">Markdown Format</a></h3>
<p>Generate human-readable markdown reports for pull request comments:</p>
<pre><code class="language-bash">debtmap compare --before before.json --after after.json --format markdown
</code></pre>
<p><strong>Implementation</strong>: src/io/writers/markdown/mod.rs, src/main.rs:1027-1072</p>
<p>The markdown output is suitable for:</p>
<ul>
<li>Pull request comments</li>
<li>Documentation</li>
<li>Email reports</li>
<li>Team dashboards</li>
</ul>
<h3 id="terminal-format"><a class="header" href="#terminal-format">Terminal Format</a></h3>
<p>Display colorized output directly in the terminal:</p>
<pre><code class="language-bash">debtmap compare --before before.json --after after.json --format terminal
</code></pre>
<p><strong>Implementation</strong>: src/io/writers/terminal.rs, src/main.rs:1011</p>
<p>The terminal format provides:</p>
<ul>
<li>Color-coded status indicators</li>
<li>Formatted tables for metrics</li>
<li>Human-readable summaries</li>
<li>Easy scanning of results</li>
</ul>
<h3 id="html-format"><a class="header" href="#html-format">HTML Format</a></h3>
<p>Generate HTML reports with structured styling:</p>
<pre><code class="language-bash">debtmap compare --before before.json --after after.json --format html
</code></pre>
<p><strong>Implementation</strong>: src/io/writers/html.rs</p>
<p>The HTML format is suitable for:</p>
<ul>
<li>Web-based dashboards</li>
<li>Archived reports</li>
<li>Integration with documentation sites</li>
</ul>
<h2 id="cicd-integration-3"><a class="header" href="#cicd-integration-3">CI/CD Integration</a></h2>
<h3 id="github-actions-example"><a class="header" href="#github-actions-example">GitHub Actions Example</a></h3>
<pre><code class="language-yaml">name: Technical Debt Check

on: [pull_request]

jobs:
  debt-check:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
        with:
          fetch-depth: 0  # Need history for before/after

      - name: Install debtmap
        run: cargo install debtmap

      - name: Analyze main branch
        run: |
          git checkout main
          debtmap analyze --output before.json

      - name: Analyze PR branch
        run: |
          git checkout ${{ github.head_ref }}
          debtmap analyze --output after.json

      - name: Compare analyses
        run: |
          debtmap compare \
            --before before.json \
            --after after.json \
            --output comparison.json

      - name: Check comparison result
        run: |
          TREND=$(jq -r '.summary.overall_debt_trend' comparison.json)
          REGRESSION_COUNT=$(jq '.regressions | length' comparison.json)
          IMPROVEMENT_COUNT=$(jq '.improvements | length' comparison.json)

          echo "Debt Trend: $TREND"
          echo "Regressions: $REGRESSION_COUNT"
          echo "Improvements: $IMPROVEMENT_COUNT"

          # Fail on regression
          if [ "$REGRESSION_COUNT" -gt 0 ]; then
            echo "❌ Regression detected"
            jq '.regressions[]' comparison.json
            exit 1
          fi

          # Warn if debt is increasing
          if [ "$TREND" = "Regressing" ]; then
            echo "⚠️ Warning: Overall debt is increasing"
          fi

      - name: Post comparison to PR
        uses: actions/github-script@v6
        with:
          script: |
            const fs = require('fs');
            const comparison = JSON.parse(fs.readFileSync('comparison.json', 'utf8'));

            const body = `## Technical Debt Comparison

            **Overall Trend:** ${comparison.summary.overall_debt_trend}
            **Regressions:** ${comparison.summary.new_critical_count}
            **Improvements:** ${comparison.summary.resolved_count}

            ${comparison.improvements.length &gt; 0 ? `
            ### Improvements
            ${comparison.improvements.map(i =&gt; `- ${i.location}: ${i.before_score.toFixed(1)} → ${i.after_score ? i.after_score.toFixed(1) : 'resolved'}`).join('\n')}
            ` : ''}

            ${comparison.regressions.length &gt; 0 ? `
            ### ⚠️ Regressions
            ${comparison.regressions.map(r =&gt; `- ${r.location}: ${r.score.toFixed(1)} (${r.debt_type})`).join('\n')}
            ` : ''}`;

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: body
            });
</code></pre>
<h3 id="gitlab-ci-example"><a class="header" href="#gitlab-ci-example">GitLab CI Example</a></h3>
<pre><code class="language-yaml">debt_check:
  stage: test
  script:
    # Analyze main branch
    - git fetch origin main
    - git checkout origin/main
    - debtmap analyze --output before.json

    # Analyze current branch
    - git checkout $CI_COMMIT_SHA
    - debtmap analyze --output after.json

    # Compare and check status
    - debtmap compare --before before.json --after after.json --output comparison.json
    - |
      TREND=$(jq -r '.summary.overall_debt_trend' comparison.json)
      REGRESSION_COUNT=$(jq '.regressions | length' comparison.json)

      echo "Debt Trend: $TREND"
      echo "Regressions: $REGRESSION_COUNT"

      if [ "$REGRESSION_COUNT" -gt 0 ]; then
        echo "Failed: Regression detected"
        jq '.regressions[]' comparison.json
        exit 1
      fi
  artifacts:
    paths:
      - before.json
      - after.json
      - comparison.json
    expire_in: 1 week
</code></pre>
<h3 id="best-practices-for-cicd"><a class="header" href="#best-practices-for-cicd">Best Practices for CI/CD</a></h3>
<ol>
<li><strong>Store analyses as artifacts</strong> - Keep before/after JSON for debugging</li>
<li><strong>Check status field</strong> - Use <code>status</code> to determine pass/fail</li>
<li><strong>Track completion percentage</strong> - Monitor progress toward debt resolution</li>
<li><strong>Review improvements</strong> - Celebrate and document successful refactorings</li>
<li><strong>Act on remaining issues</strong> - Create follow-up tasks for unresolved items</li>
<li><strong>Set completion thresholds</strong> - Require minimum completion percentage for merges</li>
</ol>
<h2 id="practical-examples"><a class="header" href="#practical-examples">Practical Examples</a></h2>
<h3 id="example-1-basic-comparison"><a class="header" href="#example-1-basic-comparison">Example 1: Basic Comparison</a></h3>
<p>Compare two analyses to track debt changes:</p>
<pre><code class="language-bash"># Run before analysis
debtmap analyze --output before.json

# Make changes to codebase...

# Run after analysis
debtmap analyze --output after.json

# Compare
debtmap compare --before before.json --after after.json --output comparison.json

# Check results
cat comparison.json | jq '.'
# Output shows: target_item, project_health, regressions, improvements, summary
</code></pre>
<h3 id="example-2-validating-function-refactoring"><a class="header" href="#example-2-validating-function-refactoring">Example 2: Validating Function Refactoring</a></h3>
<p>Validate your refactoring work with target location tracking:</p>
<pre><code class="language-bash"># Run before analysis
debtmap analyze --output before.json

# Identify critical items to fix
jq '.items[] | select(.unified_score.final_score &gt;= 60.0)' before.json

# Refactor the high-priority functions...

# Run after analysis
debtmap analyze --output after.json

# Compare and validate with target location
debtmap compare \
  --before before.json \
  --after after.json \
  --target-location "src/example.rs:complex_function:45" \
  --output comparison.json

# Check target status
STATUS=$(jq -r '.target_item.status' comparison.json)
SCORE_REDUCTION=$(jq '.target_item.improvements.score_reduction_pct' comparison.json)

echo "Target Status: $STATUS"
echo "Score Reduction: ${SCORE_REDUCTION}%"

# Review all improvements
jq '.improvements[]' comparison.json
</code></pre>
<h3 id="example-3-detecting-pr-regressions"><a class="header" href="#example-3-detecting-pr-regressions">Example 3: Detecting PR Regressions</a></h3>
<p>Check if a pull request introduces new critical debt:</p>
<pre><code class="language-bash"># Analyze base branch
git checkout main
debtmap analyze --output main.json

# Analyze PR branch
git checkout feature/new-feature
debtmap analyze --output feature.json

# Compare
debtmap compare \
  --before main.json \
  --after feature.json \
  --output comparison.json

# Check for regressions
REGRESSION_COUNT=$(jq '.regressions | length' comparison.json)
TREND=$(jq -r '.summary.overall_debt_trend' comparison.json)

echo "Regressions: $REGRESSION_COUNT"
echo "Debt Trend: $TREND"

# Example output structure:
jq '.' comparison.json
# {
#   "summary": {
#     "overall_debt_trend": "Improving",  // or "Regressing"
#     "new_critical_count": 0,
#     "resolved_count": 3
#   },
#   "regressions": [],
#   "improvements": [...],
#   "project_health": {...}
# }
</code></pre>
<h3 id="example-4-monitoring-project-health-over-releases"><a class="header" href="#example-4-monitoring-project-health-over-releases">Example 4: Monitoring Project Health Over Releases</a></h3>
<p>Track overall project health across releases:</p>
<pre><code class="language-bash"># Analyze release v1.0
git checkout v1.0
debtmap analyze --output v1.0.json

# Analyze release v1.1
git checkout v1.1
debtmap analyze --output v1.1.json

# Compare
debtmap compare \
  --before v1.0.json \
  --after v1.1.json \
  --output v1.0-to-v1.1.json

# Check project health metrics
echo "Before (v1.0):"
jq '.project_health.before' v1.0-to-v1.1.json

echo "After (v1.1):"
jq '.project_health.after' v1.0-to-v1.1.json

# Check overall trend
TREND=$(jq -r '.summary.overall_debt_trend' v1.0-to-v1.1.json)
DEBT_CHANGE=$(jq '.project_health.changes.debt_score_change_pct' v1.0-to-v1.1.json)
echo "Debt Trend: $TREND"
echo "Debt Score Change: ${DEBT_CHANGE}%"
</code></pre>
<h3 id="example-5-full-cicd-workflow"><a class="header" href="#example-5-full-cicd-workflow">Example 5: Full CI/CD Workflow</a></h3>
<p>Complete workflow for continuous debt monitoring:</p>
<pre><code class="language-bash">#!/bin/bash
# ci-debt-check.sh

set -e

BEFORE="before.json"
AFTER="after.json"
COMPARISON="comparison.json"

# Step 1: Analyze baseline (main branch)
echo "📊 Analyzing baseline..."
git checkout main
debtmap analyze --output "$BEFORE"

# Step 2: Analyze current branch
echo "📊 Analyzing current branch..."
git checkout -
debtmap analyze --output "$AFTER"

# Step 3: Run comparison
echo "🔍 Running comparison..."
debtmap compare \
  --before "$BEFORE" \
  --after "$AFTER" \
  --output "$COMPARISON"

# Step 4: Extract metrics
TREND=$(jq -r '.summary.overall_debt_trend' "$COMPARISON")
REGRESSION_COUNT=$(jq '.regressions | length' "$COMPARISON")
IMPROVEMENT_COUNT=$(jq '.improvements | length' "$COMPARISON")
RESOLVED_COUNT=$(jq '.summary.resolved_count' "$COMPARISON")

echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
echo "📈 Debt Comparison Results"
echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
echo "Trend: $TREND"
echo "Regressions: $REGRESSION_COUNT"
echo "Improvements: $IMPROVEMENT_COUNT"
echo "Resolved: $RESOLVED_COUNT"
echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"

# Step 5: Quality gate
if [ "$REGRESSION_COUNT" -gt 0 ]; then
  echo "❌ FAILED: Regression detected"
  jq '.regressions[]' "$COMPARISON"
  exit 1
fi

if [ "$TREND" = "Regressing" ]; then
  echo "⚠️  WARNING: Overall debt is increasing"
  # Don't fail, just warn
fi

if [ "$RESOLVED_COUNT" -gt 0 ]; then
  echo "🎉 SUCCESS: $RESOLVED_COUNT debt items resolved!"
fi

echo "✅ PASSED: No regressions detected"
</code></pre>
<h3 id="example-6-interpreting-comparison-results"><a class="header" href="#example-6-interpreting-comparison-results">Example 6: Interpreting Comparison Results</a></h3>
<p>Understanding the comparison output:</p>
<pre><code class="language-bash"># Run comparison
debtmap compare --before before.json --after after.json --output comparison.json

# Check debt trend
TREND=$(jq -r '.summary.overall_debt_trend' comparison.json)
REGRESSION_COUNT=$(jq '.regressions | length' comparison.json)
IMPROVEMENT_COUNT=$(jq '.improvements | length' comparison.json)

case "$TREND" in
  "Improving")
    echo "🎉 Success! Debt is decreasing"
    echo "Improvements: $IMPROVEMENT_COUNT"
    jq '.improvements[] | "\(.location): \(.improvement_type)"' comparison.json
    ;;
  "Stable")
    echo "➡️  Stable - no significant debt change"
    echo "Improvements: $IMPROVEMENT_COUNT"
    echo "Regressions: $REGRESSION_COUNT"
    ;;
  "Regressing")
    echo "❌ Warning! Debt is increasing"
    echo "New critical items: $REGRESSION_COUNT"
    jq '.regressions[] | "\(.location): \(.score) (\(.debt_type))"' comparison.json
    ;;
esac

# Check if target improved (if target was specified)
if jq -e '.target_item' comparison.json &gt; /dev/null; then
  TARGET_STATUS=$(jq -r '.target_item.status' comparison.json)
  echo "Target Status: $TARGET_STATUS"
fi
</code></pre>
<h2 id="troubleshooting-3"><a class="header" href="#troubleshooting-3">Troubleshooting</a></h2>
<h3 id="understanding-debt-trends"><a class="header" href="#understanding-debt-trends">Understanding Debt Trends</a></h3>
<p><strong>Issue</strong>: Confused about what the debt trend means</p>
<p><strong>Solution</strong>: Check the <code>summary.overall_debt_trend</code> field in comparison output:</p>
<ul>
<li><code>Improving</code> - Total debt decreased by more than 5%</li>
<li><code>Stable</code> - Total debt changed by less than 5% (within normal variance)</li>
<li><code>Regressing</code> - Total debt increased by more than 5%</li>
</ul>
<p><strong>Check the trend</strong>:</p>
<pre><code class="language-bash">TREND=$(jq -r '.summary.overall_debt_trend' comparison.json)
DEBT_CHANGE=$(jq '.project_health.changes.debt_score_change_pct' comparison.json)
echo "Debt Trend: $TREND (${DEBT_CHANGE}% change)"
</code></pre>
<h3 id="no-improvements-detected"><a class="header" href="#no-improvements-detected">No Improvements Detected</a></h3>
<p><strong>Issue</strong>: Made changes but comparison shows no improvements</p>
<p><strong>Possible causes</strong>:</p>
<ol>
<li>Changes didn’t reduce debt scores by ≥30% (improvement threshold)</li>
<li>Refactored items had scores &lt;60.0 (not tracked as critical)</li>
<li>Changes were neutral (e.g., code moved but complexity unchanged)</li>
</ol>
<p><strong>Solution</strong>: Check the details:</p>
<pre><code class="language-bash"># Compare before/after project health
jq '.project_health.before' result.json
jq '.project_health.after' result.json

# Look for critical items in before analysis
jq '.items[] | select(.unified_score.final_score &gt;= 60.0)' before.json
</code></pre>
<h3 id="json-parsing-errors"><a class="header" href="#json-parsing-errors">JSON Parsing Errors</a></h3>
<p><strong>Problem</strong>: <code>Error parsing JSON file</code></p>
<p><strong>Solutions</strong>:</p>
<ol>
<li>Verify the file is valid JSON: <code>jq . before.json</code></li>
<li>Ensure the file is a debtmap analysis output</li>
<li>Check file permissions and path</li>
<li>Regenerate the analysis if corrupted</li>
</ol>
<h3 id="understanding-target-status-values"><a class="header" href="#understanding-target-status-values">Understanding Target Status Values</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Status</th><th>Meaning</th><th>Action Required</th></tr>
</thead>
<tbody>
<tr><td><code>Resolved</code></td><td>Item eliminated completely</td><td>✅ Celebrate! Item no longer exists</td></tr>
<tr><td><code>Improved</code></td><td>Score reduced significantly</td><td>✅ Good progress, verify metrics improved</td></tr>
<tr><td><code>Unchanged</code></td><td>No significant change</td><td>⚠️ Review approach, may need different strategy</td></tr>
<tr><td><code>Regressed</code></td><td>Item got worse</td><td>❌ Investigate and fix before merging</td></tr>
<tr><td><code>NotFoundBefore</code></td><td>Item didn’t exist before</td><td>ℹ️ New code, ensure quality is acceptable</td></tr>
<tr><td><code>NotFound</code></td><td>Item not found in either</td><td>⚠️ Check target location format</td></tr>
</tbody>
</table>
</div>
<h3 id="handling-missing-files"><a class="header" href="#handling-missing-files">Handling Missing Files</a></h3>
<p><strong>Problem</strong>: <code>No such file or directory</code></p>
<p><strong>Solutions</strong>:</p>
<pre><code class="language-bash"># Verify files exist
ls -la before.json after.json

# Check current directory
pwd

# Use absolute paths if needed
debtmap compare \
  --before /absolute/path/to/before.json \
  --after /absolute/path/to/after.json
</code></pre>
<h3 id="interpreting-edge-cases"><a class="header" href="#interpreting-edge-cases">Interpreting Edge Cases</a></h3>
<p><strong>All Items Resolved</strong>:</p>
<pre><code class="language-json">{
  "summary": {
    "resolved_count": 25,
    "new_critical_count": 0,
    "overall_debt_trend": "Improving"
  },
  "project_health": {
    "after": {
      "total_items": 0,
      "critical_items": 0
    }
  }
}
</code></pre>
<p>All debt items resolved - excellent work!</p>
<p><strong>New Project (Empty Before)</strong>:</p>
<pre><code class="language-json">{
  "summary": {
    "new_critical_count": 15,
    "resolved_count": 0,
    "overall_debt_trend": "Stable"
  },
  "project_health": {
    "before": {
      "total_items": 0
    }
  }
}
</code></pre>
<p>New project or first analysis - establish baseline for future comparisons.</p>
<p><strong>No Changes</strong>:</p>
<pre><code class="language-json">{
  "summary": {
    "overall_debt_trend": "Stable",
    "new_critical_count": 0,
    "resolved_count": 0
  },
  "improvements": [],
  "regressions": []
}
</code></pre>
<p>No changes detected - either no code changes or changes were neutral to debt.</p>
<h2 id="related-documentation"><a class="header" href="#related-documentation">Related Documentation</a></h2>
<ul>
<li><a href="#validation-and-quality-gates">Validation Gates</a> - Quality gate thresholds and validation strategies</li>
<li><a href="#prodigy-integration">Prodigy Integration</a> - Automated refactoring workflows with compare validation</li>
<li><a href="#output-formats-3">Output Formats</a> - Understanding analysis JSON structure and formatters</li>
<li><a href="#scoring-strategies">Scoring Strategies</a> - How debt scores are calculated (critical ≥ 60.0, high priority ≥ 40.0)</li>
<li><a href="#threshold-configuration-1">Threshold Configuration</a> - Configuring severity thresholds for your project</li>
</ul>
<h2 id="summary"><a class="header" href="#summary">Summary</a></h2>
<p>The compare command provides validation for refactoring efforts:</p>
<p><strong>Current Capabilities:</strong></p>
<ul>
<li>✅ Target location tracking with intelligent fuzzy matching (4 strategies: Exact, FunctionLevel, ApproximateName, FileLevel)</li>
<li>✅ Detect regressions (new critical items with score ≥ 60.0)</li>
<li>✅ Track resolved items and improvements (≥30% score reduction, auto-detected)</li>
<li>✅ Detailed per-item improvement metrics with before/after scores</li>
<li>✅ Multiple output formats (JSON, Markdown, Terminal, HTML)</li>
<li>✅ Implementation plan parsing for target extraction (via <code>--plan</code> flag)</li>
<li>✅ Project-wide health metrics and debt trends</li>
<li>✅ Automate quality gates in CI/CD pipelines</li>
</ul>
<p><strong>Note</strong>: The compare command is fully implemented with all features available. For improvement classification, <code>Resolved</code> and <code>ScoreReduced</code> types are automatically detected; <code>ComplexityReduced</code> and <code>CoverageImproved</code> are defined for future use.</p>
<p>Use the compare command regularly to maintain visibility into your codebase’s technical health and ensure continuous improvement. All features are fully implemented and ready for production use.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="configuration-2"><a class="header" href="#configuration-2">Configuration</a></h1>
<p>Debtmap is highly configurable through a <code>.debtmap.toml</code> file. This chapter explains how to customize Debtmap’s behavior for your project’s specific needs.</p>
<h2 id="config-files"><a class="header" href="#config-files">Config Files</a></h2>
<p>Debtmap uses <strong>TOML format</strong> for configuration files (<code>.debtmap.toml</code>). TOML provides a clear, readable syntax well-suited for configuration.</p>
<h3 id="creating-a-configuration-file"><a class="header" href="#creating-a-configuration-file">Creating a Configuration File</a></h3>
<p>Debtmap looks for a <code>.debtmap.toml</code> file in the current directory and up to 10 parent directories. To create an initial configuration:</p>
<pre><code class="language-bash">debtmap init
</code></pre>
<p>This command creates a <code>.debtmap.toml</code> file with sensible defaults.</p>
<h3 id="configuration-file-discovery"><a class="header" href="#configuration-file-discovery">Configuration File Discovery</a></h3>
<p>When you run <code>debtmap</code>, it searches for <code>.debtmap.toml</code> starting in your current directory and traversing up to 10 parent directories. The first configuration file found is used.</p>
<p>If no configuration file is found, Debtmap uses built-in defaults that work well for most projects.</p>
<h3 id="basic-example"><a class="header" href="#basic-example">Basic Example</a></h3>
<p>Here’s a minimal <code>.debtmap.toml</code> configuration:</p>
<pre><code class="language-toml">[scoring]
coverage = 0.50      # 50% weight for test coverage gaps
complexity = 0.35    # 35% weight for code complexity
dependency = 0.15    # 15% weight for dependency criticality

[thresholds]
complexity = 10
max_file_length = 500
max_function_length = 50

[languages]
enabled = ["rust", "python", "javascript", "typescript"]
</code></pre>
<h2 id="scoring-configuration"><a class="header" href="#scoring-configuration">Scoring Configuration</a></h2>
<h3 id="scoring-weights"><a class="header" href="#scoring-weights">Scoring Weights</a></h3>
<p>The <code>[scoring]</code> section controls how different factors contribute to the overall debt score. Debtmap uses a <strong>weighted sum model</strong> where weights must sum to 1.0.</p>
<pre><code class="language-toml">[scoring]
coverage = 0.50      # Weight for test coverage gaps (default: 0.50)
complexity = 0.35    # Weight for code complexity (default: 0.35)
dependency = 0.15    # Weight for dependency criticality (default: 0.15)
</code></pre>
<p><strong>Active weights</strong> (used in scoring):</p>
<ul>
<li><code>coverage</code> - Prioritizes untested code (default: 0.50)</li>
<li><code>complexity</code> - Identifies complex areas (default: 0.35)</li>
<li><code>dependency</code> - Considers impact radius (default: 0.15)</li>
</ul>
<p><strong>Unused weights</strong> (reserved for future features):</p>
<ul>
<li><code>semantic</code> - Not currently used (default: 0.00)</li>
<li><code>security</code> - Not currently used (default: 0.00)</li>
<li><code>organization</code> - Not currently used (default: 0.00)</li>
</ul>
<p><strong>Validation rules:</strong></p>
<ul>
<li>All weights must be between 0.0 and 1.0</li>
<li>Active weights (coverage + complexity + dependency) must sum to 1.0 (±0.001 tolerance)</li>
<li>If weights don’t sum to 1.0, they will be automatically normalized</li>
</ul>
<p><strong>Example - Prioritize complexity over coverage:</strong></p>
<pre><code class="language-toml">[scoring]
coverage = 0.30
complexity = 0.55
dependency = 0.15
</code></pre>
<h3 id="role-multipliers-1"><a class="header" href="#role-multipliers-1">Role Multipliers</a></h3>
<p>Role multipliers adjust complexity scores based on a function’s semantic role:</p>
<pre><code class="language-toml">[role_multipliers]
pure_logic = 1.2        # Prioritize pure computation (default: 1.2)
orchestrator = 0.8      # Reduce for delegation functions (default: 0.8)
io_wrapper = 0.7        # Reduce for I/O wrappers (default: 0.7)
entry_point = 0.9       # Slight reduction for main/CLI (default: 0.9)
pattern_match = 0.6     # Reduce for pattern matching (default: 0.6)
debug = 0.3             # Debug/diagnostic functions (default: 0.3)
unknown = 1.0           # No adjustment (default: 1.0)
</code></pre>
<p>These multipliers help reduce false positives by recognizing that different function types have naturally different complexity levels. The <strong>debug</strong> role has the lowest multiplier (0.3) since debug and diagnostic functions typically have low testing priority.</p>
<h3 id="role-based-scoring-configuration"><a class="header" href="#role-based-scoring-configuration">Role-Based Scoring Configuration</a></h3>
<p>DebtMap uses a two-stage role adjustment mechanism to accurately score functions based on their architectural role and testing strategy. This section explains how to configure both stages.</p>
<h4 id="stage-1-role-coverage-weights"><a class="header" href="#stage-1-role-coverage-weights">Stage 1: Role Coverage Weights</a></h4>
<p>The first stage adjusts how much coverage gaps penalize different function types. This recognizes that not all functions need the same level of unit test coverage.</p>
<p><strong>Configuration</strong> (<code>.debtmap.toml</code> under <code>[scoring.role_coverage_weights]</code>):</p>
<pre><code class="language-toml">[scoring.role_coverage_weights]
entry_point = 0.6       # Reduce coverage penalty (often integration tested)
orchestrator = 0.8      # Reduce coverage penalty (tested via higher-level tests)
pure_logic = 1.0        # Pure logic should have unit tests, no reduction (default: 1.0)
io_wrapper = 0.5        # I/O wrappers are integration tested (default: 0.5)
pattern_match = 1.0     # Standard penalty
debug = 0.3             # Debug functions have lowest coverage expectations (default: 0.3)
unknown = 1.0           # Standard penalty (default behavior)
</code></pre>
<p><strong>Rationale</strong>:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Function Role</th><th>Weight</th><th>Why This Value?</th></tr>
</thead>
<tbody>
<tr><td><strong>Entry Point</strong></td><td>0.6</td><td>CLI handlers, HTTP routes, <code>main</code> functions are integration tested, not unit tested</td></tr>
<tr><td><strong>Orchestrator</strong></td><td>0.8</td><td>Coordination functions tested via higher-level tests</td></tr>
<tr><td><strong>Pure Logic</strong></td><td>1.0</td><td>Core business logic should have unit tests (default: 1.0)</td></tr>
<tr><td><strong>I/O Wrapper</strong></td><td>0.5</td><td>File/network operations tested via integration tests (default: 0.5)</td></tr>
<tr><td><strong>Pattern Match</strong></td><td>1.0</td><td>Standard coverage expectations</td></tr>
<tr><td><strong>Debug</strong></td><td>0.3</td><td>Debug/diagnostic functions have lowest testing priority (default: 0.3)</td></tr>
<tr><td><strong>Unknown</strong></td><td>1.0</td><td>Default when role cannot be determined</td></tr>
</tbody>
</table>
</div>
<p><strong>Example Impact</strong>:</p>
<pre><code class="language-toml"># Emphasize pure logic testing strongly
[scoring.role_coverage_weights]
pure_logic = 1.5        # 50% higher penalty for untested logic
entry_point = 0.5       # 50% lower penalty for untested entry points
io_wrapper = 0.4        # 60% lower penalty for untested I/O

# Conservative approach (smaller adjustments)
[scoring.role_coverage_weights]
pure_logic = 1.1        # Only 10% increase
entry_point = 0.9       # Only 10% decrease
</code></pre>
<p><strong>How It Works</strong>:</p>
<p>When a function has 0% coverage:</p>
<ul>
<li><strong>Entry Point</strong> (weight 0.6): Gets 60% penalty instead of 100% penalty</li>
<li><strong>Pure Logic</strong> (weight 1.0): Gets 100% penalty (standard emphasis on testing)</li>
<li><strong>I/O Wrapper</strong> (weight 0.5): Gets 50% penalty</li>
</ul>
<p>This prevents entry points from dominating the priority list due to low unit test coverage while emphasizing the importance of testing pure business logic.</p>
<h4 id="stage-2-role-multiplier-with-clamping"><a class="header" href="#stage-2-role-multiplier-with-clamping">Stage 2: Role Multiplier with Clamping</a></h4>
<p>The second stage applies a final role-based multiplier to reflect architectural importance. This multiplier is <strong>clamped by default</strong> to prevent extreme score variations.</p>
<p><strong>Configuration</strong> (<code>.debtmap.toml</code> under <code>[scoring.role_multiplier]</code>):</p>
<pre><code class="language-toml">[scoring.role_multiplier]
clamp_min = 0.3           # Minimum multiplier (default: 0.3)
clamp_max = 1.8           # Maximum multiplier (default: 1.8)
enable_clamping = true    # Enable clamping (default: true)
</code></pre>
<p><strong>Parameters</strong>:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Parameter</th><th>Default</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>clamp_min</code></td><td>0.3</td><td>Minimum allowed multiplier - prevents functions from becoming invisible</td></tr>
<tr><td><code>clamp_max</code></td><td>1.8</td><td>Maximum allowed multiplier - prevents extreme score spikes</td></tr>
<tr><td><code>enable_clamping</code></td><td>true</td><td>Whether to apply clamping (disable for prototyping only)</td></tr>
</tbody>
</table>
</div>
<p><strong>Clamp Range Rationale</strong>:</p>
<p><strong>Default [0.3, 1.8]</strong>: Balances differentiation with stability</p>
<ul>
<li>
<p><strong>Lower bound (0.3)</strong>: I/O wrappers still contribute 30% of their base score</p>
<ul>
<li>Prevents them from becoming invisible in the priority list</li>
<li>Ensures simple wrappers aren’t completely ignored</li>
</ul>
</li>
<li>
<p><strong>Upper bound (1.8)</strong>: Critical functions get at most 180% of base score</p>
<ul>
<li>Prevents one complex function from dominating the entire list</li>
<li>Maintains balanced prioritization across different issues</li>
</ul>
</li>
</ul>
<p><strong>When to Adjust Clamp Range</strong>:</p>
<pre><code class="language-toml"># Wider range for more differentiation
[scoring.role_multiplier]
clamp_min = 0.2           # Allow more reduction
clamp_max = 2.5           # Allow more emphasis

# Narrower range for more stability
[scoring.role_multiplier]
clamp_min = 0.5           # Less reduction
clamp_max = 1.5           # Less emphasis

# Disable clamping (not recommended for production)
[scoring.role_multiplier]
enable_clamping = false   # Allow unclamped multipliers
# Warning: May cause unstable prioritization
</code></pre>
<p><strong>When to Disable Clamping</strong>:</p>
<ul>
<li><strong>Prototyping</strong>: Testing extreme multiplier values for custom scoring strategies</li>
<li><strong>Special cases</strong>: Very specific project needs requiring wide multiplier ranges</li>
<li><strong>Not recommended</strong> for production use as it can lead to unstable prioritization</li>
</ul>
<p><strong>Example Impact</strong>:</p>
<p>Without clamping:</p>
<pre><code>Function: critical_business_logic (Pure Logic)
  Base Score: 45.0
  Role Multiplier: 2.5 (unclamped)
  Final Score: 112.5 (dominates entire list)
</code></pre>
<p>With clamping (default):</p>
<pre><code>Function: critical_business_logic (Pure Logic)
  Base Score: 45.0
  Role Multiplier: 1.8 (clamped from 2.5)
  Final Score: 81.0 (high priority, but balanced)
</code></pre>
<h4 id="complete-example-configuration"><a class="header" href="#complete-example-configuration">Complete Example Configuration</a></h4>
<p>Here’s a complete example showing both stages configured together:</p>
<pre><code class="language-toml"># Stage 1: Coverage weight adjustments
[scoring.role_coverage_weights]
pure_logic = 1.0        # Pure logic should have unit tests (default: 1.0)
entry_point = 0.6       # Reduce penalty for integration-tested entry points
orchestrator = 0.8      # Partially reduce penalty for orchestrators
io_wrapper = 0.5        # I/O wrappers are integration tested (default: 0.5)
pattern_match = 1.0     # Standard
debug = 0.3             # Debug functions have lowest coverage expectations (default: 0.3)
unknown = 1.0           # Standard

# Stage 2: Role multiplier with clamping
[scoring.role_multiplier]
clamp_min = 0.3         # I/O wrappers contribute at least 30%
clamp_max = 1.8         # Critical functions get at most 180%
enable_clamping = true  # Keep clamping enabled for stability
</code></pre>
<h4 id="how-the-two-stages-work-together"><a class="header" href="#how-the-two-stages-work-together">How the Two Stages Work Together</a></h4>
<p>The two-stage approach ensures role-based coverage adjustments and architectural importance multipliers work independently:</p>
<p><strong>Example Workflow</strong>:</p>
<pre><code>1. Calculate base score from complexity (10) and dependencies (5)
   → Base = 15.0

2. Stage 1: Apply coverage weight based on role (Entry Point, weight 0.6)
   → Coverage penalty reduced from 1.0 to 0.4
   → Preliminary score = 15.0 × 0.4 = 6.0

3. Stage 2: Apply clamped role multiplier (Entry Point, multiplier 1.2)
   → Clamped to [0.3, 1.8] → stays 1.2
   → Final score = 6.0 × 1.2 = 7.2
</code></pre>
<p><strong>Key Benefits</strong>:</p>
<ul>
<li>Coverage adjustments don’t interfere with role multiplier</li>
<li>Both mechanisms contribute independently to final score</li>
<li>Clamping prevents instability from extreme values</li>
<li>Configuration flexibility for different project needs</li>
</ul>
<h4 id="verification"><a class="header" href="#verification">Verification</a></h4>
<p>To see how role-based adjustments affect your codebase:</p>
<pre><code class="language-bash"># Show detailed scoring breakdown
debtmap analyze . --verbose

# Look for lines like:
#   Coverage Weight: 0.6 (Entry Point adjustment)
#   Adjusted Coverage Penalty: 0.4 (reduced from 1.0)
#   Role Multiplier: 1.2 (clamped from 1.5)
</code></pre>
<p>For more details on how role-based adjustments reduce false positives, see the <a href="#role-based-adjustments">Role-Based Adjustments</a> section in the Scoring Strategies guide.</p>
<h2 id="thresholds-configuration"><a class="header" href="#thresholds-configuration">Thresholds Configuration</a></h2>
<h3 id="basic-thresholds"><a class="header" href="#basic-thresholds">Basic Thresholds</a></h3>
<p>Control when code is flagged as technical debt:</p>
<pre><code class="language-toml">[thresholds]
complexity = 10                      # Cyclomatic complexity threshold
duplication = 50                     # Duplication threshold
max_file_length = 500                # Maximum lines per file
max_function_length = 50             # Maximum lines per function
</code></pre>
<p><strong>Note:</strong> The TOML configuration accepts <code>max_file_length</code> (shown above), which maps to the internal struct field <code>max_file_lines</code>. Both names refer to the same setting.</p>
<h3 id="minimum-thresholds"><a class="header" href="#minimum-thresholds">Minimum Thresholds</a></h3>
<p>Filter out trivial functions that aren’t really technical debt:</p>
<pre><code class="language-toml">[thresholds]
minimum_debt_score = 2.0              # Only show items with debt score ≥ 2.0
minimum_cyclomatic_complexity = 3     # Ignore functions with cyclomatic &lt; 3
minimum_cognitive_complexity = 5      # Ignore functions with cognitive &lt; 5
minimum_risk_score = 2.0              # Only show Risk items with score ≥ 2.0
</code></pre>
<p>These minimum thresholds help focus on significant issues by filtering out simple functions with minor complexity.</p>
<h3 id="validation-thresholds"><a class="header" href="#validation-thresholds">Validation Thresholds</a></h3>
<p>The <code>[thresholds.validation]</code> subsection configures limits for the <code>debtmap validate</code> command:</p>
<pre><code class="language-toml">[thresholds.validation]
max_average_complexity = 10.0         # Maximum allowed average complexity (default: 10.0)
max_high_complexity_count = 100       # DEPRECATED: Use max_debt_density instead (default: 100)
max_debt_items = 2000                 # DEPRECATED: Use max_debt_density instead (default: 2000)
max_total_debt_score = 10000          # Maximum total debt score (default: 10000)
max_codebase_risk_score = 7.0         # Maximum codebase risk score (default: 7.0)
max_high_risk_functions = 50          # DEPRECATED: Use max_debt_density instead (default: 50)
min_coverage_percentage = 0.0         # Minimum required coverage % (default: 0.0)
max_debt_density = 50.0               # Maximum debt per 1000 LOC (default: 50.0)
</code></pre>
<p><strong>Deprecated Fields (v0.3.0+):</strong></p>
<p>The following validation thresholds are <strong>deprecated</strong> since v0.3.0 and will be removed in v1.0:</p>
<ul>
<li><code>max_high_complexity_count</code> - Replaced by <code>max_debt_density</code> (scale-independent)</li>
<li><code>max_debt_items</code> - Replaced by <code>max_debt_density</code> (scale-independent)</li>
<li><code>max_high_risk_functions</code> - Replaced by <code>max_debt_density</code> (scale-independent)</li>
</ul>
<p><strong>Migration:</strong> Use <code>max_debt_density</code> instead, which provides a scale-independent metric (debt per 1000 lines of code). This allows the same threshold to work across codebases of different sizes.</p>
<p>Use <code>debtmap validate</code> in CI to enforce code quality standards:</p>
<pre><code class="language-bash"># Fail build if validation thresholds are exceeded
debtmap validate
</code></pre>
<h2 id="language-configuration"><a class="header" href="#language-configuration">Language Configuration</a></h2>
<h3 id="enabling-languages"><a class="header" href="#enabling-languages">Enabling Languages</a></h3>
<p>Specify which languages to analyze:</p>
<pre><code class="language-toml">[languages]
enabled = ["rust", "python", "javascript", "typescript"]
</code></pre>
<h3 id="language-specific-features"><a class="header" href="#language-specific-features">Language-Specific Features</a></h3>
<p>Configure features for individual languages:</p>
<pre><code class="language-toml">[languages.rust]
detect_dead_code = false        # Rust: disabled by default (compiler handles it)
detect_complexity = true
detect_duplication = true

[languages.python]
detect_dead_code = true
detect_complexity = true
detect_duplication = true

[languages.javascript]
detect_dead_code = true
detect_complexity = true
detect_duplication = true

[languages.typescript]
detect_dead_code = true
detect_complexity = true
detect_duplication = true
</code></pre>
<p><strong>Note:</strong> Rust’s dead code detection is disabled by default since the Rust compiler already provides excellent unused code warnings.</p>
<h2 id="exclusion-patterns"><a class="header" href="#exclusion-patterns">Exclusion Patterns</a></h2>
<h3 id="file-and-directory-exclusion"><a class="header" href="#file-and-directory-exclusion">File and Directory Exclusion</a></h3>
<p>Use glob patterns to exclude files and directories from analysis:</p>
<pre><code class="language-toml">[ignore]
patterns = [
    "target/**",              # Rust build output
    "venv/**",                # Python virtual environment
    "node_modules/**",        # JavaScript dependencies
    "*.min.js",               # Minified files
    "benches/**",             # Benchmark code
    "tests/**/*",             # Test files
    "**/test_*.rs",           # Test files (prefix)
    "**/*_test.rs",           # Test files (suffix)
    "**/fixtures/**",         # Test fixtures
    "**/mocks/**",            # Mock implementations
    "**/stubs/**",            # Stub implementations
    "**/examples/**",         # Example code
    "**/demo/**",             # Demo code
]
</code></pre>
<p><strong>Glob pattern syntax:</strong></p>
<ul>
<li><code>*</code> - Matches any characters except <code>/</code></li>
<li><code>**</code> - Matches any characters including <code>/</code> (recursive)</li>
<li><code>?</code> - Matches a single character</li>
<li><code>[abc]</code> - Matches any character in the set</li>
</ul>
<p><strong>Note:</strong> Function-level filtering (e.g., ignoring specific function name patterns) is handled by role detection and context-aware analysis rather than explicit ignore patterns. See the Context-Aware Detection section for function-level filtering options.</p>
<h2 id="display-configuration"><a class="header" href="#display-configuration">Display Configuration</a></h2>
<p>Control how results are displayed:</p>
<pre><code class="language-toml">[display]
tiered = true           # Use tiered priority display (default: true)
items_per_tier = 5      # Show 5 items per tier (default: 5)
</code></pre>
<p>When <code>tiered = true</code>, Debtmap groups results into priority tiers (Critical, High, Medium, Low) and shows the top items from each tier.</p>
<h2 id="output-configuration"><a class="header" href="#output-configuration">Output Configuration</a></h2>
<p>Set the default output format:</p>
<pre><code class="language-toml">[output]
default_format = "terminal"    # Options: "terminal", "json", "markdown"
</code></pre>
<p><strong>Supported formats:</strong></p>
<ul>
<li><code>"terminal"</code> - Human-readable colored output for the terminal (default)</li>
<li><code>"json"</code> - Machine-readable JSON for integration with other tools</li>
<li><code>"markdown"</code> - Markdown format for documentation and reports</li>
</ul>
<p>This can be overridden with the <code>--format</code> CLI flag:</p>
<pre><code class="language-bash">debtmap analyze --format json      # JSON output
debtmap analyze --format markdown  # Markdown output
</code></pre>
<h2 id="normalization-configuration"><a class="header" href="#normalization-configuration">Normalization Configuration</a></h2>
<p>Control how raw scores are normalized to a 0-10 scale:</p>
<pre><code class="language-toml">[normalization]
linear_threshold = 10.0         # Use linear scaling below this value
logarithmic_threshold = 100.0   # Use logarithmic scaling above this value
sqrt_multiplier = 3.33          # Multiplier for square root scaling
log_multiplier = 10.0           # Multiplier for logarithmic scaling
show_raw_scores = true          # Show both raw and normalized scores
</code></pre>
<p>Normalization ensures scores are comparable across different codebases and prevents extreme outliers from dominating the results.</p>
<h2 id="advanced-configuration"><a class="header" href="#advanced-configuration">Advanced Configuration</a></h2>
<h3 id="entropy-based-complexity-scoring"><a class="header" href="#entropy-based-complexity-scoring">Entropy-Based Complexity Scoring</a></h3>
<p>Entropy analysis helps identify repetitive code patterns (like large match statements) that inflate complexity metrics:</p>
<pre><code class="language-toml">[entropy]
enabled = true                      # Enable entropy analysis (default: true)
weight = 1.0                        # Weight in complexity adjustment (default: 1.0)
min_tokens = 20                     # Minimum tokens for analysis (default: 20)
pattern_threshold = 0.7             # Pattern similarity threshold (default: 0.7)
entropy_threshold = 0.4             # Low entropy threshold (default: 0.4)
branch_threshold = 0.8              # Branch similarity threshold (default: 0.8)
use_classification = false          # Use smarter token classification (default: false)

# Maximum reductions to prevent over-correction
max_repetition_reduction = 0.20     # Max 20% reduction for repetition (default: 0.20)
max_entropy_reduction = 0.15        # Max 15% reduction for low entropy (default: 0.15)
max_branch_reduction = 0.25         # Max 25% reduction for similar branches (default: 0.25)
max_combined_reduction = 0.30       # Max 30% total reduction (default: 0.30)
</code></pre>
<p>Entropy scoring reduces false positives from functions like parsers and state machines that have high cyclomatic complexity but are actually simple and maintainable.</p>
<h3 id="god-object-detection"><a class="header" href="#god-object-detection">God Object Detection</a></h3>
<p>Configure detection of classes/structs with too many responsibilities:</p>
<pre><code class="language-toml">[god_object_detection]
enabled = true

# Rust-specific thresholds
[god_object_detection.rust]
max_methods = 20        # Maximum methods before flagging (default: 20)
max_fields = 15         # Maximum fields before flagging (default: 15)
max_traits = 5          # Maximum implemented traits
max_lines = 1000        # Maximum lines of code
max_complexity = 200    # Maximum total complexity

# Python-specific thresholds
[god_object_detection.python]
max_methods = 15
max_fields = 10
max_traits = 3
max_lines = 500
max_complexity = 150

# JavaScript-specific thresholds
[god_object_detection.javascript]
max_methods = 15
max_fields = 20         # JavaScript classes often have more properties
max_traits = 3
max_lines = 500
max_complexity = 150
</code></pre>
<p><strong>Note:</strong> Different languages have different defaults. Rust allows more methods since trait implementations add methods, while JavaScript classes should be smaller.</p>
<h3 id="context-aware-detection"><a class="header" href="#context-aware-detection">Context-Aware Detection</a></h3>
<p>Enable context-aware pattern detection to reduce false positives:</p>
<pre><code class="language-toml">[context]
enabled = false         # Opt-in (default: false)

# Custom context rules
[[context.rules]]
name = "allow_blocking_in_main"
pattern = "blocking_io"
action = "allow"
priority = 100
reason = "Main function can use blocking I/O"

[context.rules.context]
role = "main"

# Function pattern configuration
[context.function_patterns]
test_patterns = ["test_*", "bench_*"]
config_patterns = ["load_*_config", "parse_*_config"]
handler_patterns = ["handle_*", "*_handler"]
init_patterns = ["initialize_*", "setup_*"]
</code></pre>
<p>Context-aware detection adjusts severity based on where code appears (main functions, test code, configuration loaders, etc.).</p>
<h3 id="error-handling-detection"><a class="header" href="#error-handling-detection">Error Handling Detection</a></h3>
<p>Configure detection of error handling anti-patterns:</p>
<pre><code class="language-toml">[error_handling]
detect_async_errors = true          # Detect async error issues (default: true)
detect_context_loss = true          # Detect error context loss (default: true)
detect_propagation = true           # Analyze error propagation (default: true)
detect_panic_patterns = true        # Detect panic/unwrap usage (default: true)
detect_swallowing = true            # Detect swallowed errors (default: true)

# Custom error patterns
[[error_handling.custom_patterns]]
name = "custom_panic"
pattern = "my_panic_macro"
pattern_type = "macro_name"
severity = "high"
description = "Custom panic macro usage"
remediation = "Replace with Result-based error handling"

# Severity overrides
[[error_handling.severity_overrides]]
pattern = "unwrap"
context = "test"
severity = "low"        # Unwrap is acceptable in test code
</code></pre>
<h3 id="pure-mapping-pattern-detection"><a class="header" href="#pure-mapping-pattern-detection">Pure Mapping Pattern Detection</a></h3>
<p>Configure detection of pure mapping patterns to reduce false positives from exhaustive match expressions:</p>
<pre><code class="language-toml">[mapping_patterns]
enabled = true                      # Enable mapping pattern detection (default: true)
complexity_reduction = 0.30         # Reduce complexity by 30% (default: 0.30)
min_branches = 3                    # Minimum match arms to consider (default: 3)
</code></pre>
<p><strong>What are pure mapping patterns?</strong></p>
<p>Pure mapping patterns are exhaustive match expressions that transform input to output without side effects. These patterns have high cyclomatic complexity due to many branches, but are actually simple and maintainable because:</p>
<ul>
<li>Each branch is independent and straightforward</li>
<li>No mutation or side effects occur</li>
<li>The pattern is predictable and easy to understand</li>
<li>Adding new cases requires minimal changes</li>
</ul>
<p><strong>Example:</strong></p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn status_to_string(status: Status) -&gt; &amp;'static str {
    match status {
        Status::Success =&gt; "success",
        Status::Pending =&gt; "pending",
        Status::Failed =&gt; "failed",
        Status::Cancelled =&gt; "cancelled",
        // ... many more cases
    }
}
<span class="boring">}</span></code></pre>
<p>This function has high cyclomatic complexity (one branch per case), but is simple to maintain. Mapping pattern detection recognizes this and reduces the complexity score appropriately.</p>
<p><strong>Configuration options:</strong></p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Parameter</th><th>Default</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>enabled</code></td><td>true</td><td>Enable mapping pattern detection</td></tr>
<tr><td><code>complexity_reduction</code></td><td>0.30</td><td>Percentage to reduce complexity (0.0-1.0)</td></tr>
<tr><td><code>min_branches</code></td><td>3</td><td>Minimum match arms to be considered a mapping pattern</td></tr>
</tbody>
</table>
</div>
<p><strong>Example configuration:</strong></p>
<pre><code class="language-toml"># Conservative reduction
[mapping_patterns]
complexity_reduction = 0.20         # Only 20% reduction

# Aggressive reduction for codebases with many mapping patterns
[mapping_patterns]
complexity_reduction = 0.50         # 50% reduction

# Disable if you want to see all match complexity
[mapping_patterns]
enabled = false
</code></pre>
<p><strong>When to adjust:</strong></p>
<ul>
<li><strong>Increase <code>complexity_reduction</code></strong> if you have many simple mapping functions being flagged as complex</li>
<li><strong>Decrease <code>complexity_reduction</code></strong> if you want more conservative adjustments</li>
<li><strong>Increase <code>min_branches</code></strong> to only apply reduction to very large match statements</li>
<li><strong>Disable entirely</strong> if you want raw complexity scores without adjustment</li>
</ul>
<h3 id="external-api-configuration"><a class="header" href="#external-api-configuration">External API Configuration</a></h3>
<p>Mark functions as public API for enhanced testing recommendations:</p>
<pre><code class="language-toml">[external_api]
detect_external_api = false         # Auto-detect public APIs (default: false)
api_functions = []                  # Explicitly mark API functions
api_files = []                      # Explicitly mark API files
</code></pre>
<p>When enabled, public API functions receive higher priority for test coverage.</p>
<h3 id="classification-configuration"><a class="header" href="#classification-configuration">Classification Configuration</a></h3>
<p>The <strong><code>[classification]</code></strong> section controls how Debtmap classifies functions by their semantic role (constructor, accessor, data flow, etc.). This classification drives role-based adjustments and reduces false positives.</p>
<pre><code class="language-toml">[classification]
# Constructor detection
[classification.constructors]
detect_constructors = true            # Enable constructor detection (default: true)
constructor_patterns = ["new", "create", "build", "from"]  # Common constructor names

# Accessor detection
[classification.accessors]
detect_accessors = true               # Enable accessor/getter detection (default: true)
accessor_patterns = ["get_*", "set_*", "is_*", "has_*"]   # Common accessor patterns

# Data flow detection
[classification.data_flow]
detect_data_flow = true               # Enable data flow analysis (default: true)
</code></pre>
<p><strong>Configuration Options:</strong></p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Section</th><th>Option</th><th>Default</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>constructors</code></td><td><code>detect_constructors</code></td><td>true</td><td>Identify constructor functions</td></tr>
<tr><td><code>constructors</code></td><td><code>constructor_patterns</code></td><td>[“new”, “create”, “build”, “from”]</td><td>Name patterns for constructors</td></tr>
<tr><td><code>accessors</code></td><td><code>detect_accessors</code></td><td>true</td><td>Identify accessor/getter functions</td></tr>
<tr><td><code>accessors</code></td><td><code>accessor_patterns</code></td><td>[“get_<em>”, “set_</em>”, “is_<em>”, “has_</em>”]</td><td>Name patterns for accessors</td></tr>
<tr><td><code>data_flow</code></td><td><code>detect_data_flow</code></td><td>true</td><td>Enable data flow analysis</td></tr>
</tbody>
</table>
</div>
<p><strong>Why Classification Matters:</strong></p>
<p>Classification helps Debtmap understand function intent and apply appropriate complexity adjustments:</p>
<ul>
<li><strong>Constructors</strong> typically have boilerplate initialization code with naturally higher complexity</li>
<li><strong>Accessors</strong> are simple getters/setters that shouldn’t be flagged as debt</li>
<li><strong>Data flow functions</strong> (mappers, filters) have predictable patterns that inflate metrics</li>
</ul>
<p>By detecting these patterns, Debtmap reduces false positives and focuses on genuine technical debt.</p>
<h3 id="parallel-processing-configuration"><a class="header" href="#parallel-processing-configuration">Parallel Processing Configuration</a></h3>
<p>The <strong><code>[batch_analysis]</code></strong> section configures parallel processing for analyzing large codebases:</p>
<pre><code class="language-toml">[batch_analysis]
fail_fast = false                   # Stop on first error (default: false)
collect_timing = true               # Collect timing metrics (default: false)

[batch_analysis.parallelism]
enabled = true                      # Enable parallel processing (default: true)
max_concurrency = 4                 # Maximum concurrent threads (default: number of CPU cores)
batch_size = 100                    # Files per batch (default: 100)
</code></pre>
<p><strong>Configuration Options:</strong></p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Option</th><th>Default</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>fail_fast</code></td><td>false</td><td>Stop analysis on first error encountered</td></tr>
<tr><td><code>collect_timing</code></td><td>false</td><td>Collect detailed timing information for performance analysis</td></tr>
<tr><td><code>parallelism.enabled</code></td><td>true</td><td>Enable parallel processing of files</td></tr>
<tr><td><code>parallelism.max_concurrency</code></td><td>CPU cores</td><td>Maximum number of concurrent analysis threads</td></tr>
<tr><td><code>parallelism.batch_size</code></td><td>100</td><td>Number of files to process in each batch</td></tr>
</tbody>
</table>
</div>
<p><strong>When to Adjust:</strong></p>
<pre><code class="language-toml"># For very large codebases
[batch_analysis.parallelism]
enabled = true
max_concurrency = 8                 # Use more threads
batch_size = 200                    # Larger batches

# For memory-constrained environments
[batch_analysis.parallelism]
max_concurrency = 2                 # Fewer threads
batch_size = 50                     # Smaller batches

# For debugging analysis issues
[batch_analysis]
fail_fast = true                    # Stop on first error
collect_timing = true               # Collect timing data
</code></pre>
<p>For detailed information on parallel processing, see the <a href="#parallel-processing">Parallel Processing</a> chapter.</p>
<p><strong>Source:</strong> <code>src/config/parallel.rs:108-143</code> (BatchAnalysisConfig)</p>
<h3 id="retry-configuration"><a class="header" href="#retry-configuration">Retry Configuration</a></h3>
<p>The <strong><code>[retry]</code></strong> section configures automatic retry behavior for resilient operations:</p>
<pre><code class="language-toml">[retry]
enabled = true                      # Enable automatic retries (default: true)
max_retries = 3                     # Maximum retry attempts (default: 3)
base_delay_ms = 100                 # Initial delay in milliseconds (default: 100)
strategy = "exponential"            # Backoff strategy (default: exponential)
timeout_seconds = 30                # Maximum total retry time (default: 30)
jitter_factor = 0.1                 # Randomness factor (default: 0.1)
</code></pre>
<p><strong>Configuration Options:</strong></p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Option</th><th>Default</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>enabled</code></td><td>true</td><td>Enable automatic retries on transient failures</td></tr>
<tr><td><code>max_retries</code></td><td>3</td><td>Maximum number of retry attempts before giving up</td></tr>
<tr><td><code>base_delay_ms</code></td><td>100</td><td>Base delay between retries in milliseconds</td></tr>
<tr><td><code>strategy</code></td><td>exponential</td><td>Backoff strategy: constant, linear, exponential, fibonacci</td></tr>
<tr><td><code>timeout_seconds</code></td><td>30</td><td>Maximum total time to spend retrying (seconds)</td></tr>
<tr><td><code>jitter_factor</code></td><td>0.1</td><td>Add randomness to delays (0.0-1.0) to prevent thundering herd</td></tr>
</tbody>
</table>
</div>
<p><strong>Retry Strategies:</strong></p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Strategy</th><th>Delay Pattern</th><th>Use Case</th></tr>
</thead>
<tbody>
<tr><td><code>constant</code></td><td>Same delay every time</td><td>Simple rate limiting</td></tr>
<tr><td><code>linear</code></td><td>Increases linearly (100ms, 200ms, 300ms)</td><td>Moderate backoff</td></tr>
<tr><td><code>exponential</code></td><td>Doubles each time (100ms, 200ms, 400ms)</td><td>Standard backoff (recommended)</td></tr>
<tr><td><code>fibonacci</code></td><td>Fibonacci sequence (100ms, 100ms, 200ms, 300ms)</td><td>Gradual backoff</td></tr>
</tbody>
</table>
</div>
<p><strong>Example Configurations:</strong></p>
<pre><code class="language-toml"># Aggressive retry for flaky file systems
[retry]
max_retries = 5
base_delay_ms = 50
strategy = "exponential"
timeout_seconds = 60

# Conservative retry for network operations
[retry]
max_retries = 2
base_delay_ms = 500
strategy = "linear"
timeout_seconds = 10

# Disable retries for fast failure
[retry]
enabled = false
</code></pre>
<p><strong>Source:</strong> <code>src/config/retry.rs:33-79</code> (RetryConfig), <code>src/config/retry.rs:141-155</code> (RetryStrategy)</p>
<h3 id="advanced-analysis-settings"><a class="header" href="#advanced-analysis-settings">Advanced Analysis Settings</a></h3>
<p>The <strong><code>[analysis]</code></strong> section configures advanced analysis features for call graph and dead code detection:</p>
<pre><code class="language-toml">[analysis]
enable_trait_analysis = true               # Enable trait method resolution (default: true)
enable_function_pointer_tracking = true    # Track function pointers and closures (default: true)
enable_framework_patterns = true           # Detect framework patterns (default: true)
enable_cross_module_analysis = true        # Analyze cross-module dependencies (default: true)
max_analysis_depth = 10                    # Maximum transitive analysis depth (default: 10)
</code></pre>
<p><strong>Configuration Options:</strong></p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Option</th><th>Default</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>enable_trait_analysis</code></td><td>true</td><td>Resolve trait method calls in call graph analysis</td></tr>
<tr><td><code>enable_function_pointer_tracking</code></td><td>true</td><td>Track function pointers and closure usage</td></tr>
<tr><td><code>enable_framework_patterns</code></td><td>true</td><td>Detect test functions, HTTP handlers, and other framework entry points</td></tr>
<tr><td><code>enable_cross_module_analysis</code></td><td>true</td><td>Analyze dependencies across module boundaries</td></tr>
<tr><td><code>max_analysis_depth</code></td><td>10</td><td>Maximum depth for transitive dependency analysis</td></tr>
</tbody>
</table>
</div>
<p><strong>When to Adjust:</strong></p>
<pre><code class="language-toml"># For deep dependency analysis in large projects
[analysis]
max_analysis_depth = 20                    # Allow deeper transitive analysis

# For faster analysis with limited resources
[analysis]
enable_cross_module_analysis = false       # Skip cross-module analysis
max_analysis_depth = 5                     # Limit depth

# For framework-heavy codebases (web apps, CLI apps)
[analysis]
enable_framework_patterns = true           # Detect entry points
enable_trait_analysis = true               # Resolve trait methods
</code></pre>
<p><strong>Framework Pattern Detection:</strong></p>
<p>When <code>enable_framework_patterns = true</code>, Debtmap recognizes common entry point patterns:</p>
<ul>
<li><strong>Test functions:</strong> Functions with <code>#[test]</code>, <code>#[tokio::test]</code>, or test prefixes</li>
<li><strong>HTTP handlers:</strong> Functions with routing attributes like <code>#[get]</code>, <code>#[post]</code></li>
<li><strong>CLI commands:</strong> Functions with CLI framework attributes</li>
<li><strong>Main functions:</strong> Entry points like <code>main()</code> and <code>async fn main()</code></li>
</ul>
<p>This prevents false positives from flagging framework entry points as dead code.</p>
<p><strong>Source:</strong> <code>src/config/core.rs:149-167</code> (AnalysisSettings)</p>
<h3 id="state-detection-configuration"><a class="header" href="#state-detection-configuration">State Detection Configuration</a></h3>
<p>The <strong><code>[state_detection]</code></strong> section configures detection of state machine patterns and state fields:</p>
<pre><code class="language-toml">[state_detection]
use_type_analysis = true              # Enable type-based detection (default: true)
use_frequency_analysis = true         # Enable frequency analysis (default: true)
use_pattern_recognition = true        # Enable semantic pattern recognition (default: true)
min_enum_variants = 3                 # Minimum enum variants for state detection (default: 3)
custom_keywords = []                  # Additional state keywords (default: [])
custom_patterns = []                  # Additional regex patterns (default: [])
</code></pre>
<p><strong>Configuration Options:</strong></p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Option</th><th>Default</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>use_type_analysis</code></td><td>true</td><td>Analyze type names and structures for state indicators</td></tr>
<tr><td><code>use_frequency_analysis</code></td><td>true</td><td>Use statistical analysis to detect state fields</td></tr>
<tr><td><code>use_pattern_recognition</code></td><td>true</td><td>Apply semantic pattern matching for state detection</td></tr>
<tr><td><code>min_enum_variants</code></td><td>3</td><td>Minimum variants for enum to be considered state machine</td></tr>
<tr><td><code>custom_keywords</code></td><td>[]</td><td>Additional keywords to identify state fields</td></tr>
<tr><td><code>custom_patterns</code></td><td>[]</td><td>Additional regex patterns for state field names</td></tr>
</tbody>
</table>
</div>
<p><strong>Built-in State Keywords:</strong></p>
<p>Debtmap recognizes these common state field patterns:</p>
<ul>
<li><strong>Field keywords:</strong> state, mode, status, phase, stage, step, kind, type, variant</li>
<li><strong>Prefixes:</strong> current_, next_, prev_, target_, desired_, actual_, expected_</li>
<li><strong>Suffixes:</strong> _state, _mode, _status, _phase, _stage, _type, _kind</li>
</ul>
<p><strong>Example Configurations:</strong></p>
<pre><code class="language-toml"># Add domain-specific state keywords
[state_detection]
custom_keywords = ["fsm_state", "machine_state", "workflow_step"]
custom_patterns = ["current_.*", "next_.*"]

# Strict enum state detection (only large enums)
[state_detection]
min_enum_variants = 5

# Disable frequency analysis for performance
[state_detection]
use_frequency_analysis = false
</code></pre>
<p><strong>Why State Detection Matters:</strong></p>
<p>Functions that manage state machines or state fields naturally have higher complexity due to branching on state. Debtmap’s state detection:</p>
<ul>
<li>Identifies state management code</li>
<li>Adjusts complexity scoring to account for state branching</li>
<li>Reduces false positives from legitimate state machine patterns</li>
</ul>
<p><strong>Source:</strong> <code>src/analyzers/state_field_detector.rs:286-340</code> (StateDetectionConfig)</p>
<h3 id="coverage-expectations-configuration"><a class="header" href="#coverage-expectations-configuration">Coverage Expectations Configuration</a></h3>
<p>The <strong><code>[coverage_expectations]</code></strong> section configures role-based test coverage expectations:</p>
<pre><code class="language-toml"># Pure functions: high expectations (90-100%)
[coverage_expectations.pure]
min = 90.0                            # Minimum acceptable coverage
target = 95.0                         # Target/ideal coverage
max = 100.0                           # Maximum meaningful coverage

# Business logic: very high expectations (80-95%)
[coverage_expectations.business_logic]
min = 80.0
target = 90.0
max = 95.0

# State management: high expectations (75-90%)
[coverage_expectations.state_management]
min = 75.0
target = 85.0
max = 90.0

# I/O operations: moderate expectations (60-80%)
[coverage_expectations.io_operations]
min = 60.0
target = 70.0
max = 80.0

# Validation: very high expectations (85-98%)
[coverage_expectations.validation]
min = 85.0
target = 92.0
max = 98.0

# Error handling: high expectations (70-90%)
[coverage_expectations.error_handling]
min = 70.0
target = 80.0
max = 90.0

# Configuration: moderate expectations (60-80%)
[coverage_expectations.configuration]
min = 60.0
target = 70.0
max = 80.0

# Initialization: moderate expectations (50-75%)
[coverage_expectations.initialization]
min = 50.0
target = 65.0
max = 75.0

# Orchestration: moderate-high expectations (65-85%)
[coverage_expectations.orchestration]
min = 65.0
target = 75.0
max = 85.0

# Utilities: high expectations (75-95%)
[coverage_expectations.utilities]
min = 75.0
target = 85.0
max = 95.0

# Debug/Development: low expectations (20-40%)
[coverage_expectations.debug]
min = 20.0
target = 30.0
max = 40.0

# Performance optimization: low-moderate expectations (40-60%)
[coverage_expectations.performance]
min = 40.0
target = 50.0
max = 60.0
</code></pre>
<p><strong>Coverage Range Parameters:</strong></p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Parameter</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>min</code></td><td>Minimum acceptable coverage - below this flags as high priority</td></tr>
<tr><td><code>target</code></td><td>Target/ideal coverage - the goal for this function role</td></tr>
<tr><td><code>max</code></td><td>Maximum meaningful coverage - diminishing returns above this</td></tr>
</tbody>
</table>
</div>
<p><strong>Role-Based Rationale:</strong></p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Function Role</th><th>Expectations</th><th>Why?</th></tr>
</thead>
<tbody>
<tr><td><strong>Pure Logic</strong></td><td>Very High (90-100%)</td><td>Easy to unit test, should have comprehensive tests</td></tr>
<tr><td><strong>Business Logic</strong></td><td>Very High (80-95%)</td><td>Core functionality requires thorough testing</td></tr>
<tr><td><strong>I/O Operations</strong></td><td>Moderate (60-80%)</td><td>Often integration tested rather than unit tested</td></tr>
<tr><td><strong>Debug</strong></td><td>Low (20-40%)</td><td>Diagnostic code has low testing priority</td></tr>
<tr><td><strong>Orchestration</strong></td><td>Moderate-High (65-85%)</td><td>Tested via higher-level tests</td></tr>
</tbody>
</table>
</div>
<p><strong>Example Configurations:</strong></p>
<pre><code class="language-toml"># Strict quality standards (increase all expectations)
[coverage_expectations.pure]
min = 95.0
target = 98.0
max = 100.0

[coverage_expectations.business_logic]
min = 90.0
target = 95.0
max = 98.0

# Legacy codebase (lower expectations)
[coverage_expectations.pure]
min = 70.0
target = 85.0
max = 95.0

[coverage_expectations.business_logic]
min = 60.0
target = 75.0
max = 85.0
</code></pre>
<p><strong>How Coverage Expectations Work:</strong></p>
<p>When Debtmap calculates coverage scores:</p>
<ol>
<li>Identifies function’s semantic role (pure, business_logic, io_operations, etc.)</li>
<li>Looks up coverage expectations for that role</li>
<li>Compares actual coverage to expected range</li>
<li>Penalizes functions below <code>min</code> more heavily</li>
<li>Rewards functions meeting or exceeding <code>target</code></li>
</ol>
<p>This ensures that scoring reflects realistic testing strategies rather than assuming all functions need 100% coverage.</p>
<p><strong>Source:</strong> <code>src/priority/scoring/coverage_expectations.rs:103-173</code> (CoverageExpectations)</p>
<h3 id="additional-advanced-options"><a class="header" href="#additional-advanced-options">Additional Advanced Options</a></h3>
<p>Debtmap supports additional advanced configuration options:</p>
<h4 id="lines-of-code-configuration"><a class="header" href="#lines-of-code-configuration">Lines of Code Configuration</a></h4>
<p>The <strong><code>[loc]</code></strong> section controls how lines of code are counted for metrics and reporting:</p>
<pre><code class="language-toml">[loc]
include_tests = false         # Exclude test files from LOC counts (default: false)
include_generated = false     # Exclude generated files from LOC counts (default: false)
count_comments = false        # Include comment lines in LOC counts (default: false)
count_blank_lines = false     # Include blank lines in LOC counts (default: false)
</code></pre>
<p><strong>Configuration options:</strong></p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Option</th><th>Default</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>include_tests</code></td><td>false</td><td>Whether to include test files in LOC metrics</td></tr>
<tr><td><code>include_generated</code></td><td>false</td><td>Whether to include generated files in LOC metrics</td></tr>
<tr><td><code>count_comments</code></td><td>false</td><td>Whether to count comment lines as LOC</td></tr>
<tr><td><code>count_blank_lines</code></td><td>false</td><td>Whether to count blank lines as LOC</td></tr>
</tbody>
</table>
</div>
<p><strong>Example - Strict LOC counting:</strong></p>
<pre><code class="language-toml">[loc]
include_tests = false         # Focus on production code
include_generated = false     # Exclude auto-generated code
count_comments = false        # Only count executable code
count_blank_lines = false     # Exclude whitespace
</code></pre>
<h4 id="tier-configuration"><a class="header" href="#tier-configuration">Tier Configuration</a></h4>
<p>The <strong><code>[tiers]</code></strong> section configures tier threshold boundaries for prioritization:</p>
<pre><code class="language-toml">[tiers]
t2_complexity_threshold = 15      # Complexity threshold for Tier 2 (default: 15)
t2_dependency_threshold = 10      # Dependency threshold for Tier 2 (default: 10)
t3_complexity_threshold = 10      # Complexity threshold for Tier 3 (default: 10)
show_t4_in_main_report = false    # Show Tier 4 items in main report (default: false)
</code></pre>
<p><strong>Tier priority levels:</strong></p>
<ul>
<li><strong>Tier 1 (Critical)</strong>: Highest priority items</li>
<li><strong>Tier 2 (High)</strong>: Items above <code>t2_*</code> thresholds</li>
<li><strong>Tier 3 (Medium)</strong>: Items above <code>t3_*</code> thresholds</li>
<li><strong>Tier 4 (Low)</strong>: Items below all thresholds</li>
</ul>
<p><strong>Example - Stricter tier boundaries:</strong></p>
<pre><code class="language-toml">[tiers]
t2_complexity_threshold = 12      # Lower threshold = more items in high priority
t2_dependency_threshold = 8
t3_complexity_threshold = 8
show_t4_in_main_report = true     # Include low-priority items
</code></pre>
<h4 id="enhanced-complexity-thresholds"><a class="header" href="#enhanced-complexity-thresholds">Enhanced Complexity Thresholds</a></h4>
<p>The <strong><code>[complexity_thresholds]</code></strong> section provides more granular control over complexity detection:</p>
<p>This supplements the basic <code>[thresholds]</code> section with minimum total, cyclomatic, and cognitive complexity thresholds for flagging functions.</p>
<p>These options are advanced features with sensible defaults. Most users won’t need to configure them explicitly.</p>
<h4 id="orchestration-adjustment"><a class="header" href="#orchestration-adjustment">Orchestration Adjustment</a></h4>
<p>The <strong><code>[orchestration_adjustment]</code></strong> section configures complexity reduction for orchestrator functions that primarily delegate to other functions:</p>
<pre><code class="language-toml">[orchestration_adjustment]
enabled = true                        # Enable orchestration detection (default: true)
min_delegation_ratio = 0.6            # Minimum ratio of delegated calls (default: 0.6)
complexity_reduction = 0.25           # Reduce complexity by 25% (default: 0.25)
</code></pre>
<p><strong>Configuration Options:</strong></p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Option</th><th>Default</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>enabled</code></td><td>true</td><td>Enable orchestration pattern detection</td></tr>
<tr><td><code>min_delegation_ratio</code></td><td>0.6</td><td>Minimum % of function that delegates to be considered orchestrator</td></tr>
<tr><td><code>complexity_reduction</code></td><td>0.25</td><td>Percentage to reduce complexity score (0.0-1.0)</td></tr>
</tbody>
</table>
</div>
<p>Orchestrator functions coordinate multiple operations but don’t contain complex logic themselves. This adjustment prevents them from being over-penalized.</p>
<h4 id="boilerplate-detection"><a class="header" href="#boilerplate-detection">Boilerplate Detection</a></h4>
<p>The <strong><code>[boilerplate_detection]</code></strong> section identifies and reduces penalties for boilerplate code patterns:</p>
<pre><code class="language-toml">[boilerplate_detection]
enabled = true                        # Enable boilerplate detection (default: true)
detect_constructors = true            # Detect constructor boilerplate (default: true)
detect_error_conversions = true       # Detect error conversion boilerplate (default: true)
complexity_reduction = 0.20           # Reduce complexity by 20% (default: 0.20)
</code></pre>
<p><strong>Configuration Options:</strong></p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Option</th><th>Default</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>enabled</code></td><td>true</td><td>Enable boilerplate pattern detection</td></tr>
<tr><td><code>detect_constructors</code></td><td>true</td><td>Identify constructor initialization boilerplate</td></tr>
<tr><td><code>detect_error_conversions</code></td><td>true</td><td>Identify error type conversion boilerplate</td></tr>
<tr><td><code>complexity_reduction</code></td><td>0.20</td><td>Percentage to reduce complexity for boilerplate (0.0-1.0)</td></tr>
</tbody>
</table>
</div>
<p>Boilerplate code often inflates complexity metrics without representing true technical debt. This detection reduces false positives from necessary but repetitive code.</p>
<h4 id="functional-analysis"><a class="header" href="#functional-analysis">Functional Analysis</a></h4>
<p>The <strong><code>[functional_analysis]</code></strong> section configures detection of functional programming patterns:</p>
<pre><code class="language-toml">[functional_analysis]
enabled = true                        # Enable functional pattern detection (default: true)
detect_pure_functions = true          # Detect pure functions (default: true)
detect_higher_order = true            # Detect higher-order functions (default: true)
detect_immutable_patterns = true      # Detect immutable data patterns (default: true)
</code></pre>
<p><strong>Configuration Options:</strong></p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Option</th><th>Default</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>enabled</code></td><td>true</td><td>Enable functional programming analysis</td></tr>
<tr><td><code>detect_pure_functions</code></td><td>true</td><td>Identify functions without side effects</td></tr>
<tr><td><code>detect_higher_order</code></td><td>true</td><td>Identify functions that take/return functions</td></tr>
<tr><td><code>detect_immutable_patterns</code></td><td>true</td><td>Identify immutable data structure usage</td></tr>
</tbody>
</table>
</div>
<p>Functional patterns often lead to cleaner, more testable code. This analysis helps Debtmap recognize and appropriately score functional programming idioms.</p>
<h2 id="cli-integration"><a class="header" href="#cli-integration">CLI Integration</a></h2>
<p>CLI flags can override configuration file settings:</p>
<pre><code class="language-bash"># Override complexity threshold
debtmap analyze --threshold-complexity 15

# Provide coverage file
debtmap analyze --coverage-file coverage.json

# Enable context-aware detection
debtmap analyze --context

# Override output format
debtmap analyze --format json
</code></pre>
<h3 id="configuration-precedence"><a class="header" href="#configuration-precedence">Configuration Precedence</a></h3>
<p>Debtmap resolves configuration values in the following order (highest to lowest priority):</p>
<ol>
<li><strong>CLI flags</strong> - Command-line arguments (e.g., <code>--threshold-complexity 15</code>)</li>
<li><strong>Configuration file</strong> - Settings from <code>.debtmap.toml</code></li>
<li><strong>Built-in defaults</strong> - Debtmap’s sensible default values</li>
</ol>
<p>This allows you to set project-wide defaults in <code>.debtmap.toml</code> while customizing specific runs with CLI flags.</p>
<h2 id="configuration-validation"><a class="header" href="#configuration-validation">Configuration Validation</a></h2>
<h3 id="automatic-validation"><a class="header" href="#automatic-validation">Automatic Validation</a></h3>
<p>Debtmap automatically validates your configuration when loading:</p>
<ul>
<li><strong>Scoring weights</strong> must sum to 1.0 (±0.001 tolerance)</li>
<li><strong>Individual weights</strong> must be between 0.0 and 1.0</li>
<li><strong>Invalid configurations</strong> fall back to defaults with a warning</li>
</ul>
<h3 id="normalization"><a class="header" href="#normalization">Normalization</a></h3>
<p>If scoring weights don’t sum exactly to 1.0, Debtmap automatically normalizes them:</p>
<pre><code class="language-toml"># Input (sums to 0.80)
[scoring]
coverage = 0.40
complexity = 0.30
dependency = 0.10

# Automatically normalized to:
# coverage = 0.50
# complexity = 0.375
# dependency = 0.125
</code></pre>
<h3 id="debug-validation"><a class="header" href="#debug-validation">Debug Validation</a></h3>
<p>To verify which configuration file is being loaded, check debug logs:</p>
<pre><code class="language-bash">RUST_LOG=debug debtmap analyze
</code></pre>
<p>Look for log messages like:</p>
<pre><code>DEBUG debtmap::config: Loaded config from /path/to/.debtmap.toml
</code></pre>
<h2 id="complete-configuration-example"><a class="header" href="#complete-configuration-example">Complete Configuration Example</a></h2>
<p>Here’s a comprehensive configuration showing all major sections:</p>
<pre><code class="language-toml"># Scoring configuration
[scoring]
coverage = 0.50
complexity = 0.35
dependency = 0.15

# Basic thresholds
[thresholds]
complexity = 10
duplication = 50
max_file_length = 500
max_function_length = 50
minimum_debt_score = 2.0
minimum_cyclomatic_complexity = 3
minimum_cognitive_complexity = 5
minimum_risk_score = 2.0

# Validation thresholds for CI
[thresholds.validation]
max_average_complexity = 10.0
max_high_complexity_count = 100       # DEPRECATED: Use max_debt_density
max_debt_items = 2000                 # DEPRECATED: Use max_debt_density
max_total_debt_score = 10000
max_codebase_risk_score = 7.0
max_high_risk_functions = 50          # DEPRECATED: Use max_debt_density
min_coverage_percentage = 0.0
max_debt_density = 50.0

# Language configuration
[languages]
enabled = ["rust", "python", "javascript", "typescript"]

[languages.rust]
detect_dead_code = false
detect_complexity = true
detect_duplication = true

# Exclusion patterns
[ignore]
patterns = [
    "target/**",
    "node_modules/**",
    "tests/**/*",
    "**/*_test.rs",
]

# Display configuration
[display]
tiered = true
items_per_tier = 5

# Output configuration
[output]
default_format = "terminal"

# Entropy configuration
[entropy]
enabled = true
weight = 1.0
min_tokens = 20

# God object detection
[god_object_detection]
enabled = true

[god_object_detection.rust]
max_methods = 20
max_fields = 15

# Classification configuration
[classification.constructors]
detect_constructors = true
constructor_patterns = ["new", "create", "build", "from"]

[classification.accessors]
detect_accessors = true
accessor_patterns = ["get_*", "set_*", "is_*", "has_*"]

[classification.data_flow]
detect_data_flow = true

# Advanced analysis
[orchestration_adjustment]
enabled = true
min_delegation_ratio = 0.6
complexity_reduction = 0.25

[boilerplate_detection]
enabled = true
detect_constructors = true
detect_error_conversions = true
complexity_reduction = 0.20

[functional_analysis]
enabled = true
detect_pure_functions = true
detect_higher_order = true
detect_immutable_patterns = true
</code></pre>
<h2 id="configuration-best-practices"><a class="header" href="#configuration-best-practices">Configuration Best Practices</a></h2>
<h3 id="for-strict-quality-standards"><a class="header" href="#for-strict-quality-standards">For Strict Quality Standards</a></h3>
<pre><code class="language-toml">[scoring]
coverage = 0.60         # Emphasize test coverage
complexity = 0.30
dependency = 0.10

[thresholds]
minimum_debt_score = 3.0        # Higher bar for flagging issues
max_function_length = 30        # Enforce smaller functions

[thresholds.validation]
max_average_complexity = 8.0    # Stricter complexity limits
max_debt_items = 500            # Stricter debt limits
min_coverage_percentage = 80.0  # Require 80% coverage
</code></pre>
<h3 id="for-legacy-codebases"><a class="header" href="#for-legacy-codebases">For Legacy Codebases</a></h3>
<pre><code class="language-toml">[scoring]
coverage = 0.30         # Reduce coverage weight (legacy code often lacks tests)
complexity = 0.50       # Focus on complexity
dependency = 0.20

[thresholds]
minimum_debt_score = 5.0        # Only show highest priority items
minimum_cyclomatic_complexity = 10   # Filter out moderate complexity

[thresholds.validation]
max_debt_items = 10000          # Accommodate large debt
max_total_debt_score = 5000     # Higher limits for legacy code
</code></pre>
<h3 id="for-open-source-libraries"><a class="header" href="#for-open-source-libraries">For Open Source Libraries</a></h3>
<pre><code class="language-toml">[scoring]
coverage = 0.55         # Prioritize test coverage (public API)
complexity = 0.30
dependency = 0.15

[external_api]
detect_external_api = true      # Flag untested public APIs

[thresholds.validation]
min_coverage_percentage = 90.0  # High coverage for public API
max_high_complexity_count = 20  # Keep complexity low
</code></pre>
<h2 id="troubleshooting-4"><a class="header" href="#troubleshooting-4">Troubleshooting</a></h2>
<h3 id="configuration-not-loading"><a class="header" href="#configuration-not-loading">Configuration Not Loading</a></h3>
<p><strong>Check file location:</strong></p>
<pre><code class="language-bash"># Ensure file is named .debtmap.toml (note the dot prefix)
ls -la .debtmap.toml

# Debtmap searches current directory + 10 parent directories
pwd
</code></pre>
<p><strong>Check file syntax:</strong></p>
<pre><code class="language-bash"># Verify TOML syntax is valid
debtmap analyze 2&gt;&amp;1 | grep -i "failed to parse"
</code></pre>
<h3 id="weights-dont-sum-to-10"><a class="header" href="#weights-dont-sum-to-10">Weights Don’t Sum to 1.0</a></h3>
<p><strong>Error message:</strong></p>
<pre><code>Warning: Invalid scoring weights: Active scoring weights must sum to 1.0, but sum to 0.800. Using defaults.
</code></pre>
<p><strong>Fix:</strong> Ensure coverage + complexity + dependency = 1.0</p>
<pre><code class="language-toml">[scoring]
coverage = 0.50
complexity = 0.35
dependency = 0.15    # Sum = 1.0 ✓
</code></pre>
<h3 id="no-results-shown"><a class="header" href="#no-results-shown">No Results Shown</a></h3>
<p><strong>Possible causes:</strong></p>
<ol>
<li>Minimum thresholds too high</li>
<li>All code excluded by ignore patterns</li>
<li>No supported languages in project</li>
</ol>
<p><strong>Solutions:</strong></p>
<pre><code class="language-toml"># Lower minimum thresholds
[thresholds]
minimum_debt_score = 1.0
minimum_cyclomatic_complexity = 1

# Check language configuration
[languages]
enabled = ["rust", "python", "javascript", "typescript"]

# Review ignore patterns
[ignore]
patterns = [
    # Make sure you're not excluding too much
]
</code></pre>
<h2 id="related-chapters"><a class="header" href="#related-chapters">Related Chapters</a></h2>
<ul>
<li><a href="#getting-started-1">Getting Started</a> - Initial setup and basic usage</li>
<li><a href="#analysis-guide">Analysis Guide</a> - Understanding scoring and prioritization</li>
<li><a href="#output-formats-3">Output Formats</a> - Formatting and exporting results</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="threshold-configuration-1"><a class="header" href="#threshold-configuration-1">Threshold Configuration</a></h1>
<p>Debtmap uses configurable thresholds to determine when code complexity, duplication, or structural issues should be flagged as technical debt. This chapter explains how to configure thresholds to match your project’s quality standards.</p>
<h2 id="overview-4"><a class="header" href="#overview-4">Overview</a></h2>
<p>Thresholds control what gets flagged as technical debt. You can configure thresholds using:</p>
<ol>
<li><strong>Preset configurations</strong> - Quick start with strict, balanced, or lenient settings</li>
<li><strong>CLI flags</strong> - Override thresholds for a single analysis run</li>
<li><strong>Configuration file</strong> - Project-specific thresholds in <code>.debtmap.toml</code></li>
</ol>
<h2 id="threshold-presets"><a class="header" href="#threshold-presets">Threshold Presets</a></h2>
<p>Debtmap provides three preset threshold configurations to match different project needs:</p>
<h3 id="preset-comparison"><a class="header" href="#preset-comparison">Preset Comparison</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Threshold</th><th>Strict</th><th>Balanced (Default)</th><th>Lenient</th></tr>
</thead>
<tbody>
<tr><td>Cyclomatic Complexity</td><td>3</td><td>5</td><td>10</td></tr>
<tr><td>Cognitive Complexity</td><td>7</td><td>10</td><td>20</td></tr>
<tr><td>Total Complexity</td><td>5</td><td>8</td><td>15</td></tr>
<tr><td>Function Length (lines)</td><td>15</td><td>20</td><td>50</td></tr>
<tr><td>Match Arms</td><td>3</td><td>4</td><td>8</td></tr>
<tr><td>If-Else Chain</td><td>2</td><td>3</td><td>5</td></tr>
<tr><td><strong>Role Multipliers</strong></td><td></td><td></td><td></td></tr>
<tr><td>Entry Point Multiplier</td><td>1.2x</td><td>1.5x</td><td>2.0x</td></tr>
<tr><td>Utility Multiplier</td><td>0.6x</td><td>0.8x</td><td>1.0x</td></tr>
<tr><td>Test Function Multiplier</td><td>3.0x</td><td>2.0x</td><td>3.0x</td></tr>
</tbody>
</table>
</div>
<h3 id="when-to-use-each-preset"><a class="header" href="#when-to-use-each-preset">When to Use Each Preset</a></h3>
<p><strong>Strict Preset</strong></p>
<ul>
<li>New projects aiming for high code quality standards</li>
<li>Libraries and reusable components</li>
<li>Critical systems requiring high reliability</li>
<li>Teams enforcing strict coding standards</li>
</ul>
<pre><code class="language-bash">debtmap analyze . --threshold-preset strict
</code></pre>
<p><strong>Balanced Preset (Default)</strong></p>
<ul>
<li>Typical production applications</li>
<li>Projects with moderate complexity tolerance</li>
<li>Recommended starting point for most projects</li>
<li>Good balance between catching issues and avoiding false positives</li>
</ul>
<pre><code class="language-bash">debtmap analyze .  # Uses balanced preset by default
</code></pre>
<p><strong>Lenient Preset</strong></p>
<ul>
<li>Legacy codebases during initial assessment</li>
<li>Complex domains (compilers, scientific computing)</li>
<li>Gradual debt reduction strategies</li>
<li>Temporary relaxation during major refactoring</li>
</ul>
<pre><code class="language-bash">debtmap analyze . --threshold-preset lenient
</code></pre>
<h2 id="understanding-complexity-thresholds"><a class="header" href="#understanding-complexity-thresholds">Understanding Complexity Thresholds</a></h2>
<p>Debtmap tracks multiple complexity metrics and uses <strong>conjunction logic</strong>: a function must exceed <strong>ALL</strong> thresholds to be flagged as technical debt.</p>
<h3 id="cyclomatic-complexity-1"><a class="header" href="#cyclomatic-complexity-1">Cyclomatic Complexity</a></h3>
<p>Counts decision points in code: <code>if</code>, <code>while</code>, <code>for</code>, <code>match</code>, <code>&amp;&amp;</code>, <code>||</code>, etc.</p>
<ul>
<li><strong>What it measures</strong>: Number of independent paths through code</li>
<li><strong>Why it matters</strong>: More paths = harder to test completely</li>
<li><strong>Default threshold</strong>: 5</li>
</ul>
<h3 id="cognitive-complexity-1"><a class="header" href="#cognitive-complexity-1">Cognitive Complexity</a></h3>
<p>Measures the mental effort required to understand code by weighing nested structures and breaks in linear flow.</p>
<ul>
<li><strong>What it measures</strong>: How hard code is to read and comprehend</li>
<li><strong>Why it matters</strong>: High cognitive load = maintenance burden</li>
<li><strong>Default threshold</strong>: 10</li>
</ul>
<h3 id="total-complexity"><a class="header" href="#total-complexity">Total Complexity</a></h3>
<p>Sum of cyclomatic and cognitive complexity.</p>
<ul>
<li><strong>What it measures</strong>: Combined complexity burden</li>
<li><strong>Why it matters</strong>: Catches functions high in either metric</li>
<li><strong>Default threshold</strong>: 8</li>
</ul>
<h3 id="function-length-1"><a class="header" href="#function-length-1">Function Length</a></h3>
<p>Number of lines of code in the function body.</p>
<ul>
<li><strong>What it measures</strong>: Physical size of function</li>
<li><strong>Why it matters</strong>: Long functions are hard to understand and test</li>
<li><strong>Default threshold</strong>: 20 lines</li>
</ul>
<h3 id="structural-complexity"><a class="header" href="#structural-complexity">Structural Complexity</a></h3>
<p>Additional metrics for specific patterns:</p>
<ul>
<li><strong>Match arms</strong>: Flags large match/switch statements (default: 4)</li>
<li><strong>If-else chains</strong>: Flags long conditional chains (default: 3)</li>
</ul>
<p><strong>Critical</strong>: Debtmap uses <strong>conjunction logic</strong> - functions are flagged only when they meet <strong>ALL</strong> of these conditions simultaneously:</p>
<ul>
<li>Cyclomatic complexity &gt;= adjusted cyclomatic threshold</li>
<li>Cognitive complexity &gt;= adjusted cognitive threshold</li>
<li>Function length &gt;= minimum function length</li>
<li>Total complexity (cyclomatic + cognitive) &gt;= adjusted total threshold</li>
</ul>
<p>The thresholds are first adjusted by role-based multipliers, then all four checks must pass for the function to be flagged.</p>
<h3 id="threshold-validation"><a class="header" href="#threshold-validation">Threshold Validation</a></h3>
<p>Debtmap validates critical thresholds to prevent misconfiguration. The following validation rules apply (src/complexity/threshold_manager.rs:191-217):</p>
<p><strong>Core Complexity Metrics</strong> (must not be zero):</p>
<ul>
<li><code>minimum_total_complexity</code> &gt; 0</li>
<li><code>minimum_cyclomatic_complexity</code> &gt; 0</li>
<li><code>minimum_cognitive_complexity</code> &gt; 0</li>
</ul>
<p><strong>Role Multipliers</strong> (must be positive):</p>
<ul>
<li><code>entry_point_multiplier</code> &gt; 0</li>
<li><code>core_logic_multiplier</code> &gt; 0</li>
<li><code>utility_multiplier</code> &gt; 0</li>
<li><code>test_function_multiplier</code> &gt; 0</li>
</ul>
<p><strong>Note</strong>: Structural thresholds (<code>minimum_match_arms</code>, <code>minimum_if_else_chain</code>, <code>minimum_function_length</code>) are not validated and can be set to any value including zero. Zero values effectively disable those checks.</p>
<p>If any validated field fails validation, Debtmap will reject the configuration with an error message and use default values instead.</p>
<h2 id="role-based-multipliers"><a class="header" href="#role-based-multipliers">Role-Based Multipliers</a></h2>
<p>Debtmap automatically adjusts thresholds based on function role, recognizing that different types of functions have different complexity expectations:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Function Role</th><th>Multiplier</th><th>Effect</th><th>Examples</th></tr>
</thead>
<tbody>
<tr><td>Entry Points</td><td>1.2x - 2.0x (preset-specific)</td><td>More lenient</td><td><code>main()</code>, HTTP handlers, CLI commands</td></tr>
<tr><td>Core Logic</td><td>1.0x</td><td>Standard</td><td>Business logic, algorithms</td></tr>
<tr><td>Utility Functions</td><td>0.6x - 1.0x (preset-specific)</td><td>Stricter</td><td>Getters, setters, simple helpers</td></tr>
<tr><td>Test Functions</td><td>2.0x - 3.0x (preset-specific)</td><td>Most lenient</td><td>Unit tests, integration tests</td></tr>
<tr><td>Unknown Functions</td><td>1.0x (defaults to core logic)</td><td>Standard</td><td>Functions that don’t match any role pattern</td></tr>
</tbody>
</table>
</div>
<p><strong>Note</strong>: Some multipliers vary by preset:</p>
<ul>
<li><strong>Entry Points</strong>: Strict=1.2x, Balanced=1.5x, Lenient=2.0x</li>
<li><strong>Utility Functions</strong>: Strict=0.6x, Balanced=0.8x, Lenient=1.0x</li>
<li><strong>Test Functions</strong>: Strict=3.0x, Balanced=2.0x, Lenient=3.0x</li>
</ul>
<p><strong>How multipliers work:</strong></p>
<p>A higher multiplier makes thresholds more lenient by adjusting ALL thresholds. The multiplier values vary by preset - for example, entry point functions use 1.2x (strict), 1.5x (balanced), or 2.0x (lenient).</p>
<p><strong>Example: Entry Point function with Balanced preset (multiplier = 1.5x):</strong></p>
<ul>
<li>Cyclomatic threshold: 7.5 (5 × 1.5)</li>
<li>Cognitive threshold: 15 (10 × 1.5)</li>
<li>Total threshold: 12 (8 × 1.5)</li>
<li>Length threshold: 30 lines (20 × 1.5)</li>
</ul>
<p><strong>The function is flagged only if ALL conditions are met:</strong></p>
<ul>
<li>Cyclomatic complexity &gt;= 7.5 AND</li>
<li>Cognitive complexity &gt;= 15 AND</li>
<li>Function length &gt;= 30 lines AND</li>
<li>Total complexity (cyclomatic + cognitive) &gt;= 12</li>
</ul>
<p><strong>Comparison across roles (Balanced preset):</strong></p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Role</th><th>Cyclomatic</th><th>Cognitive</th><th>Total</th><th>Length</th><th>Flagged When</th></tr>
</thead>
<tbody>
<tr><td>Entry Point (1.5x)</td><td>7.5</td><td>15</td><td>12</td><td>30</td><td>ALL conditions met</td></tr>
<tr><td>Core Logic (1.0x)</td><td>5</td><td>10</td><td>8</td><td>20</td><td>ALL conditions met</td></tr>
<tr><td>Utility (0.8x)</td><td>4</td><td>8</td><td>6.4</td><td>16</td><td>ALL conditions met</td></tr>
<tr><td>Test (2.0x)</td><td>10</td><td>20</td><td>16</td><td>40</td><td>ALL conditions met</td></tr>
</tbody>
</table>
</div>
<p><strong>Note</strong>: Entry point multipliers differ by preset. With the strict preset, entry points use 1.2x (cyclomatic=3.6, cognitive=8.4), while the lenient preset uses 2.0x (cyclomatic=20, cognitive=40).</p>
<p>This allows test functions and entry points to be more complex without false positives, while keeping utility functions clean and simple.</p>
<h2 id="cli-threshold-flags"><a class="header" href="#cli-threshold-flags">CLI Threshold Flags</a></h2>
<p>Override thresholds for a single analysis run using command-line flags:</p>
<h3 id="preset-based-configuration-recommended"><a class="header" href="#preset-based-configuration-recommended">Preset-Based Configuration (Recommended)</a></h3>
<p>Use <code>--threshold-preset</code> to apply a predefined threshold configuration:</p>
<pre><code class="language-bash"># Use strict preset (cyclomatic=3, cognitive=7, total=5, length=15)
debtmap analyze . --threshold-preset strict

# Use balanced preset (default - cyclomatic=5, cognitive=10, total=8, length=20)
debtmap analyze . --threshold-preset balanced

# Use lenient preset (cyclomatic=10, cognitive=20, total=15, length=50)
debtmap analyze . --threshold-preset lenient
</code></pre>
<h3 id="individual-threshold-overrides"><a class="header" href="#individual-threshold-overrides">Individual Threshold Overrides</a></h3>
<p>You can also override specific thresholds:</p>
<pre><code class="language-bash"># Override cyclomatic complexity threshold (legacy flag, default: 10)
debtmap analyze . --threshold-complexity 15

# Override duplication threshold in lines (default: 50)
debtmap analyze . --threshold-duplication 30

# Combine multiple threshold flags
debtmap analyze . --threshold-complexity 15 --threshold-duplication 30
</code></pre>
<p><strong>Note</strong>:</p>
<ul>
<li><code>--threshold-preset</code> provides the most comprehensive threshold configuration (includes all complexity metrics and role multipliers)</li>
<li>Individual flags like <code>--threshold-complexity</code> are legacy flags that only set a single cyclomatic complexity threshold, without configuring cognitive complexity, total complexity, function length, or role multipliers</li>
<li>For full control over all complexity metrics and role-based multipliers, use the <code>.debtmap.toml</code> configuration file</li>
<li>CLI flags override configuration file settings for that run only</li>
</ul>
<h2 id="configuration-file-1"><a class="header" href="#configuration-file-1">Configuration File</a></h2>
<p>For project-specific thresholds, create a <code>.debtmap.toml</code> file in your project root.</p>
<h3 id="complexity-thresholds-configuration"><a class="header" href="#complexity-thresholds-configuration">Complexity Thresholds Configuration</a></h3>
<p>The <code>[complexity_thresholds]</code> section in <code>.debtmap.toml</code> allows fine-grained control over function complexity detection:</p>
<pre><code class="language-toml">[complexity_thresholds]
# Core complexity metrics
minimum_total_complexity = 8        # Sum of cyclomatic + cognitive
minimum_cyclomatic_complexity = 5   # Decision points (if, match, etc.)
minimum_cognitive_complexity = 10   # Mental effort to understand code

# Structural complexity metrics
minimum_match_arms = 4              # Maximum match/switch arms
minimum_if_else_chain = 3           # Maximum if-else chain length
minimum_function_length = 20        # Minimum lines before flagging

# Role-based multipliers (applied to all thresholds above)
entry_point_multiplier = 1.5        # main(), handlers, CLI commands
core_logic_multiplier = 1.0         # Standard business logic
utility_multiplier = 0.8            # Getters, setters, helpers
test_function_multiplier = 2.0      # Unit tests, integration tests
</code></pre>
<p><strong>Note</strong>: The multipliers are applied to thresholds before comparison. For example, with <code>entry_point_multiplier = 1.5</code> and <code>minimum_cyclomatic_complexity = 5</code>, an entry point function would be flagged at cyclomatic complexity 7.5 (5 × 1.5).</p>
<p><strong>Validation</strong>: Core complexity metrics (<code>minimum_total_complexity</code>, <code>minimum_cyclomatic_complexity</code>, <code>minimum_cognitive_complexity</code>) and all role multipliers must be positive (&gt; 0). Zero or negative values for these fields will cause validation errors and Debtmap will use default values. Structural thresholds (<code>minimum_match_arms</code>, <code>minimum_if_else_chain</code>, <code>minimum_function_length</code>) are not validated and can be set to zero to disable those checks.</p>
<h3 id="complete-example"><a class="header" href="#complete-example">Complete Example</a></h3>
<pre><code class="language-toml"># Legacy threshold settings (simple configuration)
# Note: For comprehensive control, use [complexity_thresholds] instead
[thresholds]
complexity = 15                # Cyclomatic complexity threshold (legacy)
cognitive = 20                 # Cognitive complexity threshold (legacy)
max_file_length = 500         # Maximum file length in lines

# Validation thresholds for CI/CD
[thresholds.validation]
max_average_complexity = 10.0      # Maximum average complexity across codebase
max_debt_density = 50.0           # Maximum debt items per 1000 LOC
max_codebase_risk_score = 7.0     # Maximum overall risk score
min_coverage_percentage = 0.0     # Minimum test coverage (0 = disabled)
max_total_debt_score = 10000      # Safety net for total debt score

# God object detection
[god_object]
enabled = true

# Rust-specific thresholds
[god_object.rust]
max_methods = 20        # Maximum methods before flagging as god object
max_fields = 15         # Maximum fields
max_traits = 5          # Maximum trait implementations
max_lines = 1000        # Maximum lines in impl block
max_complexity = 200    # Maximum total complexity

# Python-specific thresholds
[god_object.python]
max_methods = 15
max_fields = 10
max_traits = 3
max_lines = 500
max_complexity = 150

# JavaScript/TypeScript-specific thresholds
[god_object.javascript]
max_methods = 15
max_fields = 20
max_traits = 3
max_lines = 500
max_complexity = 150
</code></pre>
<p><strong>Configuration Section Notes:</strong></p>
<ul>
<li><strong><code>[thresholds]</code></strong>: Legacy/simple threshold configuration. Sets basic complexity thresholds without role multipliers or comprehensive metric control.</li>
<li><strong><code>[complexity_thresholds]</code></strong>: Modern/comprehensive threshold configuration. Provides fine-grained control over all complexity metrics, structural thresholds, and role-based multipliers. Use this for full control.</li>
<li><strong>Recommendation</strong>: For new projects, use <code>[complexity_thresholds]</code> for comprehensive configuration. The <code>[thresholds]</code> section is maintained for backward compatibility.</li>
</ul>
<h3 id="using-configuration-file"><a class="header" href="#using-configuration-file">Using Configuration File</a></h3>
<pre><code class="language-bash"># Initialize with default configuration
debtmap init

# Edit .debtmap.toml to customize thresholds
# Then run analysis (automatically uses config file)
debtmap analyze .

# Validate against thresholds in CI/CD
debtmap validate . --config .debtmap.toml
</code></pre>
<h2 id="god-object-thresholds"><a class="header" href="#god-object-thresholds">God Object Thresholds</a></h2>
<p>God objects are classes/structs with too many responsibilities. Debtmap uses language-specific thresholds to detect them:</p>
<h3 id="rust-thresholds"><a class="header" href="#rust-thresholds">Rust Thresholds</a></h3>
<pre><code class="language-toml">[god_object.rust]
max_methods = 20        # Methods in impl blocks
max_fields = 15         # Struct fields
max_traits = 5          # Trait implementations
max_lines = 1000        # Lines in impl blocks
max_complexity = 200    # Total complexity
</code></pre>
<h3 id="python-thresholds"><a class="header" href="#python-thresholds">Python Thresholds</a></h3>
<pre><code class="language-toml">[god_object.python]
max_methods = 15
max_fields = 10
max_traits = 3          # Base classes
max_lines = 500
max_complexity = 150
</code></pre>
<h3 id="javascripttypescript-thresholds"><a class="header" href="#javascripttypescript-thresholds">JavaScript/TypeScript Thresholds</a></h3>
<pre><code class="language-toml">[god_object.javascript]
max_methods = 15
max_fields = 20
max_traits = 3          # Extended classes
max_lines = 500
max_complexity = 150
</code></pre>
<p><strong>Why language-specific thresholds?</strong></p>
<p>Different languages have different idioms:</p>
<ul>
<li><strong>Rust</strong>: Encourages small traits and composition, so lower thresholds</li>
<li><strong>Python</strong>: Duck typing allows more fields, but fewer methods</li>
<li><strong>JavaScript</strong>: Prototype-based, typically has more properties</li>
</ul>
<h2 id="validation-thresholds-1"><a class="header" href="#validation-thresholds-1">Validation Thresholds</a></h2>
<p>Use validation thresholds in CI/CD pipelines to enforce quality gates:</p>
<h3 id="scale-independent-metrics-recommended"><a class="header" href="#scale-independent-metrics-recommended">Scale-Independent Metrics (Recommended)</a></h3>
<p>These metrics work for codebases of any size:</p>
<pre><code class="language-toml">[thresholds.validation]
# Average complexity per function (default: 10.0)
max_average_complexity = 10.0

# Debt items per 1000 lines of code (default: 50.0)
max_debt_density = 50.0

# Overall risk score 0-10 (default: 7.0)
max_codebase_risk_score = 7.0
</code></pre>
<h3 id="optional-metrics"><a class="header" href="#optional-metrics">Optional Metrics</a></h3>
<pre><code class="language-toml">[thresholds.validation]
# Minimum test coverage percentage (default: 0.0 = disabled)
min_coverage_percentage = 80.0

# Safety net for total debt score (default: 10000)
max_total_debt_score = 5000
</code></pre>
<h3 id="using-validation-in-cicd"><a class="header" href="#using-validation-in-cicd">Using Validation in CI/CD</a></h3>
<pre><code class="language-bash"># Run validation (exits with error if thresholds exceeded)
debtmap validate . --config .debtmap.toml

# Example CI/CD workflow
debtmap analyze . --output report.json
debtmap validate . --config .debtmap.toml || exit 1
</code></pre>
<p><strong>CI/CD Best Practices:</strong></p>
<ul>
<li>Start with lenient thresholds to establish baseline</li>
<li>Gradually tighten thresholds as you pay down debt</li>
<li>Use <code>max_debt_density</code> for stable quality metric</li>
<li>Track trends over time, not just point-in-time values</li>
</ul>
<h2 id="tuning-guidelines"><a class="header" href="#tuning-guidelines">Tuning Guidelines</a></h2>
<p>How to choose and adjust thresholds for your project:</p>
<h3 id="1-start-with-defaults"><a class="header" href="#1-start-with-defaults">1. Start with Defaults</a></h3>
<p>Begin with the balanced preset to understand your codebase:</p>
<pre><code class="language-bash">debtmap analyze .
</code></pre>
<p>Review the output to see what gets flagged and what doesn’t.</p>
<h3 id="2-run-baseline-analysis"><a class="header" href="#2-run-baseline-analysis">2. Run Baseline Analysis</a></h3>
<p>Understand your current state:</p>
<pre><code class="language-bash"># Analyze and save results
debtmap analyze . --output baseline.json

# Review high-priority items
debtmap analyze . --top 20
</code></pre>
<h3 id="3-adjust-based-on-project-type"><a class="header" href="#3-adjust-based-on-project-type">3. Adjust Based on Project Type</a></h3>
<p><strong>New Projects:</strong></p>
<ul>
<li>Use strict preset to enforce high quality from the start</li>
<li>Prevents accumulation of technical debt</li>
</ul>
<p><strong>Typical Projects:</strong></p>
<ul>
<li>Use balanced preset (recommended)</li>
<li>Good middle ground for most teams</li>
</ul>
<p><strong>Legacy Codebases:</strong></p>
<ul>
<li>Use lenient preset initially</li>
<li>Focus on worst offenders first</li>
<li>Gradually tighten thresholds as you refactor</li>
</ul>
<h3 id="4-fine-tune-in-configuration-file"><a class="header" href="#4-fine-tune-in-configuration-file">4. Fine-Tune in Configuration File</a></h3>
<p>Create <code>.debtmap.toml</code> and adjust specific thresholds:</p>
<pre><code class="language-bash"># Initialize config file
debtmap init

# Edit .debtmap.toml
# Adjust thresholds based on your baseline analysis
</code></pre>
<h3 id="5-validate-and-iterate"><a class="header" href="#5-validate-and-iterate">5. Validate and Iterate</a></h3>
<pre><code class="language-bash"># Test your thresholds
debtmap validate . --config .debtmap.toml

# Adjust if needed
# Iterate until you find the right balance
</code></pre>
<h3 id="troubleshooting-threshold-configuration"><a class="header" href="#troubleshooting-threshold-configuration">Troubleshooting Threshold Configuration</a></h3>
<p><strong>Too many false positives?</strong></p>
<ul>
<li>Increase thresholds or switch to lenient preset</li>
<li>Check if role multipliers are appropriate</li>
<li>Review god object thresholds for your language</li>
</ul>
<p><strong>Missing important issues?</strong></p>
<ul>
<li>Decrease thresholds or switch to strict preset</li>
<li>Verify <code>.debtmap.toml</code> is being loaded</li>
<li>Check for suppression patterns hiding issues</li>
</ul>
<p><strong>Different standards for tests?</strong></p>
<ul>
<li>Don’t worry - role multipliers automatically handle this</li>
<li>Test functions get 2-3x multiplier by default</li>
</ul>
<p><strong>Inconsistent results?</strong></p>
<ul>
<li>Ensure <code>.debtmap.toml</code> is in project root</li>
<li>CLI flags override config file - remove them for consistency</li>
<li>Use <code>--config</code> flag to specify config file explicitly</li>
</ul>
<h2 id="examples-1"><a class="header" href="#examples-1">Examples</a></h2>
<h3 id="example-1-quick-analysis-with-strict-preset"><a class="header" href="#example-1-quick-analysis-with-strict-preset">Example 1: Quick Analysis with Strict Preset</a></h3>
<pre><code class="language-bash"># Use strict thresholds for new project
debtmap analyze . --threshold-preset strict
</code></pre>
<h3 id="example-2-custom-cli-thresholds"><a class="header" href="#example-2-custom-cli-thresholds">Example 2: Custom CLI Thresholds</a></h3>
<pre><code class="language-bash"># Analyze with custom thresholds (no config file)
debtmap analyze . \
  --threshold-complexity 15 \
  --threshold-duplication 30
</code></pre>
<h3 id="example-3-project-specific-configuration"><a class="header" href="#example-3-project-specific-configuration">Example 3: Project-Specific Configuration</a></h3>
<pre><code class="language-bash"># Initialize configuration
debtmap init

# Creates .debtmap.toml - edit to customize
# Example: Increase complexity threshold to 15

# Run analysis with project config
debtmap analyze .
</code></pre>
<h3 id="example-4-cicd-validation"><a class="header" href="#example-4-cicd-validation">Example 4: CI/CD Validation</a></h3>
<pre><code class="language-bash"># Create strict validation configuration
cat &gt; .debtmap.toml &lt;&lt; EOF
[thresholds.validation]
max_average_complexity = 8.0
max_debt_density = 30.0
max_codebase_risk_score = 6.0
min_coverage_percentage = 75.0
EOF

# Run in CI/CD pipeline
debtmap analyze . --output report.json
debtmap validate . --config .debtmap.toml
</code></pre>
<h3 id="example-5-gradual-debt-reduction"><a class="header" href="#example-5-gradual-debt-reduction">Example 5: Gradual Debt Reduction</a></h3>
<pre><code class="language-bash"># Month 1: Start lenient
debtmap analyze . --threshold-preset lenient --output month1.json

# Month 2: Switch to balanced
debtmap analyze . --threshold-preset balanced --output month2.json

# Month 3: Tighten further
debtmap analyze . --threshold-preset strict --output month3.json

# Compare progress
debtmap analyze . --output current.json
# Review trend: month1.json -&gt; month2.json -&gt; month3.json -&gt; current.json
</code></pre>
<h2 id="decision-tree-choosing-your-preset"><a class="header" href="#decision-tree-choosing-your-preset">Decision Tree: Choosing Your Preset</a></h2>
<pre><code>Start here: What kind of project are you working on?
│
├─ New project or library
│  └─ Use STRICT preset
│     └─ Prevent debt accumulation from day one
│
├─ Existing production application
│  └─ What's your goal?
│     ├─ Maintain current quality
│     │  └─ Use BALANCED preset
│     │     └─ Good default for most teams
│     │
│     └─ Reduce existing debt gradually
│        └─ Start with LENIENT preset
│           └─ Focus on worst issues first
│           └─ Tighten thresholds over time
│
└─ Legacy codebase or complex domain
   └─ Use LENIENT preset
      └─ Avoid overwhelming with false positives
      └─ Create baseline and improve incrementally
</code></pre>
<h2 id="best-practices-2"><a class="header" href="#best-practices-2">Best Practices</a></h2>
<ol>
<li><strong>Start with defaults</strong> - Don’t over-customize initially</li>
<li><strong>Track trends</strong> - Monitor debt over time, not just point values</li>
<li><strong>Be consistent</strong> - Use same thresholds across team</li>
<li><strong>Document choices</strong> - Comment your <code>.debtmap.toml</code> to explain custom thresholds</li>
<li><strong>Automate validation</strong> - Run <code>debtmap validate</code> in CI/CD</li>
<li><strong>Review regularly</strong> - Reassess thresholds quarterly</li>
<li><strong>Gradual tightening</strong> - Don’t make thresholds stricter too quickly</li>
<li><strong>Trust role multipliers</strong> - Let Debtmap handle different function types</li>
</ol>
<h2 id="related-topics"><a class="header" href="#related-topics">Related Topics</a></h2>
<ul>
<li><a href="#getting-started-1">Getting Started</a> - Initial setup and first analysis</li>
<li><a href="#cli-reference">CLI Reference</a> - Complete command-line flag documentation</li>
<li><a href="#configuration-2">Configuration</a> - Full <code>.debtmap.toml</code> reference</li>
<li><a href="#scoring-strategies">Scoring Strategies</a> - How thresholds affect debt scores</li>
<li><a href="#god-object-detection-1">God Object Detection</a> - Deep dive into god object analysis</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="suppression-patterns"><a class="header" href="#suppression-patterns">Suppression Patterns</a></h1>
<p>Debtmap provides flexible suppression mechanisms to help you focus on the technical debt that matters most. You can suppress specific debt items inline with comments, or exclude entire files and functions through configuration.</p>
<h2 id="why-use-suppressions"><a class="header" href="#why-use-suppressions">Why Use Suppressions?</a></h2>
<p>Not all detected technical debt requires immediate action. Suppressions allow you to:</p>
<ul>
<li><strong>Focus on priorities</strong>: Hide known, accepted debt to see new issues clearly</li>
<li><strong>Handle false positives</strong>: Suppress patterns that don’t apply to your context</li>
<li><strong>Document decisions</strong>: Explain why certain debt is acceptable using reason annotations</li>
<li><strong>Exclude test code</strong>: Ignore complexity in test fixtures and setup functions</li>
</ul>
<h2 id="inline-comment-suppression"><a class="header" href="#inline-comment-suppression">Inline Comment Suppression</a></h2>
<p>Debtmap supports four inline comment formats that work with your language’s comment syntax:</p>
<h3 id="single-line-suppression"><a class="header" href="#single-line-suppression">Single-Line Suppression</a></h3>
<p>Suppress debt on the same line as the comment:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// debtmap:ignore
// TODO: Implement caching later - performance is acceptable for now
<span class="boring">}</span></code></pre>
<pre><code class="language-python"># debtmap:ignore
# FIXME: Refactor this after the Q2 release
</code></pre>
<p>The suppression applies to debt detected on the same line as the comment.</p>
<h3 id="next-line-suppression"><a class="header" href="#next-line-suppression">Next-Line Suppression</a></h3>
<p>Suppress debt on the line immediately following the comment:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// debtmap:ignore-next-line
fn complex_algorithm() {
    // ...20 lines of complex code...
}
<span class="boring">}</span></code></pre>
<pre><code class="language-typescript">// debtmap:ignore-next-line
function calculateMetrics(data: DataPoint[]): Metrics {
    // ...complex implementation...
}
</code></pre>
<p>This format is useful when you want the suppression comment to appear before the code it affects.</p>
<h3 id="block-suppression"><a class="header" href="#block-suppression">Block Suppression</a></h3>
<p>Suppress multiple lines of code between start and end markers:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// debtmap:ignore-start
fn setup_test_environment() {
    // TODO: Add more test cases
    // FIXME: Handle edge cases
    // Complex test setup code...
}
// debtmap:ignore-end
<span class="boring">}</span></code></pre>
<pre><code class="language-python"># debtmap:ignore-start
def mock_api_responses():
    # TODO: Add more mock scenarios
    # Multiple lines of mock setup
    pass
# debtmap:ignore-end
</code></pre>
<p><strong>Important</strong>: Every <code>ignore-start</code> must have a matching <code>ignore-end</code>. Debtmap tracks unclosed blocks and can warn you about them.</p>
<h2 id="type-specific-suppression"><a class="header" href="#type-specific-suppression">Type-Specific Suppression</a></h2>
<p>You can suppress specific types of debt using bracket notation instead of suppressing everything:</p>
<h3 id="quick-reference-debt-type-suppression"><a class="header" href="#quick-reference-debt-type-suppression">Quick Reference: Debt Type Suppression</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Debt Type</th><th>Bracket Name(s)</th><th>Example</th><th>Notes</th></tr>
</thead>
<tbody>
<tr><td>TODO comments</td><td><code>[todo]</code></td><td><code>// debtmap:ignore[todo]</code></td><td>Also suppresses TestTodo</td></tr>
<tr><td>FIXME comments</td><td><code>[fixme]</code></td><td><code>// debtmap:ignore[fixme]</code></td><td></td></tr>
<tr><td>Code smells</td><td><code>[smell]</code> or <code>[codesmell]</code></td><td><code>// debtmap:ignore[smell]</code></td><td></td></tr>
<tr><td>High complexity</td><td><code>[complexity]</code></td><td><code>// debtmap:ignore[complexity]</code></td><td>Also suppresses TestComplexity</td></tr>
<tr><td>Code duplication</td><td><code>[duplication]</code> or <code>[duplicate]</code></td><td><code>// debtmap:ignore[duplication]</code></td><td>Also suppresses TestDuplication</td></tr>
<tr><td>Dependency issues</td><td><code>[dependency]</code></td><td><code>// debtmap:ignore[dependency]</code></td><td></td></tr>
<tr><td>Error swallowing</td><td>❌ Not supported</td><td><code>// debtmap:ignore</code></td><td>Use general suppression only</td></tr>
<tr><td>Resource management</td><td>❌ Not supported</td><td><code>// debtmap:ignore</code></td><td>Use general suppression only</td></tr>
<tr><td>Code organization</td><td>❌ Not supported</td><td><code>// debtmap:ignore</code></td><td>Use general suppression only</td></tr>
<tr><td>Test quality</td><td>❌ Not supported</td><td><code>// debtmap:ignore</code></td><td>Use general suppression only</td></tr>
<tr><td>All types</td><td><code>[*]</code></td><td><code>// debtmap:ignore[*]</code></td><td>Wildcard matches everything</td></tr>
</tbody>
</table>
</div>
<h3 id="suppress-specific-types"><a class="header" href="#suppress-specific-types">Suppress Specific Types</a></h3>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// debtmap:ignore[todo]
// TODO: This TODO is ignored, but FIXMEs and complexity are still reported
<span class="boring">}</span></code></pre>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// debtmap:ignore[todo,fixme]
// TODO: Both TODOs and FIXMEs are ignored here
// FIXME: But complexity issues would still be detected
<span class="boring">}</span></code></pre>
<h3 id="supported-debt-types"><a class="header" href="#supported-debt-types">Supported Debt Types</a></h3>
<p>You can suppress the following debt types by name in bracket notation:</p>
<p><strong>Currently Supported:</strong></p>
<ul>
<li><code>todo</code> - TODO comments (also detects test-specific TODOs)</li>
<li><code>fixme</code> - FIXME comments</li>
<li><code>smell</code> or <code>codesmell</code> - Code smell patterns</li>
<li><code>complexity</code> - High cognitive complexity (also detects test complexity)</li>
<li><code>duplication</code> or <code>duplicate</code> - Code duplication (also detects test duplication)</li>
<li><code>dependency</code> - Dependency issues</li>
<li><code>*</code> - All types (wildcard)</li>
</ul>
<p><strong>Auto-Detected Types</strong> (cannot be suppressed by name):</p>
<p>The following debt types are detected by code analysis rather than comment scanning. These types <strong>cannot</strong> be suppressed using bracket notation like <code>[error_swallowing]</code> because they are not included in the suppression parser’s type mapping.</p>
<p><strong>Why bracket notation doesn’t work</strong>: The suppression parser only recognizes specific type names in its internal mapping (<code>DEBT_TYPE_MAP</code>): <code>todo</code>, <code>fixme</code>, <code>smell</code>/<code>codesmell</code>, <code>complexity</code>, <code>duplication</code>/<code>duplicate</code>, and <code>dependency</code>. Types detected through AST analysis (like error swallowing and resource management) don’t have string identifiers in the parser. To suppress these, use the general <code>debtmap:ignore</code> marker without brackets:</p>
<ul>
<li><code>error_swallowing</code> - Error handling issues (empty catch blocks, ignored errors)</li>
<li><code>resource_management</code> - Resource cleanup issues (file handles, connections)</li>
<li><code>code_organization</code> - Structural issues (god objects, large classes)</li>
</ul>
<p><strong>Example:</strong></p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ Correct: General suppression without brackets
// debtmap:ignore -- Intentional empty catch for cleanup
match result {
    Err(_) =&gt; {} // Empty catch block
    Ok(v) =&gt; process(v)
}

// ❌ Wrong: Bracket notation not supported for auto-detected types
// debtmap:ignore[error_swallowing]
<span class="boring">}</span></code></pre>
<p><strong>Test-Specific Debt Types:</strong></p>
<p>Test-specific variants like <code>TestComplexity</code>, <code>TestTodo</code>, <code>TestDuplication</code>, and <code>TestQuality</code> are suppressed through their base types:</p>
<ul>
<li><code>TestComplexity</code> → suppressed with <code>[complexity]</code></li>
<li><code>TestTodo</code> → suppressed with <code>[todo]</code></li>
<li><code>TestDuplication</code> → suppressed with <code>[duplication]</code></li>
<li><code>TestQuality</code> → suppressed with general <code>debtmap:ignore</code> (no bracket notation)</li>
</ul>
<p><strong>Example:</strong></p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[cfg(test)]
mod tests {
    // Suppresses both Complexity and TestComplexity
    // debtmap:ignore[complexity] -- Complex test setup acceptable
    fn setup_test_environment() {
        // Complex test initialization
    }

    // debtmap:ignore[todo] -- Suppresses both Todo and TestTodo
    // TODO: Add more test cases
    fn test_feature() { }
}

## Wildcard Suppression

Use `[*]` to explicitly suppress all types (equivalent to no bracket notation):

```rust
// debtmap:ignore[*]
// Suppresses all debt types
<span class="boring">}</span></code></pre>
<h3 id="type-specific-blocks"><a class="header" href="#type-specific-blocks">Type-Specific Blocks</a></h3>
<p>Block suppressions also support type filtering:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// debtmap:ignore-start[complexity]
fn intentionally_complex_for_performance() {
    // Complex nested logic is intentional here
    // Complexity warnings suppressed, but TODOs still detected
}
// debtmap:ignore-end
<span class="boring">}</span></code></pre>
<h2 id="suppression-reasons"><a class="header" href="#suppression-reasons">Suppression Reasons</a></h2>
<p>Document why you’re suppressing debt using the <code>--</code> separator:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// debtmap:ignore -- Intentional for backward compatibility
// TODO: Remove this after all clients upgrade to v2.0
<span class="boring">}</span></code></pre>
<pre><code class="language-python"># debtmap:ignore[complexity] -- Performance-critical hot path
def optimize_query(params):
    # Complex but necessary for performance
    pass
</code></pre>
<pre><code class="language-typescript">// debtmap:ignore-next-line -- Waiting on upstream library fix
function workaroundBug() {
    // FIXME: Remove when library v3.0 is released
}
</code></pre>
<p><strong>Best Practice</strong>: Always include reasons for suppressions. This helps future maintainers understand the context and know when suppressions can be removed.</p>
<h2 id="config-file-exclusions"><a class="header" href="#config-file-exclusions">Config File Exclusions</a></h2>
<p>For broader exclusions, use the <code>[ignore]</code> section in <code>.debtmap.toml</code>:</p>
<h3 id="file-pattern-exclusions"><a class="header" href="#file-pattern-exclusions">File Pattern Exclusions</a></h3>
<pre><code class="language-toml">[ignore]
patterns = [
    "target/**",              # Build artifacts
    "node_modules/**",        # Dependencies
    "**/*_test.rs",           # Test files with _test suffix
    "tests/**",               # All test directories
    "**/fixtures/**",         # Test fixtures
    "**/mocks/**",            # Mock implementations
    "**/*.min.js",            # Minified files
    "**/demo/**",             # Demo code
    "**/*.generated.rs",      # Generated files
    "vendor/**",              # Vendor code
    "third_party/**",         # Third-party code
]
</code></pre>
<h3 id="function-name-exclusions-planned"><a class="header" href="#function-name-exclusions-planned">Function Name Exclusions (Planned)</a></h3>
<blockquote>
<p><strong>Note:</strong> Function-level exclusions by name pattern are not yet implemented. This is a planned feature for a future release.</p>
</blockquote>
<p>When implemented, you will be able to exclude entire function families by name pattern:</p>
<pre><code class="language-toml"># Planned feature - not yet available
[ignore.functions]
patterns = [
    # Test setup functions
    "setup_test_*",
    "teardown_test_*",
    "create_test_*",
    "mock_*",

    # Generated code
    "derive_*",
    "__*",                    # Python dunder methods

    # CLI parsing (naturally complex)
    "parse_args",
    "parse_cli",
    "build_cli",

    # Serialization (naturally complex pattern matching)
    "serialize_*",
    "deserialize_*",
    "to_json",
    "from_json",
]
</code></pre>
<p><strong>Current workaround:</strong> Use inline suppression comments (<code>debtmap:ignore</code>) for specific functions, or use file pattern exclusions to exclude entire test files.</p>
<h2 id="glob-pattern-syntax"><a class="header" href="#glob-pattern-syntax">Glob Pattern Syntax</a></h2>
<p>File patterns use standard glob syntax:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Pattern</th><th>Matches</th><th>Example</th></tr>
</thead>
<tbody>
<tr><td><code>*</code></td><td>Any characters within a path component</td><td><code>*.rs</code> matches <code>main.rs</code></td></tr>
<tr><td><code>**</code></td><td>Any directories (recursive)</td><td><code>tests/**</code> matches <code>tests/unit/foo.rs</code></td></tr>
<tr><td><code>?</code></td><td>Single character</td><td><code>test?.rs</code> matches <code>test1.rs</code></td></tr>
<tr><td><code>[abc]</code></td><td>Character class</td><td><code>test[123].rs</code> matches <code>test1.rs</code></td></tr>
<tr><td><code>[!abc]</code></td><td>Negated class</td><td><code>test[!0].rs</code> matches <code>test1.rs</code> but not <code>test0.rs</code></td></tr>
</tbody>
</table>
</div>
<h3 id="glob-pattern-examples"><a class="header" href="#glob-pattern-examples">Glob Pattern Examples</a></h3>
<pre><code class="language-toml">[ignore]
patterns = [
    "src/**/*_generated.rs",  # Generated files in any subdirectory
    "**/test_*.py",           # Python test files anywhere
    "legacy/**/[!i]*.js",     # Legacy JS files not starting with 'i'
    "**/*.min.js",            # Minified JavaScript
    "**/*.min.css",           # Minified CSS
]
</code></pre>
<blockquote>
<p><strong>Note:</strong> Brace expansion (e.g., <code>*.{js,css}</code>) is not supported. Use separate patterns for each file extension.</p>
</blockquote>
<h2 id="language-specific-comment-syntax"><a class="header" href="#language-specific-comment-syntax">Language-Specific Comment Syntax</a></h2>
<p>Debtmap automatically uses the correct comment syntax for each language:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Language</th><th>Comment Prefix</th><th>Example</th></tr>
</thead>
<tbody>
<tr><td>Rust</td><td><code>//</code></td><td><code>// debtmap:ignore</code></td></tr>
<tr><td>JavaScript</td><td><code>//</code></td><td><code>// debtmap:ignore</code></td></tr>
<tr><td>TypeScript</td><td><code>//</code></td><td><code>// debtmap:ignore</code></td></tr>
<tr><td>Python</td><td><code>#</code></td><td><code># debtmap:ignore</code></td></tr>
<tr><td>Other languages</td><td><code>//</code></td><td><code>// debtmap:ignore</code></td></tr>
</tbody>
</table>
</div>
<p><strong>Note</strong>: Languages not explicitly listed use <code>//</code> as the default comment prefix.</p>
<p>You don’t need to configure this—Debtmap detects the language and uses the appropriate syntax.</p>
<h2 id="explicitly-specified-files"><a class="header" href="#explicitly-specified-files">Explicitly Specified Files</a></h2>
<p><strong>Important behavior</strong>: When you analyze a specific file directly, ignore patterns are bypassed:</p>
<pre><code class="language-bash"># Respects [ignore] patterns in .debtmap.toml
debtmap analyze .
debtmap analyze src/

# Bypasses ignore patterns - analyzes the file even if patterns would exclude it
debtmap analyze src/test_helper.rs
</code></pre>
<p>This ensures you can always analyze specific files when needed, even if they match an ignore pattern.</p>
<h2 id="suppression-statistics"><a class="header" href="#suppression-statistics">Suppression Statistics</a></h2>
<p>Debtmap internally tracks suppression usage during analysis:</p>
<ul>
<li><strong>Total suppressions</strong>: Count of active suppressions across all files</li>
<li><strong>Suppressions by type</strong>: How many of each debt type are suppressed</li>
<li><strong>Unclosed blocks</strong>: Detection of <code>ignore-start</code> without matching <code>ignore-end</code></li>
</ul>
<p><strong>Current Status</strong>: These statistics are computed during analysis (via the <code>SuppressionContext::get_stats()</code> method) but are not currently displayed in any output format. The <code>SuppressionStats</code> struct exists and tracks all metrics, but there is no user-facing command or report format that exposes them. Future releases may add a dedicated command to view suppression metrics.</p>
<p><strong>Auditing Suppressions Now</strong>: You can audit your suppressions using standard tools:</p>
<pre><code class="language-bash"># Find all suppressions in Rust code
rg "debtmap:ignore" --type rust

# Count suppressions by type
rg "debtmap:ignore\[" --type rust | grep -o "\[.*\]" | sort | uniq -c

# Find unclosed blocks
rg "debtmap:ignore-start" --type rust -A 100 | grep -v "debtmap:ignore-end"

# List files with suppressions
rg "debtmap:ignore" --files-with-matches
</code></pre>
<h2 id="best-practices-3"><a class="header" href="#best-practices-3">Best Practices</a></h2>
<h3 id="use-suppressions-sparingly"><a class="header" href="#use-suppressions-sparingly">Use Suppressions Sparingly</a></h3>
<p>Suppressions hide information, so use them intentionally:</p>
<p>✅ <strong>Good use cases:</strong></p>
<ul>
<li>Test fixtures and mock data</li>
<li>Known technical debt with an accepted timeline</li>
<li>Intentional complexity for performance</li>
<li>False positives specific to your domain</li>
</ul>
<p>❌ <strong>Poor use cases:</strong></p>
<ul>
<li>Hiding all debt to make reports look clean</li>
<li>Suppressing instead of fixing simple issues</li>
<li>Using wildcards when specific types would work</li>
</ul>
<h3 id="always-include-reasons"><a class="header" href="#always-include-reasons">Always Include Reasons</a></h3>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ Good: Clear reason and timeline
// debtmap:ignore[complexity] -- Hot path optimization, profiled and necessary
fn fast_algorithm() { }

// ❌ Bad: No context for future maintainers
// debtmap:ignore
fn fast_algorithm() { }
<span class="boring">}</span></code></pre>
<h3 id="prefer-specific-over-broad"><a class="header" href="#prefer-specific-over-broad">Prefer Specific Over Broad</a></h3>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ Good: Only suppress the specific debt type
// debtmap:ignore[todo] -- Remove after v2.0 migration
// TODO: Migrate to new API

// ❌ Bad: Suppresses everything, including real issues
// debtmap:ignore
// TODO: Migrate to new API
<span class="boring">}</span></code></pre>
<h3 id="use-config-for-systematic-exclusions"><a class="header" href="#use-config-for-systematic-exclusions">Use Config for Systematic Exclusions</a></h3>
<p>For patterns that apply project-wide, use <code>.debtmap.toml</code> instead of inline comments:</p>
<pre><code class="language-toml"># ✅ Good: One config applies to all test files
[ignore]
patterns = ["tests/**"]

# ❌ Bad: Repetitive inline suppressions in every test file
</code></pre>
<h3 id="review-suppressions-periodically"><a class="header" href="#review-suppressions-periodically">Review Suppressions Periodically</a></h3>
<p>Suppressions can become outdated:</p>
<ul>
<li>Remove suppressions when the reason no longer applies</li>
<li>Check if suppressed debt can now be fixed</li>
<li>Verify reasons are still accurate after refactoring</li>
</ul>
<p><strong>Solution:</strong> Periodically search for suppressions:</p>
<pre><code class="language-bash">rg "debtmap:ignore" --type rust
</code></pre>
<h3 id="ensure-blocks-are-closed"><a class="header" href="#ensure-blocks-are-closed">Ensure Blocks Are Closed</a></h3>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ Good: Properly closed block
// debtmap:ignore-start
fn test_setup() { }
// debtmap:ignore-end

// ❌ Bad: Unclosed block affects all subsequent code
// debtmap:ignore-start
fn test_setup() { }
// (missing ignore-end)
<span class="boring">}</span></code></pre>
<p>Debtmap detects unclosed blocks and can warn you about them.</p>
<h2 id="common-patterns"><a class="header" href="#common-patterns">Common Patterns</a></h2>
<h3 id="suppressing-test-code"><a class="header" href="#suppressing-test-code">Suppressing Test Code</a></h3>
<pre><code class="language-toml"># In .debtmap.toml
[ignore]
patterns = [
    "tests/**/*",
    "**/*_test.rs",
    "**/test_*.py",
    "**/fixtures/**",
]
</code></pre>
<p>For test functions within production files, use inline suppressions:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[cfg(test)]
mod tests {
    // debtmap:ignore-start -- Test code
    fn setup_test_environment() { }
    // debtmap:ignore-end
}
<span class="boring">}</span></code></pre>
<h3 id="suppressing-generated-code"><a class="header" href="#suppressing-generated-code">Suppressing Generated Code</a></h3>
<pre><code class="language-toml">[ignore]
patterns = [
    "**/*_generated.*",
    "**/proto/**",
    "**/bindings/**",
]
</code></pre>
<h3 id="temporary-suppressions-with-timeline"><a class="header" href="#temporary-suppressions-with-timeline">Temporary Suppressions with Timeline</a></h3>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// debtmap:ignore[complexity] -- TODO: Refactor during Q2 2025 sprint
fn legacy_payment_processor() {
    // Complex legacy code scheduled for refactoring
}
<span class="boring">}</span></code></pre>
<h3 id="suppressing-false-positives"><a class="header" href="#suppressing-false-positives">Suppressing False Positives</a></h3>
<pre><code class="language-python"># debtmap:ignore[duplication] -- Similar but semantically different
def calculate_tax_us():
    # US tax calculation
    pass

# debtmap:ignore[duplication] -- Similar but semantically different
def calculate_tax_eu():
    # EU tax calculation with different rules
    pass
</code></pre>
<h3 id="conditional-suppression"><a class="header" href="#conditional-suppression">Conditional Suppression</a></h3>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[cfg(test)]
// debtmap:ignore[complexity]
fn test_helper() {
    // Complex test setup is acceptable
}
<span class="boring">}</span></code></pre>
<h3 id="suppression-with-detailed-justification"><a class="header" href="#suppression-with-detailed-justification">Suppression with Detailed Justification</a></h3>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// debtmap:ignore[complexity] -- Required by specification XYZ-123
// This function implements the state machine defined in spec XYZ-123.
// Complexity is inherent to the specification and cannot be reduced
// without violating requirements.
fn state_machine() { ... }
<span class="boring">}</span></code></pre>
<h2 id="troubleshooting-5"><a class="header" href="#troubleshooting-5">Troubleshooting</a></h2>
<h3 id="suppression-not-working"><a class="header" href="#suppression-not-working">Suppression Not Working</a></h3>
<ol>
<li><strong>Check comment syntax</strong>: Ensure you’re using the correct comment prefix for your language (<code>//</code> for Rust/JS/TS, <code>#</code> for Python)</li>
<li><strong>Verify spelling</strong>: It’s <code>debtmap:ignore</code>, not <code>debtmap-ignore</code> or <code>debtmap_ignore</code></li>
<li><strong>Check line matching</strong>: For same-line suppressions, ensure the debt is on the same line as the comment</li>
<li><strong>Verify type names</strong>: Use <code>todo</code>, <code>fixme</code>, <code>complexity</code>, etc. (lowercase)</li>
</ol>
<p><strong>Common syntax errors:</strong></p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Wrong: debtmap: ignore (space after colon)
// Right: debtmap:ignore

// Wrong: debtmap:ignore[Complexity] (capital C)
// Right: debtmap:ignore[complexity]
<span class="boring">}</span></code></pre>
<p><strong>Check placement:</strong></p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Wrong: comment after code
fn function() { } // debtmap:ignore

// Right: comment before code
// debtmap:ignore
fn function() { }
<span class="boring">}</span></code></pre>
<h3 id="unclosed-block-warning"><a class="header" href="#unclosed-block-warning">Unclosed Block Warning</a></h3>
<p>If you see warnings about unclosed blocks:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Problem: Missing ignore-end
// debtmap:ignore-start
fn test_helper() { }
// (Should have debtmap:ignore-end here)

// Solution: Add the closing marker
// debtmap:ignore-start
fn test_helper() { }
// debtmap:ignore-end
<span class="boring">}</span></code></pre>
<h3 id="file-still-being-analyzed"><a class="header" href="#file-still-being-analyzed">File Still Being Analyzed</a></h3>
<p>If a file in your ignore patterns is still being analyzed:</p>
<ol>
<li>Check if you’re analyzing the specific file directly (bypasses ignore patterns)</li>
<li>Verify the glob pattern matches the file path</li>
<li>Check for typos in the pattern</li>
<li>Test the pattern in isolation</li>
</ol>
<p><strong>Test pattern with find:</strong></p>
<pre><code class="language-bash">find . -path "tests/**/*" -type f
</code></pre>
<p><strong>Use double asterisk for subdirectories:</strong></p>
<pre><code class="language-toml"># Wrong: "tests/*" (only direct children)
# Right: "tests/**/*" (all descendants)
</code></pre>
<p><strong>Check relative paths:</strong></p>
<pre><code class="language-toml"># Patterns are relative to project root
patterns = [
    "src/legacy/**",  # ✓ Correct
    "/src/legacy/**", # ✗ Wrong (absolute path)
]
</code></pre>
<h3 id="function-suppression-not-working"><a class="header" href="#function-suppression-not-working">Function Suppression Not Working</a></h3>
<p>Function-level exclusions by name pattern are not yet implemented. To suppress specific functions:</p>
<ol>
<li>Use inline suppressions: <code>// debtmap:ignore</code> before the function</li>
<li>Use block suppressions: <code>// debtmap:ignore-start</code> … <code>// debtmap:ignore-end</code></li>
<li>Exclude entire files using <code>[ignore]</code> patterns if the functions are in dedicated files</li>
</ol>
<h2 id="related-topics-1"><a class="header" href="#related-topics-1">Related Topics</a></h2>
<ul>
<li><a href="#configuration-2">Configuration</a> - Full <code>.debtmap.toml</code> reference</li>
<li><a href="#cli-reference">CLI Reference</a> - Command-line analysis options</li>
<li><a href="#analysis-guide">Analysis Guide</a> - Understanding debt detection</li>
<li><a href="#output-formats-3">Output Formats</a> - Viewing suppressed items in reports</li>
</ul>
<h2 id="summary-1"><a class="header" href="#summary-1">Summary</a></h2>
<p>Suppressions help you focus on actionable technical debt:</p>
<ul>
<li><strong>Inline comments</strong>: <code>debtmap:ignore</code>, <code>ignore-next-line</code>, <code>ignore-start/end</code></li>
<li><strong>Type-specific</strong>: Use <code>[type1,type2]</code> to suppress selectively</li>
<li><strong>Reasons</strong>: Always use <code>-- reason</code> to document why</li>
<li><strong>Config patterns</strong>: Use <code>.debtmap.toml</code> for systematic file exclusions</li>
<li><strong>Best practices</strong>: Use sparingly, prefer specific over broad, review periodically</li>
</ul>
<p>With proper use of suppressions, your Debtmap reports show only the debt that matters to your team.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="output-formats-3"><a class="header" href="#output-formats-3">Output Formats</a></h1>
<p>Debtmap provides multiple output formats to suit different workflows, from interactive terminal reports to machine-readable JSON for CI/CD integration. This chapter covers all available formats and how to use them effectively.</p>
<h2 id="format-selection"><a class="header" href="#format-selection">Format Selection</a></h2>
<p>Select the output format using the <code>-f</code> or <code>--format</code> flag:</p>
<pre><code class="language-bash"># Terminal output (default) - human-readable with colors
debtmap analyze .

# JSON output - machine-readable for tooling
debtmap analyze . --format json

# Markdown output - documentation and reports
debtmap analyze . --format markdown

# HTML output - interactive web dashboard
debtmap analyze . --format html
</code></pre>
<p>Available formats:</p>
<ul>
<li><strong>terminal</strong> (default): Interactive output with colors, emoji, and formatting</li>
<li><strong>json</strong>: Structured data for programmatic processing</li>
<li><strong>markdown</strong>: Reports suitable for documentation and PR comments</li>
<li><strong>html</strong>: Web-viewable HTML reports with interactive dashboard</li>
</ul>
<h3 id="writing-to-files"><a class="header" href="#writing-to-files">Writing to Files</a></h3>
<p>By default, output goes to stdout. Use <code>-o</code> or <code>--output</code> to write to a file:</p>
<pre><code class="language-bash"># Write JSON to file
debtmap analyze . --format json -o report.json

# Write markdown report
debtmap analyze . --format markdown -o DEBT_REPORT.md

# Terminal output to file (preserves colors)
debtmap analyze . -o analysis.txt
</code></pre>
<h2 id="terminal-output"><a class="header" href="#terminal-output">Terminal Output</a></h2>
<p>The terminal format provides an interactive, color-coded report designed for developer workflows. It’s the default format and optimized for readability.</p>
<h3 id="output-structure"><a class="header" href="#output-structure">Output Structure</a></h3>
<p>Terminal output is organized into five main sections:</p>
<ol>
<li><strong>Header</strong> - Analysis report title</li>
<li><strong>Codebase Summary</strong> - High-level metrics and debt score</li>
<li><strong>Complexity Hotspots</strong> - Top 5 most complex functions with refactoring guidance</li>
<li><strong>Technical Debt</strong> - High-priority debt items requiring attention</li>
<li><strong>Pass/Fail Status</strong> - Overall quality assessment</li>
</ol>
<h3 id="example-terminal-output"><a class="header" href="#example-terminal-output">Example Terminal Output</a></h3>
<pre><code>═══════════════════════════════════════════
           DEBTMAP ANALYSIS REPORT
═══════════════════════════════════════════

📊 CODEBASE Summary
───────────────────────────────────────────
  Files analyzed:      42
  Total functions:     287
  Average complexity:  6.3
  Debt items:          15
  Total debt score:    156 (threshold: 100)

⚠️  COMPLEXITY HOTSPOTS (Top 5)
───────────────────────────────────────────
  1. src/analyzers/rust.rs:245 parse_function() - Cyclomatic: 18, Cognitive: 24
     ACTION: Extract 3-5 pure functions using decompose-then-transform strategy
     PATTERNS: Decompose into logical units, then apply functional patterns
     BENEFIT: Pure functions are easily testable and composable

  2. src/debt/smells.rs:196 detect_data_clumps() - Cyclomatic: 15, Cognitive: 20
     ↓ Entropy: 0.32, Repetition: 85%, Effective: 0.6x
       High pattern repetition detected (85%)

🔧 TECHNICAL DEBT (15 items)
───────────────────────────────────────────
  High Priority (5):
    - src/risk/scoring.rs:142 - TODO: Implement caching for score calculations
    - src/core/metrics.rs:89 - High complexity: cyclomatic=16
    - src/debt/patterns.rs:201 - Code duplication: 65 lines duplicated

✓ Pass/Fail: PASS
</code></pre>
<h3 id="color-coding-and-symbols"><a class="header" href="#color-coding-and-symbols">Color Coding and Symbols</a></h3>
<p>The terminal output uses colors and symbols for quick visual scanning:</p>
<p><strong>Status Indicators:</strong></p>
<ul>
<li>✓ Green: Passing, good, well-tested</li>
<li>⚠️  Yellow: Warning, moderate complexity</li>
<li>✗ Red: Failing, critical, high complexity</li>
<li>📊 Blue: Information, metrics</li>
<li>🔧 Orange: Technical debt items</li>
<li>🎯 Cyan: Recommendations</li>
</ul>
<p><strong>Complexity Classification:</strong></p>
<ul>
<li><strong>LOW</strong> (0-5): Green - Simple, easy to maintain</li>
<li><strong>MODERATE</strong> (6-10): Yellow - Consider refactoring</li>
<li><strong>HIGH</strong> (11-15): Orange - Should refactor</li>
<li><strong>SEVERE</strong> (&gt;15): Red - Urgent refactoring needed</li>
</ul>
<blockquote>
<p><strong>Note:</strong> These levels match the <code>ComplexityLevel</code> enum in the implementation.</p>
</blockquote>
<p><strong>Debt Score Thresholds:</strong></p>
<p>The default debt threshold is <strong>100</strong>. Scores are colored based on this threshold:</p>
<ul>
<li><strong>Green (≤50)</strong>: Healthy - Below half threshold (score ≤ threshold/2)</li>
<li><strong>Yellow (51-100)</strong>: Attention needed - Between half and full threshold (threshold/2 &lt; score ≤ threshold)</li>
<li><strong>Red (&gt;100)</strong>: Action required - Exceeds threshold (score &gt; threshold)</li>
</ul>
<blockquote>
<p><strong>Note:</strong> Boundary values use strict inequalities: 50 is Green, 100 is Yellow (not Red), 101+ is Red.</p>
</blockquote>
<h3 id="refactoring-guidance"><a class="header" href="#refactoring-guidance">Refactoring Guidance</a></h3>
<p>For complex functions (cyclomatic complexity &gt; 5), the terminal output provides actionable refactoring recommendations:</p>
<pre><code>ACTION: Extract 3-5 pure functions using decompose-then-transform strategy
PATTERNS: Decompose into logical units, then apply functional patterns
BENEFIT: Pure functions are easily testable and composable
</code></pre>
<p>Guidance levels:</p>
<ul>
<li><strong>Moderate</strong> (6-10): Extract 2-3 pure functions using direct functional transformation</li>
<li><strong>High</strong> (11-15): Extract 3-5 pure functions using decompose-then-transform strategy</li>
<li><strong>Severe</strong> (&gt;15): Extract 5+ pure functions into modules with functional core/imperative shell</li>
</ul>
<p>See the <a href="#analysis-guide">Analysis Guide</a> for metric explanations.</p>
<h3 id="plain-terminal-mode"><a class="header" href="#plain-terminal-mode">Plain Terminal Mode</a></h3>
<p>For environments without color support or when piping to tools, use <code>--plain</code>:</p>
<pre><code class="language-bash"># ASCII-only output, no colors
debtmap analyze . --plain
</code></pre>
<p>Plain mode:</p>
<ul>
<li>Removes ANSI color codes</li>
<li>Uses ASCII box-drawing characters</li>
<li>Machine-parseable structure</li>
</ul>
<blockquote>
<p><strong>Note:</strong> Terminal output formatting is controlled internally via <code>FormattingConfig</code> (found in <code>src/formatting</code> and <code>src/io/writers/terminal.rs</code>), which manages color mode settings. The <code>--plain</code> flag and environment variables provide user-facing control over these settings:</p>
<ul>
<li><code>--plain</code> flag - Disables colors and fancy formatting</li>
<li><code>NO_COLOR=1</code> - Disables colors (per <a href="https://no-color.org">no-color.org</a> standard)</li>
<li><code>CLICOLOR=0</code> - Disables colors</li>
<li><code>CLICOLOR_FORCE=1</code> - Forces colors even when output is not a terminal</li>
</ul>
<p><code>FormattingConfig</code> is not directly exposed to CLI users but can be accessed when using debtmap as a library through <code>TerminalWriter::with_formatting</code>.</p>
</blockquote>
<h3 id="verbosity-levels"><a class="header" href="#verbosity-levels">Verbosity Levels</a></h3>
<p>Control detail level with <code>-v</code> flags (can be repeated):</p>
<pre><code class="language-bash"># Standard output
debtmap analyze .

# Level 1: Show main score factors
debtmap analyze . -v

# Level 2: Show detailed calculations
debtmap analyze . -vv

# Level 3: Show all debug information
debtmap analyze . -vvv
</code></pre>
<p><strong>Verbosity features:</strong></p>
<ul>
<li><code>-v</code>: Show main score factors (complexity, coverage, dependency breakdown)</li>
<li><code>-vv</code>: Show detailed calculations with formulas and intermediate values</li>
<li><code>-vvv</code>: Show all debug information including entropy metrics and role detection</li>
</ul>
<blockquote>
<p><strong>Note:</strong> Verbosity flags affect terminal output only. JSON and markdown formats include all data regardless of verbosity level.</p>
</blockquote>
<p>Each level includes all information from the previous levels, progressively adding more detail to help understand how scores are calculated.</p>
<p><strong>Example Output Differences:</strong></p>
<p>Standard output shows basic metrics:</p>
<pre><code>Total debt score: 156 (threshold: 100)
</code></pre>
<p>Level 1 (<code>-v</code>) adds score breakdowns:</p>
<pre><code>Total debt score: 156 (threshold: 100)
  Complexity contribution: 85 (54%)
  Coverage gaps: 45 (29%)
  Dependency issues: 26 (17%)
</code></pre>
<p>Level 2 (<code>-vv</code>) adds detailed calculations:</p>
<pre><code>Total debt score: 156 (threshold: 100)
  Complexity contribution: 85 (54%)
    Formula: sum(cyclomatic_weight * severity_multiplier)
    High complexity functions: 5 × 12 = 60
    Medium complexity: 8 × 3 = 24
    Base penalty: 1
  Coverage gaps: 45 (29%)
    Uncovered complex functions: 3 × 15 = 45
</code></pre>
<p>Level 3 (<code>-vvv</code>) adds all internal details:</p>
<pre><code>Total debt score: 156 (threshold: 100)
  ... (all level 2 output) ...
  Debug info:
    Entropy metrics analyzed: 42/50 functions
    Function role detection: BusinessLogic=12, Utility=8, TestHelper=5
    Parse time: 245ms
</code></pre>
<h3 id="understanding-metrics-1"><a class="header" href="#understanding-metrics-1">Understanding Metrics</a></h3>
<p>To get detailed explanations of how metrics are calculated, use the <code>--explain-metrics</code> flag:</p>
<pre><code class="language-bash"># Get explanations of metric definitions and formulas
debtmap analyze . --explain-metrics
</code></pre>
<p>This flag provides:</p>
<ul>
<li><strong>Metric definitions</strong> - Detailed explanations of what each metric measures</li>
<li><strong>Calculation formulas</strong> - How scores are computed from raw data</li>
<li><strong>Measured vs estimated</strong> - Which metrics are exact and which are heuristic-based</li>
<li><strong>Interpretation guidance</strong> - How to understand and act on metric values</li>
</ul>
<p>The explanations appear inline with the analysis output, helping you understand:</p>
<ul>
<li>What cyclomatic and cognitive complexity measure</li>
<li>How debt scores are calculated</li>
<li>What entropy metrics indicate</li>
<li>How risk scores are determined</li>
</ul>
<p>This is particularly useful when:</p>
<ul>
<li>Learning how debtmap evaluates code quality</li>
<li>Understanding why certain functions have high scores</li>
<li>Explaining analysis results to team members</li>
<li>Tuning thresholds based on metric meanings</li>
</ul>
<h3 id="risk-analysis-output"><a class="header" href="#risk-analysis-output">Risk Analysis Output</a></h3>
<p>When coverage data is provided via <code>--lcov</code>, terminal output includes a dedicated risk analysis section:</p>
<pre><code>═══════════════════════════════════════════
           RISK ANALYSIS REPORT
═══════════════════════════════════════════

📈 RISK Summary
───────────────────────────────────────────
Codebase Risk Score: 45.5 (MEDIUM)
Complexity-Coverage Correlation: -0.65

Risk Distribution:
  Critical: 2 functions
  High: 5 functions
  Medium: 10 functions
  Low: 15 functions
  Well Tested: 20 functions

🎯 CRITICAL RISKS
───────────────────────────────────────────
1. src/core/parser.rs:142 parse_complex_ast()
   Risk: 85.0 | Complexity: 15 | Coverage: 0%
   Recommendation: Add 5 unit tests (est: 2-3 hours)
   Impact: -40 risk reduction

💡 RECOMMENDATIONS (by ROI)
───────────────────────────────────────────
1. test_me() - ROI: 5.0x
   Current Risk: 75 | Reduction: 40 | Effort: Moderate
   Rationale: High risk function with low coverage
</code></pre>
<p><strong>Risk Level Classification:</strong></p>
<ul>
<li><strong>LOW</strong> (&lt;30): Green - score &lt; 30.0</li>
<li><strong>MEDIUM</strong> (30-59): Yellow - 30.0 ≤ score &lt; 60.0</li>
<li><strong>HIGH</strong> (≥60): Red - score ≥ 60.0</li>
</ul>
<blockquote>
<p><strong>Note:</strong> 60 is the start of HIGH risk level.</p>
</blockquote>
<h2 id="json-output"><a class="header" href="#json-output">JSON Output</a></h2>
<p>JSON output provides complete analysis results in a machine-readable format, ideal for CI/CD pipelines, custom tooling, and programmatic analysis.</p>
<h3 id="basic-usage-1"><a class="header" href="#basic-usage-1">Basic Usage</a></h3>
<pre><code class="language-bash"># Generate JSON output
debtmap analyze . --format json

# Save to file
debtmap analyze . --format json -o report.json

# Pretty-printed by default for readability
debtmap analyze . --format json | jq .
</code></pre>
<blockquote>
<p><strong>Note:</strong> JSON output is automatically pretty-printed for readability.</p>
</blockquote>
<h3 id="json-schema-structure"><a class="header" href="#json-schema-structure">JSON Schema Structure</a></h3>
<p>Debtmap outputs a structured JSON document with the following top-level fields:</p>
<pre><code class="language-json">{
  "project_path": "/path/to/project",
  "timestamp": "2025-01-09T12:00:00Z",
  "complexity": { ... },
  "technical_debt": { ... },
  "dependencies": { ... },
  "duplications": [ ... ]
}
</code></pre>
<h3 id="full-schema-example"><a class="header" href="#full-schema-example">Full Schema Example</a></h3>
<p>Here’s a complete annotated JSON output example:</p>
<pre><code class="language-json">{
  // Project metadata
  "project_path": "/Users/dev/myproject",
  "timestamp": "2025-01-09T15:30:00Z",

  // Complexity analysis results
  "complexity": {
    "metrics": [
      {
        "name": "calculate_risk_score",
        "file": "src/risk/scoring.rs",
        "line": 142,
        "cyclomatic": 12,
        "cognitive": 18,
        "nesting": 4,
        "length": 85,
        "is_test": false,
        "visibility": "pub",
        "is_trait_method": false,
        "in_test_module": false,
        "entropy_score": {
          "token_entropy": 0.65,
          "pattern_repetition": 0.30,
          "branch_similarity": 0.45,
          "effective_complexity": 0.85
        },
        "is_pure": false,
        "purity_confidence": 0.75,
        "detected_patterns": ["nested_loops", "complex_conditionals"],
        "upstream_callers": ["analyze_codebase", "generate_report"],
        "downstream_callees": ["get_metrics", "apply_weights"]
      }
    ],
    "summary": {
      "total_functions": 287,
      "average_complexity": 6.3,
      "max_complexity": 24,
      "high_complexity_count": 12
    }
  },

  // Technical debt items
  "technical_debt": {
    "items": [
      {
        "id": "debt_001",
        "debt_type": "Complexity",
        "priority": "High",
        "file": "src/analyzers/rust.rs",
        "line": 245,
        "column": 5,
        "message": "High cyclomatic complexity: 18",
        "context": "Function parse_function has excessive branching"
      },
      {
        "id": "debt_002",
        "debt_type": "Todo",
        "priority": "Medium",
        "file": "src/core/cache.rs",
        "line": 89,
        "column": null,
        "message": "TODO: Implement LRU eviction policy",
        "context": null
      }
    ],
    "by_type": {
      "Complexity": [ /* same structure as items */ ],
      "Todo": [ /* ... */ ],
      "Duplication": [ /* ... */ ]
    },
    "priorities": ["Low", "Medium", "High", "Critical"]
  },

  // Dependency analysis
  "dependencies": {
    "modules": [
      {
        "module": "risk::scoring",
        "dependencies": ["core::metrics", "debt::patterns"],
        "dependents": ["commands::analyze", "io::output"]
      }
    ],
    "circular": [
      {
        "cycle": ["module_a", "module_b", "module_c", "module_a"]
      }
    ]
  },

  // Code duplication blocks
  "duplications": [
    {
      "hash": "abc123def456",
      "lines": 15,
      "locations": [
        {
          "file": "src/parser/rust.rs",
          "start_line": 42,
          "end_line": 57
        },
        {
          "file": "src/parser/python.rs",
          "start_line": 89,
          "end_line": 104
        }
      ]
    }
  ]
}
</code></pre>
<h3 id="field-descriptions"><a class="header" href="#field-descriptions">Field Descriptions</a></h3>
<p><strong>FunctionMetrics Fields:</strong></p>
<ul>
<li>
<p><code>name</code>: Function name</p>
</li>
<li>
<p><code>file</code>: Path to source file</p>
</li>
<li>
<p><code>line</code>: Line number where function is defined</p>
</li>
<li>
<p><code>cyclomatic</code>: Cyclomatic complexity score</p>
</li>
<li>
<p><code>cognitive</code>: Cognitive complexity score</p>
</li>
<li>
<p><code>nesting</code>: Maximum nesting depth</p>
</li>
<li>
<p><code>length</code>: Lines of code in function</p>
</li>
<li>
<p><code>is_test</code>: Whether this is a test function</p>
</li>
<li>
<p><code>visibility</code>: Rust visibility modifier (pub, pub(crate), or null)</p>
</li>
<li>
<p><code>is_trait_method</code>: Whether this implements a trait</p>
</li>
<li>
<p><code>in_test_module</code>: Whether inside #[cfg(test)]</p>
</li>
<li>
<p><code>entropy_score</code>: Optional entropy analysis with structure:</p>
<pre><code class="language-json">{
  "token_entropy": 0.65,        // Token distribution entropy (0-1): measures variety of tokens
  "pattern_repetition": 0.30,   // Pattern repetition score (0-1): detects repeated code patterns
  "branch_similarity": 0.45,    // Branch similarity metric (0-1): compares similarity between branches
  "effective_complexity": 0.85  // Adjusted complexity multiplier: complexity adjusted for entropy
}
</code></pre>
<p><strong>EntropyScore Fields:</strong></p>
<ul>
<li><code>token_entropy</code>: Measures the variety and distribution of tokens in the function (0-1, higher = more variety)</li>
<li><code>pattern_repetition</code>: Detects repeated code patterns within the function (0-1, higher = more repetition)</li>
<li><code>branch_similarity</code>: Measures similarity between different code branches (0-1, higher = more similar)</li>
<li><code>effective_complexity</code>: The overall complexity multiplier adjusted for entropy effects</li>
</ul>
</li>
<li>
<p><code>is_pure</code>: Whether function is pure (no side effects)</p>
</li>
<li>
<p><code>purity_confidence</code>: Confidence level (0.0-1.0)</p>
</li>
<li>
<p><code>detected_patterns</code>: List of detected code patterns</p>
</li>
<li>
<p><code>upstream_callers</code>: Functions that call this one</p>
</li>
<li>
<p><code>downstream_callees</code>: Functions this one calls</p>
</li>
</ul>
<p><strong>DebtItem Fields:</strong></p>
<ul>
<li><code>id</code>: Unique identifier</li>
<li><code>debt_type</code>: Type of debt (see DebtType enum below)</li>
<li><code>priority</code>: Priority level (Low, Medium, High, Critical)</li>
<li><code>file</code>: Path to file containing debt</li>
<li><code>line</code>: Line number</li>
<li><code>column</code>: Optional column number</li>
<li><code>message</code>: Human-readable description</li>
<li><code>context</code>: Optional additional context</li>
</ul>
<p><strong>DebtType Enum:</strong></p>
<ul>
<li><code>Todo</code>: TODO markers</li>
<li><code>Fixme</code>: FIXME markers</li>
<li><code>CodeSmell</code>: Code smell patterns</li>
<li><code>Duplication</code>: Duplicated code</li>
<li><code>Complexity</code>: Excessive complexity</li>
<li><code>Dependency</code>: Dependency issues</li>
<li><code>ErrorSwallowing</code>: Suppressed errors</li>
<li><code>ResourceManagement</code>: Resource management issues</li>
<li><code>CodeOrganization</code>: Organizational problems</li>
<li><code>TestComplexity</code>: Complex test code</li>
<li><code>TestTodo</code>: TODOs in tests</li>
<li><code>TestDuplication</code>: Duplicated test code</li>
<li><code>TestQuality</code>: Test quality issues</li>
</ul>
<h3 id="risk-insights-json"><a class="header" href="#risk-insights-json">Risk Insights JSON</a></h3>
<p>When coverage data is provided via <code>--lcov</code>, risk insights are included as part of the analysis output. The <code>write_risk_insights</code> method (found in <code>src/io/writers/json.rs</code>, <code>terminal.rs</code>, and <code>markdown/core.rs</code>) outputs risk analysis data in the following JSON structure:</p>
<pre><code class="language-json">{
  "items": [
    {
      "location": {
        "file": "src/risk/scoring.rs",
        "function": "calculate_priority",
        "line": 66
      },
      "debt_type": "TestGap",
      "unified_score": {
        "complexity_factor": 3.2,
        "coverage_factor": 10.0,
        "dependency_factor": 2.5,
        "role_multiplier": 1.2,
        "final_score": 9.4
      },
      "function_role": "BusinessLogic",
      "recommendation": {
        "action": "Add unit tests",
        "details": "Add 6 unit tests for full coverage",
        "effort_estimate": "2-3 hours"
      },
      "expected_impact": {
        "risk_reduction": 3.9,
        "complexity_reduction": 0,
        "coverage_improvement": 100
      },
      "upstream_dependencies": 0,
      "downstream_dependencies": 3,
      "nesting_depth": 1,
      "function_length": 13
    }
  ],
  "call_graph": {
    "total_functions": 1523,
    "entry_points": 12,
    "test_functions": 456,
    "max_depth": 8
  },
  "overall_coverage": 82.3,
  "total_impact": {
    "risk_reduction": 45.2,
    "complexity_reduction": 12.3,
    "coverage_improvement": 18.5
  }
}
</code></pre>
<h2 id="markdown-output"><a class="header" href="#markdown-output">Markdown Output</a></h2>
<p>Markdown format generates documentation-friendly reports suitable for README files, PR comments, and technical documentation.</p>
<h3 id="basic-usage-1-1"><a class="header" href="#basic-usage-1-1">Basic Usage</a></h3>
<pre><code class="language-bash"># Generate markdown report
debtmap analyze . --format markdown

# Save to documentation
debtmap analyze . --format markdown -o docs/DEBT_REPORT.md
</code></pre>
<h3 id="markdown-structure"><a class="header" href="#markdown-structure">Markdown Structure</a></h3>
<p>Markdown output includes:</p>
<ol>
<li><strong>Executive Summary</strong> - High-level metrics and health dashboard</li>
<li><strong>Complexity Analysis</strong> - Detailed complexity breakdown by file</li>
<li><strong>Technical Debt</strong> - Categorized debt items with priorities</li>
<li><strong>Dependencies</strong> - Module dependencies and circular references</li>
<li><strong>Recommendations</strong> - Prioritized action items</li>
</ol>
<h3 id="example-markdown-output"><a class="header" href="#example-markdown-output">Example Markdown Output</a></h3>
<pre><code class="language-markdown"># Debtmap Analysis Report

**Generated:** 2025-01-09 15:30:00 UTC
**Project:** /Users/dev/myproject

## Executive Summary

- **Files Analyzed:** 42
- **Total Functions:** 287
- **Average Complexity:** 6.3
- **Total Debt Items:** 15
- **Debt Score:** 156/100 ⚠️

### Health Dashboard

| Metric | Value | Status |
|--------|-------|--------|
| Complexity | 6.3 avg | ✅ Good |
| Debt Score | 156 | ⚠️ Attention |
| High Priority Items | 5 | ⚠️ Action Needed |

## Complexity Analysis

### Top 5 Complex Functions

| Function | File | Cyclomatic | Cognitive | Priority |
|----------|------|-----------|-----------|----------|
| parse_function | src/analyzers/rust.rs:245 | 18 | 24 | High |
| detect_data_clumps | src/debt/smells.rs:196 | 15 | 20 | Medium |
| analyze_dependencies | src/core/deps.rs:89 | 14 | 18 | Medium |

### Refactoring Recommendations

**src/analyzers/rust.rs:245** - `parse_function()`
- **Complexity:** Cyclomatic: 18, Cognitive: 24
- **Action:** Extract 3-5 pure functions using decompose-then-transform strategy
- **Patterns:** Decompose into logical units, then apply functional patterns
- **Benefit:** Improved testability and maintainability

## Technical Debt

### High Priority (5 items)

- **src/risk/scoring.rs:142** - TODO: Implement caching for score calculations
- **src/core/metrics.rs:89** - High complexity: cyclomatic=16
- **src/debt/patterns.rs:201** - Code duplication: 65 lines duplicated

### Medium Priority (8 items)

...

## Dependencies

### Circular Dependencies

- `risk::scoring` → `core::metrics` → `risk::scoring`

## Recommendations

1. **Refactor parse_function** (High Priority)
   - Reduce complexity from 18 to &lt;10
   - Extract helper functions
   - Estimated effort: 4-6 hours

2. **Add tests for scoring module** (High Priority)
   - Current coverage: 35%
   - Target coverage: 80%
   - Estimated effort: 2-3 hours

## Data Flow Analysis

When verbosity is enabled (`-v` or higher), detailed data flow analysis is included in markdown reports for top priority items. This section provides deep insights into mutations, I/O operations, and escape analysis.

### Mutation Analysis

Shows variable mutation patterns within functions:

**Example:**

```markdown
**Data Flow Analysis**

- Mutations: 10 total, 2 live, 2 dead stores
  - **Opportunity**: Remove 2 dead store(s) to simplify code
  - **Almost Pure**: Only 2 live mutation(s), consider extracting pure subset
</code></pre>
<p><strong>Mutation Metrics:</strong></p>
<ul>
<li><strong>Total Mutations:</strong> Count of all variable assignments and mutations</li>
<li><strong>Live Mutations:</strong> Mutations where the new value is actually used</li>
<li><strong>Dead Stores:</strong> Assignments that are never read (can be removed)</li>
</ul>
<p><strong>Refactoring Opportunities:</strong></p>
<ul>
<li>Functions with many dead stores can be simplified by removing unused assignments</li>
<li>Functions with few live mutations relative to total mutations are “almost pure” and may benefit from extracting pure subsets</li>
</ul>
<h3 id="io-operations"><a class="header" href="#io-operations">I/O Operations</a></h3>
<p>Detects and lists input/output operations within functions:</p>
<p><strong>Example:</strong></p>
<pre><code class="language-markdown">- I/O Operations: 3 detected
  - File Read at line 105
  - Network Call at line 110
  - Database Query at line 120
  - **Recommendation**: Consider isolating I/O in separate functions
</code></pre>
<p><strong>Detected I/O Types:</strong></p>
<ul>
<li>File system operations (read, write, open, close)</li>
<li>Network operations (HTTP, TCP, UDP)</li>
<li>Database queries and updates</li>
<li>Standard I/O (print, input)</li>
<li>System calls</li>
</ul>
<p><strong>Best Practices:</strong></p>
<ul>
<li>Isolate I/O operations in dedicated functions</li>
<li>Keep business logic pure and separate from I/O</li>
<li>Easier testing when I/O is at function boundaries</li>
</ul>
<h3 id="escape-analysis"><a class="header" href="#escape-analysis">Escape Analysis</a></h3>
<p>Shows which variables escape the function scope or affect the return value:</p>
<p><strong>Example:</strong></p>
<pre><code class="language-markdown">- Escape Analysis: 2 variables escape
  - Return dependencies: result, accumulator
  - **Insight**: These variables directly affect function output
</code></pre>
<p><strong>Escape Metrics:</strong></p>
<ul>
<li><strong>Escaping Variables:</strong> Variables whose values leave the function scope</li>
<li><strong>Return Dependencies:</strong> Variables that contribute to the return value</li>
</ul>
<p><strong>Implications:</strong></p>
<ul>
<li>Functions with many escaping variables have complex data flow</li>
<li>Clear return dependencies indicate focused, single-purpose functions</li>
<li>Excessive escaping may indicate the function is doing too much</li>
</ul>
<h3 id="purity-analysis"><a class="header" href="#purity-analysis">Purity Analysis</a></h3>
<p>Indicates whether a function is pure and reasons for any impurity:</p>
<p><strong>Example:</strong></p>
<pre><code class="language-markdown">- Purity: Not pure (95% confidence)
  - Reasons: Mutates shared state, Performs I/O operations
  - **Benefit**: Converting to pure functions improves testability
</code></pre>
<p><strong>Purity Assessment:</strong></p>
<ul>
<li><strong>Is Pure:</strong> Boolean indicating if the function is pure (deterministic, no side effects)</li>
<li><strong>Confidence Level:</strong> Percentage confidence in the assessment (0-100%)</li>
<li><strong>Impurity Reasons:</strong> Specific reasons why the function is not pure</li>
</ul>
<p><strong>Common Impurity Reasons:</strong></p>
<ul>
<li>Mutates shared or global state</li>
<li>Performs I/O operations</li>
<li>Calls other impure functions</li>
<li>Uses random number generation</li>
<li>Depends on system time or external state</li>
</ul>
<p><strong>Value of Pure Functions:</strong></p>
<ul>
<li>Easier to test (no setup/teardown needed)</li>
<li>Easier to reason about (same input always gives same output)</li>
<li>Safe to parallelize and memoize</li>
<li>More composable and reusable</li>
</ul>
<h3 id="enabling-data-flow-analysis"><a class="header" href="#enabling-data-flow-analysis">Enabling Data Flow Analysis</a></h3>
<p>Data flow analysis appears in markdown output when using verbose mode:</p>
<pre><code class="language-bash"># Include data flow analysis in markdown reports
debtmap analyze . --format markdown -v

# Higher verbosity includes more details
debtmap analyze . --format markdown -vv
</code></pre>
<p>The data flow section appears in the score breakdown for each of the top 3 priority items, providing actionable insights for refactoring.</p>
<pre><code>
### CLI vs Library Markdown Features

**CLI Markdown Output (`--format markdown`):**

When you use `debtmap analyze . --format markdown`, you get comprehensive reports that include:

- Executive summary with health dashboard
- Complexity analysis with refactoring recommendations
- Technical debt categorization by priority
- Dependency analysis with circular reference detection
- Actionable recommendations

This uses the base `MarkdownWriter` implementation and provides everything needed for documentation and PR comments.

**Enhanced Library Features:**

If you're using debtmap as a Rust library in your own tools, additional markdown capabilities are available:

- **`EnhancedMarkdownWriter` trait** (`src/io/writers/markdown/enhanced.rs`) - Provides advanced formatting and analysis features
- **Enhanced markdown modules** (`src/io/writers/enhanced_markdown/`) - Building blocks for custom visualizations including:
  - Priority-based debt rankings with unified scoring
  - Dead code detection and reporting
  - Call graph insights and dependency visualization
  - Testing recommendations with ROI analysis

To use enhanced features in your Rust code:

```rust
use debtmap::io::writers::markdown::enhanced::EnhancedMarkdownWriter;
use debtmap::io::writers::enhanced_markdown::*;

// Create custom reports with enhanced features
let mut writer = create_enhanced_writer(output)?;
writer.write_priority_rankings(&amp;analysis)?;
writer.write_dead_code_analysis(&amp;call_graph)?;
</code></pre>
<blockquote>
<p><strong>Note:</strong> Enhanced markdown features are only available through the library API, not via the CLI. The CLI <code>--format markdown</code> output is comprehensive for most use cases.</p>
</blockquote>
<h3 id="rendering-to-htmlpdf"><a class="header" href="#rendering-to-htmlpdf">Rendering to HTML/PDF</a></h3>
<p>Markdown reports can be converted to other formats:</p>
<pre><code class="language-bash"># Generate markdown
debtmap analyze . --format markdown -o report.md

# Convert to HTML with pandoc
pandoc report.md -o report.html --standalone --css style.css

# Convert to PDF
pandoc report.md -o report.pdf --pdf-engine=xelatex
</code></pre>
<h2 id="html-output"><a class="header" href="#html-output">HTML Output</a></h2>
<p>HTML format generates an interactive web dashboard with visual metrics and navigation. This format is ideal for viewing analysis results in a browser and sharing reports with stakeholders.</p>
<p><strong>Source:</strong> <code>src/io/writers/html.rs</code>, <code>src/cli.rs:492</code></p>
<h3 id="basic-usage-2"><a class="header" href="#basic-usage-2">Basic Usage</a></h3>
<pre><code class="language-bash"># Generate HTML dashboard
debtmap analyze . --format html

# Save to file
debtmap analyze . --format html -o dashboard.html

# Open in browser
debtmap analyze . --format html -o dashboard.html &amp;&amp; open dashboard.html
</code></pre>
<h3 id="dashboard-features"><a class="header" href="#dashboard-features">Dashboard Features</a></h3>
<p>The HTML output provides an interactive dashboard with:</p>
<ol>
<li><strong>Executive Summary</strong> - High-level metrics with visual indicators</li>
<li><strong>Debt Score Dashboard</strong> - Priority distribution (Critical, High, Medium, Low)</li>
<li><strong>Complexity Metrics</strong> - Average complexity and function counts</li>
<li><strong>Debt Density</strong> - Technical debt per function ratio</li>
<li><strong>Interactive Data</strong> - Full analysis results embedded as JSON</li>
</ol>
<h3 id="dashboard-structure"><a class="header" href="#dashboard-structure">Dashboard Structure</a></h3>
<p>The HTML dashboard uses an embedded template (<code>src/io/writers/templates/dashboard.html</code>) that includes:</p>
<ul>
<li><strong>Metrics Cards</strong> - Visual representation of key metrics</li>
<li><strong>Priority Breakdown</strong> - Count of items by priority level</li>
<li><strong>Health Indicators</strong> - Color-coded status based on thresholds</li>
<li><strong>Raw Data Access</strong> - Complete JSON analysis results in the page</li>
</ul>
<h3 id="example-output-1"><a class="header" href="#example-output-1">Example Output</a></h3>
<p>When you generate an HTML dashboard, it displays:</p>
<pre><code>┌─────────────────────────────────────┐
│     Debtmap Analysis Dashboard      │
├─────────────────────────────────────┤
│                                     │
│  Total Items: 156                   │
│  Critical: 5  High: 12              │
│  Medium: 45   Low: 94               │
│                                     │
│  Total Functions: 287               │
│  Avg Complexity: 6.3                │
│  Debt Density: 0.54                 │
│                                     │
└─────────────────────────────────────┘
</code></pre>
<h3 id="when-to-use-html-format"><a class="header" href="#when-to-use-html-format">When to Use HTML Format</a></h3>
<p><strong>Use HTML Format When:</strong></p>
<ul>
<li>Sharing reports with non-technical stakeholders</li>
<li>Creating dashboards for team visibility</li>
<li>Generating reports for management review</li>
<li>Viewing analysis results in a browser</li>
<li>Embedding reports in internal documentation sites</li>
<li>Publishing analysis results to a web server</li>
</ul>
<p><strong>HTML vs Markdown:</strong></p>
<ul>
<li><strong>HTML</strong>: Interactive, visual dashboard with embedded data</li>
<li><strong>Markdown</strong>: Text-based, suitable for conversion to multiple formats</li>
<li><strong>HTML</strong>: Better for standalone viewing in browsers</li>
<li><strong>Markdown</strong>: Better for version control and text processing</li>
</ul>
<h3 id="customization"><a class="header" href="#customization">Customization</a></h3>
<p>The HTML output uses a built-in template. For custom styling, you can:</p>
<ol>
<li>Generate the HTML file</li>
<li>Extract and modify the CSS within the file</li>
<li>Serve with custom stylesheets</li>
</ol>
<blockquote>
<p><strong>Note:</strong> Direct template customization requires modifying <code>src/io/writers/templates/dashboard.html</code> in the debtmap source code.</p>
</blockquote>
<h2 id="tool-integration"><a class="header" href="#tool-integration">Tool Integration</a></h2>
<h3 id="cicd-pipelines"><a class="header" href="#cicd-pipelines">CI/CD Pipelines</a></h3>
<p>Debtmap JSON output integrates seamlessly with CI/CD systems.</p>
<h4 id="github-actions"><a class="header" href="#github-actions">GitHub Actions</a></h4>
<pre><code class="language-yaml">name: Code Quality

on: [pull_request]

jobs:
  analyze:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Install debtmap
        run: cargo install debtmap

      - name: Run analysis
        run: |
          debtmap analyze . \
            --format json \
            --output analysis.json \
            --lcov coverage/lcov.info

      - name: Check thresholds
        run: |
          DEBT_SCORE=$(jq '.technical_debt.items | length' analysis.json)
          if [ "$DEBT_SCORE" -gt 100 ]; then
            echo "❌ Debt score too high: $DEBT_SCORE"
            exit 1
          fi

      - name: Comment on PR
        uses: actions/github-script@v6
        with:
          script: |
            const fs = require('fs');
            const analysis = JSON.parse(fs.readFileSync('analysis.json'));
            const summary = `## Debtmap Analysis

            - **Debt Items:** ${analysis.technical_debt.items.length}
            - **Average Complexity:** ${analysis.complexity.summary.average_complexity}
            - **High Complexity Functions:** ${analysis.complexity.summary.high_complexity_count}
            `;
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: summary
            });
</code></pre>
<h4 id="gitlab-ci"><a class="header" href="#gitlab-ci">GitLab CI</a></h4>
<pre><code class="language-yaml">code_quality:
  stage: test
  script:
    - cargo install debtmap
    - debtmap analyze . --format json --output gl-code-quality.json
    - |
      DEBT=$(jq '.technical_debt.items | length' gl-code-quality.json)
      if [ "$DEBT" -gt 50 ]; then
        echo "Debt threshold exceeded"
        exit 1
      fi
  artifacts:
    reports:
      codequality: gl-code-quality.json
</code></pre>
<h4 id="jenkins-pipeline"><a class="header" href="#jenkins-pipeline">Jenkins Pipeline</a></h4>
<pre><code class="language-groovy">pipeline {
    agent any

    stages {
        stage('Analyze') {
            steps {
                sh 'debtmap analyze . --format json -o report.json'

                script {
                    def json = readJSON file: 'report.json'
                    def debtScore = json.technical_debt.items.size()

                    if (debtScore &gt; 100) {
                        error("Debt score ${debtScore} exceeds threshold")
                    }
                }
            }
        }
    }

    post {
        always {
            archiveArtifacts artifacts: 'report.json'
        }
    }
}
</code></pre>
<h3 id="querying-json-with-jq"><a class="header" href="#querying-json-with-jq">Querying JSON with jq</a></h3>
<p>Common jq queries for analyzing debtmap output:</p>
<pre><code class="language-bash"># Get total debt items
jq '.technical_debt.items | length' report.json

# Get high-priority items only
jq '.technical_debt.items[] | select(.priority == "High")' report.json

# Get functions with complexity &gt; 10
jq '.complexity.metrics[] | select(.cyclomatic &gt; 10)' report.json

# Calculate average complexity
jq '.complexity.summary.average_complexity' report.json

# Get all TODO items
jq '.technical_debt.items[] | select(.debt_type == "Todo")' report.json

# Get top 5 complex functions
jq '.complexity.metrics | sort_by(-.cyclomatic) | .[0:5] | .[] | {name, file, cyclomatic}' report.json

# Get files with circular dependencies
jq '.dependencies.circular[] | .cycle' report.json

# Count debt items by type
jq '.technical_debt.items | group_by(.debt_type) | map({type: .[0].debt_type, count: length})' report.json

# Get functions with 0% coverage (when using --lcov)
jq '.complexity.metrics[] | select(.coverage == 0)' report.json

# Extract file paths with high debt
jq '.technical_debt.items[] | select(.priority == "High" or .priority == "Critical") | .file' report.json | sort -u
</code></pre>
<h3 id="filtering-and-transformation-examples"><a class="header" href="#filtering-and-transformation-examples">Filtering and Transformation Examples</a></h3>
<h4 id="python-script-to-parse-json"><a class="header" href="#python-script-to-parse-json">Python Script to Parse JSON</a></h4>
<pre><code class="language-python">#!/usr/bin/env python3
import json
import sys

def analyze_debtmap_output(json_file):
    with open(json_file) as f:
        data = json.load(f)

    # Get high-priority items
    high_priority = [
        item for item in data['technical_debt']['items']
        if item['priority'] in ['High', 'Critical']
    ]

    # Group by file
    by_file = {}
    for item in high_priority:
        file = item['file']
        if file not in by_file:
            by_file[file] = []
        by_file[file].append(item)

    # Print summary
    print(f"High-priority debt items: {len(high_priority)}")
    print(f"Files affected: {len(by_file)}")
    print("\nBy file:")
    for file, items in sorted(by_file.items(), key=lambda x: -len(x[1])):
        print(f"  {file}: {len(items)} items")

    return by_file

if __name__ == '__main__':
    analyze_debtmap_output(sys.argv[1])
</code></pre>
<h4 id="shell-script-for-threshold-checking"><a class="header" href="#shell-script-for-threshold-checking">Shell Script for Threshold Checking</a></h4>
<pre><code class="language-bash">#!/bin/bash
set -e

REPORT="$1"
DEBT_THRESHOLD=100
COMPLEXITY_THRESHOLD=10

# Check debt score
DEBT_SCORE=$(jq '.technical_debt.items | length' "$REPORT")
if [ "$DEBT_SCORE" -gt "$DEBT_THRESHOLD" ]; then
    echo "❌ Debt score $DEBT_SCORE exceeds threshold $DEBT_THRESHOLD"
    exit 1
fi

# Check average complexity
AVG_COMPLEXITY=$(jq '.complexity.summary.average_complexity' "$REPORT")
if (( $(echo "$AVG_COMPLEXITY &gt; $COMPLEXITY_THRESHOLD" | bc -l) )); then
    echo "❌ Average complexity $AVG_COMPLEXITY exceeds threshold $COMPLEXITY_THRESHOLD"
    exit 1
fi

echo "✅ All quality checks passed"
echo "   Debt score: $DEBT_SCORE/$DEBT_THRESHOLD"
echo "   Avg complexity: $AVG_COMPLEXITY"
</code></pre>
<h3 id="editor-integration"><a class="header" href="#editor-integration">Editor Integration</a></h3>
<h4 id="vs-code-tasks"><a class="header" href="#vs-code-tasks">VS Code Tasks</a></h4>
<p>Create <code>.vscode/tasks.json</code>:</p>
<pre><code class="language-json">{
  "version": "2.0.0",
  "tasks": [
    {
      "label": "Debtmap: Analyze",
      "type": "shell",
      "command": "debtmap",
      "args": [
        "analyze",
        ".",
        "--format",
        "terminal"
      ],
      "problemMatcher": [],
      "presentation": {
        "reveal": "always",
        "panel": "new"
      }
    },
    {
      "label": "Debtmap: Generate Report",
      "type": "shell",
      "command": "debtmap",
      "args": [
        "analyze",
        ".",
        "--format",
        "markdown",
        "-o",
        "DEBT_REPORT.md"
      ],
      "problemMatcher": []
    }
  ]
}
</code></pre>
<h4 id="problem-matcher-for-vs-code"><a class="header" href="#problem-matcher-for-vs-code">Problem Matcher for VS Code</a></h4>
<p>Parse debtmap output in VS Code’s Problems panel:</p>
<pre><code class="language-json">{
  "problemMatcher": {
    "owner": "debtmap",
    "fileLocation": "absolute",
    "pattern": {
      "regexp": "^(.+?):(\\d+):(\\d+)?\\s*-\\s*(.+)$",
      "file": 1,
      "line": 2,
      "column": 3,
      "message": 4
    }
  }
}
</code></pre>
<h3 id="webhook-integration"><a class="header" href="#webhook-integration">Webhook Integration</a></h3>
<p>Send debtmap results to webhooks for notifications:</p>
<pre><code class="language-bash">#!/bin/bash

# Run analysis
debtmap analyze . --format json -o report.json

# Send to Slack
DEBT_SCORE=$(jq '.technical_debt.items | length' report.json)
curl -X POST "$SLACK_WEBHOOK_URL" \
  -H 'Content-Type: application/json' \
  -d "{\"text\": \"Debtmap Analysis Complete\n• Debt Score: $DEBT_SCORE\n• High Priority: $(jq '[.technical_debt.items[] | select(.priority == "High")] | length' report.json)\"}"

# Send to custom webhook
curl -X POST "$CUSTOM_WEBHOOK_URL" \
  -H 'Content-Type: application/json' \
  -d @report.json
</code></pre>
<h2 id="output-filtering"><a class="header" href="#output-filtering">Output Filtering</a></h2>
<p>Debtmap provides several flags to filter and limit output:</p>
<blockquote>
<p><strong>Note:</strong> Filtering options (<code>--top</code>, <code>--tail</code>, <code>--summary</code>, <code>--filter</code>) apply to all output formats (terminal, JSON, and markdown). The filtered data is applied at the analysis level before formatting, ensuring consistent results across all output types.</p>
</blockquote>
<h3 id="limiting-results"><a class="header" href="#limiting-results">Limiting Results</a></h3>
<pre><code class="language-bash"># Show only top 10 priority items
debtmap analyze . --top 10

# Show bottom 5 lowest priority items
debtmap analyze . --tail 5
</code></pre>
<h3 id="priority-filtering"><a class="header" href="#priority-filtering">Priority Filtering</a></h3>
<pre><code class="language-bash"># Show only high and critical priority items
debtmap analyze . --min-priority high

# Filter by specific debt categories
debtmap analyze . --filter Architecture,Testing
</code></pre>
<p>Available categories:</p>
<ul>
<li><code>Architecture</code>: God objects, complexity hotspots, dead code</li>
<li><code>Testing</code>: Testing gaps, coverage issues</li>
<li><code>Performance</code>: Resource leaks, inefficient patterns</li>
<li><code>CodeQuality</code>: Code smells, maintainability</li>
</ul>
<h3 id="grouping-output"><a class="header" href="#grouping-output">Grouping Output</a></h3>
<pre><code class="language-bash"># Group results by debt category
debtmap analyze . --group-by-category

# Combine filters for focused analysis
debtmap analyze . --filter Architecture --min-priority high --top 5
</code></pre>
<h3 id="summary-mode"><a class="header" href="#summary-mode">Summary Mode</a></h3>
<pre><code class="language-bash"># Compact tiered priority display
debtmap analyze . --summary

# Combines well with filtering
debtmap analyze . --summary --min-priority medium
</code></pre>
<h2 id="best-practices-4"><a class="header" href="#best-practices-4">Best Practices</a></h2>
<h3 id="when-to-use-each-format"><a class="header" href="#when-to-use-each-format">When to Use Each Format</a></h3>
<p><strong>Use Terminal Format When:</strong></p>
<ul>
<li>Developing locally and reviewing code</li>
<li>Getting quick feedback on changes</li>
<li>Presenting results to team members</li>
<li>Exploring complexity hotspots interactively</li>
</ul>
<p><strong>Use JSON Format When:</strong></p>
<ul>
<li>Integrating with CI/CD pipelines</li>
<li>Building custom analysis tools</li>
<li>Tracking metrics over time</li>
<li>Programmatically processing results</li>
<li>Feeding into dashboards or monitoring systems</li>
</ul>
<p><strong>Use Markdown Format When:</strong></p>
<ul>
<li>Generating documentation</li>
<li>Creating PR comments</li>
<li>Sharing reports with stakeholders</li>
<li>Archiving analysis results</li>
<li>Producing executive summaries</li>
</ul>
<p><strong>Use HTML Format When:</strong></p>
<ul>
<li>Viewing analysis in a web browser</li>
<li>Sharing visual dashboards with stakeholders</li>
<li>Publishing reports to internal documentation sites</li>
<li>Creating interactive reports for management review</li>
<li>Embedding analysis results in web applications</li>
</ul>
<h3 id="quick-reference-table"><a class="header" href="#quick-reference-table">Quick Reference Table</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Format</th><th>Best For</th><th>Machine Readable</th><th>Human Readable</th><th>File Extension</th></tr>
</thead>
<tbody>
<tr><td>Terminal</td><td>Development</td><td>No</td><td>Yes</td><td>.txt</td></tr>
<tr><td>JSON</td><td>Automation</td><td>Yes</td><td>No</td><td>.json</td></tr>
<tr><td>Markdown</td><td>Documentation</td><td>Partially</td><td>Yes</td><td>.md</td></tr>
<tr><td>HTML</td><td>Visualization</td><td>Partially</td><td>Yes</td><td>.html</td></tr>
</tbody>
</table>
</div>
<h3 id="combining-formats"><a class="header" href="#combining-formats">Combining Formats</a></h3>
<p>Use multiple formats for comprehensive workflows:</p>
<pre><code class="language-bash"># Generate terminal output for review
debtmap analyze .

# Generate JSON for automation
debtmap analyze . --format json -o ci-report.json

# Generate markdown for documentation
debtmap analyze . --format markdown -o docs/DEBT.md
</code></pre>
<h3 id="performance-considerations"><a class="header" href="#performance-considerations">Performance Considerations</a></h3>
<ul>
<li><strong>Terminal format</strong>: Fastest, minimal overhead</li>
<li><strong>JSON format</strong>: Fast serialization, efficient for large codebases</li>
<li><strong>Markdown format</strong>: Slightly slower due to formatting, but still performant</li>
</ul>
<p>For very large codebases (&gt;10,000 files), use <code>--top</code> or <code>--filter</code> to limit output size.</p>
<h2 id="troubleshooting-6"><a class="header" href="#troubleshooting-6">Troubleshooting</a></h2>
<h3 id="common-issues"><a class="header" href="#common-issues">Common Issues</a></h3>
<p><strong>Colors not showing in terminal:</strong></p>
<ul>
<li>Check if terminal supports ANSI colors</li>
<li>Use <code>--plain</code> flag for ASCII-only output</li>
<li>Some CI systems may not support color codes</li>
</ul>
<p><strong>JSON parsing errors:</strong></p>
<ul>
<li>Ensure output is complete (check for errors during analysis)</li>
<li>Validate JSON with <code>jq</code> or online validators</li>
<li>Check for special characters in file paths</li>
</ul>
<p><strong>Markdown rendering issues:</strong></p>
<ul>
<li>Some markdown renderers don’t support all features</li>
<li>Use standard markdown for maximum compatibility</li>
<li>Test with pandoc or GitHub/GitLab preview</li>
</ul>
<p><strong>File encoding problems:</strong></p>
<ul>
<li>Ensure UTF-8 encoding for all output files</li>
<li>Use <code>--plain</code> for pure ASCII output</li>
<li>Check locale settings (LC_ALL, LANG environment variables)</li>
</ul>
<h3 id="exit-codes"><a class="header" href="#exit-codes">Exit Codes</a></h3>
<p>Current behavior (as verified in <code>src/main.rs</code>):</p>
<ul>
<li><code>0</code>: Successful analysis completed without errors</li>
<li>Non-zero: Error during analysis (invalid path, parsing error, etc.)</li>
</ul>
<blockquote>
<p><strong>Note:</strong> Threshold-based exit codes (where analysis succeeds but fails quality gates) are not currently implemented. The <code>analyze</code> command returns 0 on successful analysis regardless of debt scores or complexity thresholds.</p>
</blockquote>
<p>To enforce quality gates based on thresholds, use the <code>validate</code> command or parse JSON output:</p>
<pre><code class="language-bash"># Use validate command for threshold enforcement
debtmap validate . --config debtmap.toml

# Or parse JSON output for threshold checking
debtmap analyze . --format json -o report.json
DEBT_SCORE=$(jq '.technical_debt.items | length' report.json)
if [ "$DEBT_SCORE" -gt 100 ]; then
    echo "Debt threshold exceeded"
    exit 1
fi
</code></pre>
<h2 id="see-also-2"><a class="header" href="#see-also-2">See Also</a></h2>
<ul>
<li><a href="#getting-started-1">Getting Started</a> - Basic usage and examples</li>
<li><a href="#analysis-guide">Analysis Guide</a> - Understanding metrics and scores</li>
<li><a href="#configuration-2">Configuration</a> - Customizing analysis behavior</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="architecture"><a class="header" href="#architecture">Architecture</a></h1>
<p>This chapter explains how debtmap’s analysis pipeline works, from discovering files to producing prioritized technical debt signals.</p>
<h2 id="analysis-pipeline-overview"><a class="header" href="#analysis-pipeline-overview">Analysis Pipeline Overview</a></h2>
<p>Debtmap’s analysis follows a multi-stage pipeline that transforms source code into structured signals:</p>
<pre><code>┌─────────────────┐
│ File Discovery  │
└────────┬────────┘
         │
         ▼
┌─────────────────┐
│Language Detection│
└────────┬────────┘
         │
         ▼
    ┌────────┐
    │ Parser │
    └────┬───┘
         │
    ┌────┼────────────┐
    │    │            │
    ▼    ▼            ▼
┌─────┐ ┌──────────┐ ┌───────────┐
│ syn │ │rustpython│ │tree-sitter│
│ AST │ │   AST    │ │    AST    │
└──┬──┘ └────┬─────┘ └─────┬─────┘
   │         │             │
   └─────────┼─────────────┘
             │
             ▼
  ┌──────────────────┐
  │ Metric Extraction │
  └─────────┬────────┘
            │
    ┌───────┼───────┐
    │       │       │
    ▼       ▼       ▼
┌────────┐ ┌─────┐ ┌─────────┐
│Complexity│ │Call │ │ Pattern │
│  Calc   │ │Graph│ │Detection│
└────┬───┘ └──┬──┘ └────┬────┘
     │        │         │
     ▼        │         │
┌─────────┐   │         │
│ Entropy │   │         │
│ Analysis│   │         │
└────┬────┘   │         │
     │        │         │
     ▼        ▼         ▼
┌─────────┐ ┌────────┐ ┌──────┐    ┌──────────┐
│Effective│ │Dependency│ │ Debt │    │   LCOV   │
│Complexity│ │Analysis│ │Class │    │ Coverage │
└────┬────┘ └────┬───┘ └──┬───┘    └────┬─────┘
     │           │        │             │
     └───────────┼────────┼─────────────┘
                 │        │
                 ▼        ▼
           ┌─────────────────┐
           │  Risk Scoring   │
           └────────┬────────┘
                    │
                    ▼
         ┌───────────────────┐
         │Tiered Prioritization│
         └─────────┬─────────┘
                   │
                   ▼
        ┌──────────────────────┐
        │ Context Suggestion   │
        │    Generation        │
        └─────────┬────────────┘
                  │
                  ▼
          ┌────────────────┐
          │Output Formatting│
          └────────┬───────┘
                   │
       ┌───────────┼───────────┐
       │           │           │
       ▼           ▼           ▼
   ┌──────┐   ┌────────┐   ┌─────┐
   │ JSON │   │LLM-MD  │   │Term │
   └──────┘   └────────┘   └─────┘
</code></pre>
<h2 id="key-components"><a class="header" href="#key-components">Key Components</a></h2>
<h3 id="1-file-discovery-and-language-detection"><a class="header" href="#1-file-discovery-and-language-detection">1. File Discovery and Language Detection</a></h3>
<p><strong>Purpose:</strong> Identify source files to analyze and determine their language.</p>
<p><strong>How it works:</strong></p>
<ul>
<li>Walks the project directory tree (respecting <code>.gitignore</code> and <code>.debtmapignore</code>)</li>
<li>Detects language based on file extension (<code>.rs</code>, <code>.py</code>, <code>.js</code>, <code>.ts</code>)</li>
<li>Filters out test files, build artifacts, and vendored dependencies</li>
<li>Groups files by language for parallel processing</li>
</ul>
<p><strong>Configuration:</strong></p>
<pre><code class="language-toml">[analysis]
exclude_patterns = ["**/tests/**", "**/target/**", "**/node_modules/**"]
include_patterns = ["src/**/*.rs", "lib/**/*.py"]
</code></pre>
<h3 id="2-parser-layer"><a class="header" href="#2-parser-layer">2. Parser Layer</a></h3>
<p><strong>Purpose:</strong> Convert source code into Abstract Syntax Trees (ASTs) for analysis.</p>
<p><strong>Language-Specific Parsers:</strong></p>
<p><strong>Rust (syn):</strong></p>
<ul>
<li>Uses the <code>syn</code> crate for full Rust syntax support</li>
<li>Extracts: functions, structs, impls, traits, macros</li>
<li>Handles: async/await, generic types, lifetime annotations</li>
<li>Performance: ~10-20ms per file</li>
</ul>
<p><strong>Python (rustpython):</strong></p>
<ul>
<li>Uses rustpython’s parser for Python 3.x syntax</li>
<li>Extracts: functions, classes, methods, decorators</li>
<li>Handles: comprehensions, async/await, type hints</li>
<li>Performance: ~5-15ms per file</li>
</ul>
<p><strong>JavaScript/TypeScript (tree-sitter):</strong></p>
<ul>
<li>Uses tree-sitter for JS/TS parsing</li>
<li>Extracts: functions, classes, arrow functions, hooks</li>
<li>Handles: JSX/TSX, decorators, generics</li>
<li>Performance: ~8-18ms per file</li>
</ul>
<p><strong>Error Handling:</strong></p>
<ul>
<li>Syntax errors logged but don’t stop analysis</li>
<li>Partial ASTs used when possible</li>
<li>Files with parse errors excluded from final report</li>
</ul>
<h3 id="3-metric-extraction"><a class="header" href="#3-metric-extraction">3. Metric Extraction</a></h3>
<p><strong>Purpose:</strong> Extract raw metrics from ASTs.</p>
<p><strong>Metrics Computed:</strong></p>
<p><strong>Function-Level:</strong></p>
<ul>
<li>Lines of code (LOC)</li>
<li>Cyclomatic complexity (branch count)</li>
<li>Nesting depth (max indentation level)</li>
<li>Parameter count</li>
<li>Return path count</li>
<li>Comment ratio</li>
</ul>
<p><strong>File-Level:</strong></p>
<ul>
<li>Total LOC</li>
<li>Number of functions/classes</li>
<li>Dependency count (imports)</li>
<li>Documentation coverage</li>
</ul>
<p><strong>Implementation:</strong></p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct FunctionMetrics {
    pub name: String,
    pub location: Location,
    pub loc: u32,
    pub cyclomatic_complexity: u32,
    pub nesting_depth: u32,
    pub parameter_count: u32,
    pub return_paths: u32,
}
<span class="boring">}</span></code></pre>
<h3 id="4-complexity-calculation-and-entropy-analysis"><a class="header" href="#4-complexity-calculation-and-entropy-analysis">4. Complexity Calculation and Entropy Analysis</a></h3>
<p><strong>Purpose:</strong> Compute effective complexity using entropy-adjusted metrics.</p>
<p><strong>Traditional Cyclomatic Complexity:</strong></p>
<ul>
<li>Count decision points (if, match, loop, etc.)</li>
<li>Each branch adds +1 to complexity</li>
<li>Does not distinguish between repetitive and varied logic</li>
</ul>
<p><strong>Entropy-Based Adjustment:</strong></p>
<p>Debtmap calculates pattern entropy to adjust cyclomatic complexity:</p>
<ol>
<li><strong>Extract patterns</strong> - Identify branch structures (e.g., all if/return patterns)</li>
<li><strong>Calculate variety</strong> - Measure information entropy of patterns</li>
<li><strong>Adjust complexity</strong> - Reduce score for low-entropy (repetitive) code</li>
</ol>
<p><strong>Formula:</strong></p>
<pre><code>Entropy = -Σ(p_i * log2(p_i))

where p_i = frequency of pattern i

Effective Complexity = Cyclomatic * (1 - (1 - Entropy/Max_Entropy) * 0.75)
</code></pre>
<p><strong>Example:</strong></p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// 20 similar if/return statements
// Cyclomatic: 20, Entropy: 0.3
// Effective: 20 * (1 - (1 - 0.3/4.32) * 0.75) ≈ 5.5
<span class="boring">}</span></code></pre>
<p>This approach reduces false positives from validation/configuration code while still flagging genuinely complex logic.</p>
<h3 id="5-call-graph-construction"><a class="header" href="#5-call-graph-construction">5. Call Graph Construction</a></h3>
<p><strong>Purpose:</strong> Understand function dependencies and identify critical paths.</p>
<p><strong>What’s Tracked:</strong></p>
<ul>
<li>Function calls within the same file</li>
<li>Cross-file calls (when possible to resolve)</li>
<li>Method calls on structs/classes</li>
<li>Trait/interface implementations</li>
</ul>
<p><strong>Analysis:</strong></p>
<ul>
<li><strong>Fan-in:</strong> How many functions call this function</li>
<li><strong>Fan-out:</strong> How many functions this function calls</li>
<li><strong>Depth:</strong> Distance from entry points (main, handlers)</li>
<li><strong>Cycles:</strong> Detect recursive calls</li>
</ul>
<p><strong>Usage:</strong></p>
<ul>
<li>Prioritize functions called from many untested paths</li>
<li>Identify central functions (high fan-in/fan-out)</li>
<li>Detect test coverage gaps in critical paths</li>
</ul>
<p><strong>Limitations:</strong></p>
<ul>
<li>Dynamic dispatch not fully resolved</li>
<li>Cross-crate calls require additional analysis</li>
<li>Closures and function pointers approximated</li>
</ul>
<h3 id="6-pattern-detection-and-debt-classification"><a class="header" href="#6-pattern-detection-and-debt-classification">6. Pattern Detection and Debt Classification</a></h3>
<p><strong>Purpose:</strong> Identify specific technical debt patterns.</p>
<p><strong>Debt Categories:</strong></p>
<p><strong>Test Gaps:</strong></p>
<ul>
<li>Functions with 0% coverage and high complexity</li>
<li>Untested error paths</li>
<li>Missing edge case tests</li>
</ul>
<p><strong>Complexity Issues:</strong></p>
<ul>
<li>Functions exceeding thresholds (default: 10)</li>
<li>Deep nesting (3+ levels)</li>
<li>Long functions (200+ LOC)</li>
</ul>
<p><strong>Design Smells:</strong></p>
<ul>
<li>God functions (high fan-out)</li>
<li>Unused code (fan-in = 0)</li>
<li>Circular dependencies</li>
</ul>
<p><strong>Implementation:</strong></p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub enum DebtType {
    TestGap { missing_tests: u32 },
    HighComplexity { score: u32 },
    DeepNesting { depth: u32 },
    LongFunction { loc: u32 },
    TooManyParams { count: u32 },
}
<span class="boring">}</span></code></pre>
<h3 id="7-coverage-integration"><a class="header" href="#7-coverage-integration">7. Coverage Integration</a></h3>
<p><strong>Purpose:</strong> Map test coverage data to complexity metrics for risk scoring.</p>
<p><strong>Coverage Data Flow:</strong></p>
<ol>
<li><strong>Read LCOV file</strong> - Parse coverage report from test runners</li>
<li><strong>Map to source</strong> - Match coverage lines to functions/branches</li>
<li><strong>Calculate coverage %</strong> - For each function, compute:
<ul>
<li>Line coverage: % of lines executed</li>
<li>Branch coverage: % of branches taken</li>
</ul>
</li>
<li><strong>Identify gaps</strong> - Find untested branches in complex functions</li>
</ol>
<p><strong>Coverage Scoring:</strong></p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct CoverageMetrics {
    pub lines_covered: u32,
    pub lines_total: u32,
    pub branches_covered: u32,
    pub branches_total: u32,
    pub coverage_percent: f64,
}
<span class="boring">}</span></code></pre>
<p><strong>Special Cases:</strong></p>
<ul>
<li>Entry points (main, handlers) expect integration test coverage</li>
<li>Generated code excluded from coverage requirements</li>
<li>Test files themselves not analyzed for coverage</li>
</ul>
<h3 id="8-risk-scoring"><a class="header" href="#8-risk-scoring">8. Risk Scoring</a></h3>
<p><strong>Purpose:</strong> Combine complexity and coverage into a unified risk score.</p>
<p><strong>Risk Formula:</strong></p>
<pre><code>Risk Score = (Effective Complexity * Coverage Gap Weight) + (Call Graph Depth * Path Weight)

where:
- Effective Complexity: Entropy-adjusted cyclomatic complexity
- Coverage Gap Weight: 1.0 for 0% coverage, decreasing to 0.1 for 95%+
- Call Graph Depth: Distance from entry points
- Path Weight: Number of untested paths leading to this function
</code></pre>
<p><strong>Example Calculation:</strong></p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn calculate_risk_score():
  Effective Complexity: 8.5
  Coverage: 30%
  Coverage Gap Weight: 0.7
  Call Graph Depth: 3
  Untested Paths: 2

  Risk = (8.5 * 0.7) + (3 * 2 * 0.3) = 5.95 + 1.8 = 7.75
<span class="boring">}</span></code></pre>
<p><strong>Risk Tiers:</strong></p>
<ul>
<li><strong>Critical (8.0+):</strong> High complexity with no test coverage</li>
<li><strong>High (5.0-7.9):</strong> Moderate complexity with coverage gaps</li>
<li><strong>Moderate (2.0-4.9):</strong> Low-moderate risk, monitor</li>
<li><strong>Low (&lt;2.0):</strong> Acceptable state</li>
</ul>
<h3 id="9-tiered-prioritization"><a class="header" href="#9-tiered-prioritization">9. Tiered Prioritization</a></h3>
<p><strong>Purpose:</strong> Classify and rank technical debt items by severity.</p>
<p><strong>Prioritization Algorithm:</strong></p>
<ol>
<li><strong>Calculate base risk score</strong> (from Risk Scoring step)</li>
<li><strong>Apply context adjustments:</strong>
<ul>
<li>Entry points: -2.0 score (integration test coverage expected)</li>
<li>Core business logic: +1.5 score (higher priority)</li>
<li>Frequently changed files: +1.0 score (git history analysis)</li>
<li>Critical paths: +0.5 score per untested caller</li>
</ul>
</li>
<li><strong>Classify into tiers:</strong>
<ul>
<li>Critical: score &gt;= 8.0</li>
<li>High: score &gt;= 5.0</li>
<li>Moderate: score &gt;= 2.0</li>
<li>Low: score &lt; 2.0</li>
</ul>
</li>
<li><strong>Sort within tiers by:</strong>
<ul>
<li>Severity score</li>
<li>Coupling impact</li>
<li>File location (group related items)</li>
</ul>
</li>
</ol>
<p><strong>Output:</strong></p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct PrioritizedDebtItem {
    pub rank: u32,
    pub score: f64,
    pub tier: Tier,
    pub location: Location,
    pub debt_type: DebtType,
    pub metrics: ComplexityMetrics,
    pub coverage: Option&lt;CoverageMetrics&gt;,
    pub context: ContextSuggestion,
}
<span class="boring">}</span></code></pre>
<p>See <a href="#tiered-prioritization-3">Tiered Prioritization</a> for detailed explanation of the ranking algorithm.</p>
<h3 id="10-context-suggestion-generation"><a class="header" href="#10-context-suggestion-generation">10. Context Suggestion Generation</a></h3>
<p><strong>Purpose:</strong> Provide AI agents with specific file ranges to read for understanding the debt item.</p>
<p><strong>Context Types:</strong></p>
<p><strong>Primary Context:</strong></p>
<ul>
<li>The function/struct where debt is located</li>
<li>Start and end line numbers</li>
<li>File path</li>
</ul>
<p><strong>Related Context:</strong></p>
<ul>
<li><strong>Callers:</strong> Functions that call this function</li>
<li><strong>Callees:</strong> Functions this function calls</li>
<li><strong>Tests:</strong> Existing test files that cover related code</li>
<li><strong>Types:</strong> Struct/enum definitions used by this function</li>
</ul>
<p><strong>Selection Algorithm:</strong></p>
<ol>
<li>Include primary location (always)</li>
<li>Add top 3 callers by call frequency</li>
<li>Add callees that are untested</li>
<li>Add test files with matching function names</li>
<li>Limit total context to ~500 lines (configurable)</li>
</ol>
<p><strong>Output Format:</strong></p>
<pre><code>CONTEXT:
├─ Primary: src/parser.rs:38-85
├─ Caller: src/handler.rs:100-120 (calls 12x)
├─ Caller: src/api.rs:45-60 (calls 8x)
├─ Callee: src/tokenizer.rs:15-40 (untested)
└─ Test: tests/parser_test.rs:50-120
</code></pre>
<h3 id="11-output-formatting"><a class="header" href="#11-output-formatting">11. Output Formatting</a></h3>
<p><strong>Purpose:</strong> Present analysis results in formats optimized for different consumers.</p>
<p><strong>Output Formats:</strong></p>
<p><strong>LLM Markdown (–format llm-markdown):</strong></p>
<ul>
<li>Structured for minimal token usage</li>
<li>Context suggestions included inline</li>
<li>Metrics in consistent tabular format</li>
<li>Designed for piping to AI assistants</li>
</ul>
<p><strong>JSON (–format json):</strong></p>
<ul>
<li>Machine-readable for CI/CD integration</li>
<li>Full metadata for each debt item</li>
<li>Stable schema for programmatic consumption</li>
<li>Schema-versioned for compatibility</li>
</ul>
<p><strong>Terminal (–format terminal):</strong></p>
<ul>
<li>Color-coded by tier (red=critical, yellow=high, etc.)</li>
<li>Hierarchical tree view with unicode box characters</li>
<li>Progress bars for analysis phases</li>
<li>Summary statistics at top</li>
</ul>
<p><strong>Markdown (–format markdown):</strong></p>
<ul>
<li>Rendered in GitHub/GitLab for PR comments</li>
<li>Embedded code blocks with syntax highlighting</li>
<li>Collapsible details sections</li>
<li>Linked to source code locations</li>
</ul>
<p>See <a href="#output-formats-3">Output Formats</a> for examples and configuration options.</p>
<h2 id="data-flow-example"><a class="header" href="#data-flow-example">Data Flow Example</a></h2>
<p>Let’s trace a single function through the entire pipeline:</p>
<p><strong>Input: Source File</strong></p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// src/handlers.rs
pub fn process_request(req: Request) -&gt; Result&lt;Response&gt; {
    validate_auth(&amp;req)?;
    let data = parse_payload(&amp;req.body)?;
    let result = apply_business_logic(data)?;
    format_response(result)
}
<span class="boring">}</span></code></pre>
<p><strong>Stage 1: Parsing</strong></p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>FunctionAst {
    name: "process_request",
    location: Location { file: "src/handlers.rs", line: 2 },
    calls: ["validate_auth", "parse_payload", "apply_business_logic", "format_response"],
    ...
}
<span class="boring">}</span></code></pre>
<p><strong>Stage 2: Metric Extraction</strong></p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>FunctionMetrics {
    name: "process_request",
    cyclomatic_complexity: 4,  // 3 ?-operators + base
    nesting_depth: 1,
    loc: 5,
    ...
}
<span class="boring">}</span></code></pre>
<p><strong>Stage 3: Entropy Analysis</strong></p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Pattern: repetitive ?-operator error handling
Entropy: 0.4 (low variety)
Effective Complexity: 4 * 0.85 = 3.4
<span class="boring">}</span></code></pre>
<p><strong>Stage 4: Call Graph</strong></p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>CallGraphNode {
    function: "process_request",
    fan_in: 3,  // called from 3 handlers
    fan_out: 4,  // calls 4 functions
    depth: 1,  // direct handler (entry point)
}
<span class="boring">}</span></code></pre>
<p><strong>Stage 5: Coverage (from LCOV)</strong></p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>CoverageMetrics {
    lines_covered: 5,
    lines_total: 5,
    branches_covered: 3,
    branches_total: 4,  // Missing one error path
    coverage_percent: 75%,
}
<span class="boring">}</span></code></pre>
<p><strong>Stage 6: Risk Scoring</strong></p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>Risk = (3.4 * 0.25) + (1 * 1 * 0.2) = 0.85 + 0.2 = 1.05
Tier: LOW (entry point with decent coverage)
<span class="boring">}</span></code></pre>
<p><strong>Stage 7: Context Suggestion</strong></p>
<pre><code>CONTEXT:
├─ Primary: src/handlers.rs:2-6
├─ Callee: src/auth.rs:15-30 (validate_auth)
└─ Test: tests/integration/handlers_test.rs:10-25
</code></pre>
<p><strong>Stage 8: Output</strong></p>
<pre><code>#23 SCORE: 1.1 [LOW]
├─ src/handlers.rs:2 process_request()
├─ COMPLEXITY: cyclomatic=4, cognitive=3, nesting=1
├─ COVERAGE: 75% (1 branch untested)
└─ CONTEXT: Primary + 1 callee + 1 test file
</code></pre>
<h2 id="performance-characteristics-1"><a class="header" href="#performance-characteristics-1">Performance Characteristics</a></h2>
<p><strong>Analysis Speed:</strong></p>
<ul>
<li>Small project (&lt; 10k LOC): 1-3 seconds</li>
<li>Medium project (10-50k LOC): 5-15 seconds</li>
<li>Large project (50-200k LOC): 20-60 seconds</li>
<li>Very large project (200k+ LOC): 1-5 minutes</li>
</ul>
<p><strong>Parallelization:</strong></p>
<ul>
<li>File parsing: Parallel across all available cores</li>
<li>Metric extraction: Parallel per-file</li>
<li>Call graph construction: Parallel with work stealing</li>
<li>Risk scoring: Parallel per-function</li>
<li>Output formatting: Sequential</li>
</ul>
<p><strong>Memory Usage:</strong></p>
<ul>
<li>Approx 100-200 KB per file analyzed</li>
<li>Peak memory for large projects: 500 MB - 1 GB</li>
<li>Streaming mode available for very large codebases</li>
</ul>
<p><strong>Optimization Strategies:</strong></p>
<ul>
<li>Skip unchanged files (git diff integration)</li>
<li>Parallel processing with rayon</li>
<li>Efficient AST traversal (visitor pattern)</li>
<li>Memory-efficient streaming for large codebases</li>
</ul>
<h2 id="extension-points"><a class="header" href="#extension-points">Extension Points</a></h2>
<p><strong>Custom Analyzers:</strong>
Implement the <code>Analyzer</code> trait to add language support:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub trait Analyzer {
    fn parse(&amp;self, content: &amp;str) -&gt; Result&lt;Ast&gt;;
    fn extract_metrics(&amp;self, ast: &amp;Ast) -&gt; Vec&lt;FunctionMetrics&gt;;
    fn detect_patterns(&amp;self, ast: &amp;Ast) -&gt; Vec&lt;DebtPattern&gt;;
}
<span class="boring">}</span></code></pre>
<p><strong>Custom Scoring:</strong>
Implement the <code>RiskScorer</code> trait to adjust scoring logic:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub trait RiskScorer {
    fn calculate_risk(&amp;self, metrics: &amp;FunctionMetrics, coverage: &amp;CoverageMetrics) -&gt; f64;
    fn classify_tier(&amp;self, score: f64) -&gt; Tier;
}
<span class="boring">}</span></code></pre>
<p><strong>Custom Output:</strong>
Implement the <code>OutputFormatter</code> trait for new formats:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub trait OutputFormatter {
    fn format(&amp;self, items: &amp;[PrioritizedDebtItem]) -&gt; Result&lt;String&gt;;
}
<span class="boring">}</span></code></pre>
<h2 id="next-steps-3"><a class="header" href="#next-steps-3">Next Steps</a></h2>
<ul>
<li><strong>Understand prioritization:</strong> See <a href="#tiered-prioritization-3">Tiered Prioritization</a></li>
<li><strong>Learn scoring strategies:</strong> See <a href="#scoring-strategies">Scoring Strategies</a></li>
<li><strong>Configure analysis:</strong> See <a href="#configuration-2">Configuration</a></li>
<li><strong>Integrate with AI:</strong> See <a href="#llm-integration-guide">LLM Integration</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="architectural-analysis"><a class="header" href="#architectural-analysis">Architectural Analysis</a></h1>
<p>Debtmap provides comprehensive architectural analysis capabilities based on Robert C. Martin’s software engineering principles. These tools help identify structural issues, coupling problems, and architectural anti-patterns in your codebase.</p>
<h2 id="overview-5"><a class="header" href="#overview-5">Overview</a></h2>
<p>Architectural analysis examines module-level relationships and dependencies to identify:</p>
<ul>
<li><strong>Circular Dependencies</strong> - Modules that create dependency cycles</li>
<li><strong>Coupling Metrics</strong> - Afferent and efferent coupling measurements</li>
<li><strong>Bidirectional Dependencies</strong> - Inappropriate intimacy between modules</li>
<li><strong>Stable Dependencies Principle Violations</strong> - Unstable modules being depended upon</li>
<li><strong>Zone of Pain</strong> - Rigid, concrete implementations heavily depended upon</li>
<li><strong>Zone of Uselessness</strong> - Overly abstract, unstable modules</li>
<li><strong>Code Duplication</strong> - Identical or similar code blocks across files</li>
</ul>
<p>These analyses help you maintain clean architecture and identify refactoring opportunities.</p>
<h2 id="circular-dependency-detection"><a class="header" href="#circular-dependency-detection">Circular Dependency Detection</a></h2>
<p>Circular dependencies occur when modules form a dependency cycle (A depends on B, B depends on C, C depends on A). These violations break architectural boundaries and make code harder to understand, test, and maintain.</p>
<h3 id="how-it-works-2"><a class="header" href="#how-it-works-2">How It Works</a></h3>
<p>Debtmap builds a <strong>dependency graph</strong> from module imports and uses <strong>depth-first search (DFS)</strong> with recursion stack tracking to detect cycles:</p>
<ol>
<li>Parse all files to extract import/module dependencies</li>
<li>Build a directed graph where nodes are modules and edges are dependencies</li>
<li>Run DFS from each unvisited module</li>
<li>Track visited nodes and recursion stack</li>
<li>When a node is reached that’s already in the recursion stack, a cycle is detected</li>
</ol>
<p><strong>Implementation:</strong> <code>src/debt/circular.rs:44-66</code> (detect_circular_dependencies)</p>
<h3 id="example"><a class="header" href="#example">Example</a></h3>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Module A (src/auth.rs)
use crate::user::User;
use crate::session::validate_session;

// Module B (src/user.rs)
use crate::session::Session;

// Module C (src/session.rs)
use crate::auth::authenticate; // Creates cycle: auth → session → auth
<span class="boring">}</span></code></pre>
<p><strong>Debtmap detects:</strong></p>
<pre><code>Circular dependency detected: auth → session → auth
</code></pre>
<h3 id="refactoring-recommendations"><a class="header" href="#refactoring-recommendations">Refactoring Recommendations</a></h3>
<p>To break circular dependencies:</p>
<ol>
<li><strong>Extract Interface</strong> - Create a trait that both modules depend on</li>
<li><strong>Dependency Inversion</strong> - Introduce an abstraction layer</li>
<li><strong>Move Shared Code</strong> - Extract common functionality to a new module</li>
<li><strong>Remove Dependency</strong> - Inline or duplicate small amounts of code</li>
</ol>
<h2 id="coupling-metrics"><a class="header" href="#coupling-metrics">Coupling Metrics</a></h2>
<p>Coupling metrics measure how interconnected modules are. Debtmap calculates two primary metrics:</p>
<h3 id="afferent-coupling-ca"><a class="header" href="#afferent-coupling-ca">Afferent Coupling (Ca)</a></h3>
<p><strong>Afferent coupling</strong> is the number of modules that depend on this module. High afferent coupling means many modules rely on this code.</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct CouplingMetrics {
    pub module: String,
    pub afferent_coupling: usize, // Number depending on this module
    pub efferent_coupling: usize, // Number this module depends on
    pub instability: f64,         // Calculated from Ca and Ce
    pub abstractness: f64,        // Ratio of abstract types
}
<span class="boring">}</span></code></pre>
<p><strong>Implementation:</strong> <code>src/debt/coupling.rs:6-30</code></p>
<h3 id="efferent-coupling-ce"><a class="header" href="#efferent-coupling-ce">Efferent Coupling (Ce)</a></h3>
<p><strong>Efferent coupling</strong> is the number of modules this module depends on. High efferent coupling means this module has many dependencies.</p>
<p><strong>Note on Abstractness:</strong> The <code>abstractness</code> field in <code>CouplingMetrics</code> requires advanced type analysis to calculate properly. The current implementation uses a placeholder value (0.0) as full abstractness calculation would need semantic analysis of trait definitions, abstract types, and implementation ratios. This is similar to the cohesion analysis limitation documented below (see “Cohesion Analysis” section).</p>
<p><strong>Source:</strong> <code>src/debt/coupling.rs:44</code></p>
<h3 id="example-coupling-analysis"><a class="header" href="#example-coupling-analysis">Example Coupling Analysis</a></h3>
<pre><code>Module: api_handler
  Afferent coupling (Ca): 8  // 8 modules depend on api_handler
  Efferent coupling (Ce): 3  // api_handler depends on 3 modules
  Instability: 0.27          // Relatively stable
</code></pre>
<p>High afferent or efferent coupling (typically &gt;5) indicates potential maintainability issues.</p>
<h2 id="instability-metric"><a class="header" href="#instability-metric">Instability Metric</a></h2>
<p>The <strong>instability metric</strong> measures how resistant a module is to change. It’s calculated as:</p>
<pre><code>I = Ce / (Ca + Ce)
</code></pre>
<p><strong>Interpretation:</strong></p>
<ul>
<li><strong>I = 0.0</strong> - Maximally stable (no dependencies, many dependents)</li>
<li><strong>I = 1.0</strong> - Maximally unstable (many dependencies, no dependents)</li>
</ul>
<p><strong>Implementation:</strong> <code>src/debt/coupling.rs:16-24</code> (calculate_instability)</p>
<h3 id="stability-guidelines"><a class="header" href="#stability-guidelines">Stability Guidelines</a></h3>
<ul>
<li><strong>Stable modules (I &lt; 0.3)</strong> - Hard to change but depended upon; should contain stable abstractions</li>
<li><strong>Balanced modules (0.3 ≤ I ≤ 0.7)</strong> - Normal modules with both dependencies and dependents</li>
<li><strong>Unstable modules (I &gt; 0.7)</strong> - Change frequently; should have few or no dependents</li>
</ul>
<h3 id="example-1"><a class="header" href="#example-1">Example</a></h3>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Stable module (I = 0.1)
// core/types.rs - defines fundamental types, depended on by 20 modules
pub struct User { ... }
pub struct Session { ... }

// Unstable module (I = 0.9)
// handlers/admin_dashboard.rs - depends on 10 modules, no dependents
use crate::auth::*;
use crate::database::*;
use crate::templates::*;
// ... 7 more imports
<span class="boring">}</span></code></pre>
<h2 id="stable-dependencies-principle"><a class="header" href="#stable-dependencies-principle">Stable Dependencies Principle</a></h2>
<p>The <strong>Stable Dependencies Principle (SDP)</strong> states: <em>Depend in the direction of stability</em>. Modules should depend on modules that are more stable than themselves.</p>
<h3 id="sdp-violations"><a class="header" href="#sdp-violations">SDP Violations</a></h3>
<p>Debtmap flags violations when a module has:</p>
<ul>
<li><strong>Instability &gt; 0.8</strong> (very unstable)</li>
<li><strong>Afferent coupling &gt; 2</strong> (multiple modules depend on it)</li>
</ul>
<p>This means an unstable, frequently changing module is being depended upon by multiple other modules - a recipe for maintenance problems.</p>
<p><strong>Implementation:</strong> <code>src/debt/coupling.rs:69-76</code></p>
<h3 id="example-violation"><a class="header" href="#example-violation">Example Violation</a></h3>
<pre><code>Module 'temp_utils' violates Stable Dependencies Principle
(instability: 0.85, depended on by 5 modules)

Problem: This module changes frequently but is heavily depended upon.
Solution: Extract stable interface or reduce dependencies on this module.
</code></pre>
<h3 id="fixing-sdp-violations"><a class="header" href="#fixing-sdp-violations">Fixing SDP Violations</a></h3>
<ol>
<li><strong>Increase stability</strong> - Reduce the module’s dependencies</li>
<li><strong>Reduce afferent coupling</strong> - Extract interface, use dependency injection</li>
<li><strong>Split module</strong> - Separate stable and unstable parts</li>
</ol>
<h2 id="bidirectional-dependencies"><a class="header" href="#bidirectional-dependencies">Bidirectional Dependencies</a></h2>
<p>Bidirectional dependencies (also called <strong>inappropriate intimacy</strong>) occur when two modules depend on each other:</p>
<pre><code>Module A depends on Module B
Module B depends on Module A
</code></pre>
<p>This creates tight coupling and makes both modules harder to change, test, or reuse independently.</p>
<p><strong>Implementation:</strong> <code>src/debt/coupling.rs:98-117</code> (detect_inappropriate_intimacy)</p>
<h3 id="example-2"><a class="header" href="#example-2">Example</a></h3>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// order.rs
use crate::customer::Customer;

pub struct Order {
    customer: Customer,
}

// customer.rs
use crate::order::Order; // Bidirectional dependency!

pub struct Customer {
    orders: Vec&lt;Order&gt;,
}
<span class="boring">}</span></code></pre>
<p><strong>Debtmap detects:</strong></p>
<pre><code>Inappropriate intimacy detected between 'order' and 'customer'
</code></pre>
<h3 id="refactoring-recommendations-1"><a class="header" href="#refactoring-recommendations-1">Refactoring Recommendations</a></h3>
<ol>
<li><strong>Create Mediator</strong> - Introduce a third module to manage the relationship</li>
<li><strong>Break into Separate Modules</strong> - Split concerns more clearly</li>
<li><strong>Use Events</strong> - Replace direct dependencies with event-driven communication</li>
<li><strong>Dependency Inversion</strong> - Introduce interfaces/traits both depend on</li>
</ol>
<h2 id="zone-of-pain-detection"><a class="header" href="#zone-of-pain-detection">Zone of Pain Detection</a></h2>
<p>The <strong>zone of pain</strong> contains modules with:</p>
<ul>
<li><strong>Low abstractness (&lt; 0.2)</strong> - Concrete implementations, no abstractions</li>
<li><strong>Low instability (&lt; 0.2)</strong> - Stable, hard to change</li>
<li><strong>High afferent coupling (&gt; 3)</strong> - Many modules depend on them</li>
</ul>
<p>These modules are rigid concrete implementations that are heavily used but hard to change - causing pain when modifications are needed.</p>
<p><strong>Implementation:</strong> <code>src/debt/coupling.rs:125-138</code></p>
<h3 id="example-3"><a class="header" href="#example-3">Example</a></h3>
<pre><code>Module 'database_client' is in the zone of pain (rigid and hard to change)
  Abstractness: 0.1  (all concrete implementation)
  Instability: 0.15  (very stable, many dependents)
  Afferent coupling: 12 (12 modules depend on it)

Problem: This concrete database client is used everywhere.
Any change to its implementation requires updating many modules.
</code></pre>
<h3 id="refactoring-recommendations-2"><a class="header" href="#refactoring-recommendations-2">Refactoring Recommendations</a></h3>
<ol>
<li><strong>Extract Interfaces</strong> - Create a <code>DatabaseClient</code> trait</li>
<li><strong>Introduce Abstractions</strong> - Define abstract operations others depend on</li>
<li><strong>Break into Smaller Modules</strong> - Separate concerns to reduce coupling</li>
<li><strong>Use Dependency Injection</strong> - Pass implementations via interfaces</li>
</ol>
<h2 id="zone-of-uselessness-detection"><a class="header" href="#zone-of-uselessness-detection">Zone of Uselessness Detection</a></h2>
<p>The <strong>zone of uselessness</strong> contains modules with:</p>
<ul>
<li><strong>High abstractness (&gt; 0.8)</strong> - Mostly abstract, few concrete implementations</li>
<li><strong>High instability (&gt; 0.8)</strong> - Frequently changing</li>
</ul>
<p>These modules are overly abstract and unstable, providing little stable value to the system.</p>
<p><strong>Implementation:</strong> <code>src/debt/coupling.rs:141-153</code></p>
<h3 id="example-4"><a class="header" href="#example-4">Example</a></h3>
<pre><code>Module 'base_processor' is in the zone of uselessness
(too abstract and unstable)
  Abstractness: 0.9  (mostly traits and interfaces)
  Instability: 0.85  (changes frequently)

Problem: This module defines many abstractions but provides little
concrete value. It changes often, breaking implementations.
</code></pre>
<h3 id="refactoring-recommendations-3"><a class="header" href="#refactoring-recommendations-3">Refactoring Recommendations</a></h3>
<ol>
<li><strong>Add Concrete Implementations</strong> - Make the module useful by implementing functionality</li>
<li><strong>Remove if Unused</strong> - Delete if no real value is provided</li>
<li><strong>Stabilize Interfaces</strong> - Stop changing abstractions frequently</li>
<li><strong>Merge with Implementations</strong> - Combine abstract and concrete code</li>
</ol>
<h2 id="distance-from-main-sequence"><a class="header" href="#distance-from-main-sequence">Distance from Main Sequence</a></h2>
<p>The <strong>main sequence</strong> represents the ideal balance between abstractness and instability. Modules should lie on the line:</p>
<pre><code>A + I = 1
</code></pre>
<p>Where:</p>
<ul>
<li><strong>A</strong> = Abstractness (ratio of abstract types to total types)</li>
<li><strong>I</strong> = Instability (Ce / (Ca + Ce))</li>
</ul>
<p><strong>Distance</strong> from the main sequence:</p>
<pre><code>D = |A + I - 1|
</code></pre>
<p><strong>Implementation:</strong> <code>src/debt/coupling.rs:119-123</code></p>
<h3 id="interpretation"><a class="header" href="#interpretation">Interpretation</a></h3>
<ul>
<li><strong>D ≈ 0.0</strong> - Module is on the main sequence (ideal)</li>
<li><strong>D &gt; 0.5</strong> - Module is far from ideal
<ul>
<li>High D with low A and I → Zone of Pain</li>
<li>High D with high A and I → Zone of Uselessness</li>
</ul>
</li>
</ul>
<h3 id="visual-representation"><a class="header" href="#visual-representation">Visual Representation</a></h3>
<pre><code>Abstractness
    1.0 ┤        Zone of Uselessness
        │      ╱
        │    ╱
    0.5 ┤  ╱ Main Sequence
        │╱
        ╱
    0.0 ┤──────────────────────────
        0.0    0.5              1.0
                 Instability

        Zone of Pain
</code></pre>
<h2 id="code-duplication-detection"><a class="header" href="#code-duplication-detection">Code Duplication Detection</a></h2>
<p>Debtmap detects code duplication using <strong>hash-based chunk comparison</strong>:</p>
<ol>
<li><strong>Extract chunks</strong> - Split files into fixed-size chunks (default: 50 lines)</li>
<li><strong>Normalize</strong> - Remove whitespace and comments</li>
<li><strong>Calculate hash</strong> - Compute SHA-256 hash for each normalized chunk</li>
<li><strong>Match duplicates</strong> - Find chunks with identical hashes</li>
<li><strong>Merge adjacent</strong> - Consolidate consecutive duplicate blocks</li>
</ol>
<p><strong>Note:</strong> The minimum chunk size is configurable via the <code>--threshold-duplication</code> flag or in <code>.debtmap.toml</code> (default: 50 lines).</p>
<p><strong>Implementation:</strong> <code>src/debt/duplication.rs:6-44</code> (detect_duplication)</p>
<h3 id="algorithm-details"><a class="header" href="#algorithm-details">Algorithm Details</a></h3>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub fn detect_duplication(
    files: Vec&lt;(PathBuf, String)&gt;,
    min_lines: usize,           // Default: 50
    _similarity_threshold: f64, // Currently unused (exact matching)
) -&gt; Vec&lt;DuplicationBlock&gt;
<span class="boring">}</span></code></pre>
<p>The algorithm:</p>
<ol>
<li>Extracts overlapping chunks from each file</li>
<li>Normalizes by trimming whitespace and removing comments</li>
<li>Calculates SHA-256 hash for each normalized chunk</li>
<li>Groups chunks by hash</li>
<li>Returns groups with 2+ locations (duplicates found)</li>
</ol>
<h3 id="example-output-2"><a class="header" href="#example-output-2">Example Output</a></h3>
<pre><code>Code duplication detected:
  Hash: a3f2b9c1...
  Lines: 50
  Locations:
    - src/handlers/user.rs:120-169
    - src/handlers/admin.rs:85-134
    - src/handlers/guest.rs:200-249

Recommendation: Extract common validation logic to shared module
</code></pre>
<h2 id="duplication-configuration"><a class="header" href="#duplication-configuration">Duplication Configuration</a></h2>
<p>Configure duplication detection in <code>.debtmap.toml</code>:</p>
<pre><code class="language-toml"># Minimum lines for duplication detection
threshold_duplication = 50  # Default value

# Smaller values catch more duplications but increase noise
# threshold_duplication = 30  # More sensitive

# Larger values only catch major duplications
# threshold_duplication = 100  # Less noise
</code></pre>
<p><strong>Configuration reference:</strong> <code>src/cli.rs:69</code> (threshold_duplication flag definition)</p>
<p><strong>Implementation:</strong> <code>src/debt/duplication.rs:6-10</code></p>
<h3 id="current-limitations"><a class="header" href="#current-limitations">Current Limitations</a></h3>
<ul>
<li><strong>Exact matching only</strong> - Currently uses hash-based exact matching</li>
<li><strong>similarity_threshold parameter</strong> - Defined in function signature but not implemented yet</li>
<li><strong>Future enhancement</strong> - Fuzzy matching for near-duplicates using similarity algorithms (e.g., Levenshtein distance, token-based similarity)</li>
</ul>
<p>The <code>similarity_threshold</code> parameter exists for future extensibility but is currently unused. All duplication detection uses exact hash matching. Track progress on fuzzy matching in the project’s issue tracker or roadmap.</p>
<h2 id="refactoring-recommendations-4"><a class="header" href="#refactoring-recommendations-4">Refactoring Recommendations</a></h2>
<p>Debtmap provides specific refactoring recommendations for each architectural issue:</p>
<h3 id="for-circular-dependencies"><a class="header" href="#for-circular-dependencies">For Circular Dependencies</a></h3>
<ol>
<li><strong>Extract Interface</strong> - Create shared abstraction both modules use</li>
<li><strong>Dependency Inversion</strong> - Introduce interfaces to reverse dependency direction</li>
<li><strong>Move Shared Code</strong> - Extract to new module both can depend on</li>
<li><strong>Event-Driven</strong> - Replace direct calls with event publishing/subscribing</li>
</ol>
<h3 id="for-high-coupling"><a class="header" href="#for-high-coupling">For High Coupling</a></h3>
<ol>
<li><strong>Facade Pattern</strong> - Provide simplified interface hiding complex dependencies</li>
<li><strong>Reduce Dependencies</strong> - Remove unnecessary imports and calls</li>
<li><strong>Dependency Injection</strong> - Pass dependencies via constructors/parameters</li>
<li><strong>Interface Segregation</strong> - Split large interfaces into focused ones</li>
</ol>
<h3 id="for-zone-of-pain"><a class="header" href="#for-zone-of-pain">For Zone of Pain</a></h3>
<ol>
<li><strong>Introduce Abstractions</strong> - Extract traits/interfaces for flexibility</li>
<li><strong>Adapter Pattern</strong> - Wrap concrete implementations with adapters</li>
<li><strong>Strategy Pattern</strong> - Make algorithms pluggable via interfaces</li>
</ol>
<h3 id="for-zone-of-uselessness"><a class="header" href="#for-zone-of-uselessness">For Zone of Uselessness</a></h3>
<ol>
<li><strong>Add Concrete Implementations</strong> - Provide useful functionality</li>
<li><strong>Remove Unused Code</strong> - Delete if providing no value</li>
<li><strong>Stabilize Interfaces</strong> - Stop changing abstractions frequently</li>
</ol>
<h3 id="for-bidirectional-dependencies"><a class="header" href="#for-bidirectional-dependencies">For Bidirectional Dependencies</a></h3>
<ol>
<li><strong>Create Mediator</strong> - Third module manages relationship</li>
<li><strong>Break into Separate Modules</strong> - Clearer separation of concerns</li>
<li><strong>Observer Pattern</strong> - One-way communication via observers</li>
</ol>
<h3 id="for-code-duplication"><a class="header" href="#for-code-duplication">For Code Duplication</a></h3>
<ol>
<li><strong>Extract Common Code</strong> - Create shared function/module</li>
<li><strong>Use Inheritance/Composition</strong> - Share via traits or composition</li>
<li><strong>Parameterize Differences</strong> - Extract variable parts as parameters</li>
<li><strong>Template Method</strong> - Define algorithm structure, vary specific steps</li>
</ol>
<h2 id="examples-and-use-cases"><a class="header" href="#examples-and-use-cases">Examples and Use Cases</a></h2>
<h3 id="running-architectural-analysis"><a class="header" href="#running-architectural-analysis">Running Architectural Analysis</a></h3>
<pre><code class="language-bash"># Architectural analysis runs automatically with standard analysis
debtmap analyze .

# Duplication detection with custom chunk size
debtmap analyze . --threshold-duplication 30

# Note: Circular dependencies, coupling metrics, and SDP violations
# are analyzed automatically. There are no separate flags to enable
# or disable specific architectural checks.
</code></pre>
<h3 id="example-circular-dependency"><a class="header" href="#example-circular-dependency">Example: Circular Dependency</a></h3>
<p><strong>Before:</strong></p>
<pre><code>src/auth.rs → src/session.rs → src/user.rs → src/auth.rs

Circular dependency detected: auth → session → user → auth
</code></pre>
<p><strong>After refactoring:</strong></p>
<pre><code>src/auth.rs → src/auth_interface.rs ← src/session.rs
                      ↑
              src/user.rs

No circular dependencies found.
</code></pre>
<h3 id="example-coupling-metrics-table"><a class="header" href="#example-coupling-metrics-table">Example: Coupling Metrics Table</a></h3>
<pre><code>Module Analysis Results:

Module              Ca    Ce    Instability  Issues
-------------------------------------------------
core/types          15     0       0.00      None
api/handlers         2     8       0.80      High Ce
database/client      8     2       0.20      None
utils/temp          5    12       0.71      SDP violation
auth/session        3     3       0.50      None
</code></pre>
<h3 id="example-zone-of-pain"><a class="header" href="#example-zone-of-pain">Example: Zone of Pain</a></h3>
<p><strong>Module:</strong> <code>legacy_db_client</code></p>
<pre><code>Metrics:
  Abstractness: 0.05 (all concrete code)
  Instability: 0.12 (depended on by 25 modules)
  Afferent coupling: 25
  Distance from main sequence: 0.83

Status: Zone of Pain - rigid and hard to change

Refactoring steps:
1. Extract interface DatabaseClient trait
2. Create adapter wrapping legacy implementation
3. Gradually migrate dependents to use trait
4. Introduce alternative implementations
</code></pre>
<h2 id="interpreting-results-2"><a class="header" href="#interpreting-results-2">Interpreting Results</a></h2>
<h3 id="prioritization-1"><a class="header" href="#prioritization-1">Prioritization</a></h3>
<p>Address architectural issues in this order:</p>
<ol>
<li>
<p><strong>Circular Dependencies</strong> (Highest Priority)</p>
<ul>
<li>Break architectural boundaries</li>
<li>Make testing impossible</li>
<li>Cause build issues</li>
</ul>
</li>
<li>
<p><strong>Bidirectional Dependencies</strong> (High Priority)</p>
<ul>
<li>Create tight coupling</li>
<li>Prevent independent testing</li>
<li>Block modular changes</li>
</ul>
</li>
<li>
<p><strong>Zone of Pain Issues</strong> (Medium-High Priority)</p>
<ul>
<li>Indicate rigid architecture</li>
<li>Block future changes</li>
<li>High risk for bugs</li>
</ul>
</li>
<li>
<p><strong>SDP Violations</strong> (Medium Priority)</p>
<ul>
<li>Cause ripple effects</li>
<li>Increase maintenance cost</li>
<li>Unstable foundation</li>
</ul>
</li>
<li>
<p><strong>High Coupling</strong> (Medium Priority)</p>
<ul>
<li>Maintainability risk</li>
<li>Testing difficulty</li>
<li>Change amplification</li>
</ul>
</li>
<li>
<p><strong>Code Duplication</strong> (Lower Priority)</p>
<ul>
<li>Maintenance burden</li>
<li>Bug multiplication</li>
<li>Inconsistency risk</li>
</ul>
</li>
</ol>
<h3 id="decision-flowchart"><a class="header" href="#decision-flowchart">Decision Flowchart</a></h3>
<pre><code>Is there a circular dependency?
├─ YES → Break immediately (extract interface, DI)
└─ NO  → Continue

Is there bidirectional dependency?
├─ YES → Refactor (mediator, event-driven)
└─ NO  → Continue

Is module in zone of pain?
├─ YES → Introduce abstractions
└─ NO  → Continue

Is SDP violated?
├─ YES → Stabilize or reduce afferent coupling
└─ NO  → Continue

Is coupling &gt; threshold?
├─ YES → Reduce dependencies
└─ NO  → Continue

Is there significant duplication?
├─ YES → Extract common code
└─ NO  → Architecture is good!
</code></pre>
<h2 id="integration-with-debt-categories"><a class="header" href="#integration-with-debt-categories">Integration with Debt Categories</a></h2>
<p>Architectural analysis results are integrated with debtmap’s debt categorization system:</p>
<h3 id="debt-type-mapping"><a class="header" href="#debt-type-mapping">Debt Type Mapping</a></h3>
<p>Architectural issues are mapped to existing DebtType enum variants:</p>
<ul>
<li><strong>Duplication</strong> - Duplicated code blocks found</li>
<li><strong>Dependency</strong> - Used for circular dependencies and coupling issues</li>
<li><strong>CodeOrganization</strong> - May be used for architectural violations (SDP, zone issues)</li>
</ul>
<p><strong>Note:</strong> The DebtType enum does not have dedicated variants for CircularDependency, HighCoupling, or ArchitecturalViolation. Architectural issues are mapped to existing general-purpose debt types.</p>
<p><strong>Reference:</strong> <code>src/core/mod.rs:220-236</code> for actual DebtType enum definition</p>
<h3 id="tiered-prioritization-1"><a class="header" href="#tiered-prioritization-1">Tiered Prioritization</a></h3>
<p>Architectural issues are assigned priority tiers:</p>
<ul>
<li><strong>Tier 1 (Critical)</strong> - Circular dependencies, bidirectional dependencies</li>
<li><strong>Tier 2 (High)</strong> - Zone of pain, SDP violations</li>
<li><strong>Tier 3 (Medium)</strong> - High coupling, large duplications</li>
<li><strong>Tier 4 (Low)</strong> - Small duplications, minor coupling issues</li>
</ul>
<p><strong>Reference:</strong> See <a href="#tiered-prioritization-3">Tiered Prioritization</a> for complete priority assignment logic</p>
<h2 id="cohesion-analysis"><a class="header" href="#cohesion-analysis">Cohesion Analysis</a></h2>
<p><strong>Note:</strong> Module cohesion analysis is currently a simplified placeholder implementation.</p>
<p><strong>Current status:</strong> <code>src/debt/coupling.rs:82-95</code> (analyze_module_cohesion)</p>
<p>The function exists but provides basic cohesion calculation. Full cohesion analysis (measuring how well module elements belong together) is planned for a future release.</p>
<h3 id="future-enhancement"><a class="header" href="#future-enhancement">Future Enhancement</a></h3>
<p>Full cohesion analysis would measure:</p>
<ul>
<li>Functional cohesion (functions operating on related data)</li>
<li>Sequential cohesion (output of one function feeds another)</li>
<li>Communicational cohesion (functions operating on same data structures)</li>
</ul>
<h2 id="configuration-3"><a class="header" href="#configuration-3">Configuration</a></h2>
<h3 id="configurable-parameters"><a class="header" href="#configurable-parameters">Configurable Parameters</a></h3>
<p>Configure duplication detection in <code>.debtmap.toml</code> or via CLI:</p>
<pre><code class="language-toml"># Minimum lines for duplication detection
threshold_duplication = 50  # Default value
</code></pre>
<p>Or via command line:</p>
<pre><code class="language-bash">debtmap analyze . --threshold-duplication 50
</code></pre>
<p><strong>Configuration reference:</strong> <code>src/cli.rs:69</code> (threshold_duplication flag definition)</p>
<h3 id="hardcoded-thresholds"><a class="header" href="#hardcoded-thresholds">Hardcoded Thresholds</a></h3>
<p><strong>Note:</strong> Most architectural thresholds are currently hardcoded in the implementation and cannot be configured. These thresholds are based on industry-standard metrics from Robert C. Martin’s research and empirical software engineering studies:</p>
<ul>
<li><strong>Coupling threshold:</strong> 5 (modules with &gt;5 dependencies are flagged)</li>
<li><strong>Instability threshold:</strong> 0.8 (for SDP violations)</li>
<li><strong>SDP afferent threshold:</strong> 2 (minimum dependents for SDP violations)</li>
<li><strong>Zone of pain thresholds:</strong>
<ul>
<li>Abstractness &lt; 0.2</li>
<li>Instability &lt; 0.2</li>
<li>Afferent coupling &gt; 3</li>
</ul>
</li>
<li><strong>Zone of uselessness thresholds:</strong>
<ul>
<li>Abstractness &gt; 0.8</li>
<li>Instability &gt; 0.8</li>
</ul>
</li>
</ul>
<p>These values represent widely-accepted boundaries in software architecture literature. While they work well for most projects, configurable thresholds may be added in a future release to support domain-specific tuning.</p>
<p><strong>Source:</strong> <code>src/debt/coupling.rs:70-76, 130, 145</code> (hardcoded threshold definitions)</p>
<p>See <a href="#configuration-2">Configuration</a> for complete options.</p>
<h2 id="troubleshooting-7"><a class="header" href="#troubleshooting-7">Troubleshooting</a></h2>
<h3 id="no-circular-dependencies-detected-but-build-fails"><a class="header" href="#no-circular-dependencies-detected-but-build-fails">“No circular dependencies detected but build fails”</a></h3>
<p><strong>Cause:</strong> Circular dependencies at the package/crate level, not module level.</p>
<p><strong>Solution:</strong> Use <code>cargo tree</code> to analyze package-level dependencies.</p>
<h3 id="too-many-coupling-warnings"><a class="header" href="#too-many-coupling-warnings">“Too many coupling warnings”</a></h3>
<p><strong>Cause:</strong> Default threshold of 5 may be too strict for your codebase.</p>
<p><strong>Solution:</strong> The coupling threshold is currently hardcoded at 5 in the implementation (<code>src/debt/coupling.rs:62</code>). To adjust it, you would need to modify the source code. Consider using suppression patterns to exclude specific modules if needed. See <a href="#suppression-patterns">Suppression Patterns</a>.</p>
<h3 id="duplication-detected-in-generated-code"><a class="header" href="#duplication-detected-in-generated-code">“Duplication detected in generated code”</a></h3>
<p><strong>Cause:</strong> Code generation tools create similar patterns.</p>
<p><strong>Solution:</strong> Use suppression patterns to exclude generated files. See <a href="#suppression-patterns">Suppression Patterns</a>.</p>
<h3 id="zone-of-pain-false-positives"><a class="header" href="#zone-of-pain-false-positives">“Zone of pain false positives”</a></h3>
<p><strong>Cause:</strong> Utility modules are intentionally stable and concrete.</p>
<p><strong>Solution:</strong> This is often correct - utility modules should be stable. Consider whether the module should be more abstract.</p>
<h2 id="further-reading"><a class="header" href="#further-reading">Further Reading</a></h2>
<h3 id="robert-c-martins-principles"><a class="header" href="#robert-c-martins-principles">Robert C. Martin’s Principles</a></h3>
<p>The architectural metrics in debtmap are based on:</p>
<ul>
<li><strong>Clean Architecture</strong> by Robert C. Martin</li>
<li><strong>Agile Software Development: Principles, Patterns, and Practices</strong> by Robert C. Martin</li>
<li>Stable Dependencies Principle (SDP)</li>
<li>Stable Abstractions Principle (SAP)</li>
<li>Main Sequence distance metric</li>
</ul>
<h3 id="related-topics-2"><a class="header" href="#related-topics-2">Related Topics</a></h3>
<ul>
<li><a href="#analysis-guide">Analysis Guide</a> - Complete analysis workflow</li>
<li><a href="#configuration-2">Configuration</a> - Configuration options</li>
<li><a href="#entropy-analysis">Entropy Analysis</a> - Complexity vs. entropy</li>
<li><a href="#scoring-strategies">Scoring Strategies</a> - How debt is scored</li>
<li><a href="#tiered-prioritization-3">Tiered Prioritization</a> - Priority assignment</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="boilerplate-detection-1"><a class="header" href="#boilerplate-detection-1">Boilerplate Detection</a></h1>
<p>Debtmap identifies repetitive code patterns that could benefit from macro-ification or other abstraction techniques. This helps reduce maintenance burden and improve code consistency.</p>
<h2 id="overview-6"><a class="header" href="#overview-6">Overview</a></h2>
<p>Boilerplate detection analyzes low-complexity repetitive code to identify opportunities for:</p>
<ul>
<li><strong>Macro-ification</strong> - Convert repetitive patterns to declarative or procedural macros</li>
<li><strong>Code generation</strong> - Use build scripts to generate repetitive implementations</li>
<li><strong>Generic abstractions</strong> - Replace duplicate implementations with generic code</li>
<li><strong>Trait derivation</strong> - Use derive macros instead of manual implementations</li>
</ul>
<p>Boilerplate detection runs automatically as part of the god object analysis pipeline. When a file has many impl blocks, it’s classified as either a true god object (complex code needing splitting), a builder pattern (intentional fluent API), or a boilerplate pattern (low complexity needing macro-ification). This prevents false positives where repetitive low-complexity code is misclassified as god objects.</p>
<p><strong>Source</strong>: Integration with god object detection in src/organization/god_object/classification_types.rs:45-50, src/analyzers/file_analyzer.rs:366-372</p>
<h2 id="detection-criteria"><a class="header" href="#detection-criteria">Detection Criteria</a></h2>
<p>Debtmap identifies boilerplate using trait pattern analysis (src/organization/trait_pattern_analyzer.rs:158-176):</p>
<ul>
<li><strong>Multiple similar trait implementations</strong> - 20+ impl blocks with shared structure</li>
<li><strong>High method uniformity</strong> - 70%+ of implementations share the same methods</li>
<li><strong>Low complexity repetitive code</strong> - Average cyclomatic complexity &lt; 2.0</li>
<li><strong>Low complexity variance</strong> - Consistent complexity across implementations</li>
<li><strong>Single dominant trait</strong> - One trait accounts for 80%+ of implementations</li>
</ul>
<p>The TraitPatternAnalyzer computes these metrics:</p>
<ul>
<li><code>impl_block_count</code> - Number of trait implementations in the file</li>
<li><code>unique_traits</code> - Set of distinct traits implemented</li>
<li><code>most_common_trait</code> - Most frequently implemented trait and count</li>
<li><code>method_uniformity</code> - Ratio of most common method appearance to total impls</li>
<li><code>shared_methods</code> - Methods appearing in 50%+ of implementations</li>
<li><code>avg_method_complexity</code> - Average cyclomatic complexity per method</li>
<li><code>complexity_variance</code> - Variance in complexity across methods</li>
<li><code>avg_method_lines</code> - Average lines of code per method</li>
</ul>
<h2 id="detection-signals"><a class="header" href="#detection-signals">Detection Signals</a></h2>
<p>The boilerplate detector extracts detection signals (src/organization/boilerplate_detector.rs:246-253, 164-190):</p>
<ul>
<li><strong>HighImplCount(usize)</strong> - Number of impl blocks exceeds threshold</li>
<li><strong>HighMethodUniformity(f64)</strong> - Methods are highly uniform across implementations</li>
<li><strong>LowAvgComplexity(f64)</strong> - Average complexity is below threshold</li>
<li><strong>LowComplexityVariance(f64)</strong> - Complexity variance is low (consistent complexity)</li>
<li><strong>HighStructDensity(usize)</strong> - Many structs with similar implementations</li>
</ul>
<h2 id="boilerplate-scoring-algorithm"><a class="header" href="#boilerplate-scoring-algorithm">Boilerplate Scoring Algorithm</a></h2>
<p>The confidence score is calculated using weighted signals (src/organization/boilerplate_detector.rs:124-161):</p>
<ol>
<li>
<p><strong>High impl count (30% weight)</strong> - Files with 20+ impl blocks score higher</p>
<ul>
<li>Normalized: <code>min(impl_count / 100, 1.0) × 30%</code></li>
</ul>
</li>
<li>
<p><strong>Method uniformity (25% weight)</strong> - Methods shared across implementations</p>
<ul>
<li>Score: <code>method_uniformity × 25%</code> (if ≥ 0.7 threshold)</li>
</ul>
</li>
<li>
<p><strong>Low average complexity (20% weight)</strong> - Simple, repetitive code</p>
<ul>
<li>Score: <code>(1 - complexity / 2.0) × 20%</code> (if complexity &lt; 2.0)</li>
</ul>
</li>
<li>
<p><strong>Low complexity variance (15% weight)</strong> - Consistent complexity</p>
<ul>
<li>Score: <code>(1 - min(variance / 10.0, 1.0)) × 15%</code> (if variance &lt; 2.0)</li>
</ul>
</li>
<li>
<p><strong>Single dominant trait (10% weight)</strong> - One trait dominates</p>
<ul>
<li>Score: <code>trait_ratio × 10%</code> (if trait_ratio &gt; 0.8)</li>
</ul>
</li>
</ol>
<p><strong>Threshold</strong>: Patterns with confidence ≥ 0.7 (70%) are reported as boilerplate.</p>
<h2 id="pattern-types"><a class="header" href="#pattern-types">Pattern Types</a></h2>
<h3 id="trait-implementation-boilerplate"><a class="header" href="#trait-implementation-boilerplate">Trait Implementation Boilerplate</a></h3>
<p>Detected when a file has many similar trait implementations with low complexity.</p>
<p><strong>Example</strong> (from tests/boilerplate_integration_test.rs:14-48):</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// 26 From&lt;Format&gt; implementations with identical structure
pub enum Format { A, B, C, /* ... */ Z }
pub struct Target { name: String }

impl From&lt;Format&gt; for Target {
    fn from(f: Format) -&gt; Self {
        match f {
            Format::A =&gt; Target { name: "a".to_string() },
            Format::B =&gt; Target { name: "b".to_string() },
            // ... 24 more identical patterns
        }
    }
}

// Detected: 26 impl blocks, 1.0 method uniformity, complexity ~2.0
<span class="boring">}</span></code></pre>
<p><strong>Recommendation</strong>: Use declarative macro to reduce ~250 lines to ~30 lines.</p>
<h3 id="builder-pattern"><a class="header" href="#builder-pattern">Builder Pattern</a></h3>
<p>Detected when a file has repetitive setter methods returning <code>Self</code>.</p>
<p><strong>Example</strong> (from book/src/boilerplate-detection.md:46-61):</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>impl ConfigBuilder {
    pub fn host(mut self, host: String) -&gt; Self {
        self.host = host;
        self
    }

    pub fn port(mut self, port: u16) -&gt; Self {
        self.port = port;
        self
    }
    // ... more identical setter methods
}
<span class="boring">}</span></code></pre>
<p><strong>Recommendation</strong>: Use <code>derive_builder</code> crate or custom derive macro.</p>
<h3 id="test-boilerplate"><a class="header" href="#test-boilerplate">Test Boilerplate</a></h3>
<p>Detected when test functions have shared structure and repetitive assertions (src/organization/boilerplate_detector.rs:238-243).</p>
<p><strong>Example</strong>:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[test]
fn test_case_a() {
    let input = create_input_a();
    let result = process(input);
    assert_eq!(result.status, Status::Success);
}

#[test]
fn test_case_b() {
    let input = create_input_b();
    let result = process(input);
    assert_eq!(result.status, Status::Success);
}
// ... 20 more similar test functions
<span class="boring">}</span></code></pre>
<p><strong>Recommendation</strong>: Use parameterized tests with <code>rstest</code> or table-driven tests.</p>
<h2 id="configuration-4"><a class="header" href="#configuration-4">Configuration</a></h2>
<p>Boilerplate detection is controlled via configuration file (src/organization/boilerplate_detector.rs:256-322):</p>
<pre><code class="language-toml">[boilerplate_detection]
# Enable boilerplate detection (default: true)
enabled = true

# Minimum impl blocks to consider (default: 20)
min_impl_blocks = 20

# Method uniformity threshold 0.0-1.0 (default: 0.7)
method_uniformity_threshold = 0.7

# Maximum average complexity for boilerplate (default: 2.0)
max_avg_complexity = 2.0

# Minimum confidence to report 0.0-1.0 (default: 0.7)
confidence_threshold = 0.7

# Enable trait implementation detection (default: true)
detect_trait_impls = true

# Enable builder pattern detection (default: true)
detect_builders = true

# Enable test boilerplate detection (default: true)
detect_test_boilerplate = true
</code></pre>
<p><strong>Field reference</strong> (src/organization/boilerplate_detector.rs:48-57, 310-322):</p>
<ul>
<li>All fields have serde defaults</li>
<li>Configuration can be provided via TOML or JSON</li>
<li>Missing fields use default values</li>
</ul>
<h2 id="usage"><a class="header" href="#usage">Usage</a></h2>
<p>Boilerplate detection runs automatically when enabled in configuration:</p>
<pre><code class="language-bash"># Run analysis with default configuration
debtmap analyze .

# Use custom config file
debtmap analyze . --config custom-config.toml

# Show where configuration values came from
debtmap analyze . --show-config-sources
</code></pre>
<p><strong>Note</strong>: There are no dedicated CLI flags like <code>--detect-boilerplate</code> or <code>--show-macro-suggestions</code>. Boilerplate detection is integrated into the god object analysis pipeline and controlled via configuration file only (verified in src/cli.rs - no boilerplate-specific flags exist).</p>
<h2 id="macro-recommendations"><a class="header" href="#macro-recommendations">Macro Recommendations</a></h2>
<p>The MacroRecommendationEngine generates specific refactoring guidance (src/organization/macro_recommendations.rs:13-150):</p>
<h3 id="for-trait-implementations"><a class="header" href="#for-trait-implementations">For Trait Implementations</a></h3>
<pre><code>Detected boilerplate: 25 implementations of From trait
Estimated line reduction: 220 lines → 35 lines (84% reduction)

Recommendation:
- Use declarative macro (macro_rules!) for simple conversions
- Use procedural derive macro for complex transformations
- Consider code generation in build.rs for large enums
</code></pre>
<h3 id="for-builder-patterns"><a class="header" href="#for-builder-patterns">For Builder Patterns</a></h3>
<pre><code>Detected boilerplate: 15 setter methods in ConfigBuilder
Estimated line reduction: 75 lines → 10 lines (87% reduction)

Recommendation:
- Add derive_builder to Cargo.toml
- Use #[derive(Builder)] on struct
- Configure with #[builder(setter(into))] for ergonomics
</code></pre>
<h3 id="for-test-boilerplate"><a class="header" href="#for-test-boilerplate">For Test Boilerplate</a></h3>
<pre><code>Detected boilerplate: 20 similar test functions
Estimated line reduction: 120 lines → 25 lines (79% reduction)

Recommendation:
- Use rstest with #[rstest] and #[case(...)] for parameterized tests
- Extract common test setup into fixture functions
- Use table-driven tests with Vec&lt;TestCase&gt; for data-driven testing
</code></pre>
<h2 id="integration-with-god-object-detection"><a class="header" href="#integration-with-god-object-detection">Integration with God Object Detection</a></h2>
<p>Boilerplate detection prevents false positives in god object analysis:</p>
<ol>
<li><strong>File with many impl blocks detected</strong> → Analyze trait patterns</li>
<li><strong>High complexity + many impls</strong> → Classified as GodObject (needs module splitting)</li>
<li><strong>Low complexity + many impls</strong> → Classified as BoilerplatePattern (needs macro-ification)</li>
<li><strong>Builder pattern detected</strong> → Classified as BuilderPattern (intentional design)</li>
</ol>
<p>This distinction ensures appropriate recommendations for different code patterns.</p>
<p><strong>Source</strong>: src/organization/god_object/classification_types.rs:45-50</p>
<h2 id="see-also-3"><a class="header" href="#see-also-3">See Also</a></h2>
<ul>
<li><a href="#god-object-detection-1">God Object Detection</a> - Complexity-based refactoring</li>
<li><a href="#design-pattern-detection">Design Pattern Detection</a> - Higher-level pattern recognition</li>
<li><a href="#boilerplate-vs-complexity">Boilerplate vs Complexity</a> - Understanding the distinction</li>
<li><a href="#configuration-2">Configuration</a> - Full configuration reference</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="boilerplate-vs-complexity"><a class="header" href="#boilerplate-vs-complexity">Boilerplate vs Complexity</a></h1>
<h2 id="overview-7"><a class="header" href="#overview-7">Overview</a></h2>
<p>Debtmap distinguishes between <strong>boilerplate code</strong> (necessary but mechanical patterns) and <strong>true complexity</strong> (business logic requiring cognitive effort). This distinction is critical for:</p>
<ul>
<li>Avoiding false positives in complexity analysis</li>
<li>Focusing refactoring efforts on actual problems</li>
<li>Understanding which high-complexity code is acceptable</li>
<li>Providing actionable recommendations</li>
</ul>
<p>This chapter explains how Debtmap identifies boilerplate patterns, why they differ from complexity, and how to interpret the analysis results.</p>
<h2 id="the-distinction"><a class="header" href="#the-distinction">The Distinction</a></h2>
<h3 id="what-is-boilerplate"><a class="header" href="#what-is-boilerplate">What is Boilerplate?</a></h3>
<p>Boilerplate code consists of repetitive, mechanical patterns that are:</p>
<ol>
<li><strong>Required by language/framework</strong> - Type conversions, trait implementations, builder patterns</li>
<li><strong>Structurally necessary</strong> - Match arms for enums, error propagation, validation chains</li>
<li><strong>Low cognitive load</strong> - Pattern-based code that developers scan rather than deeply analyze</li>
<li><strong>Not actual complexity</strong> - High cyclomatic complexity but mechanistic structure</li>
</ol>
<p><strong>Examples:</strong></p>
<ul>
<li><code>From</code> trait implementations converting between types</li>
<li><code>Display</code> formatting with exhaustive enum match arms</li>
<li>Builder pattern setters with validation</li>
<li>Error conversion implementations</li>
<li>Serialization/deserialization code</li>
</ul>
<h3 id="what-is-true-complexity"><a class="header" href="#what-is-true-complexity">What is True Complexity?</a></h3>
<p>True complexity consists of business logic that requires:</p>
<ol>
<li><strong>Domain understanding</strong> - Knowledge of problem space and requirements</li>
<li><strong>Cognitive effort</strong> - Careful analysis to understand behavior</li>
<li><strong>Algorithmic decisions</strong> - Non-obvious control flow or data transformations</li>
<li><strong>Maintainability risk</strong> - Changes may introduce subtle bugs</li>
</ol>
<p><strong>Examples:</strong></p>
<ul>
<li>Graph traversal algorithms</li>
<li>Complex business rules with multiple conditions</li>
<li>State machine implementations with non-trivial transitions</li>
<li>Performance-critical optimizations</li>
<li>Error recovery with fallback strategies</li>
</ul>
<h2 id="real-example-ripgreps-defsrs"><a class="header" href="#real-example-ripgreps-defsrs">Real Example: ripgrep’s defs.rs</a></h2>
<p>The ripgrep codebase provides an excellent real-world example of boilerplate vs complexity.</p>
<h3 id="file-cratesprintersrcdefsrs"><a class="header" href="#file-cratesprintersrcdefsrs">File: <code>crates/printer/src/defs.rs</code></a></h3>
<p>This file contains type conversion implementations with high cyclomatic complexity scores but minimal actual complexity:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>impl From&lt;HyperlinkFormat&gt; for ColorHyperlink {
    fn from(format: HyperlinkFormat) -&gt; ColorHyperlink {
        match format {
            HyperlinkFormat::Default =&gt; ColorHyperlink::default(),
            HyperlinkFormat::Grep =&gt; ColorHyperlink::grep(),
            HyperlinkFormat::GrepPlus =&gt; ColorHyperlink::grep_plus(),
            HyperlinkFormat::Ripgrep =&gt; ColorHyperlink::ripgrep(),
            HyperlinkFormat::FileNone =&gt; ColorHyperlink::file_none(),
            // ... 10+ more variants
        }
    }
}
<span class="boring">}</span></code></pre>
<p><strong>Analysis:</strong></p>
<ul>
<li><strong>Cyclomatic Complexity</strong>: 15+ (one branch per enum variant)</li>
<li><strong>Cognitive Complexity</strong>: Low (simple delegation pattern)</li>
<li><strong>Boilerplate Confidence</strong>: 95% (trait implementation with mechanical structure)</li>
</ul>
<h3 id="why-this-matters"><a class="header" href="#why-this-matters">Why This Matters</a></h3>
<p>Without boilerplate detection, this file would be flagged as:</p>
<ul>
<li>High complexity debt</li>
<li>Requiring refactoring</li>
<li>Priority for review</li>
</ul>
<p>With boilerplate detection, it’s correctly classified as:</p>
<ul>
<li>Necessary type conversion code</li>
<li>Low maintenance risk</li>
<li>Can be safely skipped in debt prioritization</li>
</ul>
<h2 id="detection-methodology"><a class="header" href="#detection-methodology">Detection Methodology</a></h2>
<p>Debtmap uses a multi-phase analysis pipeline to detect boilerplate:</p>
<h3 id="phase-1-trait-analysis"><a class="header" href="#phase-1-trait-analysis">Phase 1: Trait Analysis</a></h3>
<p>Identifies trait implementations known to produce boilerplate:</p>
<p><strong>High-confidence boilerplate traits:</strong></p>
<ul>
<li><code>From</code>, <code>Into</code> - Type conversions</li>
<li><code>Display</code>, <code>Debug</code> - Formatting</li>
<li><code>Default</code> - Default value construction</li>
<li><code>Clone</code>, <code>Copy</code> - Value semantics</li>
<li><code>Eq</code>, <code>PartialEq</code>, <code>Ord</code>, <code>PartialOrd</code> - Comparisons</li>
<li><code>Hash</code> - Hashing implementations</li>
</ul>
<p><strong>Medium-confidence boilerplate traits:</strong></p>
<ul>
<li><code>Serialize</code>, <code>Deserialize</code> - Serialization</li>
<li><code>AsRef</code>, <code>AsMut</code>, <code>Deref</code>, <code>DerefMut</code> - Reference conversions</li>
<li>Custom builder traits</li>
</ul>
<p>See <code>src/debt/boilerplate/boilerplate_traits.rs:10-58</code> for complete trait categorization.</p>
<h3 id="phase-2-pattern-analysis"><a class="header" href="#phase-2-pattern-analysis">Phase 2: Pattern Analysis</a></h3>
<p>Analyzes code structure for boilerplate patterns:</p>
<p><strong>Pattern 1: Simple Delegation</strong></p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn operation(&amp;self) -&gt; Result&lt;T&gt; {
    self.inner.operation()  // Single delegation call
}
<span class="boring">}</span></code></pre>
<p>Score: 90% confidence</p>
<p><strong>Pattern 2: Trivial Match Arms</strong></p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>match variant {
    A =&gt; handler_a(),
    B =&gt; handler_b(),
    C =&gt; handler_c(),
}
<span class="boring">}</span></code></pre>
<p>Each arm calls a single function with no additional logic.
Score: 85% confidence</p>
<p><strong>Pattern 3: Validation Chains</strong></p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn validate(&amp;self) -&gt; Result&lt;()&gt; {
    check_condition_1()?;
    check_condition_2()?;
    check_condition_3()?;
    Ok(())
}
<span class="boring">}</span></code></pre>
<p>Sequential validation with early returns.
Score: 75% confidence</p>
<p><strong>Pattern 4: Builder Setters</strong></p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub fn with_field(mut self, value: T) -&gt; Self {
    self.field = value;
    self
}
<span class="boring">}</span></code></pre>
<p>Simple field assignment with fluent return.
Score: 95% confidence</p>
<p>See <code>src/debt/boilerplate/pattern_detector.rs:18-82</code> for pattern detection logic.</p>
<h3 id="phase-3-macro-analysis"><a class="header" href="#phase-3-macro-analysis">Phase 3: Macro Analysis</a></h3>
<p>Detects macro-generated code and provides recommendations:</p>
<p><strong>Derivable Traits:</strong>
Debtmap suggests using <code>#[derive(...)]</code> when it detects manual implementations of:</p>
<ul>
<li><code>Clone</code>, <code>Copy</code>, <code>Debug</code>, <code>Default</code></li>
<li><code>Eq</code>, <code>PartialEq</code>, <code>Ord</code>, <code>PartialOrd</code></li>
<li><code>Hash</code></li>
</ul>
<p><strong>Custom Macros:</strong>
Recommends creating custom derive macros for:</p>
<ul>
<li>Repeated builder pattern implementations</li>
<li>Repeated conversion trait implementations</li>
<li>Repeated validation logic</li>
</ul>
<p><strong>Existing Crates:</strong>
Suggests established crates for common patterns:</p>
<ul>
<li><code>derive_more</code> - Extended derive macros</li>
<li><code>thiserror</code> - Error type boilerplate</li>
<li><code>typed-builder</code> - Builder pattern macros</li>
<li><code>delegate</code> - Delegation patterns</li>
</ul>
<p>See <code>src/debt/boilerplate/macro_recommender.rs:9-136</code> for macro recommendation logic.</p>
<h2 id="common-boilerplate-patterns"><a class="header" href="#common-boilerplate-patterns">Common Boilerplate Patterns</a></h2>
<h3 id="type-conversions"><a class="header" href="#type-conversions">Type Conversions</a></h3>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// High complexity (15+), but boilerplate
impl From&lt;ConfigFormat&gt; for Config {
    fn from(format: ConfigFormat) -&gt; Config {
        match format {
            ConfigFormat::Json =&gt; Config::json(),
            ConfigFormat::Yaml =&gt; Config::yaml(),
            ConfigFormat::Toml =&gt; Config::toml(),
            // ... many variants
        }
    }
}
<span class="boring">}</span></code></pre>
<p><strong>Boilerplate Confidence</strong>: 90%+
<strong>Recommendation</strong>: Consider using a macro if pattern repeats</p>
<h3 id="error-propagation"><a class="header" href="#error-propagation">Error Propagation</a></h3>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// High nesting, but boilerplate pattern
fn complex_operation(&amp;self) -&gt; Result&lt;Output&gt; {
    let step1 = self.step_one()
        .context("Step one failed")?;
    let step2 = self.step_two(&amp;step1)
        .context("Step two failed")?;
    let step3 = self.step_three(&amp;step2)
        .context("Step three failed")?;
    Ok(Output::new(step3))
}
<span class="boring">}</span></code></pre>
<p><strong>Boilerplate Confidence</strong>: 75%
<strong>Recommendation</strong>: Acceptable pattern for error handling</p>
<h3 id="builder-patterns"><a class="header" href="#builder-patterns">Builder Patterns</a></h3>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Many methods, but all boilerplate
impl ConfigBuilder {
    pub fn with_timeout(mut self, timeout: Duration) -&gt; Self {
        self.timeout = Some(timeout);
        self
    }

    pub fn with_retries(mut self, retries: u32) -&gt; Self {
        self.retries = Some(retries);
        self
    }

    // ... 20+ more setters
}
<span class="boring">}</span></code></pre>
<p><strong>Boilerplate Confidence</strong>: 95%
<strong>Recommendation</strong>: Use <code>typed-builder</code> or similar crate</p>
<h3 id="display-formatting"><a class="header" href="#display-formatting">Display Formatting</a></h3>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// High complexity due to match, but boilerplate
impl Display for Status {
    fn fmt(&amp;self, f: &amp;mut Formatter) -&gt; fmt::Result {
        match self {
            Status::Pending =&gt; write!(f, "pending"),
            Status::Running =&gt; write!(f, "running"),
            Status::Success =&gt; write!(f, "success"),
            Status::Failed(err) =&gt; write!(f, "failed: {}", err),
            // ... many variants
        }
    }
}
<span class="boring">}</span></code></pre>
<p><strong>Boilerplate Confidence</strong>: 90%
<strong>Recommendation</strong>: Consider using <code>strum</code> or <code>derive_more</code></p>
<h2 id="decision-table"><a class="header" href="#decision-table">Decision Table</a></h2>
<p>Use this table to interpret boilerplate confidence scores:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Confidence</th><th>Interpretation</th><th>Action</th></tr>
</thead>
<tbody>
<tr><td>90-100%</td><td>Definite boilerplate</td><td>Exclude from complexity prioritization; consider macro optimization</td></tr>
<tr><td>70-89%</td><td>Probable boilerplate</td><td>Review pattern; likely acceptable; low refactoring priority</td></tr>
<tr><td>50-69%</td><td>Mixed boilerplate/logic</td><td>Investigate; may contain hidden complexity; medium priority</td></tr>
<tr><td>30-49%</td><td>Mostly real complexity</td><td>Standard complexity analysis; normal refactoring priority</td></tr>
<tr><td>0-29%</td><td>True complexity</td><td>High priority; focus refactoring efforts here</td></tr>
</tbody>
</table>
</div>
<h3 id="example-classifications"><a class="header" href="#example-classifications">Example Classifications</a></h3>
<p><strong>Boilerplate (90%+ confidence):</strong></p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Simple trait delegation - skip in debt analysis
impl AsRef&lt;str&gt; for CustomString {
    fn as_ref(&amp;self) -&gt; &amp;str {
        &amp;self.inner
    }
}
<span class="boring">}</span></code></pre>
<p><strong>Mixed (50-70% confidence):</strong></p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Match with some logic - review case by case
fn process_event(&amp;mut self, event: Event) -&gt; Result&lt;()&gt; {
    match event {
        Event::Simple =&gt; self.handle_simple(),  // Boilerplate
        Event::Complex(data) =&gt; {                // Real logic
            if data.priority &gt; 10 {
                self.handle_urgent(data)?;
            } else {
                self.queue_normal(data)?;
            }
            self.update_metrics()?;
            Ok(())
        }
    }
}
<span class="boring">}</span></code></pre>
<p><strong>True Complexity (0-30% confidence):</strong></p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Business logic requiring domain knowledge
fn calculate_optimal_strategy(&amp;self, market: &amp;Market) -&gt; Strategy {
    let volatility = market.calculate_volatility();
    let trend = market.detect_trend();

    if volatility &gt; self.risk_threshold {
        if trend.is_bullish() &amp;&amp; self.can_hedge() {
            Strategy::hedged_long(self.calculate_position_size())
        } else {
            Strategy::defensive()
        }
    } else {
        Strategy::momentum_based(trend, self.confidence_level())
    }
}
<span class="boring">}</span></code></pre>
<h2 id="integration-with-complexity-analysis"><a class="header" href="#integration-with-complexity-analysis">Integration with Complexity Analysis</a></h2>
<h3 id="boilerplate-scoring"><a class="header" href="#boilerplate-scoring">Boilerplate Scoring</a></h3>
<p>Debtmap calculates a <code>BoilerplateScore</code> for each function:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct BoilerplateScore {
    pub confidence: f64,              // 0.0-1.0 (0% to 100%)
    pub primary_pattern: Pattern,     // Strongest detected pattern
    pub contributing_patterns: Vec&lt;Pattern&gt;,
    pub macro_recommendation: Option&lt;MacroRecommendation&gt;,
}
<span class="boring">}</span></code></pre>
<h3 id="complexity-adjustment"><a class="header" href="#complexity-adjustment">Complexity Adjustment</a></h3>
<p>High-confidence boilerplate reduces effective complexity:</p>
<pre><code>effective_complexity = raw_complexity × (1.0 - boilerplate_confidence)
</code></pre>
<p><strong>Example:</strong></p>
<ul>
<li>Raw cyclomatic complexity: 15</li>
<li>Boilerplate confidence: 0.90 (90%)</li>
<li>Effective complexity: 15 × (1.0 - 0.90) = 1.5</li>
</ul>
<p>This prevents boilerplate from dominating debt prioritization.</p>
<h3 id="output-display"><a class="header" href="#output-display">Output Display</a></h3>
<p>Debtmap annotates boilerplate functions in analysis output:</p>
<pre><code>src/types/conversions.rs:
  ├─ from (complexity: 15, boilerplate: 92%)
  │    Pattern: Trait Implementation (From)
  │    Recommendation: Consider #[derive(From)] via derive_more
  │    Priority: Low (boilerplate)

  ├─ process_request (complexity: 12, boilerplate: 15%)
  │    Priority: High (true complexity)
</code></pre>
<h2 id="best-practices-5"><a class="header" href="#best-practices-5">Best Practices</a></h2>
<h3 id="when-to-accept-boilerplate"><a class="header" href="#when-to-accept-boilerplate">When to Accept Boilerplate</a></h3>
<p><strong>Accept</strong> high-complexity boilerplate when:</p>
<ol>
<li><strong>Required by language</strong> - Trait implementations, type conversions</li>
<li><strong>Pattern is clear</strong> - Developers can scan quickly without deep analysis</li>
<li><strong>Covered by tests</strong> - Mechanical patterns verified by unit tests</li>
<li><strong>No simpler alternative</strong> - Refactoring would reduce clarity</li>
</ol>
<p><strong>Example:</strong> Exhaustive match arms for enum variants with simple delegation.</p>
<h3 id="when-to-refactor-boilerplate"><a class="header" href="#when-to-refactor-boilerplate">When to Refactor Boilerplate</a></h3>
<p><strong>Refactor</strong> boilerplate when:</p>
<ol>
<li><strong>Pattern repeats extensively</strong> - 10+ similar implementations</li>
<li><strong>Macro alternative exists</strong> - Can use derive or custom macro</li>
<li><strong>Maintenance burden</strong> - Changes require updating many copies</li>
<li><strong>Error-prone</strong> - Manual pattern increases bug risk</li>
</ol>
<p><strong>Example:</strong> 50+ builder setters that could use <code>typed-builder</code> crate.</p>
<h3 id="configuring-thresholds"><a class="header" href="#configuring-thresholds">Configuring Thresholds</a></h3>
<p>Adjust boilerplate sensitivity in <code>.debtmap.toml</code>:</p>
<pre><code class="language-toml">[boilerplate_detection]
enabled = true
min_confidence_to_exclude = 0.85  # Only exclude 85%+ confidence
trait_delegation_threshold = 0.90  # Trait impl confidence
pattern_match_threshold = 0.75     # Match pattern confidence
</code></pre>
<p><strong>Strict mode</strong> (minimize false negatives):</p>
<pre><code class="language-toml">min_confidence_to_exclude = 0.95  # Very high bar for exclusion
</code></pre>
<p><strong>Lenient mode</strong> (minimize false positives):</p>
<pre><code class="language-toml">min_confidence_to_exclude = 0.70  # More aggressive exclusion
</code></pre>
<h2 id="validation-and-testing"><a class="header" href="#validation-and-testing">Validation and Testing</a></h2>
<h3 id="integration-test-example"><a class="header" href="#integration-test-example">Integration Test Example</a></h3>
<p>Debtmap’s test suite includes real-world boilerplate validation:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[test]
fn test_ripgrep_defs_boilerplate() {
    let code = r#"
        impl From&lt;HyperlinkFormat&gt; for ColorHyperlink {
            fn from(format: HyperlinkFormat) -&gt; ColorHyperlink {
                match format {
                    HyperlinkFormat::Default =&gt; ColorHyperlink::default(),
                    // ... 15 variants
                }
            }
        }
    "#;

    let result = analyze_boilerplate(code);
    assert!(result.confidence &gt;= 0.85, "Should detect trait boilerplate");
    assert_eq!(result.primary_pattern, Pattern::TraitImplementation);
}
<span class="boring">}</span></code></pre>
<p>See <code>tests/boilerplate_integration_test.rs</code> for complete test cases.</p>
<h3 id="performance-overhead"><a class="header" href="#performance-overhead">Performance Overhead</a></h3>
<p>Boilerplate detection adds minimal overhead:</p>
<p><strong>Measurement:</strong> &lt;5% increase in analysis time
<strong>Reason:</strong> Single-pass AST analysis with cached pattern matching
<strong>Optimization:</strong> Trait analysis uses fast HashMap lookups</p>
<p>See <code>tests/boilerplate_performance_test.rs</code> for benchmark details.</p>
<h2 id="troubleshooting-8"><a class="header" href="#troubleshooting-8">Troubleshooting</a></h2>
<h3 id="why-is-my-code-marked-as-boilerplate"><a class="header" href="#why-is-my-code-marked-as-boilerplate">“Why is my code marked as boilerplate?”</a></h3>
<p><strong>Check:</strong></p>
<ol>
<li>Is it a trait implementation? (From, Display, etc.)</li>
<li>Does it follow a mechanical pattern?</li>
<li>Are all branches simple delegations?</li>
</ol>
<p><strong>If incorrectly classified:</strong></p>
<ul>
<li>Adjust <code>min_confidence_to_exclude</code> threshold</li>
<li>Report false positive if confidence is very high</li>
</ul>
<h3 id="my-boilerplate-isnt-detected"><a class="header" href="#my-boilerplate-isnt-detected">“My boilerplate isn’t detected”</a></h3>
<p><strong>Common causes:</strong></p>
<ol>
<li>Custom logic mixed with boilerplate pattern</li>
<li>Non-standard trait names</li>
<li>Complex match arm logic</li>
</ol>
<p><strong>Solutions:</strong></p>
<ul>
<li>Extract pure boilerplate into separate functions</li>
<li>Use standard traits when possible</li>
<li>Check confidence score - may be detected with lower confidence</li>
</ul>
<h3 id="boilerplate-detection-seems-too-aggressive"><a class="header" href="#boilerplate-detection-seems-too-aggressive">“Boilerplate detection seems too aggressive”</a></h3>
<p><strong>Adjust configuration:</strong></p>
<pre><code class="language-toml">[boilerplate_detection]
min_confidence_to_exclude = 0.95  # Raise threshold
trait_delegation_threshold = 0.95
</code></pre>
<h2 id="related-documentation-1"><a class="header" href="#related-documentation-1">Related Documentation</a></h2>
<ul>
<li><a href="#metrics-reference">Complexity Metrics</a> - Understanding cyclomatic complexity</li>
<li><a href="#configuration-2">Configuration</a> - Complete <code>.debtmap.toml</code> reference</li>
<li><a href="#tiered-prioritization-3">Tiered Prioritization</a> - How boilerplate affects debt ranking</li>
</ul>
<h2 id="summary-2"><a class="header" href="#summary-2">Summary</a></h2>
<p>Boilerplate detection is a critical feature that:</p>
<ul>
<li>Distinguishes mechanical patterns from true complexity</li>
<li>Reduces false positives in debt analysis</li>
<li>Provides actionable macro recommendations</li>
<li>Integrates seamlessly with complexity scoring</li>
<li>Helps teams focus on real maintainability issues</li>
</ul>
<p>By identifying boilerplate with 85%+ confidence, Debtmap ensures that high-complexity scores reflect actual cognitive burden rather than necessary but mechanical code patterns.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="call-graph-analysis"><a class="header" href="#call-graph-analysis">Call Graph Analysis</a></h1>
<p>Debtmap constructs detailed call graphs to track function relationships and dependencies across your codebase. This enables critical path identification, circular dependency detection, and transitive coverage propagation.</p>
<h2 id="overview-8"><a class="header" href="#overview-8">Overview</a></h2>
<p>Call graph analysis builds a comprehensive map of which functions call which other functions. This information powers several key features:</p>
<ul>
<li><strong>Critical path identification</strong> - Find frequently-called functions that deserve extra attention</li>
<li><strong>Circular dependency detection</strong> - Identify problematic circular call patterns</li>
<li><strong>Transitive coverage</strong> - Propagate test coverage through the call graph</li>
<li><strong>Dependency visualization</strong> - See caller/callee relationships in output</li>
<li><strong>Risk assessment</strong> - Factor calling patterns into priority scoring</li>
</ul>
<h2 id="call-graph-construction"><a class="header" href="#call-graph-construction">Call Graph Construction</a></h2>
<p>Debtmap builds call graphs through a three-phase AST-based construction process:</p>
<ol>
<li><strong>Extract functions and collect unresolved calls</strong> - Parse each file to identify function definitions and call expressions</li>
<li><strong>Resolve calls using CallResolver and PathResolver</strong> - Match call expressions to function definitions within the same file</li>
<li><strong>Final cross-file resolution</strong> - Resolve remaining calls across module boundaries</li>
</ol>
<p>This multi-phase approach ensures accurate resolution while handling complex scenarios like trait methods, macros, and module imports.</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Example: Debtmap tracks these relationships
fn process_data(input: &amp;str) -&gt; Result&lt;Data&gt; {
    validate_input(input)?;  // Call edge: process_data -&gt; validate_input
    parse_data(input)        // Call edge: process_data -&gt; parse_data
}

fn validate_input(input: &amp;str) -&gt; Result&lt;()&gt; {
    // Call graph tracks this function as a callee
    Ok(())
}
<span class="boring">}</span></code></pre>
<p><strong>Source</strong>: Example pattern from tests/call_graph_comprehensive_test.rs:48-94</p>
<h3 id="resolution-mechanisms"><a class="header" href="#resolution-mechanisms">Resolution Mechanisms</a></h3>
<p>The call graph analyzer handles complex resolution scenarios:</p>
<ul>
<li><strong>Trait method resolution</strong> - Resolves trait method calls to implementations using struct prefixes (e.g., <code>Processor::process</code>)</li>
<li><strong>Macro expansion tracking</strong> - Classifies and tracks calls within macros (collection, formatting, assertion, and logging macros)</li>
<li><strong>Module path resolution</strong> - Resolves fully-qualified paths across module boundaries</li>
<li><strong>Cross-file resolution</strong> - Matches unresolved calls (marked with line 0) to actual function definitions</li>
</ul>
<p><strong>Source</strong>: Resolution mechanisms from src/analyzers/call_graph/trait_handling.rs, src/analyzers/call_graph/macro_expansion.rs, src/analyzers/call_graph/path_resolver.rs</p>
<h3 id="parallel-construction"><a class="header" href="#parallel-construction">Parallel Construction</a></h3>
<p>Call graph construction runs in parallel by default for improved performance. You can disable parallel processing with <code>--no-parallel</code> for debugging purposes, though this affects overall analysis performance, not just call graph construction.</p>
<h2 id="configuration-5"><a class="header" href="#configuration-5">Configuration</a></h2>
<p>Call graph behavior is controlled through two configuration sections:</p>
<h3 id="analysis-settings"><a class="header" href="#analysis-settings">Analysis Settings</a></h3>
<p>Configure advanced analysis features in the <code>[analysis]</code> section:</p>
<pre><code class="language-toml">[analysis]
# Enable trait method resolution (default: depends on context)
enable_trait_analysis = true

# Enable function pointer and closure tracking (default: depends on context)
enable_function_pointer_tracking = true

# Enable framework pattern detection for tests and handlers (default: depends on context)
enable_framework_patterns = true

# Enable cross-module dependency analysis (default: depends on context)
enable_cross_module_analysis = true

# Maximum depth for transitive analysis (optional)
max_analysis_depth = 10
</code></pre>
<p><strong>Source</strong>: Configuration fields from src/config/core.rs:149-167 (AnalysisSettings)</p>
<h3 id="callercallee-display-settings"><a class="header" href="#callercallee-display-settings">Caller/Callee Display Settings</a></h3>
<p>Configure how dependencies are displayed in the <code>[classification.caller_callee]</code> section:</p>
<pre><code class="language-toml">[classification.caller_callee]
# Maximum number of callers to display per function (default: 5)
max_callers = 5

# Maximum number of callees to display per function (default: 5)
max_callees = 5

# Show external crate calls in dependencies (default: false)
show_external = false

# Show standard library calls in dependencies (default: false)
show_std_lib = false
</code></pre>
<p><strong>Source</strong>: Configuration fields from src/config/classification.rs:5-50 (CallerCalleeConfig)</p>
<h2 id="cli-reference-1"><a class="header" href="#cli-reference-1">CLI Reference</a></h2>
<h3 id="display-control-flags"><a class="header" href="#display-control-flags">Display Control Flags</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Flag</th><th>Default</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>--show-dependencies</code></td><td>false</td><td>Show dependency information (callers/callees) in output</td></tr>
<tr><td><code>--no-dependencies</code></td><td>false</td><td>Hide dependency information (conflicts with –show-dependencies)</td></tr>
<tr><td><code>--max-callers &lt;N&gt;</code></td><td>5</td><td>Maximum number of callers to display per function</td></tr>
<tr><td><code>--max-callees &lt;N&gt;</code></td><td>5</td><td>Maximum number of callees to display per function</td></tr>
<tr><td><code>--show-external-calls</code></td><td>false</td><td>Show external crate calls in dependencies</td></tr>
<tr><td><code>--show-std-lib-calls</code></td><td>false</td><td>Show standard library calls in dependencies</td></tr>
</tbody>
</table>
</div>
<h3 id="analysis-control-flags"><a class="header" href="#analysis-control-flags">Analysis Control Flags</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Flag</th><th>Default</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>--no-parallel</code></td><td>false</td><td>Disable parallel processing (enabled by default)</td></tr>
</tbody>
</table>
</div>
<h3 id="debug-and-validation-flags"><a class="header" href="#debug-and-validation-flags">Debug and Validation Flags</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Flag</th><th>Default</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>--debug-call-graph</code></td><td>false</td><td>Enable detailed call graph debugging output</td></tr>
<tr><td><code>--validate-call-graph</code></td><td>false</td><td>Validate call graph structure and report issues</td></tr>
<tr><td><code>--call-graph-stats</code></td><td>false</td><td>Show call graph statistics with resolution percentiles (p50, p95, p99)</td></tr>
<tr><td><code>--trace-function &lt;NAMES&gt;</code></td><td>none</td><td>Trace specific functions during call resolution (comma-separated)</td></tr>
</tbody>
</table>
</div>
<p><strong>Source</strong>: CLI flags from src/cli.rs:163-289</p>
<h2 id="usage-1"><a class="header" href="#usage-1">Usage</a></h2>
<h3 id="basic-call-graph-analysis"><a class="header" href="#basic-call-graph-analysis">Basic Call Graph Analysis</a></h3>
<pre><code class="language-bash"># Analyze with call graph enabled (default)
debtmap analyze .

# Show caller/callee relationships in output
debtmap analyze . --show-dependencies

# Limit displayed relationships
debtmap analyze . --show-dependencies --max-callers 3 --max-callees 3
</code></pre>
<h3 id="filtering-external-calls"><a class="header" href="#filtering-external-calls">Filtering External Calls</a></h3>
<p>By default, Debtmap filters both external crate calls and standard library calls for cleaner output. The call graph contains all edges; filtering only affects display output.</p>
<pre><code class="language-bash"># Default: external and standard library calls are hidden
debtmap analyze .

# Show external crate calls (e.g., from dependencies)
debtmap analyze . --show-dependencies --show-external-calls

# Show standard library calls (std::, core::, alloc::)
debtmap analyze . --show-dependencies --show-std-lib-calls

# Show both external and standard library calls
debtmap analyze . --show-dependencies --show-external-calls --show-std-lib-calls
</code></pre>
<p><strong>Important</strong>: External call filtering happens at display time, not during graph construction. This means <code>--debug-call-graph</code> may show more calls than regular output.</p>
<p><strong>Source</strong>: Filtering logic from src/priority/formatter/dependencies.rs:filter_dependencies</p>
<h3 id="debugging-call-resolution"><a class="header" href="#debugging-call-resolution">Debugging Call Resolution</a></h3>
<pre><code class="language-bash"># Enable detailed call graph debugging
debtmap analyze . --debug-call-graph

# Trace specific functions during resolution
debtmap analyze . --trace-function "process_data,validate_input"

# Show call graph statistics with percentiles
debtmap analyze . --call-graph-stats

# Validate call graph structure
debtmap analyze . --validate-call-graph

# Disable parallel processing for debugging
debtmap analyze . --no-parallel
</code></pre>
<h3 id="debug-output-format"><a class="header" href="#debug-output-format">Debug Output Format</a></h3>
<p>Debug output includes:</p>
<ul>
<li><strong>Resolution statistics</strong> - Success rates with percentiles (p50, p95, p99)</li>
<li><strong>Timing information</strong> - Performance metrics for each resolution phase</li>
<li><strong>Function tracing</strong> - Detailed resolution attempts for specified functions</li>
<li><strong>Unresolved calls</strong> - Calls that couldn’t be matched to definitions</li>
</ul>
<p>Macro expansion statistics show classification breakdown (collection macros, formatting macros, assertion macros, logging macros).</p>
<p><strong>Source</strong>: Debug capabilities from src/analyzers/call_graph/debug.rs (DebugConfig, ResolutionStatistics)</p>
<h2 id="visualization"><a class="header" href="#visualization">Visualization</a></h2>
<p>Call graph information appears in output using Unicode tree-style rendering:</p>
<pre><code>├─ DEPENDENCIES:
│  ├─ Called by (2):
│  │     * main
│  │     * handle_request
│  │     ... (showing 2 of 2)
│  ├─ Calls (3):
│       * validate_input
│       * parse_data
│       * transform
│       ... (showing 3 of 5)
</code></pre>
<p><strong>Source</strong>: Tree-style rendering from src/priority/formatter/sections.rs:240-329</p>
<h3 id="path-simplification"><a class="header" href="#path-simplification">Path Simplification</a></h3>
<p>Long paths are simplified for readability:</p>
<ul>
<li>Short names: unchanged (e.g., <code>my_function</code>)</li>
<li>Two-segment paths: unchanged (e.g., <code>helper::read_file</code>)</li>
<li>Long paths: simplified to last two segments (e.g., <code>crate::utils::io::helper::read_file</code> → <code>helper::read_file</code>)</li>
</ul>
<h3 id="empty-states"><a class="header" href="#empty-states">Empty States</a></h3>
<ul>
<li><strong>No callers</strong>: “Called by: No direct callers detected”</li>
<li><strong>No callees</strong>: “Calls: Calls no other functions”</li>
</ul>
<h3 id="standard-library-detection"><a class="header" href="#standard-library-detection">Standard Library Detection</a></h3>
<p>Standard library calls are filtered by default and include:</p>
<ul>
<li>Functions starting with <code>std::</code>, <code>core::</code>, or <code>alloc::</code></li>
<li>Common macros: <code>println</code>, <code>print</code>, <code>eprintln</code>, <code>eprint</code>, <code>write</code>, <code>writeln</code>, <code>format</code>, <code>panic</code>, <code>assert</code>, <code>debug_assert</code></li>
</ul>
<p>External crate calls are identified as functions containing <code>::</code> that aren’t in the standard library or the current crate (<code>crate::</code>).</p>
<p><strong>Source</strong>: Detection logic from src/priority/formatter/dependencies.rs:is_standard_library_call, is_external_crate_call</p>
<h2 id="validation-and-health-scoring"><a class="header" href="#validation-and-health-scoring">Validation and Health Scoring</a></h2>
<p>The call graph validator checks for structural issues:</p>
<pre><code class="language-bash">debtmap analyze . --validate-call-graph
</code></pre>
<p>Validation reports include:</p>
<ul>
<li><strong>Health score</strong> - Overall graph quality (0-100)</li>
<li><strong>Structural issues</strong> - Orphaned functions, disconnected components</li>
<li><strong>Warnings</strong> - Potential resolution problems</li>
</ul>
<p><strong>Source</strong>: Validation implementation from tests/call_graph_debug_output_test.rs:134-151</p>
<h2 id="performance-tuning-2"><a class="header" href="#performance-tuning-2">Performance Tuning</a></h2>
<p>For large codebases, consider these performance optimizations:</p>
<ul>
<li><strong>Disable parallel processing</strong> (<code>--no-parallel</code>) - Only for debugging; reduces performance</li>
<li><strong>Control analysis depth</strong> - Use <code>max_analysis_depth</code> in configuration to limit transitive analysis</li>
<li><strong>Disable optional analysis</strong> - Turn off <code>enable_trait_analysis</code>, <code>enable_function_pointer_tracking</code>, or <code>enable_framework_patterns</code> if not needed</li>
</ul>
<h2 id="troubleshooting-9"><a class="header" href="#troubleshooting-9">Troubleshooting</a></h2>
<h3 id="unresolved-calls"><a class="header" href="#unresolved-calls">Unresolved Calls</a></h3>
<p>If you see unresolved calls in debug output:</p>
<ol>
<li><strong>Check imports</strong> - Ensure all modules are properly imported</li>
<li><strong>Verify visibility</strong> - Confirm functions are accessible (not private across module boundaries)</li>
<li><strong>Review module structure</strong> - Complex module hierarchies may require explicit path configuration</li>
<li><strong>Use tracing</strong> - Run with <code>--trace-function</code> to see detailed resolution attempts</li>
</ol>
<h3 id="incorrect-callercallee-counts"><a class="header" href="#incorrect-callercallee-counts">Incorrect Caller/Callee Counts</a></h3>
<p>If counts seem wrong:</p>
<ol>
<li><strong>Check filtering</strong> - Use <code>--show-external-calls</code> and <code>--show-std-lib-calls</code> to see all edges</li>
<li><strong>Validate structure</strong> - Run <code>--validate-call-graph</code> to check for structural issues</li>
<li><strong>Review debug output</strong> - Use <code>--debug-call-graph</code> to see complete graph before filtering</li>
</ol>
<h2 id="see-also-4"><a class="header" href="#see-also-4">See Also</a></h2>
<ul>
<li><a href="#architectural-analysis">Architectural Analysis</a> - Circular dependency detection</li>
<li><a href="#context-providers">Context Providers</a> - Critical path analysis</li>
<li><a href="#coverage-integration-1">Coverage Integration</a> - Transitive coverage propagation</li>
<li><a href="#configuration-2">Configuration</a> - Complete configuration reference</li>
<li><a href="#cli-reference">CLI Reference</a> - All command-line flags</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="context-providers"><a class="header" href="#context-providers">Context Providers</a></h1>
<p>Context providers enhance debtmap’s risk analysis by incorporating additional factors beyond complexity and test coverage. They analyze critical execution paths, dependency relationships, and version control history to provide a more comprehensive understanding of technical risk.</p>
<h2 id="overview-9"><a class="header" href="#overview-9">Overview</a></h2>
<p>Context providers implement the <code>ContextProvider</code> trait, which gathers risk-relevant information about functions and modules. Each provider analyzes a specific dimension of risk:</p>
<ul>
<li><strong>Critical Path Provider</strong>: Identifies functions on critical execution paths</li>
<li><strong>Dependency Provider</strong>: Analyzes call graph relationships and blast radius</li>
<li><strong>Git History Provider</strong>: Integrates version control history for change patterns</li>
</ul>
<p>Context providers help debtmap understand:</p>
<ul>
<li>Which code paths are most critical</li>
<li>How functions depend on each other</li>
<li>Which code changes most frequently</li>
<li>Where bugs are likely to occur</li>
</ul>
<p>This context-aware analysis improves prioritization accuracy and reduces false positives.</p>
<p>The <code>ContextAggregator</code> combines context from multiple enabled providers and adjusts risk scores using the formula:</p>
<pre><code>contextual_risk = base_risk × (1.0 + context_contribution)
</code></pre>
<p>Where <code>context_contribution</code> is the weighted sum of all provider contributions:</p>
<pre><code>context_contribution = Σ(provider.contribution × provider.weight)
</code></pre>
<blockquote>
<p><strong>Configuration Note</strong>: Provider-specific TOML configuration (like <code>[context.critical_path]</code>) is a planned feature not yet implemented. All providers currently use hard-coded defaults from the implementation. Use CLI flags (<code>--context</code>, <code>--context-providers</code>, <code>--disable-context</code>) to control providers. See the <a href="#enabling-context-providers">Enabling Context Providers</a> section for working examples.</p>
</blockquote>
<h2 id="critical-path-provider"><a class="header" href="#critical-path-provider">Critical Path Provider</a></h2>
<p>The Critical Path provider identifies functions that lie on critical execution paths through your application. Functions on these paths have elevated risk because failures directly impact user-facing functionality.</p>
<h3 id="entry-point-detection"><a class="header" href="#entry-point-detection">Entry Point Detection</a></h3>
<p>The provider automatically detects entry points based on function names and file paths. These weights determine the base criticality of execution paths:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Entry Type</th><th>Weight</th><th>Detection Pattern</th><th>User-Facing</th></tr>
</thead>
<tbody>
<tr><td>Main</td><td>10.0</td><td>Function named <code>main</code></td><td>Yes</td></tr>
<tr><td>API Endpoint</td><td>8.0</td><td><code>handle_*</code>, <code>*_handler</code>, <code>get_*</code>, <code>post_*</code> in <code>api/</code>, <code>handler/</code>, <code>route/</code> paths</td><td>Yes</td></tr>
<tr><td>CLI Command</td><td>7.0</td><td><code>cmd_*</code>, <code>command_*</code>, <code>*_command</code> in <code>cli/</code>, <code>command/</code> paths</td><td>Yes</td></tr>
<tr><td>Web Handler</td><td>7.0</td><td>Functions with <code>route</code>, <code>handler</code> in <code>web/</code>, <code>http/</code> paths</td><td>Yes</td></tr>
<tr><td>Event Handler</td><td>5.0</td><td><code>on_*</code>, <code>*_listener</code>, contains <code>event</code></td><td>No</td></tr>
<tr><td>Test Entry</td><td>2.0</td><td><code>test_*</code>, in <code>test/</code> paths</td><td>No</td></tr>
</tbody>
</table>
</div>
<p><strong>Note on API Endpoint detection:</strong> Detection requires BOTH conditions: (1) path contains <code>api/</code>, <code>handler/</code>, or <code>route/</code> AND (2) function starts with <code>handle_*</code>, <code>get_*</code>, <code>post_*</code>, <code>put_*</code>, <code>delete_*</code> or ends with <code>*_handler</code>. This combined matching ensures accurate classification of HTTP endpoint handlers.</p>
<p><strong>What it detects:</strong></p>
<ul>
<li>Entry points (main functions, CLI handlers, API endpoints)</li>
<li>Error handling paths</li>
<li>Data processing pipelines</li>
<li>Resource initialization</li>
</ul>
<h3 id="path-weighting"><a class="header" href="#path-weighting">Path Weighting</a></h3>
<p>Functions on critical paths receive contribution scores based on:</p>
<ul>
<li><strong>Path weight</strong>: The maximum entry point weight leading to the function</li>
<li><strong>User-facing flag</strong>: Doubles contribution for user-facing paths</li>
</ul>
<p>The contribution formula consists of two steps:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Step 1: Calculate base contribution (normalized 0-1)
base_contribution = path_weight / max_weight

// Step 2: Apply user-facing multiplier
final_contribution = base_contribution × user_facing_multiplier

// Example: main entry path (weight 10.0, user-facing)
base = 10.0 / 10.0 = 1.0
final = 1.0 × 2.0 = 2.0

// Example: event handler path (weight 5.0, non-user-facing)
base = 5.0 / 10.0 = 0.5
final = 0.5 × 1.0 = 0.5
<span class="boring">}</span></code></pre>
<p><strong>Impact on scoring:</strong></p>
<ul>
<li>Functions on critical paths get higher priority</li>
<li>Entry point multiplier: 1.5x</li>
<li>Business logic multiplier: 1.2x</li>
</ul>
<h3 id="use-cases"><a class="header" href="#use-cases">Use Cases</a></h3>
<ul>
<li><strong>API prioritization</strong>: Identify critical endpoints that need careful review</li>
<li><strong>Refactoring safety</strong>: Avoid breaking user-facing execution paths</li>
<li><strong>Test coverage</strong>: Ensure critical paths have adequate test coverage</li>
</ul>
<h3 id="enable"><a class="header" href="#enable">Enable</a></h3>
<pre><code class="language-bash">debtmap analyze . --context-providers critical_path
</code></pre>
<p><strong>Configuration:</strong></p>
<pre><code class="language-toml">[analysis]
context_providers = ["critical_path"]

# Note: Provider-specific TOML sections below are planned features.
# Currently, providers use hard-coded defaults. Use CLI flags for now.

[context.critical_path]
# Multiplier for entry points (default: 1.5)
entry_point_multiplier = 1.5

# Multiplier for business logic (default: 1.2)
business_logic_multiplier = 1.2
</code></pre>
<h2 id="dependency-provider"><a class="header" href="#dependency-provider">Dependency Provider</a></h2>
<p>The Dependency provider analyzes call graph relationships to identify functions with high architectural impact. It calculates how changes propagate through the dependency graph and determines the blast radius of modifications.</p>
<h3 id="dependency-chain-analysis"><a class="header" href="#dependency-chain-analysis">Dependency Chain Analysis</a></h3>
<p>The provider builds a dependency graph where:</p>
<ul>
<li><strong>Modules</strong> contain functions and have intrinsic risk scores</li>
<li><strong>Edges</strong> represent dependencies with coupling strength (0.0-1.0)</li>
<li><strong>Risk propagation</strong> flows through dependencies using iterative refinement</li>
</ul>
<p><strong>Convergence Parameters:</strong> The risk propagation algorithm uses iterative convergence with a maximum of 10 iterations. Convergence is reached when the maximum risk change between iterations falls below 0.01. This ensures risk stabilizes throughout the dependency graph.</p>
<p><strong>What it detects:</strong></p>
<ul>
<li>Upstream dependencies (functions this function calls)</li>
<li>Downstream dependencies (functions that call this function)</li>
<li>Transitive dependencies through the call graph</li>
<li>Dependency criticality</li>
</ul>
<h3 id="blast-radius-calculation"><a class="header" href="#blast-radius-calculation">Blast Radius Calculation</a></h3>
<p>The blast radius represents how many modules would be affected by changes to a function. It counts unique modules reachable through transitive dependencies by traversing the dependency graph edges.</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Blast Radius</th><th>Contribution</th><th>Impact Level</th></tr>
</thead>
<tbody>
<tr><td>&gt; 10 modules</td><td>1.5</td><td>Critical dependency affecting many modules</td></tr>
<tr><td>&gt; 5 modules</td><td>1.0</td><td>Important dependency with moderate impact</td></tr>
<tr><td>&gt; 2 modules</td><td>0.5</td><td>Medium impact</td></tr>
<tr><td>≤ 2 modules</td><td>0.2</td><td>Minimal or isolated component</td></tr>
</tbody>
</table>
</div>
<h3 id="risk-propagation-formula"><a class="header" href="#risk-propagation-formula">Risk Propagation Formula</a></h3>
<p>Risk propagation uses an iterative convergence algorithm to stabilize risk scores throughout the dependency graph:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>propagated_risk = base_risk × criticality_factor + Σ(caller.risk × 0.3)

where:
  criticality_factor = 1.0 + min(0.5, dependents.len() × 0.1)
  The 0.3 factor dampens risk propagation from callers
<span class="boring">}</span></code></pre>
<p><strong>Iterative Convergence:</strong> The algorithm runs with a maximum of 10 iterations and converges when the maximum risk change between iterations falls below 0.01. This ensures risk stabilizes throughout the dependency graph without requiring manual tuning.</p>
<p><strong>Note</strong>: The constants (0.5, 0.1, 0.3) are currently hard-coded based on empirical analysis. Future versions may make these configurable.</p>
<p><strong>Impact on scoring:</strong></p>
<pre><code>dependency_factor = normalized_to_0_10(upstream + downstream)

Ranges:
- Entry points: 8-10 (critical path)
- Business logic: 6-8 (core functionality)
- Data access: 5-7 (important but stable)
- Utilities: 3-5 (lower priority)
- Test helpers: 1-3 (lowest priority)
</code></pre>
<h3 id="use-cases-1"><a class="header" href="#use-cases-1">Use Cases</a></h3>
<ul>
<li><strong>Architectural refactoring</strong>: Identify high-impact modules to refactor carefully</li>
<li><strong>Change impact analysis</strong>: Understand downstream effects of modifications</li>
<li><strong>Module decoupling</strong>: Find tightly coupled modules with high blast radius</li>
</ul>
<h3 id="enable-1"><a class="header" href="#enable-1">Enable</a></h3>
<pre><code class="language-bash">debtmap analyze . --context-providers dependency
</code></pre>
<p><strong>Configuration:</strong></p>
<pre><code class="language-toml">[analysis]
context_providers = ["dependency"]

# Note: Provider-specific TOML sections below are planned features.
# Currently, providers use hard-coded defaults. Use CLI flags for now.

[context.dependency]
# Include transitive dependencies (default: true)
include_transitive = true

# Maximum depth for transitive analysis (default: 5)
max_depth = 5
</code></pre>
<h2 id="git-history-provider"><a class="header" href="#git-history-provider">Git History Provider</a></h2>
<p>The Git History provider integrates version control data to detect change-prone code and bug patterns. Files with frequent changes and bug fixes indicate higher maintenance risk.</p>
<h3 id="metrics-collected"><a class="header" href="#metrics-collected">Metrics Collected</a></h3>
<p>The provider analyzes Git history to calculate:</p>
<ul>
<li><strong>Change frequency</strong>: Commits per month (recent activity indicator)</li>
<li><strong>Bug density</strong>: Ratio of bug fix commits to total commits</li>
<li><strong>Age</strong>: Days since first commit (maturity indicator)</li>
<li><strong>Author count</strong>: Number of unique contributors (complexity indicator)</li>
<li><strong>Total commits</strong>: Total number of commits to the file</li>
<li><strong>Last modified</strong>: Timestamp of the most recent commit</li>
<li><strong>Stability score</strong>: Weighted combination of churn, bug fixes, and age (0.0-1.0)</li>
</ul>
<p><strong>What it analyzes:</strong></p>
<ul>
<li>Commit frequency per file/function</li>
<li>Bug fix patterns (commits with “fix” in message)</li>
<li>Code churn (lines added/removed)</li>
<li>Recent activity</li>
</ul>
<h3 id="risk-classification"><a class="header" href="#risk-classification">Risk Classification</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Category</th><th>Conditions</th><th>Contribution</th><th>Explanation</th></tr>
</thead>
<tbody>
<tr><td>Very unstable</td><td>freq &gt; 5.0 AND bug_density &gt; 0.3</td><td>2.0</td><td>High churn with many bug fixes</td></tr>
<tr><td>Moderately unstable</td><td>freq &gt; 2.0 OR bug_density &gt; 0.2</td><td>1.0</td><td>Frequent changes or bug-prone</td></tr>
<tr><td>Slightly unstable</td><td>freq &gt; 1.0 OR bug_density &gt; 0.1</td><td>0.5</td><td>Some instability</td></tr>
<tr><td>Stable</td><td>freq ≤ 1.0 AND bug_density ≤ 0.1</td><td>0.1</td><td>Low change rate, few bugs</td></tr>
</tbody>
</table>
</div>
<h3 id="bug-fix-detection"><a class="header" href="#bug-fix-detection">Bug Fix Detection</a></h3>
<p>The provider identifies bug fixes using sophisticated pattern matching with <strong>word boundary detection</strong> to minimize false positives from substring matches like “prefix” or “debug”.</p>
<p><strong>Detection Patterns:</strong></p>
<p>The analyzer searches commit messages for these word-boundary-matched patterns (case-insensitive):</p>
<ul>
<li><code>\bfix\b</code>, <code>\bfixes\b</code>, <code>\bfixed\b</code>, <code>\bfixing\b</code> - Matches “fix” as a complete word, excluding “prefix”, “suffix”, “fixture”</li>
<li><code>\bbug\b</code> - Matches “bug” as a complete word, excluding “debug”, “debugging”</li>
<li><code>\bhotfix\b</code> - Matches emergency fixes</li>
</ul>
<p><strong>Git Command:</strong></p>
<pre><code class="language-bash">git log --oneline \
  --grep='\bfix\b' --grep='\bfixes\b' --grep='\bfixed\b' \
  --grep='\bfixing\b' --grep='\bbug\b' --grep='\bhotfix\b' \
  -i -- &lt;file&gt;
</code></pre>
<p><strong>Exclusion Filters:</strong></p>
<p>To further reduce false positives, commits are filtered out if they match non-bug-fix patterns:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Exclusion Type</th><th>Keywords</th><th>Rationale</th></tr>
</thead>
<tbody>
<tr><td>Conventional Commits</td><td><code>style:</code>, <code>chore:</code>, <code>docs:</code>, <code>test:</code></td><td>Not bug fixes, just maintenance</td></tr>
<tr><td>Maintenance</td><td><code>formatting</code>, <code>linting</code>, <code>whitespace</code>, <code>typo</code></td><td>Cosmetic changes, not functional bugs</td></tr>
<tr><td>Refactoring</td><td><code>refactor:</code> (without bug keywords)</td><td>Code improvements without fixing bugs</td></tr>
</tbody>
</table>
</div>
<p><strong>Examples of Detection:</strong></p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Commit Message</th><th>Detected?</th><th>Reason</th></tr>
</thead>
<tbody>
<tr><td><code>fix: resolve login bug</code></td><td>✅ Yes</td><td>Contains “fix” and “bug” as complete words</td></tr>
<tr><td><code>Fixed the payment issue</code></td><td>✅ Yes</td><td>Contains “fixed” as complete word</td></tr>
<tr><td><code>hotfix: urgent database fix</code></td><td>✅ Yes</td><td>Contains “hotfix” and “fix”</td></tr>
<tr><td><code>Bug fix for issue #123</code></td><td>✅ Yes</td><td>Contains “bug” and “fix”</td></tr>
<tr><td><code>style: apply formatting fixes</code></td><td>❌ No</td><td>Excluded: conventional commit type “style:”</td></tr>
<tr><td><code>refactor: improve prefix handling</code></td><td>❌ No</td><td>Excluded: refactor without bug keywords, “prefix” is substring</td></tr>
<tr><td><code>Add debugging tools</code></td><td>❌ No</td><td>“debugging” contains “bug” but not as word boundary</td></tr>
<tr><td><code>chore: fix linting issues</code></td><td>❌ No</td><td>Excluded: conventional commit type “chore:”</td></tr>
<tr><td><code>update: add fixture for testing</code></td><td>❌ No</td><td>“fixture” contains “fix” but not as word boundary</td></tr>
</tbody>
</table>
</div>
<p><strong>Bug Density Calculation:</strong></p>
<pre><code>bug_density = bug_fix_count / total_commits
</code></pre>
<p>For example, if a file has 10 total commits and 3 are genuine bug fixes (after exclusion filtering):</p>
<ul>
<li>Bug density = 3/10 = 0.30 (30%)</li>
</ul>
<p>A file with 100% bug density means every commit to that file was a bug fix, which is a strong signal that the code is problematic.</p>
<h3 id="research-background-the-bug-magnet-hypothesis"><a class="header" href="#research-background-the-bug-magnet-hypothesis">Research Background: The Bug Magnet Hypothesis</a></h3>
<p>The use of git history for risk assessment is backed by extensive empirical research in software engineering. The core theory, often called the <strong>“Bug Magnet Hypothesis”</strong>, states that <strong>code with a history of bugs is significantly more likely to contain future bugs</strong>.</p>
<h4 id="empirical-evidence"><a class="header" href="#empirical-evidence">Empirical Evidence</a></h4>
<p>Multiple large-scale studies have validated this approach:</p>
<ul>
<li>
<p><strong>Microsoft Study (Nagappan &amp; Ball, 2005)</strong>: Analyzed Windows Server 2003 and found that modules with prior bugs were <strong>4-16 times more likely</strong> to have future bugs than modules without bug history.</p>
</li>
<li>
<p><strong>Mozilla Study (Hassan, 2009)</strong>: Found that bug prediction models based on change history achieved <strong>73% accuracy</strong> in identifying future buggy files.</p>
</li>
<li>
<p><strong>Linux Kernel Study (Kim et al., 2007)</strong>: Showed that files with bug fixes in their recent history had a significantly higher probability of containing latent defects.</p>
</li>
</ul>
<h4 id="why-past-bugs-predict-future-bugs"><a class="header" href="#why-past-bugs-predict-future-bugs">Why Past Bugs Predict Future Bugs</a></h4>
<p>There are four key mechanisms that explain this phenomenon:</p>
<ol>
<li>
<p><strong>Inherent Complexity</strong>: Code that attracted bugs in the past is often more complex, making it harder to fix correctly and more prone to regression.</p>
</li>
<li>
<p><strong>Incomplete Understanding</strong>: If developers repeatedly introduce bugs in the same area, it suggests the code is difficult to understand or has subtle edge cases.</p>
</li>
<li>
<p><strong>Technical Debt Accumulation</strong>: Bug fixes under time pressure often introduce workarounds rather than proper solutions, creating technical debt that leads to more bugs.</p>
</li>
<li>
<p><strong>Broken Window Effect</strong>: Once code develops a reputation for being buggy, it may receive less careful maintenance, perpetuating the cycle.</p>
</li>
</ol>
<h4 id="interpreting-bug-density-scores"><a class="header" href="#interpreting-bug-density-scores">Interpreting Bug Density Scores</a></h4>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Bug Density</th><th>Interpretation</th><th>Action</th></tr>
</thead>
<tbody>
<tr><td>0% - 10%</td><td>Healthy</td><td>Typical for stable, well-tested code</td></tr>
<tr><td>10% - 30%</td><td>Moderate concern</td><td>Consider adding tests or documentation</td></tr>
<tr><td>30% - 50%</td><td>High risk</td><td>Strong candidate for refactoring</td></tr>
<tr><td>50% - 100%</td><td>Critical</td><td>Almost certainly needs redesign or major refactoring</td></tr>
</tbody>
</table>
</div>
<p><strong>Example from real output:</strong></p>
<pre><code>├─ GIT HISTORY: 2.0 changes/month, 100.0% bugs, 30 days old, 1 authors
│  └─ Risk Impact: base_risk=39.7 → contextual_risk=88.9 (2.2x multiplier)
</code></pre>
<p>This shows a file where <strong>every single commit</strong> was a bug fix (100% bug density), resulting in a 2.2x risk multiplier. This is a critical red flag indicating code that needs immediate attention.</p>
<h4 id="limitations-and-false-positives"><a class="header" href="#limitations-and-false-positives">Limitations and False Positives</a></h4>
<p>The improved detection methodology with word boundary matching and exclusion filters significantly reduces false positives, but some limitations remain:</p>
<p><strong>Reduced Issues</strong> (handled by word boundaries and exclusion filters):</p>
<ul>
<li>✅ <strong>Substring matches</strong>: Word boundary matching (<code>\bfix\b</code>) now correctly excludes “prefix”, “fixture”, “suffix”, and “debugging”</li>
<li>✅ <strong>Style/formatting commits</strong>: Conventional commit prefixes (<code>style:</code>, <code>chore:</code>) are automatically excluded</li>
<li>✅ <strong>Maintenance changes</strong>: Keywords like “formatting”, “linting”, “whitespace”, “typo” are filtered out</li>
<li>✅ <strong>Non-bug refactorings</strong>: Refactoring commits without bug-related keywords are excluded</li>
</ul>
<p><strong>Remaining Limitations:</strong></p>
<ul>
<li><strong>Commit message quality</strong>: Still relies on developers mentioning “fix” or “bug” in commit messages
<ul>
<li>Underreporting: Some bug fixes may not be mentioned in commit messages</li>
<li>Example: A commit “Update authentication logic” that actually fixes a bug won’t be detected</li>
</ul>
</li>
<li><strong>New code</strong>: Cannot assess code without commit history
<ul>
<li>Files with &lt;5 commits may not have enough data for reliable bug density calculation</li>
</ul>
</li>
<li><strong>Language barriers</strong>: Non-English commit messages may use different bug-fix keywords</li>
<li><strong>Informal commits</strong>: Internal repos may use different conventions (e.g., ticket IDs only)</li>
</ul>
<p><strong>Accuracy Improvements:</strong></p>
<p>Before word boundary matching and exclusion filters:</p>
<ul>
<li>False positive rate: ~15-25% (substring matches, style commits, etc.)</li>
</ul>
<p>After improvements:</p>
<ul>
<li>False positive rate: ~5-10% (primarily commit message quality issues)</li>
<li>Precision: Significantly improved, particularly for conventional commit style repositories</li>
</ul>
<p><strong>Recommended practice</strong>: Use git history as one signal among many. Combine it with complexity metrics, test coverage, and dependency analysis for a complete risk picture. The bug density metric is most reliable when:</p>
<ul>
<li>Repository uses consistent commit message conventions</li>
<li>Files have at least 10+ commits in their history</li>
<li>Development team follows English-based conventional commit style</li>
</ul>
<h3 id="stability-score"><a class="header" href="#stability-score">Stability Score</a></h3>
<p>Stability is calculated using weighted factors:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>stability = (churn_factor × 0.4) + (bug_factor × 0.4) + (age_factor × 0.2)

where:
  churn_factor = 1.0 / (1.0 + monthly_churn)
  bug_factor = 1.0 - (bug_fixes / total_commits)
  age_factor = min(1.0, age_days / 365.0)
<span class="boring">}</span></code></pre>
<h3 id="stability-status-classifications"><a class="header" href="#stability-status-classifications">Stability Status Classifications</a></h3>
<p>The provider internally classifies files into stability statuses based on the calculated metrics:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Status</th><th>Criteria</th><th>Explanation</th></tr>
</thead>
<tbody>
<tr><td>HighlyUnstable</td><td>freq &gt; 5.0 AND bug_density &gt; 0.3</td><td>Extremely high churn combined with many bug fixes</td></tr>
<tr><td>FrequentlyChanged</td><td>freq &gt; 2.0</td><td>High change rate regardless of bug density</td></tr>
<tr><td>BugProne</td><td>bug_density &gt; 0.2</td><td>High proportion of bug fix commits</td></tr>
<tr><td>MatureStable</td><td>age &gt; 365 days</td><td>Code older than one year (unless unstable)</td></tr>
<tr><td>RelativelyStable</td><td>(default)</td><td>Moderate activity, typical stability</td></tr>
</tbody>
</table>
</div>
<p>These classifications are used internally for contribution calculations and appear in verbose output.</p>
<p><strong>Impact on scoring:</strong></p>
<ul>
<li>High-churn functions get higher priority</li>
<li>Recently fixed bugs indicate risk areas</li>
<li>Stable code (no recent changes) gets lower priority</li>
</ul>
<h3 id="use-cases-2"><a class="header" href="#use-cases-2">Use Cases</a></h3>
<ul>
<li><strong>Find change-prone code</strong>: Identify files that change frequently and need attention</li>
<li><strong>Detect bug hotspots</strong>: Locate areas with high bug fix rates</li>
<li><strong>Prioritize refactoring</strong>: Target unstable code for improvement</li>
<li><strong>Team collaboration patterns</strong>: Files touched by many authors may need better documentation</li>
</ul>
<h3 id="enable-2"><a class="header" href="#enable-2">Enable</a></h3>
<pre><code class="language-bash">debtmap analyze . --context-providers git_history
</code></pre>
<p><strong>Configuration:</strong></p>
<pre><code class="language-toml">[analysis]
context_providers = ["git_history"]

# Note: Provider-specific TOML sections below are planned features.
# Currently, providers use hard-coded defaults. Use CLI flags for now.

[context.git_history]
# Commits to analyze (default: 100)
max_commits = 100

# Time range in days (default: 90)
time_range_days = 90

# Minimum commits to consider "high churn" (default: 10)
high_churn_threshold = 10
</code></pre>
<h3 id="troubleshooting-10"><a class="header" href="#troubleshooting-10">Troubleshooting</a></h3>
<p><strong>Git repository not found</strong>: The provider requires a Git repository. If analysis fails:</p>
<pre><code class="language-bash"># Verify you're in a git repository
git rev-parse --git-dir

# If not a git repo, initialize one or disable git_history provider
# Option 1: Enable context but exclude git_history
debtmap analyze --context --disable-context git_history

# Option 2: Use only specific providers
debtmap analyze --context-providers critical_path,dependency
</code></pre>
<p><strong>Performance issues</strong>: Git history analysis can be slow for large repositories:</p>
<pre><code class="language-bash"># Use only lightweight providers
debtmap analyze --context-providers critical_path,dependency
</code></pre>
<h2 id="enabling-context-providers"><a class="header" href="#enabling-context-providers">Enabling Context Providers</a></h2>
<p>Context-aware analysis is disabled by default. Enable it using CLI flags:</p>
<h3 id="enable-all-providers"><a class="header" href="#enable-all-providers">Enable All Providers</a></h3>
<pre><code class="language-bash"># Enable all available context providers
debtmap analyze --context
# or
debtmap analyze --enable-context
</code></pre>
<h3 id="enable-specific-providers"><a class="header" href="#enable-specific-providers">Enable Specific Providers</a></h3>
<pre><code class="language-bash"># Enable only critical_path and dependency
debtmap analyze --context-providers critical_path,dependency

# Enable only git_history
debtmap analyze --context-providers git_history

# Enable all three explicitly
debtmap analyze --context-providers critical_path,dependency,git_history
</code></pre>
<h3 id="disable-specific-providers"><a class="header" href="#disable-specific-providers">Disable Specific Providers</a></h3>
<pre><code class="language-bash"># Enable context but disable git_history (useful for non-git repos)
debtmap analyze --context --disable-context git_history

# Enable context but disable dependency analysis
debtmap analyze --context --disable-context dependency
</code></pre>
<h3 id="enabling-multiple-providers"><a class="header" href="#enabling-multiple-providers">Enabling Multiple Providers</a></h3>
<p>Combine providers for comprehensive analysis:</p>
<pre><code class="language-bash">debtmap analyze . --context-providers critical_path,dependency,git_history
</code></pre>
<p>Or via config:</p>
<pre><code class="language-toml">[analysis]
context_providers = ["critical_path", "dependency", "git_history"]
</code></pre>
<h2 id="provider-weights"><a class="header" href="#provider-weights">Provider Weights</a></h2>
<p>Each provider has a weight that determines its influence on the final risk score:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Provider</th><th>Weight</th><th>Rationale</th></tr>
</thead>
<tbody>
<tr><td>critical_path</td><td>1.5</td><td>Critical paths have high impact on users</td></tr>
<tr><td>dependency_risk</td><td>1.2</td><td>Architectural dependencies affect many modules</td></tr>
<tr><td>git_history</td><td>1.0</td><td>Historical patterns indicate future risk</td></tr>
</tbody>
</table>
</div>
<p>The total context contribution is calculated as:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>total_contribution = Σ(contribution_i × weight_i)

Example with all providers:
  critical_path: 2.0 × 1.5 = 3.0
  dependency:    1.0 × 1.2 = 1.2
  git_history:   0.5 × 1.0 = 0.5
  ────────────────────────────
  total_contribution = 4.7

contextual_risk = base_risk × (1.0 + 4.7) = base_risk × 5.7
<span class="boring">}</span></code></pre>
<h2 id="how-context-affects-scoring"><a class="header" href="#how-context-affects-scoring">How Context Affects Scoring</a></h2>
<h3 id="base-scoring-no-context"><a class="header" href="#base-scoring-no-context">Base Scoring (No Context)</a></h3>
<pre><code>Base Score = (Complexity × 0.40) + (Coverage × 0.40) + (Dependency × 0.20)
</code></pre>
<h3 id="with-context-providers"><a class="header" href="#with-context-providers">With Context Providers</a></h3>
<pre><code>Context-Adjusted Score = Base Score × Role Multiplier × Churn Multiplier

Role Multiplier (from critical path &amp; dependency analysis):
- Entry points: 1.5x
- Business logic: 1.2x
- Data access: 1.0x
- Infrastructure: 0.8x
- Utilities: 0.5x
- Test code: 0.1x

Churn Multiplier (from git history):
- High churn (10+ commits/month): 1.3x
- Medium churn (5-10 commits/month): 1.1x
- Low churn (1-5 commits/month): 1.0x
- Stable (0 commits/6 months): 0.8x
</code></pre>
<h2 id="context-details-structure"><a class="header" href="#context-details-structure">Context Details Structure</a></h2>
<p>When using <code>--format json</code>, context information is included in the output. The <code>ContextDetails</code> enum contains provider-specific data:</p>
<h3 id="criticalpath"><a class="header" href="#criticalpath">CriticalPath</a></h3>
<pre><code class="language-json">{
  "provider": "critical_path",
  "weight": 1.5,
  "contribution": 2.0,
  "details": {
    "CriticalPath": {
      "entry_points": ["main (Main)", "handle_request (ApiEndpoint)"],
      "path_weight": 10.0,
      "is_user_facing": true
    }
  }
}
</code></pre>
<h3 id="dependencychain"><a class="header" href="#dependencychain">DependencyChain</a></h3>
<pre><code class="language-json">{
  "provider": "dependency_risk",
  "weight": 1.2,
  "contribution": 1.5,
  "details": {
    "DependencyChain": {
      "depth": 3,
      "propagated_risk": 8.5,
      "dependents": ["module_a", "module_b", "module_c"],
      "blast_radius": 12
    }
  }
}
</code></pre>
<h3 id="historical"><a class="header" href="#historical">Historical</a></h3>
<pre><code class="language-json">{
  "provider": "git_history",
  "weight": 1.0,
  "contribution": 1.0,
  "details": {
    "Historical": {
      "change_frequency": 3.5,
      "bug_density": 0.25,
      "age_days": 180,
      "author_count": 5
    }
  }
}
</code></pre>
<h2 id="practical-examples-1"><a class="header" href="#practical-examples-1">Practical Examples</a></h2>
<h3 id="example-1-entry-point-vs-utility"><a class="header" href="#example-1-entry-point-vs-utility">Example 1: Entry Point vs Utility</a></h3>
<p><strong>Without context providers:</strong></p>
<pre><code>Function: main() - Entry point
Complexity: 8
Coverage: 50%
Score: 6.0 [MEDIUM]

Function: format_string() - Utility
Complexity: 8
Coverage: 50%
Score: 6.0 [MEDIUM]
</code></pre>
<p>Both functions have the same score.</p>
<p><strong>With context providers:</strong></p>
<pre><code>Function: main() - Entry point
Complexity: 8
Coverage: 50%
Base Score: 6.0
Role Multiplier: 1.5x (entry point)
Final Score: 9.0 [CRITICAL]

Function: format_string() - Utility
Complexity: 8
Coverage: 50%
Base Score: 6.0
Role Multiplier: 0.5x (utility)
Final Score: 3.0 [LOW]
</code></pre>
<p>Entry point is prioritized over utility.</p>
<h3 id="example-2-high-churn-function"><a class="header" href="#example-2-high-churn-function">Example 2: High-Churn Function</a></h3>
<p><strong>Without git history:</strong></p>
<pre><code>Function: process_payment()
Complexity: 12
Coverage: 60%
Score: 7.5 [HIGH]
</code></pre>
<p><strong>With git history:</strong></p>
<pre><code>Function: process_payment()
Complexity: 12
Coverage: 60%
Base Score: 7.5
Churn: 15 commits in last month (bug fixes)
Churn Multiplier: 1.3x
Final Score: 9.75 [CRITICAL]
</code></pre>
<p>High-churn function is elevated to critical.</p>
<h3 id="example-3-stable-well-tested-code"><a class="header" href="#example-3-stable-well-tested-code">Example 3: Stable Well-Tested Code</a></h3>
<p><strong>Without context:</strong></p>
<pre><code>Function: legacy_parser()
Complexity: 15
Coverage: 95%
Score: 3.5 [LOW]
</code></pre>
<p><strong>With context:</strong></p>
<pre><code>Function: legacy_parser()
Complexity: 15
Coverage: 95%
Base Score: 3.5
Churn: 0 commits in last 2 years
Churn Multiplier: 0.8x
Role: Data access (stable)
Role Multiplier: 1.0x
Final Score: 2.8 [LOW]
</code></pre>
<p>Stable, well-tested code gets even lower priority.</p>
<h3 id="example-4-api-endpoint-prioritization"><a class="header" href="#example-4-api-endpoint-prioritization">Example 4: API Endpoint Prioritization</a></h3>
<p>Analyze a web service to identify critical API endpoints:</p>
<pre><code class="language-bash">debtmap analyze --context-providers critical_path --format json
</code></pre>
<p>Functions on API endpoint paths will receive elevated risk scores. Use this to prioritize code review and testing for user-facing functionality.</p>
<h3 id="example-5-finding-change-prone-code"><a class="header" href="#example-5-finding-change-prone-code">Example 5: Finding Change-Prone Code</a></h3>
<p>Identify files with high change frequency and bug fixes:</p>
<pre><code class="language-bash">debtmap analyze --context-providers git_history --top 20
</code></pre>
<p>This highlights unstable areas of the codebase that may benefit from refactoring or increased test coverage.</p>
<h3 id="example-6-architectural-impact-analysis"><a class="header" href="#example-6-architectural-impact-analysis">Example 6: Architectural Impact Analysis</a></h3>
<p>Find high-impact modules with large blast radius:</p>
<pre><code class="language-bash">debtmap analyze --context-providers dependency --format json | \
  jq '.[] | select(.blast_radius &gt; 10)'
</code></pre>
<p>Use this to identify architectural choke points that require careful change management.</p>
<h3 id="example-7-comprehensive-risk-assessment"><a class="header" href="#example-7-comprehensive-risk-assessment">Example 7: Comprehensive Risk Assessment</a></h3>
<p>Combine all providers for holistic risk analysis:</p>
<pre><code class="language-bash">debtmap analyze --context -v
</code></pre>
<p>The verbose output shows how each provider contributes to the final risk score:</p>
<pre><code>function: process_payment
  base_risk: 5.0
  critical_path: +3.0 (on main path, user-facing)
  dependency: +1.2 (12 dependent modules)
  git_history: +1.0 (3.5 changes/month, 0.25 bug density)
  ──────────────────
  contextual_risk: 26.0
</code></pre>
<h2 id="configuration-6"><a class="header" href="#configuration-6">Configuration</a></h2>
<blockquote>
<p>⚠️ <strong>Configuration Limitation</strong>: Provider-specific TOML configuration sections shown below are planned features not yet implemented. Currently, all provider settings use hard-coded defaults from the implementation. Use CLI flags (<code>--context</code>, <code>--context-providers</code>, <code>--disable-context</code>) to control providers. See the CLI examples throughout this chapter for working configurations.</p>
</blockquote>
<p>Configure context providers in <code>.debtmap.toml</code>:</p>
<pre><code class="language-toml">[analysis]
# Enable context-aware analysis (default: false)
enable_context = true

# Specify which providers to use
context_providers = ["critical_path", "dependency", "git_history"]

# Disable specific providers (use CLI flag --disable-context instead)
# disable_context = ["git_history"]  # Not yet implemented in config

[context.git_history]
# Commits to analyze (default: 100) - PLANNED
max_commits = 100

# Time range in days (default: 90) - PLANNED
time_range_days = 90

# Minimum commits to consider "high churn" (default: 10) - PLANNED
high_churn_threshold = 10

[context.critical_path]
# Multiplier for entry points (default: 1.5) - PLANNED
entry_point_multiplier = 1.5

# Multiplier for business logic (default: 1.2) - PLANNED
business_logic_multiplier = 1.2

[context.dependency]
# Include transitive dependencies (default: true) - PLANNED
include_transitive = true

# Maximum depth for transitive analysis (default: 5) - PLANNED
max_depth = 5
</code></pre>
<h2 id="performance-considerations-1"><a class="header" href="#performance-considerations-1">Performance Considerations</a></h2>
<p>Context providers add computational overhead to analysis:</p>
<p><strong>Impact on analysis time:</strong></p>
<ul>
<li>Critical path: +10-15% (fast - call graph traversal)</li>
<li>Dependency: +20-30% (moderate - iterative risk propagation)</li>
<li>Git history: +30-50% (slow for large repos - multiple git commands per file)</li>
</ul>
<p><strong>Combined overhead:</strong> ~60-80% increase in analysis time</p>
<h3 id="optimization-tips"><a class="header" href="#optimization-tips">Optimization Tips</a></h3>
<ol>
<li><strong>Start minimal</strong>: Use <code>--context-providers critical_path,dependency</code> initially</li>
<li><strong>Add git_history selectively</strong>: Enable for critical modules only</li>
<li><strong>Use caching</strong>: The <code>ContextAggregator</code> caches results by <code>file:function</code> key</li>
<li><strong>Profile with verbose flags</strong>: Use <code>-vvv</code> to see provider execution times</li>
</ol>
<h3 id="for-large-projects"><a class="header" href="#for-large-projects">For Large Projects</a></h3>
<pre><code class="language-bash"># Disable git history for faster analysis
debtmap analyze . --disable-context git_history

# Or disable all context
debtmap analyze . --no-context-aware
</code></pre>
<h3 id="for-cicd"><a class="header" href="#for-cicd">For CI/CD</a></h3>
<pre><code class="language-bash"># Full analysis with context (run nightly)
debtmap analyze . --context-providers critical_path,dependency,git_history

# Fast analysis without context (run on every commit)
debtmap analyze . --no-context-aware
</code></pre>
<h3 id="when-to-use-each-provider"><a class="header" href="#when-to-use-each-provider">When to Use Each Provider</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Scenario</th><th>Recommended Providers</th></tr>
</thead>
<tbody>
<tr><td>API service refactoring</td><td><code>critical_path</code></td></tr>
<tr><td>Legacy codebase analysis</td><td><code>git_history</code></td></tr>
<tr><td>Microservice boundaries</td><td><code>dependency</code></td></tr>
<tr><td>Pre-release risk review</td><td>All providers (<code>--context</code>)</td></tr>
<tr><td>CI/CD integration</td><td><code>critical_path,dependency</code> (faster)</td></tr>
</tbody>
</table>
</div>
<h2 id="troubleshooting-1-1"><a class="header" href="#troubleshooting-1-1">Troubleshooting</a></h2>
<h3 id="git-history-analysis-slow"><a class="header" href="#git-history-analysis-slow">Git History Analysis Slow</a></h3>
<p><strong>Issue:</strong> Analysis takes much longer with git history enabled</p>
<p><strong>Solutions:</strong></p>
<p><strong>Reduce commit history:</strong></p>
<pre><code class="language-toml">[context.git_history]
max_commits = 50
time_range_days = 30
</code></pre>
<p><strong>Use shallow clone in CI:</strong></p>
<pre><code class="language-bash">git clone --depth 50 repo.git
debtmap analyze . --context-providers critical_path,dependency
</code></pre>
<h3 id="incorrect-role-classification"><a class="header" href="#incorrect-role-classification">Incorrect Role Classification</a></h3>
<p><strong>Issue:</strong> Function classified as wrong role (e.g., utility instead of business logic)</p>
<p><strong>Possible causes:</strong></p>
<ol>
<li>Function naming doesn’t match patterns</li>
<li>Call graph analysis incomplete</li>
<li>Function is misplaced in codebase</li>
</ol>
<p><strong>Solutions:</strong></p>
<p><strong>Check with verbose output:</strong></p>
<pre><code class="language-bash">debtmap analyze . -vv | grep "Role classification"
</code></pre>
<p><strong>Manually verify call graph:</strong></p>
<pre><code class="language-bash">debtmap analyze . --show-call-graph
</code></pre>
<h3 id="context-providers-not-available"><a class="header" href="#context-providers-not-available">Context Providers Not Available</a></h3>
<p><strong>Issue:</strong> <code>--context-providers</code> flag not recognized</p>
<p><strong>Solution:</strong> Ensure you’re using a recent version:</p>
<pre><code class="language-bash">debtmap --version
# Should be 0.2.0 or later
</code></pre>
<p>Update debtmap:</p>
<pre><code class="language-bash">cargo install debtmap --force
</code></pre>
<h3 id="common-issues-1"><a class="header" href="#common-issues-1">Common Issues</a></h3>
<p><strong>Issue</strong>: Context providers not affecting scores</p>
<p><strong>Solution</strong>: Ensure providers are enabled with <code>--context</code> or <code>--context-providers</code></p>
<pre><code class="language-bash"># Wrong: context flag missing
debtmap analyze

# Correct: context enabled
debtmap analyze --context
</code></pre>
<hr>
<p><strong>Issue</strong>: Git history provider fails with “Not a git repository”</p>
<p><strong>Solution</strong>: Disable git_history if not using version control</p>
<pre><code class="language-bash">debtmap analyze --context --disable-context git_history
</code></pre>
<hr>
<p><strong>Issue</strong>: Dependency analysis errors</p>
<p><strong>Solution</strong>: Check for circular dependencies or disable dependency provider</p>
<pre><code class="language-bash">debtmap analyze --context --disable-context dependency
</code></pre>
<hr>
<p><strong>Issue</strong>: Slow analysis with all providers</p>
<p><strong>Solution</strong>: Use selective providers or increase verbosity to identify bottlenecks</p>
<pre><code class="language-bash"># Faster: skip git_history
debtmap analyze --context-providers critical_path,dependency

# Debug: see provider execution times
debtmap analyze --context -vvv
</code></pre>
<hr>
<p>For more troubleshooting guidance, see the <a href="#troubleshooting-23">Troubleshooting</a> chapter.</p>
<h2 id="advanced-usage"><a class="header" href="#advanced-usage">Advanced Usage</a></h2>
<h3 id="interpreting-context-contribution"><a class="header" href="#interpreting-context-contribution">Interpreting Context Contribution</a></h3>
<p>Enable verbose output to see detailed context contributions:</p>
<pre><code class="language-bash">debtmap analyze --context -v
</code></pre>
<p>Each function shows:</p>
<ul>
<li>Base risk score from complexity/coverage</li>
<li>Individual provider contributions</li>
<li>Total contextual risk score</li>
<li>Provider-specific explanations</li>
</ul>
<h3 id="architecture-exploration"><a class="header" href="#architecture-exploration">Architecture Exploration</a></h3>
<p>The <code>ContextAggregator</code> caches context by <code>file:function</code> key to avoid redundant analysis during a single run.</p>
<p><strong>Cache Lifetime:</strong> The cache is in-memory per <code>ContextAggregator</code> instance and is cleared when a new instance is created or when analyzing a different codebase. This enables efficient re-analysis within the same run without requiring external cache management:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let mut aggregator = ContextAggregator::new()
    .with_provider(Box::new(CriticalPathProvider::new(analyzer)))
    .with_provider(Box::new(DependencyRiskProvider::new(graph)))
    .with_provider(Box::new(GitHistoryProvider::new(repo_root)?));

let context = aggregator.analyze(&amp;target);
let contribution = context.total_contribution();
<span class="boring">}</span></code></pre>
<h3 id="custom-provider-implementation"><a class="header" href="#custom-provider-implementation">Custom Provider Implementation</a></h3>
<p>Advanced users can implement custom context providers by implementing the <code>ContextProvider</code> trait:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub trait ContextProvider: Send + Sync {
    fn name(&amp;self) -&gt; &amp;str;
    fn gather(&amp;self, target: &amp;AnalysisTarget) -&gt; Result&lt;Context&gt;;
    fn weight(&amp;self) -&gt; f64;
    fn explain(&amp;self, context: &amp;Context) -&gt; String;
}
<span class="boring">}</span></code></pre>
<p>See <code>src/risk/context/mod.rs</code> for the trait definition and <code>src/risk/context/</code> for built-in provider implementations.</p>
<h2 id="future-enhancements"><a class="header" href="#future-enhancements">Future Enhancements</a></h2>
<h3 id="business-context-provider-planned"><a class="header" href="#business-context-provider-planned">Business Context Provider (Planned)</a></h3>
<p>A Business context provider is defined but not yet implemented. It will support:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>Business {
    priority: Priority,      // Critical, High, Medium, Low
    impact: Impact,          // Revenue, UserExperience, Security, Compliance
    annotations: Vec&lt;String&gt; // Custom business metadata
}
<span class="boring">}</span></code></pre>
<p>This will allow manual prioritization based on business requirements through code annotations or configuration files.</p>
<h2 id="best-practices-6"><a class="header" href="#best-practices-6">Best Practices</a></h2>
<ol>
<li><strong>Use all providers for comprehensive analysis</strong> - Especially for production code</li>
<li><strong>Disable git history in CI</strong> - Use shallow clones or disable for speed</li>
<li><strong>Verify role classifications</strong> - Use <code>-vv</code> to see how functions are classified</li>
<li><strong>Tune multipliers for your project</strong> - Adjust in config based on architecture</li>
<li><strong>Combine with coverage data</strong> - Context providers enhance coverage-based risk analysis</li>
</ol>
<h2 id="summary-3"><a class="header" href="#summary-3">Summary</a></h2>
<p>Context providers transform debtmap from a static complexity analyzer into a comprehensive risk assessment tool. By combining:</p>
<ul>
<li><strong>Critical path analysis</strong> for user impact</li>
<li><strong>Dependency analysis</strong> for architectural risk</li>
<li><strong>Git history analysis</strong> for maintenance patterns</li>
</ul>
<p>You gain actionable insights for prioritizing technical debt and refactoring efforts. Start with <code>--context</code> to enable all providers, then refine based on your project’s needs.</p>
<h2 id="see-also-5"><a class="header" href="#see-also-5">See Also</a></h2>
<ul>
<li><a href="#analysis-guide">Analysis Guide</a> - Core analysis concepts</li>
<li><a href="#risk-scoring-1">Risk Scoring</a> - Risk scoring methodology</li>
<li><a href="#scoring-strategies">Scoring Strategies</a> - File-level and function-level scoring</li>
<li><a href="#configuration-2">Configuration</a> - Complete configuration reference</li>
<li><a href="#parallel-processing">Parallel Processing</a> - Performance optimization</li>
<li><a href="#troubleshooting-23">Troubleshooting</a> - Common issues and solutions</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="coverage-gap-analysis"><a class="header" href="#coverage-gap-analysis">Coverage Gap Analysis</a></h1>
<p>Debtmap provides precise line-level coverage gap reporting to help you understand exactly which lines of code lack test coverage, rather than relying on misleading function-level percentages.</p>
<h2 id="understanding-coverage-gaps"><a class="header" href="#understanding-coverage-gaps">Understanding Coverage Gaps</a></h2>
<p>A <strong>coverage gap</strong> represents the portion of a function that is not executed during tests. Traditional tools report this as a simple percentage (e.g., “50% covered”), but this can be misleading:</p>
<ul>
<li>A 100-line function with 1 uncovered line shows “99% covered” - sounds great!</li>
<li>A 10-line function with 1 uncovered line shows “90% covered” - sounds worse, but is actually better</li>
</ul>
<p>Debtmap improves on this by:</p>
<ol>
<li>Reporting the actual number of uncovered lines</li>
<li>Showing which specific lines are uncovered</li>
<li>Calculating the gap as a percentage of instrumented lines (not total lines)</li>
<li>Providing visual severity indicators based on gap size</li>
</ol>
<h2 id="precise-vs-estimated-gaps"><a class="header" href="#precise-vs-estimated-gaps">Precise vs Estimated Gaps</a></h2>
<p>Debtmap uses different precision levels depending on available coverage data:</p>
<h3 id="precise-gaps-line-level-data-available"><a class="header" href="#precise-gaps-line-level-data-available">Precise Gaps (Line-Level Data Available)</a></h3>
<p>When LCOV coverage data is available, debtmap provides exact line-level reporting:</p>
<pre><code>Business logic - 1 line uncovered (11% gap) - line 52
Complex calculation - 4 lines uncovered (20% gap) - lines 10-12, 15
</code></pre>
<p><strong>Benefits:</strong></p>
<ul>
<li>Exact line numbers for uncovered code</li>
<li>Accurate gap percentage based on instrumented lines</li>
<li>Compact line range formatting (e.g., “10-12, 15, 20-21”)</li>
<li>Distinguishes between code that can’t be instrumented vs uncovered code</li>
</ul>
<p><strong>How it works:</strong></p>
<ul>
<li>Debtmap reads LCOV coverage data from your test runs</li>
<li>Matches functions by file path and name</li>
<li>Extracts precise uncovered line numbers</li>
<li>Calculates percentage as: <code>(uncovered_lines / instrumented_lines) * 100</code></li>
</ul>
<h3 id="estimated-gaps-function-level-data-only"><a class="header" href="#estimated-gaps-function-level-data-only">Estimated Gaps (Function-Level Data Only)</a></h3>
<p>When only function-level coverage percentages are available:</p>
<pre><code>Data processing - ~50% gap (estimated, ~25 lines)
Helper function - ~100% gap (estimated, 15 lines)
Utility - ~3% gap (mostly covered)
</code></pre>
<p><strong>Characteristics:</strong></p>
<ul>
<li>Estimates uncovered line count from percentage</li>
<li>Uses tilde (~) prefix to indicate estimation</li>
<li>Special cases:
<ul>
<li>≥99% gap → “~100% gap”</li>
<li>&lt;5% gap → “mostly covered”</li>
<li>Otherwise → “~X% gap (estimated, ~Y lines)”</li>
</ul>
</li>
</ul>
<p><strong>How it works:</strong></p>
<ul>
<li>Falls back when LCOV data unavailable or function not found</li>
<li>Calculates: <code>estimated_uncovered = total_lines * (gap_percentage / 100)</code></li>
<li>Useful for quick overview but less actionable than precise gaps</li>
</ul>
<h3 id="unknown-coverage"><a class="header" href="#unknown-coverage">Unknown Coverage</a></h3>
<p>When no coverage data is available:</p>
<pre><code>Untested module - Coverage data unavailable (42 lines)
</code></pre>
<p>This typically occurs when:</p>
<ul>
<li>No coverage collection has been run</li>
<li>File not included in coverage report</li>
<li>Coverage data file path mismatch</li>
</ul>
<h2 id="gap-severity-indicators"><a class="header" href="#gap-severity-indicators">Gap Severity Indicators</a></h2>
<p>Debtmap uses visual indicators to quickly identify the severity of coverage gaps:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Indicator</th><th>Range</th><th>Severity</th><th>Meaning</th></tr>
</thead>
<tbody>
<tr><td>🟡</td><td>1-25%</td><td>LOW</td><td>Minor gaps, mostly covered</td></tr>
<tr><td>🟠</td><td>26-50%</td><td>MODERATE</td><td>Significant gaps, needs attention</td></tr>
<tr><td>🔴</td><td>51-75%</td><td>HIGH</td><td>Major gaps, high priority</td></tr>
<tr><td>🔴🔴</td><td>76-100%</td><td>CRITICAL</td><td>Severe gaps, critical priority</td></tr>
</tbody>
</table>
</div>
<p>These indicators appear in debtmap’s priority output to help you quickly identify which functions need testing most urgently.</p>
<h3 id="severity-calculation"><a class="header" href="#severity-calculation">Severity Calculation</a></h3>
<p>Gap severity is based on the percentage of uncovered code:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn get_severity(gap_percentage: f64) -&gt; &amp;'static str {
    match gap_percentage {
        p if p &lt;= 25.0 =&gt; "🟡 LOW",
        p if p &lt;= 50.0 =&gt; "🟠 MODERATE",
        p if p &lt;= 75.0 =&gt; "🔴 HIGH",
        _ =&gt; "🔴🔴 CRITICAL"
    }
}
<span class="boring">}</span></code></pre>
<p>This works for both precise and estimated gaps, ensuring consistent severity classification across your codebase.</p>
<h2 id="example-output-3"><a class="header" href="#example-output-3">Example Output</a></h2>
<h3 id="high-verbosity-mode"><a class="header" href="#high-verbosity-mode">High Verbosity Mode</a></h3>
<pre><code>Priority 1: Authentication Logic (CRITICAL)
  File: src/auth/login.rs:45
  Coverage Gap: 2 lines uncovered (89% gap) 🔴🔴 - lines 67, 89
  Complexity: Cyclomatic 8, Cognitive 12
  Impact: High-risk business logic with critical coverage gaps

Priority 2: Data Validation (HIGH)
  File: src/validation/rules.rs:120
  Coverage Gap: 15 lines uncovered (65% gap) 🔴 - lines 145-152, 167-173
  Complexity: Cyclomatic 5, Cognitive 8
  Impact: Complex validation logic needs comprehensive testing

Priority 3: Helper Function (MODERATE)
  File: src/utils/helpers.rs:30
  Coverage Gap: ~45% gap (estimated, ~12 lines) 🟠
  Complexity: Cyclomatic 3, Cognitive 4
  Impact: Moderate complexity with estimated coverage gaps
</code></pre>
<h3 id="standard-mode"><a class="header" href="#standard-mode">Standard Mode</a></h3>
<pre><code>1. Authentication Logic (src/auth/login.rs:45)
   Gap: 2 lines uncovered (89%) 🔴🔴 [lines 67, 89]

2. Data Validation (src/validation/rules.rs:120)
   Gap: 15 lines uncovered (65%) 🔴 [lines 145-152, 167-173]

3. Helper Function (src/utils/helpers.rs:30)
   Gap: ~45% (estimated) 🟠
</code></pre>
<h2 id="integration-with-coverage-tools"><a class="header" href="#integration-with-coverage-tools">Integration with Coverage Tools</a></h2>
<h3 id="generating-lcov-data"><a class="header" href="#generating-lcov-data">Generating LCOV Data</a></h3>
<p>For precise gap reporting, generate LCOV coverage data with your test framework:</p>
<p><strong>Rust (using cargo-tarpaulin):</strong></p>
<pre><code class="language-bash">cargo tarpaulin --out Lcov --output-dir ./coverage
</code></pre>
<p><strong>Python (using pytest-cov):</strong></p>
<pre><code class="language-bash">pytest --cov=mypackage --cov-report=lcov:coverage/lcov.info
</code></pre>
<p><strong>JavaScript (using Jest):</strong></p>
<pre><code class="language-bash">jest --coverage --coverageReporters=lcov
</code></pre>
<h3 id="configuring-debtmap"><a class="header" href="#configuring-debtmap">Configuring Debtmap</a></h3>
<p>Point debtmap to your coverage data:</p>
<pre><code class="language-bash">debtmap analyze --coverage-path ./coverage/lcov.info
</code></pre>
<p>Or in <code>.debtmap.toml</code>:</p>
<pre><code class="language-toml">[coverage]
lcov_path = "./coverage/lcov.info"
</code></pre>
<h2 id="best-practices-7"><a class="header" href="#best-practices-7">Best Practices</a></h2>
<h3 id="1-use-precise-gaps-when-possible"><a class="header" href="#1-use-precise-gaps-when-possible">1. Use Precise Gaps When Possible</a></h3>
<p>Always generate LCOV data for actionable coverage insights:</p>
<ul>
<li>Precise line numbers help you quickly locate untested code</li>
<li>Accurate percentages prevent over/under-estimating gaps</li>
<li>Line ranges show if gaps are concentrated or scattered</li>
</ul>
<h3 id="2-focus-on-high-severity-gaps-first"><a class="header" href="#2-focus-on-high-severity-gaps-first">2. Focus on High Severity Gaps First</a></h3>
<p>Prioritize based on severity indicators:</p>
<ol>
<li>🔴🔴 CRITICAL (76-100%) - Address immediately</li>
<li>🔴 HIGH (51-75%) - Schedule for next sprint</li>
<li>🟠 MODERATE (26-50%) - Address when convenient</li>
<li>🟡 LOW (1-25%) - Acceptable for some code</li>
</ol>
<h3 id="3-consider-context"><a class="header" href="#3-consider-context">3. Consider Context</a></h3>
<p>Gap severity should be weighted by:</p>
<ul>
<li><strong>Function role</strong>: Business logic vs utilities</li>
<li><strong>Complexity</strong>: High complexity + high gap = top priority</li>
<li><strong>Change frequency</strong>: Frequently changed code needs better coverage</li>
<li><strong>Risk</strong>: Security, data integrity, financial calculations</li>
</ul>
<h3 id="4-track-progress-over-time"><a class="header" href="#4-track-progress-over-time">4. Track Progress Over Time</a></h3>
<p>Run debtmap regularly to track coverage improvements:</p>
<pre><code class="language-bash"># Weekly coverage check
debtmap analyze --coverage-path ./coverage/lcov.info &gt; weekly-gaps.txt
</code></pre>
<p>Compare reports to see gap reduction progress.</p>
<h2 id="troubleshooting-11"><a class="header" href="#troubleshooting-11">Troubleshooting</a></h2>
<h3 id="coverage-data-unavailable-for-all-functions"><a class="header" href="#coverage-data-unavailable-for-all-functions">“Coverage data unavailable” for all functions</a></h3>
<p><strong>Cause</strong>: Debtmap can’t find or parse LCOV file</p>
<p><strong>Solutions</strong>:</p>
<ul>
<li>Verify <code>--coverage-path</code> points to valid LCOV file</li>
<li>Ensure LCOV file was generated recently</li>
<li>Check file permissions (readable by debtmap)</li>
<li>Validate LCOV format: <code>head -20 ./coverage/lcov.info</code></li>
</ul>
<h3 id="line-numbers-dont-match-source-code"><a class="header" href="#line-numbers-dont-match-source-code">Line numbers don’t match source code</a></h3>
<p><strong>Cause</strong>: Source code changed since coverage was generated</p>
<p><strong>Solutions</strong>:</p>
<ul>
<li>Re-run tests with coverage collection</li>
<li>Ensure clean build before coverage run</li>
<li>Commit code before running coverage</li>
</ul>
<h3 id="estimated-gaps-for-functions-with-lcov-data"><a class="header" href="#estimated-gaps-for-functions-with-lcov-data">Estimated gaps for functions with LCOV data</a></h3>
<p><strong>Cause</strong>: Function name or path mismatch</p>
<p><strong>Solutions</strong>:</p>
<ul>
<li>Check function names match exactly (case-sensitive)</li>
<li>Verify file paths are consistent (relative vs absolute)</li>
<li>Enable debug logging: <code>debtmap analyze --log-level debug</code></li>
</ul>
<h3 id="missing-functions-in-coverage-report"><a class="header" href="#missing-functions-in-coverage-report">Missing functions in coverage report</a></h3>
<p><strong>Cause</strong>: Functions not instrumented or filtered out</p>
<p><strong>Solutions</strong>:</p>
<ul>
<li>Check coverage tool configuration</li>
<li>Ensure test execution reaches those functions</li>
<li>Verify functions aren’t in excluded paths</li>
</ul>
<h2 id="related-topics-3"><a class="header" href="#related-topics-3">Related Topics</a></h2>
<ul>
<li><a href="#coverage-integration-1">Coverage Integration</a> - Detailed coverage tool setup</li>
<li><a href="#tiered-prioritization-3">Tiered Prioritization</a> - How coverage gaps affect priority</li>
<li><a href="#scoring-strategies">Scoring Strategies</a> - Coverage weight in debt scoring</li>
<li><a href="#metrics-reference">Metrics Reference</a> - All coverage-related metrics</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="coverage-integration-1"><a class="header" href="#coverage-integration-1">Coverage Integration</a></h1>
<p>Coverage integration is one of Debtmap’s most powerful capabilities, enabling <strong>risk-based prioritization</strong> by correlating complexity metrics with test coverage. This helps you identify truly risky code—functions that are both complex and untested—rather than just highlighting complex but well-tested functions.</p>
<h2 id="why-coverage-matters"><a class="header" href="#why-coverage-matters">Why Coverage Matters</a></h2>
<p>Without coverage data, complexity analysis shows you <em>what’s complex</em>, but not <em>what’s risky</em>. A complex function with 100% test coverage poses far less risk than a simple function with 0% coverage on a critical path.</p>
<p>Coverage integration transforms Debtmap from a complexity analyzer into a <strong>risk assessment tool</strong>:</p>
<ul>
<li><strong>Prioritize testing efforts</strong>: Focus on high-complexity functions with low coverage</li>
<li><strong>Validate refactoring safety</strong>: See which complex code is already protected by tests</li>
<li><strong>Risk-based sprint planning</strong>: Surface truly risky code ahead of well-tested complexity</li>
<li><strong>Quantify risk reduction</strong>: Measure how coverage improvements reduce project risk</li>
</ul>
<h2 id="lcov-format-the-universal-standard"><a class="header" href="#lcov-format-the-universal-standard">LCOV Format: The Universal Standard</a></h2>
<p>Debtmap uses the <strong>LCOV format</strong> for coverage data. LCOV is a language-agnostic standard supported by virtually all coverage tools across all major languages.</p>
<h3 id="why-lcov"><a class="header" href="#why-lcov">Why LCOV?</a></h3>
<ul>
<li><strong>Universal compatibility</strong>: Works with Rust, Python, JavaScript, TypeScript, Go, and more</li>
<li><strong>Tool independence</strong>: Not tied to any specific test framework</li>
<li><strong>Simple text format</strong>: Easy to inspect and debug</li>
<li><strong>Widely supported</strong>: Generated by most modern coverage tools</li>
</ul>
<h3 id="lcov-file-structure"><a class="header" href="#lcov-file-structure">LCOV File Structure</a></h3>
<p>An LCOV file contains line-by-line coverage information:</p>
<pre><code class="language-lcov">SF:src/analyzer.rs
FN:42,calculate_complexity
FNDA:15,calculate_complexity
DA:42,15
DA:43,15
DA:44,12
DA:45,0
LH:3
LF:4
end_of_record
</code></pre>
<ul>
<li><code>SF:</code> - Source file path</li>
<li><code>FN:</code> - Function name and starting line</li>
<li><code>FNDA:</code> - Function execution count</li>
<li><code>DA:</code> - Line execution data (line number, hit count)</li>
<li><code>LH:</code> - Lines hit</li>
<li><code>LF:</code> - Lines found (total)</li>
</ul>
<h3 id="rust-name-demangling"><a class="header" href="#rust-name-demangling">Rust Name Demangling</a></h3>
<p>For Rust projects, Debtmap includes sophisticated name demangling to correlate LCOV coverage with analyzed functions. The demangling system handles:</p>
<p><strong>Mangling Schemes</strong>:</p>
<ul>
<li><strong>v0 scheme</strong>: Starts with <code>_RNv</code> (modern Rust, default since 1.38)</li>
<li><strong>Legacy scheme</strong>: Starts with <code>_ZN</code> (older Rust versions)</li>
</ul>
<p><strong>Normalization Process</strong> (see <code>src/risk/lcov.rs:demangle_function_name</code> and <code>normalize_demangled_name</code>):</p>
<ol>
<li><strong>Demangle</strong>: Convert mangled symbols to human-readable names</li>
<li><strong>Strip crate hashes</strong>: Remove build-specific hash IDs (e.g., <code>debtmap[71f4b4990cdcf1ab]</code> → <code>debtmap</code>)</li>
<li><strong>Strip generic parameters</strong>: Remove type parameters (e.g., <code>HashMap&lt;K,V&gt;::insert</code> → <code>HashMap::insert</code>)</li>
<li><strong>Extract method names</strong>: Store both full path and method name for flexible matching</li>
</ol>
<p><strong>Examples</strong>:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Original (in LCOV)</th><th>After Demangling</th><th>Normalized Full Path</th><th>Method Name</th></tr>
</thead>
<tbody>
<tr><td><code>_RNvXs0_14debtmap...visit_expr</code></td><td><code>&lt;debtmap[hash]::Type&gt;::visit_expr</code></td><td><code>debtmap::Type::visit_expr</code></td><td><code>visit_expr</code></td></tr>
<tr><td><code>Type::method::&lt;T&gt;</code></td><td><code>Type::method::&lt;T&gt;</code></td><td><code>Type::method</code></td><td><code>method</code></td></tr>
<tr><td><code>std::vec::Vec&lt;T&gt;::push</code></td><td><code>std::vec::Vec&lt;T&gt;::push</code></td><td><code>std::vec::Vec::push</code></td><td><code>push</code></td></tr>
</tbody>
</table>
</div>
<p>This normalization enables Debtmap to match coverage data even when:</p>
<ul>
<li>Crate hashes change between builds</li>
<li>Multiple monomorphizations of generic functions exist</li>
<li>LCOV stores simplified names while Debtmap uses qualified names</li>
</ul>
<h2 id="generating-coverage-data"><a class="header" href="#generating-coverage-data">Generating Coverage Data</a></h2>
<h3 id="rust-cargo-tarpaulin"><a class="header" href="#rust-cargo-tarpaulin">Rust: cargo-tarpaulin</a></h3>
<p><strong>Installation:</strong></p>
<pre><code class="language-bash">cargo install cargo-tarpaulin
</code></pre>
<p><strong>Generate LCOV:</strong></p>
<pre><code class="language-bash">cargo tarpaulin --out lcov --output-dir target/coverage
</code></pre>
<p><strong>Analyze with Debtmap:</strong></p>
<pre><code class="language-bash">debtmap analyze . --lcov target/coverage/lcov.info
</code></pre>
<p><strong>Common Issues:</strong></p>
<ul>
<li>Ensure tests compile before running tarpaulin</li>
<li>Use <code>--ignore-tests</code> if tests themselves show up in coverage</li>
<li>Check paths match your project structure (relative to project root)</li>
</ul>
<h3 id="javascripttypescript-jest"><a class="header" href="#javascripttypescript-jest">JavaScript/TypeScript: Jest</a></h3>
<p><strong>Configuration (package.json or jest.config.js):</strong></p>
<pre><code class="language-json">{
  "jest": {
    "coverageReporters": ["lcov", "text"]
  }
}
</code></pre>
<p><strong>Generate Coverage:</strong></p>
<pre><code class="language-bash">npm test -- --coverage
</code></pre>
<p><strong>Analyze with Debtmap:</strong></p>
<pre><code class="language-bash">debtmap analyze . --lcov coverage/lcov.info
</code></pre>
<h3 id="python-pytest-cov"><a class="header" href="#python-pytest-cov">Python: pytest-cov</a></h3>
<p><strong>Installation:</strong></p>
<pre><code class="language-bash">pip install pytest-cov
</code></pre>
<p><strong>Generate LCOV:</strong></p>
<pre><code class="language-bash">pytest --cov=src --cov-report=lcov
</code></pre>
<p><strong>Analyze with Debtmap:</strong></p>
<pre><code class="language-bash">debtmap analyze . --lcov coverage.lcov
</code></pre>
<h3 id="go-go-test-with-gocover-cobertura"><a class="header" href="#go-go-test-with-gocover-cobertura">Go: go test with gocover-cobertura</a></h3>
<p><strong>Generate Coverage:</strong></p>
<pre><code class="language-bash">go test -coverprofile=coverage.out ./...
gocover-cobertura &lt; coverage.out &gt; coverage.xml
# Convert to LCOV using lcov tools
</code></pre>
<p><strong>Note</strong>: Go’s native coverage format requires conversion. Most CI systems support LCOV conversion plugins.</p>
<h2 id="role-based-coverage-expectations"><a class="header" href="#role-based-coverage-expectations">Role-Based Coverage Expectations</a></h2>
<p>Not all functions need the same level of test coverage. Debtmap uses a <strong>role-based coverage expectation system</strong> to adjust scoring based on function purpose (see <code>src/priority/scoring/coverage_expectations.rs</code> and <code>src/risk/evidence/coverage_analyzer.rs</code>).</p>
<h3 id="function-roles-and-coverage-targets"><a class="header" href="#function-roles-and-coverage-targets">Function Roles and Coverage Targets</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Role</th><th>Coverage Range</th><th>Rationale</th><th>Role Weight</th></tr>
</thead>
<tbody>
<tr><td><strong>PureLogic</strong></td><td>90-95%</td><td>Business logic requires comprehensive testing</td><td>1.0</td></tr>
<tr><td><strong>EntryPoint</strong></td><td>70-80%</td><td>Better tested with integration tests</td><td>0.9</td></tr>
<tr><td><strong>Orchestrator</strong></td><td>60-80%</td><td>Coordinates other functions, moderate complexity</td><td>0.6</td></tr>
<tr><td><strong>IOWrapper</strong></td><td>40-60%</td><td>Thin I/O layer, often integration-tested</td><td>0.4</td></tr>
<tr><td><strong>PatternMatch</strong></td><td>50-70%</td><td>Simple pattern matching, lower complexity</td><td>0.3</td></tr>
<tr><td><strong>Debug</strong></td><td>20-30%</td><td>Diagnostic functions, low priority</td><td>0.2</td></tr>
<tr><td><strong>Unknown</strong></td><td>70-90%</td><td>Default for unclassified functions</td><td>0.8</td></tr>
</tbody>
</table>
</div>
<p><strong>Source</strong>: See <code>src/priority/semantic_classifier/mod.rs:25-32</code> for role definitions and <code>src/risk/evidence/coverage_analyzer.rs:63-73</code> for role weight calculation.</p>
<h3 id="coverage-gap-severity-classification"><a class="header" href="#coverage-gap-severity-classification">Coverage Gap Severity Classification</a></h3>
<p>Debtmap classifies coverage gaps into 4 severity levels (see <code>src/priority/scoring/coverage_expectations.rs:GapSeverity</code>):</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Severity</th><th>Condition</th><th>Impact on Score</th><th>Visual</th></tr>
</thead>
<tbody>
<tr><td><strong>None</strong></td><td>Coverage ≥ target</td><td>No penalty</td><td>🟢</td></tr>
<tr><td><strong>Minor</strong></td><td>Coverage between min and target</td><td>Small penalty</td><td>🟡</td></tr>
<tr><td><strong>Moderate</strong></td><td>Coverage between 50% of min and min</td><td>Medium penalty</td><td>🟠</td></tr>
<tr><td><strong>Critical</strong></td><td>Coverage &lt; 50% of min</td><td>High penalty</td><td>🔴</td></tr>
</tbody>
</table>
</div>
<p><strong>Example</strong>: For <code>PureLogic</code> (target: 95%, min: 90%):</p>
<ul>
<li>96% coverage → None (🟢)</li>
<li>92% coverage → Minor (🟡)</li>
<li>75% coverage → Moderate (🟠)</li>
<li>40% coverage → Critical (🔴)</li>
</ul>
<h3 id="test-quality-classification"><a class="header" href="#test-quality-classification">Test Quality Classification</a></h3>
<p>Coverage is classified into quality tiers based on both percentage and complexity (see <code>src/risk/evidence/coverage_analyzer.rs:44-52</code>):</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn classify_test_quality(coverage: f64, complexity: u32) -&gt; TestQuality {
    match () {
        _ if coverage &gt;= 90.0 &amp;&amp; complexity &lt;= 5 =&gt; TestQuality::Excellent,
        _ if coverage &gt;= 80.0 =&gt; TestQuality::Good,
        _ if coverage &gt;= 60.0 =&gt; TestQuality::Adequate,
        _ if coverage &gt; 0.0 =&gt; TestQuality::Poor,
        _ =&gt; TestQuality::Missing,
    }
}
<span class="boring">}</span></code></pre>
<p><strong>Quality Levels</strong>:</p>
<ul>
<li><strong>Excellent</strong>: ≥90% coverage AND complexity ≤5 (simple, well-tested)</li>
<li><strong>Good</strong>: ≥80% coverage (comprehensive testing)</li>
<li><strong>Adequate</strong>: ≥60% coverage (basic testing)</li>
<li><strong>Poor</strong>: &gt;0% but &lt;60% coverage (incomplete testing)</li>
<li><strong>Missing</strong>: 0% coverage (no tests)</li>
</ul>
<h3 id="how-roles-affect-scoring"><a class="header" href="#how-roles-affect-scoring">How Roles Affect Scoring</a></h3>
<p>Role weights adjust the coverage penalty applied to functions (see <code>src/risk/evidence/coverage_analyzer.rs:63-73</code>):</p>
<p><strong>Example</strong>: A function with 50% coverage:</p>
<ul>
<li><strong>PureLogic</strong> (weight: 1.0): Full penalty, high urgency</li>
<li><strong>Orchestrator</strong> (weight: 0.6): 60% of full penalty</li>
<li><strong>Debug</strong> (weight: 0.2): Only 20% of full penalty, low urgency</li>
</ul>
<p>This ensures that:</p>
<ol>
<li>Business logic functions are prioritized for testing</li>
<li>Entry points rely more on integration tests</li>
<li>Diagnostic/debug functions don’t create noise</li>
</ol>
<h2 id="how-coverage-affects-scoring"><a class="header" href="#how-coverage-affects-scoring">How Coverage Affects Scoring</a></h2>
<p>Coverage data fundamentally changes how Debtmap calculates debt scores. The scoring system operates in <strong>two different modes</strong> depending on whether coverage data is available.</p>
<h3 id="scoring-modes"><a class="header" href="#scoring-modes">Scoring Modes</a></h3>
<p><strong>Mode 1: With Coverage Data (Dampening Multiplier)</strong></p>
<p>When you provide an LCOV file with <code>--lcov</code>, coverage acts as a <strong>dampening multiplier</strong> that reduces scores for well-tested code:</p>
<pre><code>Base Score = (Complexity Factor × 0.50) + (Dependency Factor × 0.25)
Coverage Multiplier = 1.0 - coverage_percentage
Final Score = Base Score × Coverage Multiplier
</code></pre>
<p>This is the <strong>current implementation</strong> as of spec 122. Coverage dampens the base score rather than contributing as an additive component.</p>
<p><strong>Mode 2: Without Coverage Data (Weighted Sum)</strong></p>
<p>When no coverage data is available, Debtmap falls back to a weighted sum model:</p>
<pre><code>Final Score = (Coverage × 0.50) + (Complexity × 0.35) + (Dependency × 0.15)
</code></pre>
<p>In this mode, coverage is assumed to be 0% (worst case), giving it a weight of 50% in the total score. See <code>src/priority/scoring/calculation.rs:119-129</code> for the implementation.</p>
<h3 id="coverage-dampening-multiplier"><a class="header" href="#coverage-dampening-multiplier">Coverage Dampening Multiplier</a></h3>
<p>When coverage data is provided, it acts as a <strong>multiplier</strong> that dampens the base score:</p>
<pre><code>Coverage Multiplier = 1.0 - coverage_percentage
Final Score = Base Score × Coverage Multiplier
</code></pre>
<p><strong>Examples:</strong></p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Base Score</th><th>Coverage</th><th>Multiplier</th><th>Final Score</th><th>Priority</th></tr>
</thead>
<tbody>
<tr><td>8.5</td><td>100%</td><td>0.0</td><td>0.0</td><td>Minimal (well-tested)</td></tr>
<tr><td>8.5</td><td>50%</td><td>0.5</td><td>4.25</td><td>Medium</td></tr>
<tr><td>8.5</td><td>0%</td><td>1.0</td><td>8.5</td><td>High (untested)</td></tr>
</tbody>
</table>
</div>
<p><strong>Key Insight</strong>: Complex but well-tested code automatically drops in priority, while untested complex code rises to the top.</p>
<p><strong>Special Cases:</strong></p>
<ul>
<li><strong>Test functions</strong>: Coverage multiplier = 0.0 (tests get near-zero scores regardless of complexity)</li>
<li><strong>Entry points</strong>: Handled through semantic classification (FunctionRole) system with role multipliers, not coverage-specific weighting</li>
</ul>
<p><strong>Invariant</strong>: Total debt score with coverage ≤ total debt score without coverage.</p>
<p><strong>Implementation</strong>: See <code>src/priority/scoring/calculation.rs:68-82</code> for the coverage dampening calculation.</p>
<h2 id="transitive-coverage-propagation"><a class="header" href="#transitive-coverage-propagation">Transitive Coverage Propagation</a></h2>
<p>Debtmap doesn’t just look at <em>direct</em> coverage—it propagates coverage through the <strong>call graph</strong> using transitive analysis.</p>
<h3 id="how-it-works-3"><a class="header" href="#how-it-works-3">How It Works</a></h3>
<p>A function’s effective coverage considers:</p>
<ol>
<li><strong>Direct coverage</strong>: Lines executed by tests</li>
<li><strong>Caller coverage</strong>: Coverage of functions that call this function</li>
</ol>
<pre><code>Transitive Coverage = Direct Coverage + Σ(Caller Coverage × Weight)
</code></pre>
<h3 id="algorithm-parameters"><a class="header" href="#algorithm-parameters">Algorithm Parameters</a></h3>
<p>The transitive coverage propagation uses carefully tuned parameters to balance accuracy and performance:</p>
<ul>
<li><strong>Well-Tested Threshold</strong>: 80% - Only functions with ≥80% direct coverage contribute to indirect coverage, ensuring high confidence</li>
<li><strong>Distance Discount</strong>: 70% per hop - Each level of indirection reduces contribution by 30%, reflecting decreased confidence</li>
<li><strong>Maximum Distance</strong>: 3 hops - Limits recursion depth to prevent exponential complexity (after 3 hops, contribution drops to ~34%)</li>
</ul>
<p>These parameters ensure that indirect coverage signals are meaningful while preventing false confidence from distant call relationships. See <code>src/priority/coverage_propagation.rs:38-46</code> for the implementation.</p>
<h3 id="why-it-matters"><a class="header" href="#why-it-matters">Why It Matters</a></h3>
<p>A function with 0% direct coverage might have high transitive coverage if it’s only called by well-tested functions:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// direct_coverage = 0%
// But called only by `process_request` (100% coverage)
// → transitive_coverage = 85%
fn validate_input(data: &amp;str) -&gt; bool {
    data.len() &gt; 0
}

// direct_coverage = 100%
fn process_request(input: String) -&gt; Result&lt;()&gt; {
    if !validate_input(&amp;input) {
        return Err("Invalid");
    }
    // ...
}
<span class="boring">}</span></code></pre>
<p><strong>Effect</strong>: <code>validate_input</code> has reduced urgency because it’s only reachable through well-tested code paths.</p>
<h3 id="generic-function-coverage-monomorphization"><a class="header" href="#generic-function-coverage-monomorphization">Generic Function Coverage (Monomorphization)</a></h3>
<p><strong>Challenge</strong>: Generic functions in Rust get monomorphized into multiple versions, each appearing as a separate function in LCOV with different coverage.</p>
<p>For example, <code>execute::&lt;T&gt;()</code> might appear as:</p>
<ul>
<li><code>execute::&lt;WorkflowExecutor&gt;</code> - 70% coverage, uncovered: [10, 20, 30]</li>
<li><code>execute::&lt;MockExecutor&gt;</code> - 80% coverage, uncovered: [20, 40]</li>
</ul>
<p><strong>Debtmap’s Solution</strong> (see <code>src/risk/coverage_index.rs:merge_coverage</code>):</p>
<ol>
<li>
<p><strong>Base Function Index</strong>: Maps base names to all monomorphized versions</p>
<ul>
<li><code>(file, "execute")</code> → <code>["execute::&lt;WorkflowExecutor&gt;", "execute::&lt;MockExecutor&gt;"]</code></li>
</ul>
</li>
<li>
<p><strong>Intersection Merge Strategy</strong>: A line is uncovered ONLY if ALL versions leave it uncovered</p>
<ul>
<li>Coverage percentage: Average across all versions (75% in example)</li>
<li>Uncovered lines: Intersection of uncovered sets ([20] in example)</li>
</ul>
</li>
<li>
<p><strong>Conservative Approach</strong>: Ensures we don’t claim coverage that doesn’t exist in all code paths</p>
</li>
</ol>
<p><strong>Example Aggregation</strong>:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Version</th><th>Coverage</th><th>Uncovered Lines</th></tr>
</thead>
<tbody>
<tr><td><code>execute::&lt;WorkflowExecutor&gt;</code></td><td>70%</td><td>[10, 20, 30]</td></tr>
<tr><td><code>execute::&lt;MockExecutor&gt;</code></td><td>80%</td><td>[20, 40]</td></tr>
<tr><td><strong>Aggregated Result</strong></td><td><strong>75%</strong></td><td><strong>[20]</strong></td></tr>
</tbody>
</table>
</div>
<p>Line 20 is uncovered in BOTH versions, so it’s risky. Lines 10, 30, 40 are covered in at least one version, so they’re considered safer.</p>
<p><strong>Implementation</strong>: See <code>src/risk/coverage_index.rs:AggregateCoverage</code> and <code>merge_coverage</code> (lines 50-139).</p>
<h3 id="trait-method-coverage-matching"><a class="header" href="#trait-method-coverage-matching">Trait Method Coverage Matching</a></h3>
<p><strong>Challenge</strong>: LCOV files may store trait method implementations with simplified names while Debtmap tracks fully qualified names.</p>
<p><strong>Example Mismatch</strong>:</p>
<ul>
<li><strong>LCOV stores</strong>: <code>visit_expr</code> (demangled method name)</li>
<li><strong>Debtmap queries</strong>: <code>RecursiveMatchDetector::visit_expr</code> (fully qualified)</li>
</ul>
<p><strong>Debtmap’s Solution</strong> (see <code>src/risk/coverage_index.rs:generate_name_variants</code> and <code>method_name_index</code>):</p>
<ol>
<li>
<p><strong>Name Variant Generation</strong>: Extract method name from qualified paths</p>
<ul>
<li><code>RecursiveMatchDetector::visit_expr</code> → generates variant <code>visit_expr</code></li>
</ul>
</li>
<li>
<p><strong>Method Name Index</strong>: O(1) lookup from method name to all implementations</p>
<ul>
<li><code>(file, "visit_expr")</code> → <code>["RecursiveMatchDetector::visit_expr", "_RNvXs0_...visit_expr"]</code></li>
</ul>
</li>
<li>
<p><strong>Multi-Strategy Matching</strong>: Try variants if exact match fails</p>
<ul>
<li>First: Exact qualified name match</li>
<li>Second: Method name variant match</li>
<li>Third: Line-based fallback</li>
</ul>
</li>
</ol>
<p><strong>Implementation</strong>: See <code>src/risk/coverage_index.rs:192</code> (method_name_index) and <code>generate_name_variants</code> (lines 12-48).</p>
<h2 id="performance-characteristics-2"><a class="header" href="#performance-characteristics-2">Performance Characteristics</a></h2>
<p>Coverage integration is highly optimized for large codebases using a multi-strategy lookup system.</p>
<h3 id="coverage-index-structure"><a class="header" href="#coverage-index-structure">Coverage Index Structure</a></h3>
<p>The coverage index uses nested HashMaps plus supporting indexes for O(1) lookups (see <code>src/risk/coverage_index.rs:172-196</code>):</p>
<ol>
<li><strong>by_file</strong>: <code>HashMap&lt;PathBuf, HashMap&lt;String, FunctionCoverage&gt;&gt;</code> - Primary nested index</li>
<li><strong>by_line</strong>: <code>HashMap&lt;PathBuf, BTreeMap&lt;usize, FunctionCoverage&gt;&gt;</code> - Line-based range queries</li>
<li><strong>base_function_index</strong>: Maps base names to monomorphized versions (generic handling)</li>
<li><strong>method_name_index</strong>: Maps method names to full qualified names (trait methods)</li>
</ol>
<h3 id="lookup-strategy-waterfall"><a class="header" href="#lookup-strategy-waterfall">Lookup Strategy Waterfall</a></h3>
<p>Debtmap tries 5 strategies in order, stopping at the first match (see <code>src/risk/coverage_index.rs:get_function_coverage_with_line</code>):</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Strategy</th><th>Complexity</th><th>When It Matches</th><th>Typical Latency</th></tr>
</thead>
<tbody>
<tr><td><strong>1. Exact Match</strong></td><td>O(1)</td><td>File path and function name exactly match LCOV</td><td>~0.5μs</td></tr>
<tr><td><strong>2. Suffix Matching</strong></td><td>O(files)</td><td>Query path ends with LCOV file path, then O(1) function lookup</td><td>~5-8μs</td></tr>
<tr><td><strong>3. Reverse Suffix</strong></td><td>O(files)</td><td>LCOV file path ends with query path, then O(1) function lookup</td><td>~5-8μs</td></tr>
<tr><td><strong>4. Normalized Equality</strong></td><td>O(files)</td><td>Paths equal after normalizing <code>./</code> prefix, then O(1) function lookup</td><td>~5-8μs</td></tr>
<tr><td><strong>5. Line-Based Fallback</strong></td><td>O(log n)</td><td>Match by line number ±2 tolerance using BTreeMap range query</td><td>~10-15μs</td></tr>
</tbody>
</table>
</div>
<p><strong>Strategy Optimizations</strong>:</p>
<ul>
<li>Path strategies iterate over FILES (typically ~375) not functions (~1,500+), providing 4x-50x speedup</li>
<li>Each path strategy tries 3 name matching techniques per file:
<ol>
<li>Exact name match</li>
<li>Function name suffix match (handles qualified vs short names)</li>
<li>Method name match (handles trait implementations)</li>
</ol>
</li>
</ul>
<h3 id="performance-benchmarks"><a class="header" href="#performance-benchmarks">Performance Benchmarks</a></h3>
<ul>
<li><strong>Index Build</strong>: O(n), ~20-30ms for 5,000 functions</li>
<li><strong>Exact Lookup</strong>: O(1), ~0.5μs per lookup</li>
<li><strong>Path Strategy Fallback</strong>: O(files) × O(1), ~5-8μs when exact match fails</li>
<li><strong>Line-Based Fallback</strong>: O(log n), ~10-15μs when all path strategies fail</li>
<li><strong>Memory Usage</strong>: ~200 bytes per record (~2MB for 5,000 functions)</li>
<li><strong>Thread Safety</strong>: Lock-free parallel access via <code>Arc&lt;CoverageIndex&gt;</code></li>
<li><strong>Analysis Overhead</strong>: ~2.5x baseline (target: ≤3x)</li>
</ul>
<p><strong>Result</strong>: Coverage integration adds minimal overhead even on projects with thousands of functions.</p>
<h3 id="debugging-lookup-performance"><a class="header" href="#debugging-lookup-performance">Debugging Lookup Performance</a></h3>
<p>The coverage index tracks detailed statistics for performance analysis (see <code>src/risk/coverage_index.rs:CoverageIndexStats</code>):</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct CoverageIndexStats {
    pub total_files: usize,
    pub total_records: usize,
    pub index_build_time: Duration,
    pub estimated_memory_bytes: usize,
}
<span class="boring">}</span></code></pre>
<p>Enable verbose logging to see which strategy matched:</p>
<pre><code class="language-bash">debtmap analyze . --lcov coverage.info -vv
</code></pre>
<p>Output shows strategy attempts:</p>
<pre><code>Looking up coverage for function 'visit_expr' in file 'src/detector.rs'
Strategy 1: Suffix matching...
  Found path match: 'src/detector.rs'
  ✓ Matched method name 'visit_expr' -&gt; 'RecursiveMatchDetector::visit_expr': 85%
</code></pre>
<h2 id="cli-options-reference-1"><a class="header" href="#cli-options-reference-1">CLI Options Reference</a></h2>
<h3 id="primary-coverage-options"><a class="header" href="#primary-coverage-options">Primary Coverage Options</a></h3>
<pre><code class="language-bash"># Provide LCOV coverage file
debtmap analyze . --coverage-file path/to/lcov.info

# Shorthand alias
debtmap analyze . --lcov path/to/lcov.info
</code></pre>
<h3 id="context-providers-1"><a class="header" href="#context-providers-1">Context Providers</a></h3>
<p>Coverage can be combined with other context providers for nuanced risk assessment:</p>
<pre><code class="language-bash"># Enable all context providers (includes coverage propagation)
debtmap analyze . --lcov coverage.info --enable-context

# Specify specific providers
debtmap analyze . --lcov coverage.info \
  --context-providers critical_path,dependency,git_history

# Disable specific providers
debtmap analyze . --lcov coverage.info \
  --disable-context git_history
</code></pre>
<p><strong>Available Context Providers</strong>:</p>
<ul>
<li><code>critical_path</code>: Identifies functions on critical execution paths</li>
<li><code>dependency</code>: Analyzes dependency relationships and impact</li>
<li><code>git_history</code>: Uses change frequency from version control</li>
</ul>
<p>See <a href="#scoring-strategies">Scoring Strategies</a> for details on how these combine.</p>
<h3 id="validate-command-support"><a class="header" href="#validate-command-support">Validate Command Support</a></h3>
<p>The <code>validate</code> command also supports coverage integration for risk-based quality gates:</p>
<pre><code class="language-bash"># Fail CI builds if untested complex code exceeds thresholds
debtmap validate . --lcov coverage.info --max-debt-density 50
</code></pre>
<p>See <a href="#cli-reference">CLI Reference</a> for complete validation options.</p>
<h2 id="troubleshooting-coverage-integration"><a class="header" href="#troubleshooting-coverage-integration">Troubleshooting Coverage Integration</a></h2>
<h3 id="coverage-not-correlating-with-functions"><a class="header" href="#coverage-not-correlating-with-functions">Coverage Not Correlating with Functions</a></h3>
<p><strong>Symptoms</strong>:</p>
<ul>
<li>Debtmap shows 0% coverage for all functions</li>
<li>Warning: “No coverage data correlated with analyzed functions”</li>
</ul>
<p><strong>Solutions</strong>:</p>
<ol>
<li><strong>Verify LCOV Format</strong>:</li>
</ol>
<pre><code class="language-bash">head coverage.info
# Should show: SF:, FN:, DA: lines
</code></pre>
<ol start="2">
<li><strong>Check Path Matching</strong>:
Coverage file paths must be relative to project root:</li>
</ol>
<pre><code class="language-bash"># Good: SF:src/analyzer.rs
# Bad:  SF:/home/user/project/src/analyzer.rs
</code></pre>
<ol start="3">
<li><strong>Use explain-coverage Command</strong>:</li>
</ol>
<pre><code class="language-bash">debtmap explain-coverage . --lcov coverage.info \
  --function validate_input \
  --file src/validator.rs \
  --format json
</code></pre>
<p>The <code>explain-coverage</code> command provides detailed diagnostics:</p>
<p><strong>JSON Output Structure</strong> (see <code>src/commands/explain_coverage.rs:ExplainCoverageResult</code>):</p>
<pre><code class="language-json">{
  "function_name": "validate_input",
  "file_path": "src/validator.rs",
  "coverage_found": true,
  "coverage_percentage": 85.0,
  "matched_by_strategy": "Suffix Matching",
  "attempts": [
    {
      "strategy": "Exact Match",
      "success": false,
      "matched_function": null,
      "coverage_percentage": null
    },
    {
      "strategy": "Suffix Matching",
      "success": true,
      "matched_function": "validator::validate_input",
      "matched_file": "src/validator.rs",
      "coverage_percentage": 85.0
    }
  ],
  "available_functions": [
    "src/validator.rs::validate_input",
    "src/processor.rs::process_request"
  ],
  "available_files": [
    "src/validator.rs",
    "src/processor.rs"
  ]
}
</code></pre>
<p><strong>Key Fields</strong>:</p>
<ul>
<li><code>attempts[]</code>: Shows all 5 strategies tried and which succeeded</li>
<li><code>available_functions[]</code>: All functions found in LCOV (helps identify naming mismatches)</li>
<li><code>available_files[]</code>: All files in LCOV (helps debug path issues)</li>
</ul>
<ol start="4">
<li><strong>Enable Verbose Logging</strong>:</li>
</ol>
<pre><code class="language-bash">debtmap analyze . --lcov coverage.info -vv
</code></pre>
<p>This shows coverage lookup details for each function during analysis.</p>
<ol start="5">
<li><strong>Verify Coverage Tool Output</strong>:</li>
</ol>
<pre><code class="language-bash"># Ensure your coverage tool generated line data (DA: records)
grep "^DA:" coverage.info | head
</code></pre>
<h3 id="functions-still-show-up-despite-100-coverage"><a class="header" href="#functions-still-show-up-despite-100-coverage">Functions Still Show Up Despite 100% Coverage</a></h3>
<p><strong>This is expected behavior</strong> when:</p>
<ul>
<li>Function has high complexity (cyclomatic &gt; 10)</li>
<li>Function has other debt issues (duplication, nesting, etc.)</li>
<li>You’re viewing function-level output (coverage dampens but doesn’t eliminate)</li>
</ul>
<p><strong>Coverage reduces priority but doesn’t hide issues</strong>. Use filters to focus:</p>
<pre><code class="language-bash"># Show only critical and high priority items
debtmap analyze . --lcov coverage.info --min-priority high

# Show top 10 most urgent items
debtmap analyze . --lcov coverage.info --top 10
</code></pre>
<h3 id="coverage-file-path-issues"><a class="header" href="#coverage-file-path-issues">Coverage File Path Issues</a></h3>
<p><strong>Problem</strong>: Can’t find coverage file</p>
<p><strong>Solutions</strong>:</p>
<pre><code class="language-bash"># Use absolute path
debtmap analyze . --lcov /absolute/path/to/coverage.info

# Or ensure relative path is from project root
debtmap analyze . --lcov ./target/coverage/lcov.info
</code></pre>
<h3 id="lcov-format-errors"><a class="header" href="#lcov-format-errors">LCOV Format Errors</a></h3>
<p><strong>Problem</strong>: “Invalid LCOV format” error</p>
<p><strong>Causes</strong>:</p>
<ul>
<li>Non-LCOV format (Cobertura XML, JaCoCo, etc.)</li>
<li>Corrupted file</li>
<li>Wrong file encoding</li>
</ul>
<p><strong>Solutions</strong>:</p>
<ul>
<li>Verify your coverage tool is configured for LCOV output</li>
<li>Check for binary/encoding issues: <code>file coverage.info</code></li>
<li>Regenerate coverage with explicit LCOV format flag</li>
</ul>
<p>See <a href="#troubleshooting-23">Troubleshooting</a> for more debugging tips.</p>
<h2 id="best-practices-8"><a class="header" href="#best-practices-8">Best Practices</a></h2>
<h3 id="analysis-workflow"><a class="header" href="#analysis-workflow">Analysis Workflow</a></h3>
<ol>
<li>
<p><strong>Generate Coverage Before Analysis</strong>:</p>
<pre><code class="language-bash"># Rust example
cargo tarpaulin --out lcov --output-dir target/coverage
debtmap analyze . --lcov target/coverage/lcov.info
</code></pre>
</li>
<li>
<p><strong>Use Coverage for Sprint Planning</strong>:</p>
<pre><code class="language-bash"># Focus on untested complex code
debtmap analyze . --lcov coverage.info --top 20
</code></pre>
</li>
<li>
<p><strong>Combine with Tiered Prioritization</strong>:
Coverage automatically feeds into <a href="#tiered-prioritization-3">Tiered Prioritization</a>:</p>
<ul>
<li><strong>Tier 1</strong>: Architectural issues (less affected by coverage)</li>
<li><strong>Tier 2</strong>: Complex untested code (coverage &lt; 50%, complexity &gt; 15)</li>
<li><strong>Tier 3</strong>: Testing gaps (coverage &lt; 80%, complexity 10-15)</li>
</ul>
</li>
<li>
<p><strong>Validate Refactoring Impact</strong>:</p>
<pre><code class="language-bash"># Before refactoring
debtmap analyze . --lcov coverage-before.info -o before.json

# After refactoring
debtmap analyze . --lcov coverage-after.info -o after.json

# Compare
debtmap compare --before before.json --after after.json
</code></pre>
</li>
</ol>
<h3 id="testing-strategy"><a class="header" href="#testing-strategy">Testing Strategy</a></h3>
<p><strong>Prioritize testing based on risk</strong>:</p>
<ol>
<li>
<p><strong>High Complexity + Low Coverage = Highest Priority</strong>:</p>
<pre><code class="language-bash">debtmap analyze . --lcov coverage.info \
  --filter Risk --min-priority high
</code></pre>
</li>
<li>
<p><strong>Focus on Business Logic</strong>:
Entry points and infrastructure code have natural coverage patterns. Focus unit tests on business logic functions.</p>
</li>
<li>
<p><strong>Use Dependency Analysis</strong>:</p>
<pre><code class="language-bash">debtmap analyze . --lcov coverage.info \
  --context-providers dependency -vv
</code></pre>
<p>Tests high-dependency functions first—they have the most impact.</p>
</li>
<li>
<p><strong>Don’t Over-Test Entry Points</strong>:
Entry points (main, handlers) are better tested with integration tests, not unit tests. Debtmap applies role multipliers through its semantic classification system (FunctionRole) to adjust scoring for different function types. See <code>src/priority/unified_scorer.rs:149</code> and <code>src/priority/scoring/classification.rs</code> for the classification system.</p>
</li>
</ol>
<h3 id="configuration-7"><a class="header" href="#configuration-7">Configuration</a></h3>
<p>In <code>.debtmap.toml</code>:</p>
<pre><code class="language-toml">[scoring]
# Default weights for scoring WITHOUT coverage data
# When coverage data IS provided, it acts as a dampening multiplier instead
coverage = 0.50  # Default: 50% (only used when no LCOV provided)
complexity = 0.35  # Default: 35%
dependency = 0.15  # Default: 15%

[thresholds]
# Set minimum risk score to filter low-priority items
minimum_risk_score = 15.0

# Skip simple functions even if uncovered
minimum_cyclomatic_complexity = 5
</code></pre>
<p><strong>Important</strong>: These weights are from the deprecated additive scoring model. The current implementation (spec 122) calculates a base score from complexity (50%) and dependency (25%) factors, then applies coverage as a dampening multiplier: <code>Final Score = Base Score × (1.0 - coverage_pct)</code>. These weights only apply when coverage data is <strong>not</strong> available. See <code>src/priority/scoring/calculation.rs:68-82</code> for the coverage dampening calculation and <code>src/priority/scoring/calculation.rs:119-129</code> for the fallback weighted sum mode.</p>
<p>See <a href="#configuration-2">Configuration</a> for complete options.</p>
<h3 id="ci-integration"><a class="header" href="#ci-integration">CI Integration</a></h3>
<p><strong>Example GitHub Actions Workflow</strong>:</p>
<pre><code class="language-yaml">- name: Generate Coverage
  run: cargo tarpaulin --out lcov --output-dir target/coverage

- name: Analyze with Debtmap
  run: |
    debtmap analyze . \
      --lcov target/coverage/lcov.info \
      --format json \
      --output debtmap-report.json

- name: Validate Quality Gates
  run: |
    debtmap validate . \
      --lcov target/coverage/lcov.info \
      --max-debt-density 50
</code></pre>
<p><strong>Quality Gate Strategy</strong>:</p>
<ul>
<li>Fail builds on new critical debt (Tier 1 architectural issues)</li>
<li>Warn on new high-priority untested code (Tier 2)</li>
<li>Track coverage trends over time with <code>compare</code> command</li>
</ul>
<h2 id="complete-language-examples"><a class="header" href="#complete-language-examples">Complete Language Examples</a></h2>
<h3 id="rust-end-to-end"><a class="header" href="#rust-end-to-end">Rust End-to-End</a></h3>
<pre><code class="language-bash"># 1. Generate coverage
cargo tarpaulin --out lcov --output-dir target/coverage

# 2. Verify LCOV output
head target/coverage/lcov.info

# 3. Run Debtmap with coverage
debtmap analyze . --lcov target/coverage/lcov.info

# 4. Interpret results (look for [UNTESTED] markers on high-complexity functions)
</code></pre>
<h3 id="javascripttypescript-end-to-end"><a class="header" href="#javascripttypescript-end-to-end">JavaScript/TypeScript End-to-End</a></h3>
<pre><code class="language-bash"># 1. Configure Jest for LCOV (in package.json or jest.config.js)
# "coverageReporters": ["lcov", "text"]

# 2. Generate coverage
npm test -- --coverage

# 3. Verify LCOV output
head coverage/lcov.info

# 4. Run Debtmap
debtmap analyze . --lcov coverage/lcov.info --languages javascript,typescript
</code></pre>
<h3 id="python-end-to-end"><a class="header" href="#python-end-to-end">Python End-to-End</a></h3>
<pre><code class="language-bash"># 1. Install pytest-cov
pip install pytest-cov

# 2. Generate LCOV coverage
pytest --cov=src --cov-report=lcov

# 3. Verify output
head coverage.lcov

# 4. Run Debtmap
debtmap analyze . --lcov coverage.lcov --languages python
</code></pre>
<h3 id="go-end-to-end"><a class="header" href="#go-end-to-end">Go End-to-End</a></h3>
<pre><code class="language-bash"># 1. Generate native coverage
go test -coverprofile=coverage.out ./...

# 2. Convert to LCOV (requires gocover-cobertura or similar)
# Note: This step is tool-dependent

# 3. Run Debtmap
debtmap analyze . --lcov coverage.lcov --languages go
</code></pre>
<h2 id="faq"><a class="header" href="#faq">FAQ</a></h2>
<h3 id="why-does-my-100-covered-function-still-show-up"><a class="header" href="#why-does-my-100-covered-function-still-show-up">Why does my 100% covered function still show up?</a></h3>
<p>Coverage dampens debt scores but doesn’t eliminate debt. A function with cyclomatic complexity 25 and 100% coverage still represents technical debt—it’s just lower priority than untested complex code.</p>
<p><strong>Use filters to focus on high-priority items</strong>:</p>
<pre><code class="language-bash">debtmap analyze . --lcov coverage.info --top 10
</code></pre>
<h3 id="whats-the-difference-between-direct-and-transitive-coverage"><a class="header" href="#whats-the-difference-between-direct-and-transitive-coverage">What’s the difference between direct and transitive coverage?</a></h3>
<ul>
<li><strong>Direct coverage</strong>: Lines executed directly by tests</li>
<li><strong>Transitive coverage</strong>: Coverage considering call graph (functions called by well-tested code)</li>
</ul>
<p>Transitive coverage reduces urgency for functions only reachable through well-tested paths.</p>
<h3 id="should-i-test-everything-to-100-coverage"><a class="header" href="#should-i-test-everything-to-100-coverage">Should I test everything to 100% coverage?</a></h3>
<p><strong>No.</strong> Use Debtmap’s risk scores to prioritize:</p>
<ol>
<li>Test high-complexity, low-coverage functions first</li>
<li>Entry points are better tested with integration tests</li>
<li>Simple utility functions (complexity &lt; 5) may not need dedicated unit tests</li>
</ol>
<p>Debtmap helps you achieve <strong>optimal coverage</strong>, not maximal coverage.</p>
<h3 id="how-do-i-debug-coverage-correlation-issues"><a class="header" href="#how-do-i-debug-coverage-correlation-issues">How do I debug coverage correlation issues?</a></h3>
<p>Use verbose logging:</p>
<pre><code class="language-bash">debtmap analyze . --lcov coverage.info -vv
</code></pre>
<p>This shows:</p>
<ul>
<li>Coverage file parsing details</li>
<li>Function-to-coverage correlation attempts</li>
<li>Path matching diagnostics</li>
</ul>
<h3 id="can-i-use-coverage-with-validate-command"><a class="header" href="#can-i-use-coverage-with-validate-command">Can I use coverage with validate command?</a></h3>
<p>Yes! The <code>validate</code> command supports <code>--lcov</code> for risk-based quality gates:</p>
<pre><code class="language-bash">debtmap validate . --lcov coverage.info --max-debt-density 50
</code></pre>
<p>See <a href="#validate-command">CLI Reference</a> for details.</p>
<h2 id="further-reading-1"><a class="header" href="#further-reading-1">Further Reading</a></h2>
<ul>
<li><a href="#scoring-strategies">Scoring Strategies</a> - Deep dive into how coverage affects unified scoring</li>
<li><a href="#tiered-prioritization-3">Tiered Prioritization</a> - How coverage fits into tiered priority levels</li>
<li><a href="#cli-reference">CLI Reference</a> - Complete coverage option documentation</li>
<li><a href="#configuration-2">Configuration</a> - Customizing coverage scoring weights</li>
<li><a href="#troubleshooting-23">Troubleshooting</a> - More debugging tips for coverage issues</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="dead-code-analysis"><a class="header" href="#dead-code-analysis">Dead Code Analysis</a></h1>
<p>Debtmap’s Python dead code detection system uses advanced static analysis to identify unused functions with high accuracy and low false positive rates. The analyzer integrates multiple detection systems to provide confidence-scored results that help you make informed decisions about code removal.</p>
<h2 id="overview-10"><a class="header" href="#overview-10">Overview</a></h2>
<p>The dead code analyzer combines several detection techniques:</p>
<ul>
<li><strong>Static call graph analysis</strong> - Tracks which functions call each other across your codebase</li>
<li><strong>Framework pattern detection</strong> - Recognizes entry points from Flask, Django, FastAPI, Click, pytest, and more</li>
<li><strong>Test detection</strong> - Identifies test functions and test files to avoid false positives</li>
<li><strong>Callback tracking</strong> - Detects functions registered as callbacks or event handlers</li>
<li><strong>Import analysis</strong> - Tracks which functions are imported and exported by other modules</li>
<li><strong>Coverage integration</strong> - Uses test coverage data when available to identify live code</li>
<li><strong>Public API detection</strong> - Uses heuristics to identify external API functions</li>
</ul>
<p>This multi-layered approach significantly reduces false positives compared to naive call graph analysis, with the goal of achieving a target false positive rate of less than 10% (see Spec 116 for confidence scoring validation).</p>
<h2 id="confidence-scoring"><a class="header" href="#confidence-scoring">Confidence Scoring</a></h2>
<p>Every analysis result includes a confidence score to help you prioritize code removal:</p>
<h3 id="high-confidence-08-10"><a class="header" href="#high-confidence-08-10">High Confidence (0.8-1.0)</a></h3>
<p><strong>Safe to remove</strong> - These functions are very likely dead code.</p>
<p>Characteristics:</p>
<ul>
<li>No static callers found in the codebase</li>
<li>Not a framework entry point (route, command, view, etc.)</li>
<li>Not a test function or in a test file</li>
<li>Not registered as a callback or event handler</li>
<li>Not exported in <code>__all__</code> or used in public API patterns</li>
<li>Often private functions (starting with <code>_</code>)</li>
</ul>
<p>Example output:</p>
<pre><code>Function: _old_helper
Confidence: High (0.95)
Suggestion: High confidence this function is dead code and can be safely removed.
</code></pre>
<h3 id="medium-confidence-05-08"><a class="header" href="#medium-confidence-05-08">Medium Confidence (0.5-0.8)</a></h3>
<p><strong>Review recommended</strong> - These functions might be dead code but require manual verification.</p>
<p>Characteristics:</p>
<ul>
<li>No static callers but function is public</li>
<li>In a test file but not called by any tests</li>
<li>Might be used dynamically (via <code>getattr</code>, plugins, etc.)</li>
<li>Public API that might be used by external code</li>
</ul>
<p>Example output:</p>
<pre><code>Function: legacy_api_method
Confidence: Medium (0.65)
Suggestion: Medium confidence this function is dead code. Manual verification recommended.
Risks:
  - Function is public and may be used by external code.
</code></pre>
<h3 id="low-confidence-00-05"><a class="header" href="#low-confidence-00-05">Low Confidence (0.0-0.5)</a></h3>
<p><strong>Likely in use</strong> - These functions are probably not dead code.</p>
<p>Characteristics:</p>
<ul>
<li>Has static callers in the codebase</li>
<li>Framework entry point (Flask route, Django view, Click command)</li>
<li>Test function (starts with <code>test_</code>, in test file)</li>
<li>Callback target or event handler</li>
<li>Magic method (<code>__init__</code>, <code>__str__</code>, etc.)</li>
<li>Property accessor or descriptor</li>
</ul>
<p>Example output:</p>
<pre><code>Function: index
Confidence: Low (0.15)
Result: LIVE
Reasons:
  - Framework entry point (Flask route)
  - Function is public
</code></pre>
<h2 id="public-api-detection"><a class="header" href="#public-api-detection">Public API Detection</a></h2>
<p>Debtmap uses advanced heuristics to identify functions that are likely part of your project’s external API (introduced in Spec 113). This prevents false positives when analyzing library code.</p>
<h3 id="detection-heuristics"><a class="header" href="#detection-heuristics">Detection Heuristics</a></h3>
<p>The public API detector considers:</p>
<ol>
<li><strong>Public visibility</strong> - Function doesn’t start with <code>_</code></li>
<li><strong>File location patterns</strong> - Functions in <code>api/</code>, <code>public/</code>, or top-level <code>__init__.py</code> files</li>
<li><strong>Naming conventions</strong> - Functions following API naming patterns</li>
<li><strong>Export declarations</strong> - Functions listed in <code>__all__</code></li>
<li><strong>Explicit configuration</strong> - Functions marked as API in <code>.debtmap.toml</code></li>
</ol>
<h3 id="configuration-8"><a class="header" href="#configuration-8">Configuration</a></h3>
<p>Configure public API detection in <code>.debtmap.toml</code>:</p>
<pre><code class="language-toml">[external_api]
# Enable/disable automatic public API detection (default: true)
detect_external_api = true

# Explicitly mark specific functions as external APIs
api_functions = [
    "calculate_score",           # Just function name
    "mylib.api::process_data",   # Module-qualified name
    "public_handler",            # Any function matching this name
]

# Mark entire files as containing external APIs (supports glob patterns)
api_files = [
    "src/api/**/*.py",           # All Python files in api directory recursively
    "src/lib.rs",                # Rust library entry point (all public functions)
    "src/public_interface.py",   # Specific Python file
    "**/__init__.py",            # All __init__.py files in any directory
    "**/public_*.py",            # Any file starting with 'public_'
    "myapp/api.py",              # Specific API module
]
</code></pre>
<p>Functions identified as public APIs receive lower dead code confidence scores, even if they have no internal callers.</p>
<h2 id="framework-support"><a class="header" href="#framework-support">Framework Support</a></h2>
<p>The analyzer recognizes entry points from popular Python frameworks to avoid false positives:</p>
<h3 id="web-frameworks"><a class="header" href="#web-frameworks">Web Frameworks</a></h3>
<ul>
<li><strong>Flask</strong>: <code>@app.route</code>, <code>@app.before_request</code>, <code>@app.after_request</code>, <code>@app.errorhandler</code></li>
<li><strong>Django</strong>: View functions, admin actions, signal handlers, middleware methods</li>
<li><strong>FastAPI</strong>: <code>@app.get</code>, <code>@app.post</code>, <code>@app.put</code>, <code>@app.delete</code>, route decorators</li>
</ul>
<h3 id="cli-frameworks"><a class="header" href="#cli-frameworks">CLI Frameworks</a></h3>
<ul>
<li><strong>Click</strong>: <code>@click.command</code>, <code>@click.group</code>, subcommand handlers</li>
<li><strong>argparse</strong>: Functions registered as subcommand handlers</li>
</ul>
<h3 id="testing-frameworks"><a class="header" href="#testing-frameworks">Testing Frameworks</a></h3>
<ul>
<li><strong>pytest</strong>: Functions starting with <code>test_</code>, <code>@pytest.fixture</code>, parametrized tests</li>
<li><strong>unittest</strong>: <code>TestCase</code> methods, <code>setUp</code>, <code>tearDown</code>, <code>setUpClass</code>, <code>tearDownClass</code></li>
</ul>
<h3 id="event-systems"><a class="header" href="#event-systems">Event Systems</a></h3>
<ul>
<li><strong>Qt/PyQt</strong>: Signal connections, slot decorators (<code>@pyqtSlot</code>)</li>
<li><strong>Tkinter</strong>: Event bindings, button command callbacks, widget event handlers</li>
</ul>
<h3 id="framework-detection-matrix"><a class="header" href="#framework-detection-matrix">Framework Detection Matrix</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Framework</th><th>Pattern</th><th>Decorator/Keyword</th><th>Detection Method</th><th>Example</th></tr>
</thead>
<tbody>
<tr><td><strong>Flask</strong></td><td>Routes</td><td><code>@app.route</code></td><td>Decorator analysis</td><td><code>@app.route('/')</code></td></tr>
<tr><td>Flask</td><td>Before request</td><td><code>@app.before_request</code></td><td>Decorator analysis</td><td>Handler hooks</td></tr>
<tr><td>Flask</td><td>Error handlers</td><td><code>@app.errorhandler</code></td><td>Decorator analysis</td><td>Custom error pages</td></tr>
<tr><td><strong>Django</strong></td><td>Views</td><td>Function-based views</td><td>Module structure</td><td><code>def my_view(request):</code></td></tr>
<tr><td>Django</td><td>Admin actions</td><td><code>@admin.action</code></td><td>Decorator analysis</td><td>Admin panel actions</td></tr>
<tr><td>Django</td><td>Signals</td><td><code>@receiver</code></td><td>Decorator analysis</td><td>Signal handlers</td></tr>
<tr><td><strong>FastAPI</strong></td><td>Routes</td><td><code>@app.get</code>, <code>@app.post</code></td><td>Decorator analysis</td><td>REST endpoints</td></tr>
<tr><td>FastAPI</td><td>Dependencies</td><td><code>Depends()</code></td><td>Call graph analysis</td><td>Dependency injection</td></tr>
<tr><td><strong>Click</strong></td><td>Commands</td><td><code>@click.command</code></td><td>Decorator analysis</td><td>CLI commands</td></tr>
<tr><td>Click</td><td>Groups</td><td><code>@click.group</code></td><td>Decorator analysis</td><td>Command groups</td></tr>
<tr><td><strong>pytest</strong></td><td>Tests</td><td><code>test_*</code> prefix</td><td>Naming convention</td><td><code>def test_foo():</code></td></tr>
<tr><td>pytest</td><td>Fixtures</td><td><code>@pytest.fixture</code></td><td>Decorator analysis</td><td>Test fixtures</td></tr>
<tr><td><strong>unittest</strong></td><td>Tests</td><td><code>TestCase</code> methods</td><td>Class hierarchy</td><td><code>class TestFoo(TestCase):</code></td></tr>
<tr><td>unittest</td><td>Setup/Teardown</td><td><code>setUp</code>, <code>tearDown</code></td><td>Method naming</td><td>Lifecycle methods</td></tr>
<tr><td><strong>Qt/PyQt</strong></td><td>Slots</td><td><code>@pyqtSlot</code></td><td>Decorator analysis</td><td>Signal handlers</td></tr>
<tr><td>Qt</td><td>Connections</td><td><code>.connect()</code> calls</td><td>Call graph analysis</td><td>Event wiring</td></tr>
<tr><td><strong>Tkinter</strong></td><td>Callbacks</td><td><code>command=func</code></td><td>Assignment tracking</td><td>Button callbacks</td></tr>
</tbody>
</table>
</div>
<p>See <a href="#context-providers">Framework Patterns documentation</a> for comprehensive framework support details and language-specific patterns.</p>
<h2 id="confidence-thresholds"><a class="header" href="#confidence-thresholds">Confidence Thresholds</a></h2>
<p>You can customize confidence thresholds based on your project’s tolerance for false positives vs. false negatives:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use debtmap::analysis::python_dead_code_enhanced::AnalysisConfig;

let config = AnalysisConfig {
    high_confidence_threshold: 0.8,      // Default: 0.8
    medium_confidence_threshold: 0.5,    // Default: 0.5
    respect_suppression_comments: true,  // Default: true
    include_private_api: true,           // Default: true
    enable_public_api_detection: true,   // Default: true (Spec 113)
    ..Default::default()
};
<span class="boring">}</span></code></pre>
<p><strong>Tuning recommendations:</strong></p>
<ul>
<li><strong>Conservative projects</strong> (libraries, public APIs): Raise thresholds to 0.9/0.7 to reduce false positives</li>
<li><strong>Aggressive cleanup</strong> (internal tools): Lower thresholds to 0.7/0.4 to catch more dead code</li>
<li><strong>Balanced approach</strong> (most projects): Use defaults of 0.8/0.5</li>
</ul>
<h2 id="suppressing-false-positives-1"><a class="header" href="#suppressing-false-positives-1">Suppressing False Positives</a></h2>
<p>Mark functions as intentionally unused with suppression comments:</p>
<pre><code class="language-python"># debtmap: not-dead
def future_api_endpoint():
    """Will be activated in v2.0"""
    pass

def compatibility_shim():  # noqa: dead-code
    """Kept for backwards compatibility"""
    pass
</code></pre>
<h3 id="supported-comment-formats"><a class="header" href="#supported-comment-formats">Supported Comment Formats</a></h3>
<p>All of these formats are recognized:</p>
<ul>
<li><code># debtmap: not-dead</code> (recommended)</li>
<li><code># debtmap:not-dead</code></li>
<li><code># noqa: dead-code</code></li>
<li><code># noqa:dead-code</code></li>
</ul>
<h3 id="comment-placement"><a class="header" href="#comment-placement">Comment Placement</a></h3>
<p>Suppression comments can appear:</p>
<ul>
<li>
<p><strong>Above the function</strong> (most common):</p>
<pre><code class="language-python"># debtmap: not-dead
def my_function():
    pass
</code></pre>
</li>
<li>
<p><strong>Same line as function definition</strong>:</p>
<pre><code class="language-python">def my_function():  # debtmap: not-dead
    pass
</code></pre>
</li>
<li>
<p><strong>Below the function definition</strong> (less common):</p>
<pre><code class="language-python">def my_function():
# debtmap: not-dead
    pass
</code></pre>
</li>
</ul>
<h2 id="coverage-integration-2"><a class="header" href="#coverage-integration-2">Coverage Integration</a></h2>
<p>When test coverage data is available, the analyzer uses it to dramatically improve accuracy by marking covered functions as live:</p>
<h3 id="generating-coverage-data-1"><a class="header" href="#generating-coverage-data-1">Generating Coverage Data</a></h3>
<pre><code class="language-bash"># With pytest and pytest-cov
pytest --cov=myapp --cov-report=json

# With coverage.py directly
coverage run -m pytest
coverage json

# Debtmap automatically detects and uses coverage.json
debtmap analyze myapp/
</code></pre>
<h3 id="how-it-works-4"><a class="header" href="#how-it-works-4">How It Works</a></h3>
<p>Functions that appear in coverage data are considered live, even if:</p>
<ul>
<li>No static callers are found</li>
<li>They’re private functions</li>
<li>They’re not framework entry points</li>
</ul>
<p>This catches functions called:</p>
<ul>
<li>Dynamically via <code>getattr()</code> or <code>exec()</code></li>
<li>Through plugin systems</li>
<li>By external libraries or C extensions</li>
</ul>
<h3 id="programmatic-coverage-usage"><a class="header" href="#programmatic-coverage-usage">Programmatic Coverage Usage</a></h3>
<p>In Rust code, you can provide coverage data programmatically:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use debtmap::analysis::python_dead_code_enhanced::{EnhancedDeadCodeAnalyzer, CoverageData};

// Load coverage from coverage.json file
let coverage = CoverageData::from_coverage_json("coverage.json")?;

// Create analyzer with coverage data
let analyzer = EnhancedDeadCodeAnalyzer::new()
    .with_coverage(coverage);

// Analyze functions - covered functions will have higher "live" confidence
let result = analyzer.analyze_function(&amp;func, &amp;call_graph);
<span class="boring">}</span></code></pre>
<h3 id="accuracy-improvement"><a class="header" href="#accuracy-improvement">Accuracy Improvement</a></h3>
<p>Coverage integration substantially improves accuracy by:</p>
<ul>
<li><strong>Significantly reducing false positives</strong> - Eliminates most false positives in complex codebases</li>
<li><strong>High accuracy for covered functions</strong> - Functions with test coverage are correctly identified as live</li>
<li><strong>Clear removal candidates</strong> - Uncovered code with no static callers is more confidently dead</li>
<li><strong>Dynamic call detection</strong> - Catches functions called via <code>getattr()</code>, plugins, or other dynamic mechanisms that static analysis misses</li>
</ul>
<p><strong>Coverage data format</strong>: Debtmap uses the standard <code>coverage.json</code> format produced by <code>coverage.py</code> and <code>pytest-cov</code>. The file should be in your project root and contain executed line numbers for each source file.</p>
<h2 id="configuration-reference"><a class="header" href="#configuration-reference">Configuration Reference</a></h2>
<h3 id="toml-configuration"><a class="header" href="#toml-configuration">TOML Configuration</a></h3>
<p>Complete dead code analysis configuration in <code>.debtmap.toml</code>:</p>
<pre><code class="language-toml"># Language-specific dead code detection
[languages.python]
detect_dead_code = true           # Enable Python dead code analysis (default: true)

# External API detection (Spec 113)
[external_api]
detect_external_api = true        # Enable automatic public API detection (default: true)

api_functions = [
    "public_function_name",       # Function name only
    "module::qualified_name",     # Module-qualified format
]

api_files = [
    "src/api/**/*.py",            # Glob patterns supported
    "src/public_interface.py",    # Exact file paths
    "**/__init__.py",             # All package entry points
]
</code></pre>
<h3 id="programmatic-configuration-rust-api"><a class="header" href="#programmatic-configuration-rust-api">Programmatic Configuration (Rust API)</a></h3>
<p><strong>Note</strong>: Confidence thresholds and analysis behavior are configured programmatically via the Rust API. These settings are <strong>not available</strong> in <code>.debtmap.toml</code> - only the <code>detect_external_api</code> and API detection settings can be configured via TOML (see above).</p>
<p>For Rust API users, you can customize thresholds:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use debtmap::analysis::python_dead_code_enhanced::AnalysisConfig;

let config = AnalysisConfig {
    // Confidence thresholds (Rust API only - not in .debtmap.toml)
    high_confidence_threshold: 0.8,       // Default: 0.8 (80%)
    medium_confidence_threshold: 0.5,     // Default: 0.5 (50%)

    // Analysis options (Rust API only)
    respect_suppression_comments: true,   // Honor # debtmap: not-dead (default: true)
    include_private_api: true,            // Analyze private functions (default: true)
    enable_public_api_detection: true,    // Use public API heuristics (default: true)

    // Public API detector configuration (optional)
    public_api_config: None,              // Use default PublicApiConfig
};

let analyzer = EnhancedDeadCodeAnalyzer::new()
    .with_config(config);
<span class="boring">}</span></code></pre>
<h3 id="configuration-tuning-by-project-type"><a class="header" href="#configuration-tuning-by-project-type">Configuration Tuning by Project Type</a></h3>
<p><strong>Libraries and Public APIs</strong> (conservative):</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>AnalysisConfig {
    high_confidence_threshold: 0.9,       // Very strict
    medium_confidence_threshold: 0.7,
    enable_public_api_detection: true,    // Critical for libraries
    ..Default::default()
}
<span class="boring">}</span></code></pre>
<p><strong>Internal Applications</strong> (aggressive):</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>AnalysisConfig {
    high_confidence_threshold: 0.7,       // More lenient
    medium_confidence_threshold: 0.4,
    include_private_api: true,            // Analyze everything
    ..Default::default()
}
<span class="boring">}</span></code></pre>
<p><strong>Balanced Approach</strong> (recommended default):</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>AnalysisConfig::default()  // Uses 0.8/0.5 thresholds
<span class="boring">}</span></code></pre>
<h2 id="understanding-results"><a class="header" href="#understanding-results">Understanding Results</a></h2>
<h3 id="interpreting-output"><a class="header" href="#interpreting-output">Interpreting Output</a></h3>
<p>When you run dead code analysis, you’ll see results like:</p>
<pre><code>Dead code analysis for 'calculate_total':
  Result: LIVE
  Confidence: Low (0.2)

  Reasons it's LIVE:
    - HasStaticCallers (called by 3 functions)
    - PublicApi

  Suggestion:
    Function appears to be in use or is a framework/test entry point.
</code></pre>
<h3 id="decision-guide"><a class="header" href="#decision-guide">Decision Guide</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Result</th><th>Confidence</th><th>Action</th></tr>
</thead>
<tbody>
<tr><td><code>is_dead: true</code></td><td>High (0.8-1.0)</td><td><strong>Safe to remove</strong> - Very likely unused</td></tr>
<tr><td><code>is_dead: true</code></td><td>Medium (0.5-0.8)</td><td><strong>Review manually</strong> - Might be dead, verify first</td></tr>
<tr><td><code>is_dead: true</code></td><td>Low (0.0-0.5)</td><td><strong>Keep</strong> - Likely used dynamically</td></tr>
<tr><td><code>is_dead: false</code></td><td>Any</td><td><strong>Keep</strong> - Function is in use</td></tr>
</tbody>
</table>
</div>
<h3 id="decision-tree-for-confidence-interpretation"><a class="header" href="#decision-tree-for-confidence-interpretation">Decision Tree for Confidence Interpretation</a></h3>
<p>Use this decision tree to determine what action to take:</p>
<pre><code>Is the function flagged as dead?
│
├─ NO → Keep the function (it's in use)
│
└─ YES → What is the confidence level?
    │
    ├─ HIGH (0.8-1.0)
    │   ├─ Is it a public API function? → Review, add suppression comment if keeping
    │   └─ Is it private (_prefix)? → **SAFE TO REMOVE**
    │
    ├─ MEDIUM (0.5-0.8)
    │   ├─ Check git history: recently added? → Keep for now, review in next sprint
    │   ├─ Has coverage data been generated? → Run with coverage first
    │   ├─ Is it used dynamically (getattr, plugins)? → Add suppression comment
    │   └─ No clear reason to keep? → **REVIEW MANUALLY, likely safe to remove**
    │
    └─ LOW (0.0-0.5)
        ├─ Review "Reasons it's LIVE" → If reasons are valid, keep it
        ├─ Function is public and might be external API? → Keep it
        └─ Truly unused but marked live incorrectly? → Report issue or use suppression
</code></pre>
<h3 id="confidence-level-quick-reference"><a class="header" href="#confidence-level-quick-reference">Confidence Level Quick Reference</a></h3>
<p><strong>When to act without review:</strong></p>
<ul>
<li><code>is_dead: true</code> + <code>confidence: HIGH</code> + <code>private function (_prefix)</code> → <strong>Remove immediately</strong></li>
<li><code>is_dead: true</code> + <code>confidence: HIGH</code> + <code>in test file</code> + <code>not test function</code> → <strong>Remove immediately</strong></li>
</ul>
<p><strong>When to review before acting:</strong></p>
<ul>
<li><code>is_dead: true</code> + <code>confidence: MEDIUM</code> → <strong>Manual review required</strong></li>
<li><code>is_dead: true</code> + <code>confidence: HIGH</code> + <code>public function</code> → <strong>Check git history, verify external usage</strong></li>
</ul>
<p><strong>When to keep:</strong></p>
<ul>
<li><code>is_dead: false</code> → <strong>Always keep (function is live)</strong></li>
<li><code>is_dead: true</code> + <code>confidence: LOW</code> → <strong>Keep (too uncertain to remove)</strong></li>
</ul>
<h3 id="filtering-results-by-confidence"><a class="header" href="#filtering-results-by-confidence">Filtering Results by Confidence</a></h3>
<p>To filter dead code results by confidence level, you can process the JSON output:</p>
<pre><code class="language-bash"># Analyze and output JSON
debtmap analyze --format=json &gt; results.json

# Filter for high confidence dead code using jq
jq '.dead_code | map(select(.confidence &gt;= 0.8))' results.json

# Filter for high and medium confidence
jq '.dead_code | map(select(.confidence &gt;= 0.5))' results.json
</code></pre>
<p>Note: CLI filtering by confidence threshold (e.g., <code>--min-confidence</code>) is planned for a future release (see Spec 116). Currently, filtering must be done via JSON post-processing.</p>
<p>See <a href="#cli-reference">CLI Reference</a> for complete command options.</p>
<h2 id="common-patterns-1"><a class="header" href="#common-patterns-1">Common Patterns</a></h2>
<h3 id="false-positives-and-how-to-handle-them"><a class="header" href="#false-positives-and-how-to-handle-them">False Positives (and How to Handle Them)</a></h3>
<p><strong>Public API methods</strong></p>
<pre><code class="language-python">class Calculator:
    def add(self, a, b):  # Might be used by external code
        return a + b
</code></pre>
<p><em>Solution</em>: Add to <code>api_functions</code> in <code>.debtmap.toml</code> or use suppression comment</p>
<p><strong>Dynamic imports</strong></p>
<pre><code class="language-python"># Module loaded dynamically via importlib
def handle_command(cmd):  # Called via getattr()
    pass
</code></pre>
<p><em>Solution</em>: Add <code># debtmap: not-dead</code> suppression comment</p>
<p><strong>Plugin registration</strong></p>
<pre><code class="language-python">@registry.register
def handler():  # Registered at import time
    pass
</code></pre>
<p><em>Solution</em>: Should be detected by callback tracker; if not, add suppression comment</p>
<h3 id="true-positives-safe-to-remove"><a class="header" href="#true-positives-safe-to-remove">True Positives (Safe to Remove)</a></h3>
<p><strong>Old implementations</strong></p>
<pre><code class="language-python">def _old_calculate(x):  # Replaced but not removed
    return x * 2
</code></pre>
<p><em>Action</em>: Safe to remove (high confidence)</p>
<p><strong>Unused helper functions</strong></p>
<pre><code class="language-python">def _format_date(date):  # Was used but caller was removed
    return date.strftime("%Y-%m-%d")
</code></pre>
<p><em>Action</em>: Safe to remove (high confidence)</p>
<p><strong>Commented-out code alternatives</strong></p>
<pre><code class="language-python">def process_v1(data):  # Old version, v2 is now used
    pass
</code></pre>
<p><em>Action</em>: Safe to remove (high confidence)</p>
<h2 id="best-practices-9"><a class="header" href="#best-practices-9">Best Practices</a></h2>
<h3 id="workflow-recommendations"><a class="header" href="#workflow-recommendations">Workflow Recommendations</a></h3>
<ol>
<li>
<p><strong>Start with high confidence items</strong> - Remove functions with 0.8+ confidence first to build trust in the tool</p>
</li>
<li>
<p><strong>Run with coverage data</strong> - Generate <code>coverage.json</code> to dramatically improve accuracy:</p>
<pre><code class="language-bash">pytest --cov=myapp --cov-report=json
debtmap analyze myapp/
</code></pre>
</li>
<li>
<p><strong>Review medium confidence items</strong> - These often find real dead code but need manual verification</p>
</li>
<li>
<p><strong>Use suppression comments liberally</strong> - Better to mark something as intentionally unused than to have noise in results</p>
</li>
<li>
<p><strong>Check git history</strong> - Before removing, verify the function wasn’t recently added:</p>
<pre><code class="language-bash">git log -p -- path/to/file.py | grep -A5 "def function_name"
</code></pre>
</li>
<li>
<p><strong>Remove incrementally</strong> - Remove a few functions, run tests, commit. Don’t remove everything at once:</p>
<pre><code class="language-bash"># Remove 3-5 high confidence functions
pytest  # Verify tests still pass
git commit -m "Remove dead code: _old_helper, _unused_formatter"
</code></pre>
</li>
<li>
<p><strong>Look for patterns</strong> - If multiple related functions are flagged, they might all be part of an abandoned feature</p>
</li>
</ol>
<h3 id="cicd-integration-4"><a class="header" href="#cicd-integration-4">CI/CD Integration</a></h3>
<p>Prevent dead code from accumulating by integrating into your CI pipeline:</p>
<pre><code class="language-bash"># .github/workflows/dead-code.yml
- name: Check for dead code
  run: |
    debtmap analyze --min-confidence=0.8 --format=json &gt; dead-code.json
    # Fail if high-confidence dead code is found
    if [ $(jq '.dead_code | length' dead-code.json) -gt 0 ]; then
      echo "High-confidence dead code detected!"
      jq '.dead_code[] | "\(.file):\(.line) - \(.function)"' dead-code.json
      exit 1
    fi
</code></pre>
<h2 id="limitations-1"><a class="header" href="#limitations-1">Limitations</a></h2>
<h3 id="what-the-analyzer-can-detect"><a class="header" href="#what-the-analyzer-can-detect">What the Analyzer CAN Detect</a></h3>
<ul>
<li>✅ Static function calls across modules</li>
<li>✅ Framework entry points via decorators</li>
<li>✅ Test functions in test files</li>
<li>✅ Callback registrations and event handlers</li>
<li>✅ Functions in <code>__all__</code> exports</li>
<li>✅ Property decorators and descriptors</li>
<li>✅ Magic methods (<code>__init__</code>, <code>__str__</code>, etc.)</li>
<li>✅ Functions covered by test coverage data</li>
</ul>
<h3 id="what-the-analyzer-cannot-detect"><a class="header" href="#what-the-analyzer-cannot-detect">What the Analyzer CANNOT Detect</a></h3>
<ul>
<li>❌ <code>eval()</code> or <code>exec()</code> usage - arbitrary code execution</li>
<li>❌ <code>getattr()</code> with dynamic string names - runtime attribute lookup</li>
<li>❌ Reflection-based calls - <code>inspect</code> module usage</li>
<li>❌ Functions called from C extensions</li>
<li>❌ Plugin systems using string-based loading - dynamic imports</li>
</ul>
<h3 id="mitigation-strategies"><a class="header" href="#mitigation-strategies">Mitigation Strategies</a></h3>
<p>For functions the analyzer cannot detect, use suppression comments:</p>
<pre><code class="language-python"># Called dynamically via getattr in plugin system
# debtmap: not-dead
def handle_dynamic_command():
    pass

# Loaded via string-based plugin system
# debtmap: not-dead
def plugin_entrypoint():
    pass
</code></pre>
<h2 id="troubleshooting-12"><a class="header" href="#troubleshooting-12">Troubleshooting</a></h2>
<h3 id="function-marked-as-dead-but-its-actually-used"><a class="header" href="#function-marked-as-dead-but-its-actually-used">“Function marked as dead but it’s actually used”</a></h3>
<p><strong>Possible causes and solutions:</strong></p>
<ol>
<li>
<p><strong>Dynamic call via <code>getattr()</code> or <code>exec()</code></strong></p>
<ul>
<li><em>Solution</em>: Add <code># debtmap: not-dead</code> suppression comment</li>
<li><em>Example</em>: Plugin systems, command dispatchers</li>
</ul>
</li>
<li>
<p><strong>Called from external code or C extension</strong></p>
<ul>
<li><em>Solution</em>: Add function to <code>api_functions</code> in <code>.debtmap.toml</code></li>
<li><em>Example</em>: Public library APIs</li>
</ul>
</li>
<li>
<p><strong>Framework pattern not recognized</strong></p>
<ul>
<li><em>Solution</em>: Report issue on GitHub with framework details</li>
<li><em>Workaround</em>: Add suppression comment</li>
</ul>
</li>
<li>
<p><strong>Callback registration not detected</strong></p>
<ul>
<li><em>Solution</em>: Check if decorator is supported; add suppression if not</li>
<li><em>Example</em>: Custom registration decorators</li>
</ul>
</li>
</ol>
<h3 id="too-many-false-positives-in-my-codebase"><a class="header" href="#too-many-false-positives-in-my-codebase">“Too many false positives in my codebase”</a></h3>
<p><strong>Solutions to try (in order):</strong></p>
<ol>
<li>
<p><strong>Run with coverage data</strong> - Biggest impact on accuracy:</p>
<pre><code class="language-bash">pytest --cov=myapp --cov-report=json
debtmap analyze myapp/
</code></pre>
</li>
<li>
<p><strong>Configure public API detection</strong> - Mark your external APIs:</p>
<pre><code class="language-toml">[external_api]
api_files = ["src/api/**/*.py", "src/public/**/*.py"]
</code></pre>
</li>
<li>
<p><strong>Add framework patterns</strong> - Report unrecognized frameworks on GitHub</p>
</li>
<li>
<p><strong>Add suppression comments</strong> - Mark intentionally unused functions</p>
</li>
<li>
<p><strong>Adjust confidence thresholds</strong> - Raise to 0.9/0.7 for conservative analysis</p>
</li>
</ol>
<h3 id="low-confidence-on-obviously-dead-code"><a class="header" href="#low-confidence-on-obviously-dead-code">“Low confidence on obviously dead code”</a></h3>
<p>This is working as intended - the analyzer is <strong>conservative</strong> to avoid false positives.</p>
<p><strong>What to do:</strong></p>
<ol>
<li><strong>Review the “Reasons it’s LIVE”</strong> - Understand why confidence is low</li>
<li><strong>Check if function is truly unused</strong> - Verify no dynamic calls</li>
<li><strong>Run with coverage</strong> - Coverage data will increase confidence for truly dead code</li>
<li><strong>Accept medium/low confidence</strong> - Manual review is valuable for complex cases</li>
</ol>
<h2 id="examples-2"><a class="header" href="#examples-2">Examples</a></h2>
<h3 id="example-1-flask-application"><a class="header" href="#example-1-flask-application">Example 1: Flask Application</a></h3>
<pre><code class="language-python">from flask import Flask
app = Flask(__name__)

@app.route('/')
def index():  # ✅ LIVE - Framework entry point
    return helper()

def helper():  # ✅ LIVE - Called by index()
    return format_response("Hello")

def format_response(msg):  # ✅ LIVE - Called by helper()
    return f"&lt;html&gt;{msg}&lt;/html&gt;"

def _old_route():  # ❌ DEAD - No callers, not a route (High: 0.95)
    return "Unused"
</code></pre>
<p><strong>Analysis results:</strong></p>
<ul>
<li><code>index</code>: LIVE (Low: 0.15) - Flask route decorator detected</li>
<li><code>helper</code>: LIVE (Low: 0.25) - Has static caller (index)</li>
<li><code>format_response</code>: LIVE (Low: 0.30) - Has static caller (helper)</li>
<li><code>_old_route</code>: DEAD (High: 0.95) - No callers, private function</li>
</ul>
<h3 id="example-2-test-file"><a class="header" href="#example-2-test-file">Example 2: Test File</a></h3>
<pre><code class="language-python">import pytest

def test_addition():  # ✅ LIVE - Test function
    assert add(1, 2) == 3

def add(a, b):  # ✅ LIVE - Called by test
    return a + b

@pytest.fixture
def sample_data():  # ✅ LIVE - pytest fixture
    return [1, 2, 3]

def _unused_helper():  # ❌ DEAD - No callers (High: 0.90)
    return 42

def _old_test_helper():  # ❌ DEAD - Was used, now orphaned (High: 0.92)
    return "test data"
</code></pre>
<p><strong>Analysis results:</strong></p>
<ul>
<li><code>test_addition</code>: LIVE (Low: 0.10) - Test function pattern</li>
<li><code>add</code>: LIVE (Low: 0.20) - Called by test</li>
<li><code>sample_data</code>: LIVE (Low: 0.15) - pytest fixture decorator</li>
<li><code>_unused_helper</code>: DEAD (High: 0.90) - No callers in test file</li>
<li><code>_old_test_helper</code>: DEAD (High: 0.92) - Orphaned helper</li>
</ul>
<h3 id="example-3-public-api-with-configuration"><a class="header" href="#example-3-public-api-with-configuration">Example 3: Public API with Configuration</a></h3>
<pre><code class="language-python"># src/api/calculator.py

__all__ = ['calculate', 'format_result']

def calculate(x):  # ✅ LIVE - Exported in __all__
    return _internal_multiply(x, 2)

def format_result(x):  # ✅ LIVE - Exported in __all__
    return f"Result: {x}"

def _internal_multiply(a, b):  # ✅ LIVE - Called by calculate
    return a * b

def _internal_helper():  # ❌ DEAD - Not exported, no callers (High: 0.88)
    return None

# Public API but not in __all__
def legacy_api():  # ⚠️ MEDIUM - Public but no callers (Medium: 0.65)
    """Kept for backwards compatibility"""
    pass
</code></pre>
<p><strong>.debtmap.toml configuration:</strong></p>
<pre><code class="language-toml">[external_api]
api_files = ["src/api/**/*.py"]

# Explicitly mark legacy API
api_functions = ["legacy_api"]
</code></pre>
<p><strong>Analysis results:</strong></p>
<ul>
<li><code>calculate</code>: LIVE (Low: 0.20) - In <code>__all__</code>, has callers</li>
<li><code>format_result</code>: LIVE (Low: 0.25) - In <code>__all__</code></li>
<li><code>_internal_multiply</code>: LIVE (Low: 0.30) - Called by calculate</li>
<li><code>_internal_helper</code>: DEAD (High: 0.88) - Private, no callers</li>
<li><code>legacy_api</code>: LIVE (Low: 0.35) - Marked as API in config</li>
</ul>
<h2 id="getting-help-1"><a class="header" href="#getting-help-1">Getting Help</a></h2>
<ul>
<li><strong>Documentation</strong>: See <a href="#troubleshooting-23">Troubleshooting Guide</a> for common issues</li>
<li><strong>Report issues</strong>: https://github.com/anthropics/debtmap/issues</li>
<li><strong>Examples</strong>: Check the <a href="#examples-5">Examples chapter</a> for more scenarios</li>
<li><strong>Related topics</strong>:
<ul>
<li><a href="#coverage-integration-1">Coverage Integration</a> - Detailed coverage setup</li>
<li><a href="#suppression-patterns">Suppression Patterns</a> - Advanced suppression techniques</li>
<li><a href="#configuration-2">Configuration</a> - Complete configuration reference</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="design-pattern-detection"><a class="header" href="#design-pattern-detection">Design Pattern Detection</a></h1>
<p>Debtmap automatically detects common design patterns in your codebase to provide better architectural insights and reduce false positives in complexity analysis. When recognized design patterns are detected, Debtmap applies appropriate complexity adjustments to avoid penalizing idiomatic code.</p>
<h2 id="overview-11"><a class="header" href="#overview-11">Overview</a></h2>
<p>Debtmap detects 7 design patterns across Python, JavaScript, TypeScript, and Rust:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Pattern</th><th>Primary Language</th><th>Detection Confidence</th></tr>
</thead>
<tbody>
<tr><td>Observer</td><td>Python, Rust</td><td>High (0.8-0.9)</td></tr>
<tr><td>Singleton</td><td>Python</td><td>High (0.85-0.95)</td></tr>
<tr><td>Factory</td><td>Python</td><td>Medium-High (0.7-0.85)</td></tr>
<tr><td>Strategy</td><td>Python</td><td>Medium (0.7-0.8)</td></tr>
<tr><td>Callback</td><td>Python, JavaScript</td><td>High (0.8-0.9)</td></tr>
<tr><td>Template Method</td><td>Python</td><td>Medium (0.7-0.8)</td></tr>
<tr><td>Dependency Injection</td><td>Python</td><td>Medium (0.65-0.75)</td></tr>
</tbody>
</table>
</div>
<p>Pattern detection serves multiple purposes:</p>
<ul>
<li><strong>Reduces false positives</strong>: Avoids flagging idiomatic pattern implementations as overly complex</li>
<li><strong>Documents architecture</strong>: Automatically identifies architectural patterns in your codebase</li>
<li><strong>Validates consistency</strong>: Helps ensure patterns are used correctly and completely</li>
<li><strong>Guides refactoring</strong>: Identifies incomplete pattern implementations</li>
</ul>
<h2 id="pattern-detection-details"><a class="header" href="#pattern-detection-details">Pattern Detection Details</a></h2>
<h3 id="observer-pattern"><a class="header" href="#observer-pattern">Observer Pattern</a></h3>
<p>The Observer pattern is detected in Python and Rust by identifying abstract base classes with concrete implementations.</p>
<p><strong>Detection Criteria (Python)</strong>:</p>
<ul>
<li>Abstract base class with <code>ABC</code>, <code>Protocol</code>, or <code>Interface</code> markers</li>
<li>Abstract methods decorated with <code>@abstractmethod</code></li>
<li>Concrete implementations inheriting from the interface</li>
<li>Methods prefixed with <code>on_</code>, <code>handle_</code>, or <code>notify_</code></li>
<li>Registration methods like <code>add_observer</code>, <code>register</code>, or <code>subscribe</code></li>
<li>Notification methods like <code>notify</code>, <code>notify_all</code>, <code>trigger</code>, <code>emit</code></li>
</ul>
<p><strong>Detection Criteria (Rust)</strong>:</p>
<ul>
<li>Trait definitions with callback-style methods</li>
<li>Multiple implementations of the same trait</li>
<li>Trait registry tracking for cross-module detection</li>
</ul>
<p><strong>Example (Python)</strong>:</p>
<pre><code class="language-python">from abc import ABC, abstractmethod

class EventObserver(ABC):
    @abstractmethod
    def on_event(self, data):
        """Handle event notification"""
        pass

class LoggingObserver(EventObserver):
    def on_event(self, data):
        print(f"Event occurred: {data}")

class EmailObserver(EventObserver):
    def on_event(self, data):
        send_email(f"Alert: {data}")

class EventManager:
    def __init__(self):
        self.observers = []

    def add_observer(self, observer: EventObserver):
        self.observers.append(observer)

    def notify_all(self, data):
        for observer in self.observers:
            observer.on_event(data)
</code></pre>
<p><strong>Confidence</strong>: High (0.8-0.9) when abstract base class, implementations, and registration/notification methods are present. Lower confidence (0.5-0.7) for partial implementations.</p>
<h3 id="singleton-pattern"><a class="header" href="#singleton-pattern">Singleton Pattern</a></h3>
<p>Singleton pattern detection identifies three common Python implementations: module-level singletons, <code>__new__</code> override, and decorator-based patterns.</p>
<p><strong>Detection Criteria</strong>:</p>
<ul>
<li>Module-level variable assignments (e.g., <code>instance = MyClass()</code>)</li>
<li>Classes overriding <code>__new__</code> to enforce single instance</li>
<li>Classes decorated with <code>@singleton</code> or similar decorators</li>
<li>Presence of instance caching logic</li>
</ul>
<p><strong>Example (Module-level)</strong>:</p>
<pre><code class="language-python"># config.py
class Config:
    def __init__(self):
        self.settings = {}

    def load(self, path):
        # Load configuration
        pass

# Single instance created at module level
config = Config()
</code></pre>
<p><strong>Example (<code>__new__</code> override)</strong>:</p>
<pre><code class="language-python">class DatabaseConnection:
    _instance = None

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance

    def __init__(self):
        if not hasattr(self, 'initialized'):
            self.initialized = True
            self.connect()
</code></pre>
<p><strong>Example (Decorator-based)</strong>:</p>
<pre><code class="language-python">def singleton(cls):
    instances = {}
    def get_instance(*args, **kwargs):
        if cls not in instances:
            instances[cls] = cls(*args, **kwargs)
        return instances[cls]
    return get_instance

@singleton
class Logger:
    def __init__(self):
        self.log_file = open('app.log', 'a')
</code></pre>
<p><strong>Confidence</strong>: Very High (0.9-0.95) for <code>__new__</code> override and decorator patterns. High (0.85) for module-level singletons with clear naming.</p>
<h3 id="factory-pattern"><a class="header" href="#factory-pattern">Factory Pattern</a></h3>
<p>Factory pattern detection identifies factory functions, factory classes, and factory registries based on naming conventions and structural patterns.</p>
<p><strong>Detection Criteria</strong>:</p>
<ul>
<li>Functions with names containing <code>create_</code>, <code>make_</code>, <code>build_</code>, or <code>_factory</code></li>
<li>Factory registry patterns (dictionaries mapping types to constructors)</li>
<li>Functions that return instances of different types based on parameters</li>
<li>Classes with factory methods</li>
</ul>
<p><strong>Example (Factory Function)</strong>:</p>
<pre><code class="language-python">def create_logger(log_type: str):
    if log_type == "file":
        return FileLogger()
    elif log_type == "console":
        return ConsoleLogger()
    elif log_type == "network":
        return NetworkLogger()
    else:
        raise ValueError(f"Unknown logger type: {log_type}")
</code></pre>
<p><strong>Example (Registry-based Factory)</strong>:</p>
<pre><code class="language-python"># Parser registry
PARSERS = {
    'json': JSONParser,
    'xml': XMLParser,
    'yaml': YAMLParser,
}

def create_parser(format: str):
    parser_class = PARSERS.get(format)
    if parser_class is None:
        raise ValueError(f"No parser for format: {format}")
    return parser_class()
</code></pre>
<p><strong>Example (Factory Method)</strong>:</p>
<pre><code class="language-python">class DocumentFactory:
    @staticmethod
    def create_document(doc_type: str):
        if doc_type == "pdf":
            return PDFDocument()
        elif doc_type == "word":
            return WordDocument()
        else:
            return PlainTextDocument()
</code></pre>
<p><strong>Confidence</strong>: Medium-High (0.75-0.85) for functions with factory naming patterns. Lower confidence (0.6-0.7) for registry patterns without factory names.</p>
<h3 id="strategy-pattern"><a class="header" href="#strategy-pattern">Strategy Pattern</a></h3>
<p>Strategy pattern detection identifies interfaces with multiple implementations representing interchangeable algorithms.</p>
<p><strong>Detection Criteria</strong>:</p>
<ul>
<li>Abstract base class or Protocol defining strategy interface</li>
<li>Multiple concrete implementations</li>
<li>Strategy interface typically has 1-2 core methods</li>
<li>Used via composition (strategy object passed to context)</li>
</ul>
<p><strong>Example</strong>:</p>
<pre><code class="language-python">from abc import ABC, abstractmethod

class CompressionStrategy(ABC):
    @abstractmethod
    def compress(self, data: bytes) -&gt; bytes:
        pass

class ZipCompression(CompressionStrategy):
    def compress(self, data: bytes) -&gt; bytes:
        return zlib.compress(data)

class GzipCompression(CompressionStrategy):
    def compress(self, data: bytes) -&gt; bytes:
        return gzip.compress(data)

class LzmaCompression(CompressionStrategy):
    def compress(self, data: bytes) -&gt; bytes:
        return lzma.compress(data)

class FileCompressor:
    def __init__(self, strategy: CompressionStrategy):
        self.strategy = strategy

    def compress_file(self, path):
        data = read_file(path)
        return self.strategy.compress(data)
</code></pre>
<p><strong>Confidence</strong>: Medium (0.7-0.8) based on interface structure and implementation count.</p>
<h3 id="callback-pattern"><a class="header" href="#callback-pattern">Callback Pattern</a></h3>
<p>Callback pattern detection identifies decorator-based callbacks commonly used in web frameworks and event handlers.</p>
<p><strong>Detection Criteria</strong>:</p>
<ul>
<li>Decorators with patterns like <code>@route</code>, <code>@handler</code>, <code>@app.</code>, <code>@on</code>, <code>@callback</code></li>
<li>Framework-specific decorators (Flask routes, FastAPI endpoints, event handlers)</li>
<li>Functions registered as callbacks for events or hooks</li>
</ul>
<p><strong>Example (Flask Routes)</strong>:</p>
<pre><code class="language-python">from flask import Flask

app = Flask(__name__)

@app.route('/api/users')
def get_users():
    return {"users": []}

@app.route('/api/users/&lt;id&gt;')
def get_user(id):
    return {"user": find_user(id)}
</code></pre>
<p><strong>Example (Event Handler)</strong>:</p>
<pre><code class="language-python">class EventBus:
    def __init__(self):
        self.handlers = {}

    def on(self, event_name):
        def decorator(func):
            self.handlers[event_name] = func
            return func
        return decorator

bus = EventBus()

@bus.on('user.created')
def handle_user_created(user):
    send_welcome_email(user)

@bus.on('order.placed')
def handle_order_placed(order):
    process_payment(order)
</code></pre>
<p><strong>Confidence</strong>: High (0.8-0.9) for framework decorator patterns. Medium (0.6-0.7) for custom callback implementations.</p>
<h3 id="template-method-pattern"><a class="header" href="#template-method-pattern">Template Method Pattern</a></h3>
<p>Template method pattern detection identifies base classes with template methods that call abstract hook methods.</p>
<p><strong>Detection Criteria</strong>:</p>
<ul>
<li>Base class with concrete methods (template methods)</li>
<li>Abstract methods intended to be overridden (hook methods)</li>
<li>Template method calls hook methods in a defined sequence</li>
<li>Subclasses override hook methods but not template method</li>
</ul>
<p><strong>Example</strong>:</p>
<pre><code class="language-python">from abc import ABC, abstractmethod

class DataProcessor(ABC):
    def process(self, data):
        """Template method defining the algorithm skeleton"""
        raw = self.load_data(data)
        validated = self.validate(raw)
        transformed = self.transform(validated)
        self.save(transformed)

    @abstractmethod
    def load_data(self, source):
        """Hook: Load data from source"""
        pass

    @abstractmethod
    def validate(self, data):
        """Hook: Validate data"""
        pass

    def transform(self, data):
        """Hook: Transform data (optional override)"""
        return data

    @abstractmethod
    def save(self, data):
        """Hook: Save processed data"""
        pass

class CSVProcessor(DataProcessor):
    def load_data(self, source):
        return read_csv(source)

    def validate(self, data):
        return [row for row in data if row]

    def save(self, data):
        write_csv('output.csv', data)
</code></pre>
<p><strong>Confidence</strong>: Medium (0.7-0.8) based on combination of abstract and concrete methods in base class.</p>
<h3 id="dependency-injection-pattern"><a class="header" href="#dependency-injection-pattern">Dependency Injection Pattern</a></h3>
<p>Dependency injection pattern detection identifies classes that receive dependencies through constructors or setters rather than creating them internally.</p>
<p><strong>Detection Criteria</strong>:</p>
<ul>
<li>Constructor parameters accepting interface/protocol types</li>
<li>Setter methods for injecting dependencies</li>
<li>Optional dependencies with default values</li>
<li>Absence of hard-coded object instantiation inside the class</li>
</ul>
<p><strong>Example (Constructor Injection)</strong>:</p>
<pre><code class="language-python">class UserService:
    def __init__(self,
                 user_repository: UserRepository,
                 email_service: EmailService,
                 logger: Logger):
        self.user_repo = user_repository
        self.email_service = email_service
        self.logger = logger

    def create_user(self, username, email):
        user = self.user_repo.create(username, email)
        self.email_service.send_welcome(email)
        self.logger.info(f"Created user: {username}")
        return user
</code></pre>
<p><strong>Example (Setter Injection)</strong>:</p>
<pre><code class="language-python">class ReportGenerator:
    def __init__(self):
        self.data_source = None
        self.formatter = None

    def set_data_source(self, source):
        self.data_source = source

    def set_formatter(self, formatter):
        self.formatter = formatter

    def generate(self):
        data = self.data_source.fetch()
        return self.formatter.format(data)
</code></pre>
<p><strong>Confidence</strong>: Medium (0.65-0.75) based on constructor signatures and absence of direct instantiation.</p>
<h2 id="internal-pattern-detection"><a class="header" href="#internal-pattern-detection">Internal Pattern Detection</a></h2>
<p>Debtmap also detects certain patterns internally for analysis purposes, but these are not exposed as user-facing design pattern detection features. These internal patterns help improve the accuracy of other analyses like god object detection and complexity calculations.</p>
<h3 id="builder-pattern-internal-use-only"><a class="header" href="#builder-pattern-internal-use-only">Builder Pattern (Internal Use Only)</a></h3>
<p>The Builder pattern is detected internally during <strong>god object detection</strong> to avoid false positives. Classes that follow the builder pattern are given adjusted scores in god object analysis since builder classes naturally have many methods and fields.</p>
<p><strong>Note</strong>: Builder pattern detection is <strong>not available</strong> via the <code>--patterns</code> CLI flag. It’s used only internally for scoring adjustments.</p>
<p><strong>Internal Detection Criteria</strong>:</p>
<ul>
<li>Struct with builder suffix or builder-related naming</li>
<li>Methods returning <code>Self</code> for chaining</li>
<li>Final <code>build()</code> method returning the constructed type</li>
<li>Type-state pattern usage (optional)</li>
</ul>
<p><strong>Example</strong> (Internal Detection):</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct HttpClientBuilder {
    base_url: Option&lt;String&gt;,
    timeout: Duration,
    headers: HashMap&lt;String, String&gt;,
}

impl HttpClientBuilder {
    pub fn new() -&gt; Self { /* ... */ }

    // Chaining methods detected internally
    pub fn base_url(mut self, url: impl Into&lt;String&gt;) -&gt; Self { /* ... */ }
    pub fn timeout(mut self, timeout: Duration) -&gt; Self { /* ... */ }
    pub fn header(mut self, key: String, value: String) -&gt; Self { /* ... */ }

    pub fn build(self) -&gt; Result&lt;HttpClient&gt; { /* ... */ }
}
<span class="boring">}</span></code></pre>
<p><strong>Why Internal Only</strong>: Builder patterns are a legitimate design choice for complex object construction. Debtmap detects them to prevent flagging builder classes as god objects, but doesn’t report them as design patterns since they don’t require complexity adjustments like other patterns.</p>
<p><strong>Source</strong>: <code>src/organization/builder_pattern.rs</code> - Used for god object detection score adjustment</p>
<h3 id="visitor-pattern-internal-use-only"><a class="header" href="#visitor-pattern-internal-use-only">Visitor Pattern (Internal Use Only)</a></h3>
<p>The Visitor pattern is detected internally for <strong>complexity analysis normalization</strong>. When exhaustive pattern matching is detected (typical of visitor patterns), Debtmap applies logarithmic complexity scaling instead of linear scaling to avoid penalizing idiomatic exhaustive match expressions.</p>
<p><strong>Note</strong>: Visitor pattern detection is <strong>not available</strong> via the <code>--patterns</code> CLI flag. It’s used only internally for complexity scaling adjustments.</p>
<p><strong>Internal Detection Criteria</strong>:</p>
<ul>
<li>Trait with visit methods for different types</li>
<li>Implementations providing behavior for each visited type</li>
<li>Exhaustive pattern matching across enum variants</li>
<li>Used primarily for AST traversal or data structure processing</li>
</ul>
<p><strong>Example</strong> (Internal Detection):</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>trait Visitor {
    fn visit_function(&amp;mut self, func: &amp;Function);
    fn visit_class(&amp;mut self, class: &amp;Class);
    fn visit_module(&amp;mut self, module: &amp;Module);
}

impl Visitor for ComplexityVisitor {
    fn visit_function(&amp;mut self, func: &amp;Function) {
        // Exhaustive matching detected for complexity scaling
        match &amp;func.body {
            FunctionBody::Simple =&gt; { /* ... */ }
            FunctionBody::Complex(statements) =&gt; { /* ... */ }
        }
    }
}
<span class="boring">}</span></code></pre>
<p><strong>Why Internal Only</strong>: Visitor patterns often involve exhaustive pattern matching which can appear complex by traditional metrics. Debtmap detects these patterns to apply logarithmic scaling (<code>log2(match_arms) * avg_complexity</code>) instead of linear, preventing false positives in complexity analysis. This is a complexity adjustment mechanism, not a user-visible pattern detection feature.</p>
<p><strong>Source</strong>: <code>src/complexity/visitor_detector.rs</code> - Used for complexity analysis, not pattern reporting</p>
<h2 id="configuration-9"><a class="header" href="#configuration-9">Configuration</a></h2>
<h3 id="current-implementation-status"><a class="header" href="#current-implementation-status">Current Implementation Status</a></h3>
<p>Pattern detection is currently <strong>internal-only</strong> and used for analysis adjustments. The CLI flags for pattern detection exist in the codebase but are not yet fully integrated into the analysis pipeline.</p>
<p><strong>Status Summary</strong>:</p>
<ul>
<li>✅ Pattern detection logic implemented (7 user-facing patterns)</li>
<li>✅ CLI flags defined (<code>--no-pattern-detection</code>, <code>--patterns</code>, <code>--pattern-threshold</code>, <code>--show-pattern-warnings</code>)</li>
<li>⚠️ CLI flags not yet wired to analysis pipeline</li>
<li>⚠️ Pattern detection results not currently exposed in output formats</li>
</ul>
<p>Pattern detection is primarily used internally for:</p>
<ul>
<li>Adjusting complexity scores to avoid false positives</li>
<li>Informing god object detection (Builder pattern)</li>
<li>Normalizing exhaustive pattern matching complexity (Visitor pattern)</li>
</ul>
<h3 id="cli-options-defined-but-not-yet-active"><a class="header" href="#cli-options-defined-but-not-yet-active">CLI Options (Defined but Not Yet Active)</a></h3>
<p>The following CLI flags are defined in the codebase (<code>src/cli.rs:228-241</code>) but are not yet fully integrated:</p>
<pre><code class="language-bash"># Disable all pattern detection (planned)
debtmap analyze --no-pattern-detection

# Enable only specific patterns (planned)
debtmap analyze --patterns observer,singleton,factory,strategy,callback,template_method,dependency_injection

# Set confidence threshold (planned)
debtmap analyze --pattern-threshold 0.8

# Show warnings for uncertain pattern detections (planned)
debtmap analyze --show-pattern-warnings
</code></pre>
<p><strong>Planned Patterns for <code>--patterns</code> Flag</strong> (when integration is complete):</p>
<ul>
<li><code>observer</code> - Observer pattern detection</li>
<li><code>singleton</code> - Singleton pattern detection</li>
<li><code>factory</code> - Factory pattern detection</li>
<li><code>strategy</code> - Strategy pattern detection</li>
<li><code>callback</code> - Callback pattern detection</li>
<li><code>template_method</code> - Template method pattern detection</li>
<li><code>dependency_injection</code> - Dependency injection detection</li>
</ul>
<p><strong>Note</strong>: Builder and Visitor patterns are detected internally but will not be available via the <code>--patterns</code> flag. See <a href="#internal-pattern-detection">Internal Pattern Detection</a> for details.</p>
<h3 id="roadmap-pattern-detection-output"><a class="header" href="#roadmap-pattern-detection-output">Roadmap: Pattern Detection Output</a></h3>
<p>When fully integrated, pattern detection results will appear in debtmap’s output formats:</p>
<p><strong>Planned Terminal Format</strong>:</p>
<pre><code>Design Patterns Detected:
  Observer Pattern (confidence: 0.88)
    Interface: EventListener (event_system.py:4)
    Implementations: AuditLogger, SessionManager
</code></pre>
<p><strong>Planned JSON Format</strong>:</p>
<pre><code class="language-json">{
  "pattern_instances": [
    {
      "pattern_type": "Observer",
      "confidence": 0.88,
      "location": "event_system.py:4",
      "implementations": ["AuditLogger", "SessionManager"]
    }
  ]
}
</code></pre>
<p><strong>Current Workaround</strong>: Pattern detection is used internally during analysis to improve accuracy. To see the effects of pattern detection:</p>
<ol>
<li>Run analysis with and without <code>--no-pattern-detection</code> (when implemented)</li>
<li>Compare complexity scores and god object detection results</li>
<li>Patterns are being detected, but not explicitly reported in output</li>
</ol>
<h2 id="confidence-scoring-1"><a class="header" href="#confidence-scoring-1">Confidence Scoring</a></h2>
<p>Pattern detection uses a confidence scoring system (0.0-1.0) to indicate match quality:</p>
<ul>
<li><strong>0.9-1.0</strong>: Very High - Strong structural match with all key elements present</li>
<li><strong>0.8-0.9</strong>: High - Clear pattern with most elements present</li>
<li><strong>0.7-0.8</strong>: Medium-High - Pattern present with some uncertainty</li>
<li><strong>0.6-0.7</strong>: Medium - Possible pattern with limited evidence</li>
<li><strong>0.5-0.6</strong>: Low - Weak match, may be false positive</li>
</ul>
<p><strong>Default Threshold</strong>: 0.7 - Only patterns with 70% or higher confidence are reported by default.</p>
<p><strong>Adjusting Thresholds</strong>:</p>
<pre><code class="language-bash"># More strict (fewer patterns, higher confidence)
debtmap analyze --pattern-threshold 0.85

# More lenient (more patterns, lower confidence)
debtmap analyze --pattern-threshold 0.6 --show-pattern-warnings
</code></pre>
<p><strong>How Confidence is Calculated</strong>:</p>
<p>Each pattern detector calculates confidence holistically based on multiple factors:</p>
<ol>
<li><strong>Structural completeness</strong>: Are all expected elements present?</li>
<li><strong>Naming conventions</strong>: Do names match expected patterns?</li>
<li><strong>Implementation count</strong>: Are there enough implementations to confirm the pattern?</li>
<li><strong>Cross-validation</strong>: Do different detection heuristics agree?</li>
</ol>
<p>For example, Observer pattern confidence is calculated holistically based on:</p>
<ul>
<li>Presence of abstract base class with appropriate markers (<code>ABC</code>, <code>Protocol</code>, etc.)</li>
<li>Number of concrete implementations found</li>
<li>Detection of registration methods (<code>add_observer</code>, <code>register</code>, <code>subscribe</code>)</li>
<li>Detection of notification methods (<code>notify</code>, <code>notify_all</code>, <code>trigger</code>, <code>emit</code>)</li>
<li>Naming conventions matching observer patterns</li>
</ul>
<p>Higher confidence requires more structural elements to be present. The calculation is not a simple sum of individual weights but rather a holistic assessment of pattern completeness.</p>
<h2 id="cross-file-pattern-detection"><a class="header" href="#cross-file-pattern-detection">Cross-File Pattern Detection</a></h2>
<p>Debtmap can detect patterns that span multiple files, particularly for the Observer pattern where interfaces and implementations may be in separate modules.</p>
<p><strong>How Cross-File Detection Works</strong>:</p>
<ol>
<li><strong>Import Tracking</strong>: Debtmap tracks imports to understand module dependencies</li>
<li><strong>Interface Registry</strong>: Abstract base classes are registered globally</li>
<li><strong>Implementation Matching</strong>: Implementations in other files are matched to registered interfaces</li>
<li><strong>Cross-Module Context</strong>: A shared context links related files</li>
</ol>
<p><strong>Example</strong>:</p>
<pre><code class="language-python"># interfaces/observer.py
from abc import ABC, abstractmethod

class EventObserver(ABC):
    @abstractmethod
    def on_event(self, data):
        pass

# observers/logging_observer.py
from interfaces.observer import EventObserver

class LoggingObserver(EventObserver):
    def on_event(self, data):
        log(data)

# observers/email_observer.py
from interfaces.observer import EventObserver

class EmailObserver(EventObserver):
    def on_event(self, data):
        send_email(data)
</code></pre>
<p>Debtmap detects this as a single Observer pattern with cross-file implementations.</p>
<p><strong>Limitations</strong>:</p>
<ul>
<li>Only works for explicitly imported interfaces</li>
<li>Requires static import analysis (dynamic imports may not be tracked)</li>
<li>Most effective within a single project (not across external dependencies)</li>
</ul>
<h2 id="rust-specific-pattern-detection"><a class="header" href="#rust-specific-pattern-detection">Rust-Specific Pattern Detection</a></h2>
<h3 id="trait-based-patterns"><a class="header" href="#trait-based-patterns">Trait-Based Patterns</a></h3>
<p>Rust pattern detection leverages the trait system for identifying patterns:</p>
<p><strong>Trait Registry</strong>: Tracks trait definitions and implementations across modules</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Trait registered for pattern detection
pub trait EventHandler {
    fn handle(&amp;self, event: &amp;Event);
}

// Multiple implementations tracked
impl EventHandler for LogHandler { /* ... */ }
impl EventHandler for MetricsHandler { /* ... */ }
impl EventHandler for AlertHandler { /* ... */ }
<span class="boring">}</span></code></pre>
<p><strong>Observer Pattern via Traits</strong>:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub trait Observable {
    fn subscribe(&amp;mut self, observer: Box&lt;dyn Observer&gt;);
    fn notify(&amp;self, event: &amp;Event);
}

pub trait Observer {
    fn on_event(&amp;self, event: &amp;Event);
}
<span class="boring">}</span></code></pre>
<p><strong>Differences from Python Detection</strong>:</p>
<ul>
<li>Traits are more explicit than Python’s ABC</li>
<li>Type system ensures implementation correctness</li>
<li>No runtime reflection needed for detection</li>
<li>Pattern matching exhaustiveness helps identify Visitor pattern</li>
</ul>
<h2 id="integration-with-complexity-analysis-1"><a class="header" href="#integration-with-complexity-analysis-1">Integration with Complexity Analysis</a></h2>
<p>Debtmap has two separate but complementary systems for patterns:</p>
<h3 id="1-design-pattern-detection-this-feature"><a class="header" href="#1-design-pattern-detection-this-feature">1. Design Pattern Detection (This Feature)</a></h3>
<p>The 7 user-facing design patterns documented in this chapter (Observer, Singleton, Factory, Strategy, Callback, Template Method, Dependency Injection) are <strong>detected and reported</strong> to users. These patterns appear in the output to document architectural choices but do not directly adjust complexity scores.</p>
<p><strong>Purpose</strong>: Architectural documentation and pattern identification</p>
<p><strong>Output</strong>: Pattern instances with confidence scores in terminal, JSON, and markdown formats</p>
<h3 id="2-complexity-pattern-adjustments-internal-system"><a class="header" href="#2-complexity-pattern-adjustments-internal-system">2. Complexity Pattern Adjustments (Internal System)</a></h3>
<p>Debtmap has a separate internal system in <code>src/complexity/python_pattern_adjustments.rs</code> that detects specific complexity patterns and applies multipliers. These are <strong>different patterns</strong> from the user-facing design patterns:</p>
<p><strong>Internal complexity patterns include</strong>:</p>
<ul>
<li>Dictionary Dispatch (0.5x multiplier)</li>
<li>Strategy Pattern detection via conditionals (0.6x multiplier)</li>
<li>Comprehension patterns (0.8x multiplier)</li>
<li>Other Python-specific complexity patterns</li>
</ul>
<p><strong>Purpose</strong>: Adjust complexity scores to avoid penalizing idiomatic code</p>
<p><strong>Output</strong>: Applied automatically during complexity calculation, not reported separately</p>
<h3 id="relationship-between-the-systems"><a class="header" href="#relationship-between-the-systems">Relationship Between the Systems</a></h3>
<p>Currently, these are <strong>independent systems</strong>:</p>
<ul>
<li>Design pattern detection focuses on architectural patterns</li>
<li>Complexity adjustments focus on implementation patterns</li>
</ul>
<p>The design pattern detection results are primarily for documentation and architectural insights. The complexity scoring uses its own pattern recognition to apply appropriate adjustments.</p>
<h3 id="visitor-pattern-special-case"><a class="header" href="#visitor-pattern-special-case">Visitor Pattern Special Case</a></h3>
<p>The Visitor pattern (internal-only) is used for complexity analysis. When exhaustive pattern matching is detected, debtmap applies <strong>logarithmic scaling</strong>:</p>
<pre><code>visitor_complexity = log2(match_arms) * average_arm_complexity
</code></pre>
<p>This prevents exhaustive pattern matching from being flagged as overly complex. See <a href="#visitor-pattern-internal-use-only">Visitor Pattern (Internal Use Only)</a> for more details.</p>
<p><strong>See Also</strong>:</p>
<ul>
<li><a href="#complexity-metrics-1">Complexity Analysis</a> - How complexity is calculated</li>
<li><a href="#scoring-strategies">Scoring Strategies</a> - Complexity adjustments and multipliers</li>
</ul>
<h2 id="practical-examples-2"><a class="header" href="#practical-examples-2">Practical Examples</a></h2>
<h3 id="example-1-observer-pattern-code-structure"><a class="header" href="#example-1-observer-pattern-code-structure">Example 1: Observer Pattern Code Structure</a></h3>
<p>Pattern detection identifies Observer implementations even though results are not yet shown in output:</p>
<pre><code class="language-python"># event_system.py
from abc import ABC, abstractmethod

class EventListener(ABC):
    @abstractmethod
    def on_user_login(self, user):
        pass

class AuditLogger(EventListener):
    def on_user_login(self, user):
        audit_log.write(f"User {user.id} logged in")

class SessionManager(EventListener):
    def on_user_login(self, user):
        create_session(user)

class EventDispatcher:
    def __init__(self):
        self.listeners = []

    def add_listener(self, listener):
        self.listeners.append(listener)

    def notify_login(self, user):
        for listener in self.listeners:
            listener.on_user_login(user)
</code></pre>
<p><strong>Current Behavior</strong>: Debtmap internally detects this Observer pattern (confidence ~0.88) and uses it to adjust complexity scoring. The pattern structure is recognized but not reported in output.</p>
<p><strong>Source</strong>: Pattern detection logic in <code>src/analysis/patterns/observer.rs</code></p>
<h3 id="example-2-factory-pattern-detection-criteria"><a class="header" href="#example-2-factory-pattern-detection-criteria">Example 2: Factory Pattern Detection Criteria</a></h3>
<p>Debtmap detects factory patterns based on naming and structure:</p>
<pre><code class="language-python">def create_logger(log_type: str):
    """Factory function - detected by 'create_' prefix"""
    if log_type == "file":
        return FileLogger()
    elif log_type == "console":
        return ConsoleLogger()
    else:
        return NetworkLogger()
</code></pre>
<p><strong>Current Behavior</strong>: The factory pattern detector (in <code>src/analysis/patterns/factory.rs</code>) identifies this as a Factory pattern with medium-high confidence (~0.75-0.85) based on:</p>
<ul>
<li>Function name contains <code>create_</code></li>
<li>Returns different types based on parameter</li>
<li>Multiple instantiation paths</li>
</ul>
<p>This information is used internally to adjust complexity scores for factory functions.</p>
<h3 id="example-3-impact-on-complexity-analysis"><a class="header" href="#example-3-impact-on-complexity-analysis">Example 3: Impact on Complexity Analysis</a></h3>
<p>While pattern detection results aren’t directly shown, their effect can be observed:</p>
<pre><code class="language-bash"># Run standard analysis
debtmap analyze myapp/

# When --no-pattern-detection is fully integrated, compare results
# (currently this flag exists but isn't fully wired)
</code></pre>
<p><strong>Expected Differences</strong>:</p>
<ul>
<li>Factory functions: Lower complexity scores with pattern detection</li>
<li>Observer implementations: Adjusted scores for callback registration</li>
<li>Template methods: Reduced penalty for abstract method patterns</li>
<li>Builder classes: Not flagged as god objects despite many methods</li>
</ul>
<h2 id="use-cases-3"><a class="header" href="#use-cases-3">Use Cases</a></h2>
<h3 id="1-false-positive-reduction-active"><a class="header" href="#1-false-positive-reduction-active">1. False Positive Reduction (Active)</a></h3>
<p><strong>Problem</strong>: Complex factory functions flagged as too complex
<strong>Solution</strong>: Pattern detection automatically adjusts complexity scores for recognized factory patterns</p>
<p><strong>Current Behavior</strong>:</p>
<pre><code class="language-bash">debtmap analyze myapp/
</code></pre>
<p>Factory functions are automatically detected and receive adjusted complexity scores. This happens internally without requiring specific flags.</p>
<p><strong>Source</strong>: <code>src/analysis/patterns/factory.rs</code> applies multipliers to factory function complexity</p>
<h3 id="2-builder-pattern-god-object-prevention-active"><a class="header" href="#2-builder-pattern-god-object-prevention-active">2. Builder Pattern God Object Prevention (Active)</a></h3>
<p><strong>Problem</strong>: Builder classes flagged as god objects due to many chaining methods
<strong>Solution</strong>: Builder pattern detection automatically excludes builder classes from god object analysis</p>
<p><strong>Current Behavior</strong>:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// This builder class is automatically recognized
pub struct HttpClientBuilder {
    // Many fields and methods
    pub fn base_url(mut self, url: String) -&gt; Self { /* ... */ }
    pub fn timeout(mut self, duration: Duration) -&gt; Self { /* ... */ }
    pub fn build(self) -&gt; HttpClient { /* ... */ }
}
<span class="boring">}</span></code></pre>
<p>Debtmap detects the builder pattern (chaining methods returning <code>Self</code>, final <code>build()</code> method) and adjusts scoring accordingly.</p>
<p><strong>Source</strong>: <code>src/organization/builder_pattern.rs</code> for god object detection adjustment</p>
<h3 id="3-future-use-case-architecture-documentation-planned"><a class="header" href="#3-future-use-case-architecture-documentation-planned">3. Future Use Case: Architecture Documentation (Planned)</a></h3>
<p><strong>Problem</strong>: Undocumented design patterns in legacy codebase
<strong>Solution</strong>: When pattern output is integrated, users will be able to generate architectural pattern reports</p>
<p><strong>Planned Command</strong>:</p>
<pre><code class="language-bash">debtmap analyze --show-pattern-warnings --output-format json &gt; architecture-report.json
</code></pre>
<p><strong>Current Workaround</strong>: Review complexity adjustments and god object scoring to infer where patterns are detected</p>
<h3 id="4-future-use-case-pattern-consistency-validation-planned"><a class="header" href="#4-future-use-case-pattern-consistency-validation-planned">4. Future Use Case: Pattern Consistency Validation (Planned)</a></h3>
<p><strong>Problem</strong>: Inconsistent Observer implementations across the codebase
<strong>Solution</strong>: When integrated, users can filter analysis to specific pattern types</p>
<p><strong>Planned Command</strong>:</p>
<pre><code class="language-bash">debtmap analyze --patterns observer --output-format json &gt; observers.json
</code></pre>
<p><strong>Current Status</strong>: Pattern detection logic exists and works internally, but results aren’t yet exposed in output</p>
<h2 id="troubleshooting-13"><a class="header" href="#troubleshooting-13">Troubleshooting</a></h2>
<h3 id="pattern-detection-not-visible-in-output"><a class="header" href="#pattern-detection-not-visible-in-output">Pattern Detection Not Visible in Output</a></h3>
<p><strong>Symptoms</strong>: Cannot see detected patterns in analysis output</p>
<p><strong>Explanation</strong>: Pattern detection is currently <strong>internal-only</strong>. Patterns are detected and used to adjust complexity scoring, but results are not exposed in terminal, JSON, or markdown output.</p>
<p><strong>Current Behavior</strong>:</p>
<ul>
<li>Patterns ARE being detected (see <code>src/analysis/patterns/mod.rs</code>)</li>
<li>Detection results affect complexity scores and god object analysis</li>
<li>Pattern information is not included in output formatting</li>
</ul>
<p><strong>Solution</strong>: To benefit from pattern detection:</p>
<ol>
<li>Run standard analysis - patterns are automatically detected</li>
<li>Check if complexity scores seem adjusted for factory/callback patterns</li>
<li>Verify builder classes aren’t flagged as god objects</li>
</ol>
<p><strong>Future Integration</strong>: CLI flags and output formatting will be connected when pattern output integration is complete.</p>
<h3 id="cli-flags-have-no-effect"><a class="header" href="#cli-flags-have-no-effect">CLI Flags Have No Effect</a></h3>
<p><strong>Symptoms</strong>: Using <code>--patterns</code>, <code>--pattern-threshold</code>, or <code>--show-pattern-warnings</code> doesn’t change results</p>
<p><strong>Explanation</strong>: These CLI flags are defined in <code>src/cli.rs:228-241</code> but are not yet fully wired to the analysis pipeline.</p>
<p><strong>Current Status</strong>:</p>
<ul>
<li>✅ CLI argument parsing works</li>
<li>⚠️ Values not passed to pattern detector</li>
<li>⚠️ Pattern detection runs with default settings regardless of flags</li>
</ul>
<p><strong>Workaround</strong>: Pattern detection runs automatically with default settings (threshold 0.7, all 7 patterns enabled).</p>
<h3 id="builder-or-visitor-pattern-not-available-via-cli"><a class="header" href="#builder-or-visitor-pattern-not-available-via-cli">Builder or Visitor Pattern Not Available via CLI</a></h3>
<p><strong>Symptoms</strong>: Cannot specify <code>--patterns builder</code> or <code>--patterns visitor</code></p>
<p><strong>Explanation</strong>: Builder and Visitor patterns are <strong>intentionally internal-only</strong> and will not be available as user-facing pattern detection features:</p>
<ul>
<li><strong>Builder</strong>: Used during god object detection to adjust scores for builder classes (see <code>src/organization/builder_pattern.rs</code>)</li>
<li><strong>Visitor</strong>: Used for complexity analysis to apply logarithmic scaling to exhaustive match expressions (see <code>src/complexity/visitor_detector.rs</code>)</li>
</ul>
<p><strong>Solution</strong>: These patterns are automatically detected when needed for internal analyses. They won’t appear in the <code>--patterns</code> flag even when that feature is fully integrated.</p>
<p><strong>Available user-facing patterns</strong>: <code>observer</code>, <code>singleton</code>, <code>factory</code>, <code>strategy</code>, <code>callback</code>, <code>template_method</code>, <code>dependency_injection</code></p>
<h3 id="false-positive-complexity-adjustments"><a class="header" href="#false-positive-complexity-adjustments">False Positive Complexity Adjustments</a></h3>
<p><strong>Symptoms</strong>: Function with <code>create_</code> prefix gets lower complexity score but isn’t actually a factory</p>
<p><strong>Possible Causes</strong>:</p>
<ol>
<li>Naming collision (e.g., <code>create_session()</code> that doesn’t create objects)</li>
<li>Overly broad pattern matching heuristics</li>
</ol>
<p><strong>Current Workaround</strong>: Pattern detection cannot currently be disabled or tuned per-file. The <code>--no-pattern-detection</code> flag exists but isn’t yet wired to the detector.</p>
<p><strong>Future Solution</strong>: When CLI integration is complete:</p>
<pre><code class="language-bash">debtmap analyze --no-pattern-detection  # Disable all pattern detection
debtmap analyze --pattern-threshold 0.9  # Require very high confidence
</code></pre>
<h2 id="best-practices-10"><a class="header" href="#best-practices-10">Best Practices</a></h2>
<h3 id="current-recommendations"><a class="header" href="#current-recommendations">Current Recommendations</a></h3>
<ol>
<li><strong>Trust automatic detection</strong>: Pattern detection runs automatically with sensible defaults (threshold 0.7, all 7 patterns enabled)</li>
<li><strong>Review complexity scores</strong>: Lower-than-expected complexity for factory/callback functions indicates pattern detection is working</li>
<li><strong>Check builder classes</strong>: If builder classes aren’t flagged as god objects, builder pattern detection is working correctly</li>
<li><strong>Follow pattern idioms</strong>: Use standard naming conventions (<code>create_</code>, <code>make_</code>, <code>@abstractmethod</code>, etc.) to ensure patterns are recognized</li>
<li><strong>Structure code clearly</strong>: Well-structured patterns (clear base classes, explicit implementations) have higher confidence scores</li>
</ol>
<h3 id="when-cli-integration-is-complete"><a class="header" href="#when-cli-integration-is-complete">When CLI Integration is Complete</a></h3>
<p>Future best practices when CLI flags are fully wired:</p>
<ol>
<li><strong>Start with defaults</strong>: The default 0.7 threshold will work well for most projects</li>
<li><strong>Use <code>--show-pattern-warnings</code></strong> during initial analysis to see borderline detections</li>
<li><strong>Tune thresholds per-project</strong>: Adjust <code>--pattern-threshold</code> based on your codebase’s idioms</li>
<li><strong>Disable selectively</strong>: Use <code>--no-pattern-detection</code> to compare scores with/without adjustments</li>
<li><strong>Review pattern reports</strong>: Examine detected patterns to understand architectural decisions</li>
</ol>
<h2 id="summary-4"><a class="header" href="#summary-4">Summary</a></h2>
<h3 id="current-state"><a class="header" href="#current-state">Current State</a></h3>
<p>Debtmap’s design pattern detection <strong>exists and works internally</strong> with the following characteristics:</p>
<p><strong>Implemented Features</strong>:</p>
<ul>
<li>✅ <strong>7 user-facing patterns</strong>: Observer, Singleton, Factory, Strategy, Callback, Template Method, Dependency Injection</li>
<li>✅ <strong>2 internal patterns</strong>: Builder (for god object detection), Visitor (for complexity normalization)</li>
<li>✅ <strong>Pattern detection logic</strong>: Fully implemented in <code>src/analysis/patterns/</code></li>
<li>✅ <strong>Confidence scoring</strong>: 0.0-1.0 scale with holistic assessment</li>
<li>✅ <strong>Cross-file detection</strong>: Tracks imports and interfaces across modules</li>
<li>✅ <strong>Rust trait support</strong>: Leverages trait system for pattern detection</li>
<li>✅ <strong>Complexity integration</strong>: Automatically adjusts scores to reduce false positives</li>
</ul>
<p><strong>Partially Implemented</strong>:</p>
<ul>
<li>⚠️ <strong>CLI flags</strong>: Defined in <code>src/cli.rs</code> but not wired to pattern detector</li>
<li>⚠️ <strong>Output formatting</strong>: <code>PatternInstance</code> type exists but not exposed in output</li>
</ul>
<p><strong>Not Yet Implemented</strong>:</p>
<ul>
<li>❌ <strong>Pattern output in terminal/JSON/markdown</strong>: Detection results not shown to users</li>
<li>❌ <strong>User configuration</strong>: Cannot currently control pattern detection via CLI or config file</li>
<li>❌ <strong>Pattern-specific reports</strong>: Cannot filter or focus on specific pattern types</li>
</ul>
<h3 id="impact"><a class="header" href="#impact">Impact</a></h3>
<p>Pattern detection <strong>significantly improves analysis accuracy</strong> even without visible output:</p>
<ul>
<li><strong>Reduces false positives</strong>: Factory functions, callbacks, and template methods get appropriate complexity scores</li>
<li><strong>Prevents god object misclassification</strong>: Builder classes recognized and excluded from god object detection</li>
<li><strong>Normalizes exhaustive matching</strong>: Visitor pattern detection applies logarithmic scaling to pattern matching</li>
<li><strong>Supports multiple languages</strong>: Works across Python, JavaScript, TypeScript, and Rust</li>
</ul>
<h3 id="future-integration"><a class="header" href="#future-integration">Future Integration</a></h3>
<p>When CLI and output integration is complete, users will be able to:</p>
<ul>
<li>View detected patterns in analysis output</li>
<li>Control pattern detection via <code>--patterns</code>, <code>--pattern-threshold</code>, and <code>--show-pattern-warnings</code> flags</li>
<li>Generate architectural documentation from pattern detection results</li>
<li>Validate pattern consistency across codebases</li>
</ul>
<p>The foundation is solid - pattern detection works correctly and provides value. The remaining work is connecting the detection logic to user-facing configuration and output.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="entropy-analysis"><a class="header" href="#entropy-analysis">Entropy Analysis</a></h1>
<p>Entropy analysis is Debtmap’s unique approach to distinguishing genuinely complex code from repetitive pattern-based code. This reduces false positives by 60-75% compared to traditional cyclomatic complexity metrics.</p>
<h2 id="overview-12"><a class="header" href="#overview-12">Overview</a></h2>
<p>Traditional static analysis tools flag code as “complex” based purely on cyclomatic complexity or lines of code. However, not all complexity is equal:</p>
<ul>
<li><strong>Repetitive patterns</strong> (validation functions, dispatchers) have high cyclomatic complexity but low cognitive load</li>
<li><strong>Diverse logic</strong> (state machines, business rules) may have moderate cyclomatic complexity but high cognitive load</li>
</ul>
<p>Entropy analysis uses information theory to distinguish between these cases.</p>
<h2 id="how-it-works-5"><a class="header" href="#how-it-works-5">How It Works</a></h2>
<p>Debtmap’s entropy analysis is <strong>language-agnostic</strong>, working across Rust, Python, JavaScript, and TypeScript codebases using a universal token classification approach. This ensures consistent complexity assessment regardless of the programming language used.</p>
<h3 id="language-agnostic-analysis"><a class="header" href="#language-agnostic-analysis">Language-Agnostic Analysis</a></h3>
<p>The same entropy concepts apply consistently across all supported languages. Here’s how a validation function would be analyzed in different languages:</p>
<p><strong>Rust:</strong></p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn validate_config(config: &amp;Config) -&gt; Result&lt;()&gt; {
    if config.output_dir.is_none() { return Err(anyhow!("output_dir required")); }
    if config.max_workers.is_none() { return Err(anyhow!("max_workers required")); }
    if config.timeout_secs.is_none() { return Err(anyhow!("timeout_secs required")); }
    Ok(())
}
// Entropy: ~0.3, Pattern Repetition: 0.9, Effective Complexity: ~5
<span class="boring">}</span></code></pre>
<p><strong>Python:</strong></p>
<pre><code class="language-python">def validate_config(config: Config) -&gt; None:
    if config.output_dir is None: raise ValueError("output_dir required")
    if config.max_workers is None: raise ValueError("max_workers required")
    if config.timeout_secs is None: raise ValueError("timeout_secs required")
# Entropy: ~0.3, Pattern Repetition: 0.9, Effective Complexity: ~5
</code></pre>
<p><strong>JavaScript/TypeScript:</strong></p>
<pre><code class="language-typescript">function validateConfig(config: Config): void {
    if (!config.outputDir) throw new Error("outputDir required");
    if (!config.maxWorkers) throw new Error("maxWorkers required");
    if (!config.timeoutSecs) throw new Error("timeoutSecs required");
}
// Entropy: ~0.3, Pattern Repetition: 0.9, Effective Complexity: ~5
</code></pre>
<p>All three receive similar entropy scores because they share the same repetitive validation pattern, demonstrating how Debtmap’s analysis transcends language syntax to identify underlying code structure patterns.</p>
<h3 id="shannon-entropy"><a class="header" href="#shannon-entropy">Shannon Entropy</a></h3>
<p>Shannon entropy measures the variety and unpredictability of code patterns:</p>
<pre><code>H(X) = -Σ p(x) × log₂(p(x))
</code></pre>
<p>Where:</p>
<ul>
<li><code>p(x)</code> = probability of each token type</li>
<li>High entropy (0.8-1.0) = many different patterns</li>
<li>Low entropy (0.0-0.3) = repetitive patterns</li>
</ul>
<h3 id="token-classification"><a class="header" href="#token-classification">Token Classification</a></h3>
<p>Debtmap can classify tokens by importance to give more weight to semantically significant tokens in entropy calculations. This is controlled by the <code>use_classification</code> configuration option.</p>
<p><strong>When enabled</strong> (<code>use_classification = false</code> by default for backward compatibility), tokens are weighted by importance:</p>
<p><strong>High importance (weight: 1.0):</strong></p>
<ul>
<li>Control flow keywords (<code>if</code>, <code>match</code>, <code>for</code>, <code>while</code>)</li>
<li>Error handling (<code>try</code>, <code>catch</code>, <code>?</code>, <code>unwrap</code>)</li>
<li>Async keywords (<code>async</code>, <code>await</code>)</li>
</ul>
<p><strong>Medium importance (weight: 0.7):</strong></p>
<ul>
<li>Function calls</li>
<li>Method invocations</li>
<li>Operators</li>
</ul>
<p><strong>Low importance (weight: 0.3):</strong></p>
<ul>
<li>Identifiers (variable names)</li>
<li>Literals (strings, numbers)</li>
<li>Punctuation</li>
</ul>
<p><strong>When disabled</strong> (<code>use_classification = false</code>), all tokens are treated equally, which may be useful for debugging or when you want unweighted entropy scores.</p>
<h3 id="pattern-repetition-detection"><a class="header" href="#pattern-repetition-detection">Pattern Repetition Detection</a></h3>
<p>Detects repetitive structures in the AST:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Low pattern repetition (0.2) - all branches identical
if a.is_none() { return Err(...) }
if b.is_none() { return Err(...) }
if c.is_none() { return Err(...) }

// High pattern repetition (0.9) - diverse branches
match state {
    Active =&gt; transition_to_standby(),
    Standby =&gt; transition_to_active(),
    Maintenance =&gt; schedule_restart(),
}
<span class="boring">}</span></code></pre>
<h3 id="branch-similarity-analysis"><a class="header" href="#branch-similarity-analysis">Branch Similarity Analysis</a></h3>
<p>Analyzes similarity between conditional branches:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// High branch similarity (0.9) - branches are nearly identical
if condition_a {
    log("A happened");
    process_a();
}
if condition_b {
    log("B happened");
    process_b();
}

// Low branch similarity (0.2) - branches are very different
if needs_auth {
    authenticate_user()?;
    load_profile()?;
} else {
    show_guest_ui();
}
<span class="boring">}</span></code></pre>
<h3 id="effective-complexity-adjustment"><a class="header" href="#effective-complexity-adjustment">Effective Complexity Adjustment</a></h3>
<p>Debtmap uses a multi-factor dampening approach that analyzes three dimensions of code repetitiveness:</p>
<ol>
<li><strong>Pattern Repetition</strong> - Detects repetitive AST structures</li>
<li><strong>Token Entropy</strong> - Measures variety in token usage</li>
<li><strong>Branch Similarity</strong> - Compares similarity between conditional branches</li>
</ol>
<p>These factors are combined multiplicatively with a minimum floor of 0.7 (preserving at least 70% of original complexity):</p>
<pre><code>dampening_factor = (repetition_factor × entropy_factor × branch_factor).max(0.7)
effective_complexity = raw_complexity × dampening_factor
</code></pre>
<h4 id="historical-note-spec-68"><a class="header" href="#historical-note-spec-68">Historical Note: Spec 68</a></h4>
<p><strong>Spec 68: Graduated Entropy Dampening</strong> was the original simple algorithm that only considered entropy &lt; 0.2:</p>
<pre><code>dampening_factor = 0.5 + 0.5 × (entropy / 0.2)  [when entropy &lt; 0.2]
</code></pre>
<p>The current implementation uses a more sophisticated <strong>graduated dampening</strong> approach that considers all three factors (repetition, entropy, branch similarity) with separate thresholds and ranges for each. The test suite references Spec 68 to verify backward compatibility with the original behavior.</p>
<h4 id="when-dampening-applies"><a class="header" href="#when-dampening-applies">When Dampening Applies</a></h4>
<p>Dampening is applied based on multiple thresholds:</p>
<ul>
<li><strong>Pattern Repetition</strong>: Values approaching 1.0 trigger dampening (high repetition detected)</li>
<li><strong>Token Entropy</strong>: Values below 0.4 trigger graduated dampening (low variety)</li>
<li><strong>Branch Similarity</strong>: Values above 0.8 trigger dampening (similar branches)</li>
</ul>
<h4 id="graduated-dampening-formula"><a class="header" href="#graduated-dampening-formula">Graduated Dampening Formula</a></h4>
<p>Each factor is dampened individually using a graduated calculation:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Conceptual pseudocode showing the three-factor approach
// Actual implementation in src/complexity/entropy.rs:185-195 and :429-439
fn calculate_dampening_factor(
    repetition: f64,     // 0.0-1.0
    entropy: f64,        // 0.0-1.0
    branch_similarity: f64  // 0.0-1.0
) -&gt; f64 {
    // Each factor uses calculate_graduated_dampening with its own threshold/range
    let repetition_factor = graduated_dampening(repetition, threshold=1.0, max_reduction=0.20);
    let entropy_factor = graduated_dampening(entropy, threshold=0.4, max_reduction=0.15);
    let branch_factor = graduated_dampening(branch_similarity, threshold=0.8, max_reduction=0.25);

    (repetition_factor * entropy_factor * branch_factor).max(0.7)  // Never reduce below 70%
}
<span class="boring">}</span></code></pre>
<p><strong>Key Parameters:</strong></p>
<ul>
<li><strong>Repetition</strong>: Threshold 1.0, max 20% reduction (configurable via <code>max_repetition_reduction</code>)</li>
<li><strong>Entropy</strong>: Threshold 0.4 (hardcoded), max 15% reduction (configurable via <code>max_entropy_reduction</code>)</li>
<li><strong>Branch Similarity</strong>: Threshold 0.8 (configurable via <code>branch_threshold</code>), max 25% reduction (configurable via <code>max_branch_reduction</code>)</li>
<li><strong>Combined Floor</strong>: Minimum 70% of original complexity preserved (configurable via <code>max_combined_reduction</code>)</li>
</ul>
<h4 id="example-repetitive-validation-function"><a class="header" href="#example-repetitive-validation-function">Example: Repetitive Validation Function</a></h4>
<pre><code>Raw Complexity: 20
Pattern Repetition: 0.95 (very high)
Token Entropy: 0.3 (low variety)
Branch Similarity: 0.9 (very similar branches)

repetition_factor ≈ 0.85 (15% reduction)
entropy_factor ≈ 0.90 (10% reduction)
branch_factor ≈ 0.80 (20% reduction)

dampening_factor = (0.85 × 0.90 × 0.80) = 0.612
dampening_factor = max(0.612, 0.7) = 0.7  // Floor applied

Effective Complexity = 20 × 0.7 = 14

Result: 30% reduction (maximum allowed)
</code></pre>
<h4 id="example-diverse-state-machine"><a class="header" href="#example-diverse-state-machine">Example: Diverse State Machine</a></h4>
<pre><code>Raw Complexity: 20
Pattern Repetition: 0.2 (low - not repetitive)
Token Entropy: 0.8 (high variety)
Branch Similarity: 0.3 (diverse branches)

repetition_factor ≈ 1.0 (no reduction)
entropy_factor ≈ 1.0 (no reduction)
branch_factor ≈ 1.0 (no reduction)

dampening_factor = (1.0 × 1.0 × 1.0) = 1.0

Effective Complexity = 20 × 1.0 = 20

Result: 0% reduction (complexity preserved)
</code></pre>
<h2 id="real-world-examples"><a class="header" href="#real-world-examples">Real-World Examples</a></h2>
<h3 id="example-1-validation-function"><a class="header" href="#example-1-validation-function">Example 1: Validation Function</a></h3>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn validate_config(config: &amp;Config) -&gt; Result&lt;()&gt; {
    if config.output_dir.is_none() {
        return Err(anyhow!("output_dir required"));
    }
    if config.max_workers.is_none() {
        return Err(anyhow!("max_workers required"));
    }
    if config.timeout_secs.is_none() {
        return Err(anyhow!("timeout_secs required"));
    }
    // ... 17 more similar checks
    Ok(())
}
<span class="boring">}</span></code></pre>
<p><strong>Traditional analysis:</strong></p>
<ul>
<li>Cyclomatic Complexity: 20</li>
<li>Assessment: CRITICAL</li>
</ul>
<p><strong>Entropy analysis:</strong></p>
<ul>
<li>Shannon Entropy: 0.3 (low variety)</li>
<li>Pattern Repetition: 0.9 (highly repetitive)</li>
<li>Branch Similarity: 0.95 (nearly identical)</li>
<li>Effective Complexity: 5</li>
<li>Assessment: LOW PRIORITY</li>
</ul>
<h3 id="example-2-state-machine-logic"><a class="header" href="#example-2-state-machine-logic">Example 2: State Machine Logic</a></h3>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn reconcile_state(current: &amp;State, desired: &amp;State) -&gt; Vec&lt;Action&gt; {
    let mut actions = vec![];

    match (current.mode, desired.mode) {
        (Mode::Active, Mode::Standby) =&gt; {
            if current.has_active_connections() {
                actions.push(Action::DrainConnections);
                actions.push(Action::WaitForDrain);
            }
            actions.push(Action::TransitionToStandby);
        }
        (Mode::Standby, Mode::Active) =&gt; {
            if desired.requires_warmup() {
                actions.push(Action::Warmup);
            }
            actions.push(Action::TransitionToActive);
        }
        // ... more diverse state transitions
        _ =&gt; {}
    }

    actions
}
<span class="boring">}</span></code></pre>
<p><strong>Traditional analysis:</strong></p>
<ul>
<li>Cyclomatic Complexity: 8</li>
<li>Assessment: MODERATE</li>
</ul>
<p><strong>Entropy analysis:</strong></p>
<ul>
<li>Shannon Entropy: 0.85 (high variety)</li>
<li>Pattern Repetition: 0.2 (not repetitive)</li>
<li>Branch Similarity: 0.3 (diverse branches)</li>
<li>Effective Complexity: 9</li>
<li>Assessment: HIGH PRIORITY</li>
</ul>
<h2 id="configuration-10"><a class="header" href="#configuration-10">Configuration</a></h2>
<p>Configure entropy analysis in <code>.debtmap.toml</code> or disable via the <code>--semantic-off</code> CLI flag.</p>
<pre><code class="language-toml">[entropy]
# Enable entropy analysis (default: true)
enabled = true

# Weight of entropy in overall complexity scoring (0.0-1.0, default: 1.0)
# Note: This affects scoring, not dampening thresholds
weight = 1.0

# Minimum tokens required for entropy calculation (default: 20)
min_tokens = 20

# Pattern similarity threshold for repetition detection (0.0-1.0, default: 0.7)
pattern_threshold = 0.7

# Enable advanced token classification (default: false for backward compatibility)
# When true, weights tokens by semantic importance (control flow &gt; operators &gt; identifiers)
use_classification = false

# Branch similarity threshold (0.0-1.0, default: 0.8)
# Branches with similarity above this threshold contribute to dampening
branch_threshold = 0.8

# Maximum reduction limits (these are configurable)
max_repetition_reduction = 0.20  # Max 20% reduction from pattern repetition
max_entropy_reduction = 0.15     # Max 15% reduction from low token entropy
max_branch_reduction = 0.25      # Max 25% reduction from branch similarity
max_combined_reduction = 0.30    # Overall cap at 30% reduction (minimum 70% preserved)
</code></pre>
<p><strong>Important Notes:</strong></p>
<ol>
<li>
<p><strong>Dampening thresholds</strong> - Some are configurable, some are hardcoded (<code>src/complexity/entropy.rs:185-195</code>):</p>
<ul>
<li><strong>Entropy factor threshold: 0.4</strong> - Hardcoded in implementation (src/complexity/entropy.rs:192). Although an <code>entropy_threshold</code> field exists in the config struct (src/config/languages.rs:84), it is not wired up to be used in the actual dampening calculation. The value 0.4 was chosen based on empirical analysis across multiple codebases to balance false positive reduction with sensitivity to genuinely complex code.</li>
<li><strong>Branch threshold: 0.8</strong> - Configurable via <code>branch_threshold</code> in config file</li>
<li><strong>Pattern threshold: 0.7/1.0</strong> - Configurable via <code>pattern_threshold</code> in config file</li>
</ul>
</li>
<li>
<p><strong>The <code>weight</code> parameter</strong> affects how entropy scores contribute to overall complexity scoring, but does not change the dampening thresholds or reductions.</p>
</li>
<li>
<p><strong>Token classification</strong> defaults to <code>false</code> (disabled) for backward compatibility, even though it provides more accurate entropy analysis when enabled.</p>
</li>
</ol>
<h3 id="tuning-for-your-project"><a class="header" href="#tuning-for-your-project">Tuning for Your Project</a></h3>
<p><strong>Enable token classification for better accuracy:</strong></p>
<pre><code class="language-toml">[entropy]
enabled = true
use_classification = true  # Weight control flow keywords more heavily
</code></pre>
<p><strong>Strict mode (fewer reductions, flag more code):</strong></p>
<pre><code class="language-toml">[entropy]
enabled = true
max_repetition_reduction = 0.10  # Reduce from default 0.20
max_entropy_reduction = 0.08     # Reduce from default 0.15
max_branch_reduction = 0.12      # Reduce from default 0.25
max_combined_reduction = 0.20    # Reduce from default 0.30 (preserve 80%)
</code></pre>
<p><strong>Lenient mode (more aggressive reduction):</strong></p>
<pre><code class="language-toml">[entropy]
enabled = true
max_repetition_reduction = 0.30  # Increase from default 0.20
max_entropy_reduction = 0.25     # Increase from default 0.15
max_branch_reduction = 0.35      # Increase from default 0.25
max_combined_reduction = 0.50    # Increase from default 0.30 (preserve 50%)
</code></pre>
<p><strong>Disable entropy dampening entirely:</strong></p>
<pre><code class="language-toml">[entropy]
enabled = false
</code></pre>
<p>Or via CLI (disables entropy-based complexity adjustments):</p>
<pre><code class="language-bash"># Disables semantic analysis features including entropy dampening
debtmap analyze . --semantic-off
</code></pre>
<p><strong>Note</strong>: The <code>--semantic-off</code> flag disables all semantic analysis features, including entropy-based complexity adjustments. This is useful when you want raw cyclomatic complexity without any dampening.</p>
<h2 id="interpreting-entropy-adjusted-output"><a class="header" href="#interpreting-entropy-adjusted-output">Interpreting Entropy-Adjusted Output</a></h2>
<p>When entropy analysis detects repetitive patterns, debtmap displays both the original and adjusted complexity values to help you understand the adjustment. This transparency allows you to verify the analysis and understand why certain code receives lower priority.</p>
<h3 id="output-format"><a class="header" href="#output-format">Output Format</a></h3>
<p>When viewing detailed output (verbosity level 2 with <code>-vv</code>), entropy-adjusted complexity is shown in the <strong>COMPLEXITY</strong> section:</p>
<pre><code>COMPLEXITY: cyclomatic=20 (dampened: 14, factor: 0.70), est_branches=40, cognitive=25, nesting=3, entropy=0.30
</code></pre>
<p>And in the <strong>Entropy Impact</strong> scoring section:</p>
<pre><code>  - Entropy Impact: 30% dampening (entropy: 0.30, repetition: 95%)
</code></pre>
<h3 id="understanding-the-values"><a class="header" href="#understanding-the-values">Understanding the Values</a></h3>
<p><strong>cyclomatic=20</strong>: Original cyclomatic complexity before adjustment
<strong>dampened: 14</strong>: Adjusted complexity after entropy analysis (20 × 0.70 = 14)
<strong>factor: 0.70</strong>: The dampening factor applied (0.70 = 30% reduction)
<strong>entropy=0.30</strong>: Shannon entropy score (0.0-1.0, lower = more repetitive)
<strong>repetition: 95%</strong>: Pattern repetition score (higher = more repetitive)</p>
<h3 id="reconstructing-the-calculation"><a class="header" href="#reconstructing-the-calculation">Reconstructing the Calculation</a></h3>
<p>You can verify the adjustment by multiplying:</p>
<pre><code>original_complexity × dampening_factor = adjusted_complexity
20 × 0.70 = 14
</code></pre>
<p>The dampening percentage shown in the Entropy Impact section is:</p>
<pre><code>dampening_percentage = (1.0 - dampening_factor) × 100%
(1.0 - 0.70) × 100% = 30%
</code></pre>
<h3 id="when-entropy-data-is-unavailable"><a class="header" href="#when-entropy-data-is-unavailable">When Entropy Data is Unavailable</a></h3>
<p>If a function is too small for entropy analysis (&lt; 20 tokens) or entropy is disabled, the output shows complexity without dampening:</p>
<pre><code>COMPLEXITY: cyclomatic=5, est_branches=10, cognitive=8, nesting=2
</code></pre>
<p>No “dampened” or “factor” values are shown, indicating the raw complexity is used for scoring.</p>
<h3 id="example-output-comparison"><a class="header" href="#example-output-comparison">Example Output Comparison</a></h3>
<p><strong>Before entropy-adjustment:</strong></p>
<pre><code>#1 SCORE: 95.5 [CRITICAL]
├─ COMPLEXITY: cyclomatic=20, est_branches=40, cognitive=25, nesting=3
</code></pre>
<p><strong>After entropy-adjustment:</strong></p>
<pre><code>#15 SCORE: 68.2 [HIGH]
├─ COMPLEXITY: cyclomatic=20 (dampened: 14, factor: 0.70), est_branches=40, cognitive=25, nesting=3, entropy=0.30
  - Entropy Impact: 30% dampening (entropy: 0.30, repetition: 95%)
</code></pre>
<p>The item dropped from rank #1 to #15 because entropy analysis detected the high complexity was primarily due to repetitive validation patterns rather than genuine cognitive complexity.</p>
<h2 id="understanding-the-impact"><a class="header" href="#understanding-the-impact">Understanding the Impact</a></h2>
<h3 id="measuring-false-positive-reduction"><a class="header" href="#measuring-false-positive-reduction">Measuring False Positive Reduction</a></h3>
<p>Run analysis with and without entropy:</p>
<pre><code class="language-bash"># Without entropy
debtmap analyze . --semantic-off --top 20 &gt; without_entropy.txt

# With entropy (default)
debtmap analyze . --top 20 &gt; with_entropy.txt

# Compare
diff without_entropy.txt with_entropy.txt
</code></pre>
<p><strong>Expected results:</strong></p>
<ul>
<li>60-75% reduction in flagged validation functions</li>
<li>40-50% reduction in flagged dispatcher functions</li>
<li>20-30% reduction in flagged configuration parsers</li>
<li>No reduction in genuinely complex state machines or business logic</li>
</ul>
<h3 id="verifying-correctness"><a class="header" href="#verifying-correctness">Verifying Correctness</a></h3>
<p>Entropy analysis should:</p>
<ul>
<li><strong>Reduce</strong> flags on repetitive code (validators, dispatchers)</li>
<li><strong>Preserve</strong> flags on genuinely complex code (state machines, business logic)</li>
</ul>
<p>If entropy analysis incorrectly reduces flags on genuinely complex code, adjust configuration:</p>
<pre><code class="language-toml">[entropy]
max_combined_reduction = 0.20  # Reduce from default 0.30 (preserve 80%)
max_repetition_reduction = 0.10  # Reduce individual factors
max_entropy_reduction = 0.08
max_branch_reduction = 0.12
</code></pre>
<h2 id="best-practices-11"><a class="header" href="#best-practices-11">Best Practices</a></h2>
<ol>
<li><strong>Use default settings</strong> - They work well for most projects</li>
<li><strong>Verify results</strong> - Spot-check top-priority items to ensure correctness</li>
<li><strong>Tune conservatively</strong> - Start with default settings, adjust if needed</li>
<li><strong>Disable for debugging</strong> - Use <code>--semantic-off</code> if entropy seems incorrect</li>
<li><strong>Report issues</strong> - If entropy incorrectly flags code, report it</li>
</ol>
<h2 id="limitations-2"><a class="header" href="#limitations-2">Limitations</a></h2>
<p>Entropy analysis works best for:</p>
<ul>
<li>Functions with cyclomatic complexity 10-50</li>
<li>Code with clear repetitive patterns</li>
<li>Validation, dispatch, and configuration functions</li>
</ul>
<p>Entropy analysis is less effective for:</p>
<ul>
<li>Very simple functions (complexity &lt; 5)</li>
<li>Very complex functions (complexity &gt; 100)</li>
<li>Obfuscated or generated code</li>
</ul>
<h2 id="comparison-with-other-approaches"><a class="header" href="#comparison-with-other-approaches">Comparison with Other Approaches</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Approach</th><th>False Positive Rate</th><th>Complexity</th><th>Speed</th></tr>
</thead>
<tbody>
<tr><td>Raw Cyclomatic Complexity</td><td>High (many false positives)</td><td>Low</td><td>Fast</td></tr>
<tr><td>Cognitive Complexity</td><td>Medium</td><td>Medium</td><td>Medium</td></tr>
<tr><td>Entropy Analysis (Debtmap)</td><td>Low</td><td>High</td><td>Fast</td></tr>
<tr><td>Manual Code Review</td><td>Very Low</td><td>Very High</td><td>Very Slow</td></tr>
</tbody>
</table>
</div>
<p>Debtmap’s entropy analysis provides the best balance of accuracy and speed.</p>
<h2 id="see-also-6"><a class="header" href="#see-also-6">See Also</a></h2>
<ul>
<li><a href="#why-debtmap">Why Debtmap?</a> - Real-world examples of entropy analysis</li>
<li><a href="#analysis-guide">Analysis Guide</a> - General analysis concepts</li>
<li><a href="#configuration-2">Configuration</a> - Complete configuration reference</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="error-handling-analysis"><a class="header" href="#error-handling-analysis">Error Handling Analysis</a></h1>
<p>Debtmap provides comprehensive error handling analysis for Rust codebases, detecting anti-patterns that lead to silent failures, production panics, and difficult-to-debug issues.</p>
<h2 id="implementation-status-1"><a class="header" href="#implementation-status-1">Implementation Status</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Language</th><th>Error Swallowing</th><th>Panic Patterns</th><th>Async Errors</th><th>Context Loss</th><th>Propagation Analysis</th></tr>
</thead>
<tbody>
<tr><td><strong>Rust</strong></td><td>✅ Full (7 patterns)</td><td>✅ Full (6 patterns)</td><td>✅ Full (5 patterns)</td><td>✅ Full (5 patterns)</td><td>✅ Full (4 patterns)</td></tr>
<tr><td><strong>Python</strong></td><td>⚠️ Planned</td><td>⚠️ Planned</td><td>N/A</td><td>⚠️ Planned</td><td>⚠️ Planned</td></tr>
<tr><td><strong>JavaScript/TypeScript</strong></td><td>⚠️ Limited</td><td>⚠️ Limited</td><td>⚠️ Planned</td><td>❌ Not yet</td><td>❌ Not yet</td></tr>
</tbody>
</table>
</div>
<p><strong>Current Focus:</strong> Rust error handling analysis is fully implemented and production-ready. Python and JavaScript/TypeScript support is limited or planned for future releases.</p>
<h2 id="overview-13"><a class="header" href="#overview-13">Overview</a></h2>
<p>Error handling issues are classified as <strong>ErrorSwallowing</strong> debt with <strong>Major severity</strong> (weight 4), reflecting their significant impact on code reliability and debuggability.</p>
<p><strong>Fully Implemented for Rust:</strong></p>
<ul>
<li><strong>Error swallowing</strong> (7 patterns): Exception handlers that silently catch errors without logging or re-raising</li>
<li><strong>Panic patterns</strong> (6 patterns): Code that can panic in production (unwrap, expect, panic!)</li>
<li><strong>Error propagation issues</strong> (4 patterns): Missing error context in Result chains</li>
<li><strong>Async error handling</strong> (5 patterns): Dropped futures, unhandled JoinHandles, silent task panics</li>
<li><strong>Context loss detection</strong> (5 patterns): Error propagation without meaningful context</li>
</ul>
<p>All error handling patterns are filtered intelligently - code detected in test modules (e.g., <code>#[cfg(test)]</code>, <code>#[test]</code> attributes) receives lower priority or is excluded entirely.</p>
<h2 id="rust-error-handling-analysis"><a class="header" href="#rust-error-handling-analysis">Rust Error Handling Analysis</a></h2>
<h3 id="panic-pattern-detection"><a class="header" href="#panic-pattern-detection">Panic Pattern Detection</a></h3>
<p>Debtmap identifies Rust code that can panic at runtime instead of returning <code>Result</code>:</p>
<p><strong>Detected patterns:</strong></p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ❌ CRITICAL: Direct panic in production code
fn process_data(value: Option&lt;i32&gt;) -&gt; i32 {
    panic!("not implemented");  // Detected: PanicInNonTest
}

// ❌ HIGH: Unwrap on Result
fn read_config(path: &amp;Path) -&gt; Config {
    let content = fs::read_to_string(path).unwrap();  // Detected: UnwrapOnResult
    parse_config(&amp;content)
}

// ❌ HIGH: Unwrap on Option
fn get_user(id: u32) -&gt; User {
    users.get(&amp;id).unwrap()  // Detected: UnwrapOnOption
}

// ❌ MEDIUM: Expect with generic message
fn parse_value(s: &amp;str) -&gt; i32 {
    s.parse().expect("parse failed")  // Detected: ExpectWithGenericMessage
}

// ❌ MEDIUM: TODO in production
fn calculate_tax(amount: f64) -&gt; f64 {
    todo!("implement tax calculation")  // Detected: TodoInProduction
}
<span class="boring">}</span></code></pre>
<p><strong>Recommended alternatives:</strong></p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ GOOD: Propagate errors with ?
fn read_config(path: &amp;Path) -&gt; Result&lt;Config&gt; {
    let content = fs::read_to_string(path)?;
    parse_config(&amp;content)
}

// ✅ GOOD: Handle Option explicitly
fn get_user(id: u32) -&gt; Result&lt;User&gt; {
    users.get(&amp;id)
        .ok_or_else(|| anyhow!("User {} not found", id))
}

// ✅ GOOD: Add meaningful context
fn parse_value(s: &amp;str) -&gt; Result&lt;i32&gt; {
    s.parse()
        .with_context(|| format!("Failed to parse '{}' as integer", s))
}
<span class="boring">}</span></code></pre>
<p><strong>Test code exceptions:</strong></p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[cfg(test)]
mod tests {
    #[test]
    fn test_parsing() {
        let result = "42".parse::&lt;i32&gt;().unwrap();  // ✅ OK in tests (LOW priority)
        assert_eq!(result, 42);
    }
}
<span class="boring">}</span></code></pre>
<p>Debtmap detects <code>#[cfg(test)]</code> attributes and test function contexts, automatically assigning <strong>Low priority</strong> to panic patterns in test code.</p>
<h3 id="error-propagation-analysis"><a class="header" href="#error-propagation-analysis">Error Propagation Analysis</a></h3>
<p>Debtmap detects missing error context in Result chains.</p>
<p><strong>Note:</strong> Context loss detection uses AST-based heuristics without full type information. This means some edge cases may produce false positives or negatives. Type information would improve accuracy but would require a full compilation environment.</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ❌ Missing context - which file failed? What was the error?
fn load_multiple_configs(paths: &amp;[PathBuf]) -&gt; Result&lt;Vec&lt;Config&gt;&gt; {
    paths.iter()
        .map(|p| fs::read_to_string(p))  // Error loses file path information
        .collect::&lt;Result&lt;Vec&lt;_&gt;&gt;&gt;()?
        .into_iter()
        .map(|c| parse_config(&amp;c))  // Error loses which config failed
        .collect()
}

// ✅ GOOD: Preserve context through the chain
fn load_multiple_configs(paths: &amp;[PathBuf]) -&gt; Result&lt;Vec&lt;Config&gt;&gt; {
    paths.iter()
        .map(|p| {
            fs::read_to_string(p)
                .with_context(|| format!("Failed to read config from {}", p.display()))
        })
        .collect::&lt;Result&lt;Vec&lt;_&gt;&gt;&gt;()?
        .into_iter()
        .enumerate()
        .map(|(i, content)| {
            parse_config(&amp;content)
                .with_context(|| format!("Failed to parse config #{}", i))
        })
        .collect()
}
<span class="boring">}</span></code></pre>
<p><strong>Best practices:</strong></p>
<ul>
<li>Use <code>.context()</code> or <code>.with_context()</code> from <code>anyhow</code> or <code>thiserror</code></li>
<li>Include relevant values in error messages (file paths, indices, input values)</li>
<li>Maintain error context at each transformation in the chain</li>
</ul>
<h3 id="error-swallowing-in-rust"><a class="header" href="#error-swallowing-in-rust">Error Swallowing in Rust</a></h3>
<p>Debtmap detects seven distinct patterns of error swallowing in Rust, where errors are silently ignored without logging or propagation:</p>
<h4 id="1-ifletoknoelse---missing-else-branch"><a class="header" href="#1-ifletoknoelse---missing-else-branch">1. IfLetOkNoElse - Missing else branch</a></h4>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ❌ Detected: if let Ok without else branch
fn try_update(value: &amp;str) {
    if let Ok(parsed) = value.parse::&lt;i32&gt;() {
        update_value(parsed);
    }
    // Error case silently ignored - no logging or handling
}

// ✅ GOOD: Handle both cases
fn try_update(value: &amp;str) -&gt; Result&lt;()&gt; {
    if let Ok(parsed) = value.parse::&lt;i32&gt;() {
        update_value(parsed);
        Ok(())
    } else {
        Err(anyhow!("Failed to parse value: {}", value))
    }
}
<span class="boring">}</span></code></pre>
<h4 id="2-ifletokemptyelse---empty-else-branch"><a class="header" href="#2-ifletokemptyelse---empty-else-branch">2. IfLetOkEmptyElse - Empty else branch</a></h4>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ❌ Detected: if let Ok with empty else
fn process_result(result: Result&lt;Data, Error&gt;) {
    if let Ok(data) = result {
        process(data);
    } else {
        // Empty else - error silently swallowed
    }
}

// ✅ GOOD: Log the error
fn process_result(result: Result&lt;Data, Error&gt;) {
    if let Ok(data) = result {
        process(data);
    } else {
        log::error!("Failed to process: {:?}", result);
    }
}
<span class="boring">}</span></code></pre>
<h4 id="3-letunderscoreresult---discarding-result-with-let-_"><a class="header" href="#3-letunderscoreresult---discarding-result-with-let-_">3. LetUnderscoreResult - Discarding Result with let _</a></h4>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ❌ Detected: Result discarded with let _
fn save_data(data: &amp;Data) {
    let _ = fs::write("data.json", serde_json::to_string(data).unwrap());
    // Write failure silently ignored
}

// ✅ GOOD: Handle or propagate the error
fn save_data(data: &amp;Data) -&gt; Result&lt;()&gt; {
    fs::write("data.json", serde_json::to_string(data)?)
        .context("Failed to save data")?;
    Ok(())
}
<span class="boring">}</span></code></pre>
<h4 id="4-okmethoddiscard---calling-ok-and-discarding"><a class="header" href="#4-okmethoddiscard---calling-ok-and-discarding">4. OkMethodDiscard - Calling .ok() and discarding</a></h4>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ❌ Detected: .ok() called but result discarded
fn try_parse(s: &amp;str) -&gt; Option&lt;i32&gt; {
    s.parse::&lt;i32&gt;().ok();  // Result immediately discarded
    None
}

// ✅ GOOD: Use the Ok value or log the error
fn try_parse(s: &amp;str) -&gt; Option&lt;i32&gt; {
    match s.parse::&lt;i32&gt;() {
        Ok(v) =&gt; Some(v),
        Err(e) =&gt; {
            log::warn!("Failed to parse '{}': {}", s, e);
            None
        }
    }
}
<span class="boring">}</span></code></pre>
<h4 id="5-matchignorederr---match-with-ignored-error-variant"><a class="header" href="#5-matchignorederr---match-with-ignored-error-variant">5. MatchIgnoredErr - Match with ignored error variant</a></h4>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ❌ Detected: match with _ in Err branch
fn try_load(path: &amp;Path) -&gt; Option&lt;String&gt; {
    match fs::read_to_string(path) {
        Ok(content) =&gt; Some(content),
        Err(_) =&gt; None,  // Error details ignored
    }
}

// ✅ GOOD: Log the error with context
fn try_load(path: &amp;Path) -&gt; Option&lt;String&gt; {
    match fs::read_to_string(path) {
        Ok(content) =&gt; Some(content),
        Err(e) =&gt; {
            log::error!("Failed to read {}: {}", path.display(), e);
            None
        }
    }
}
<span class="boring">}</span></code></pre>
<h4 id="6-unwrapornolog---unwrap_or-without-logging"><a class="header" href="#6-unwrapornolog---unwrap_or-without-logging">6. UnwrapOrNoLog - .unwrap_or() without logging</a></h4>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ❌ Detected: unwrap_or without logging
fn get_config_value(key: &amp;str) -&gt; String {
    load_config()
        .and_then(|c| c.get(key))
        .unwrap_or_else(|| "default".to_string())
    // Error silently replaced with default
}

// ✅ GOOD: Log before falling back to default
fn get_config_value(key: &amp;str) -&gt; String {
    match load_config().and_then(|c| c.get(key)) {
        Ok(value) =&gt; value,
        Err(e) =&gt; {
            log::warn!("Config key '{}' not found: {}. Using default.", key, e);
            "default".to_string()
        }
    }
}
<span class="boring">}</span></code></pre>
<h4 id="7-unwrapordefaultnolog---unwrap_or_default-without-logging"><a class="header" href="#7-unwrapordefaultnolog---unwrap_or_default-without-logging">7. UnwrapOrDefaultNoLog - .unwrap_or_default() without logging</a></h4>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ❌ Detected: unwrap_or_default without logging
fn load_settings() -&gt; Settings {
    read_settings_file().unwrap_or_default()
    // Error silently replaced with default settings
}

// ✅ GOOD: Log the fallback to defaults
fn load_settings() -&gt; Settings {
    match read_settings_file() {
        Ok(settings) =&gt; settings,
        Err(e) =&gt; {
            log::warn!("Failed to load settings: {}. Using defaults.", e);
            Settings::default()
        }
    }
}
<span class="boring">}</span></code></pre>
<p><strong>Summary of Error Swallowing Patterns:</strong></p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Pattern</th><th>Description</th><th>Common Cause</th></tr>
</thead>
<tbody>
<tr><td>IfLetOkNoElse</td><td><code>if let Ok(..)</code> without else</td><td>Quick prototyping, forgotten error path</td></tr>
<tr><td>IfLetOkEmptyElse</td><td><code>if let Ok(..)</code> with empty else</td><td>Incomplete implementation</td></tr>
<tr><td>LetUnderscoreResult</td><td><code>let _ = result</code></td><td>Intentional ignore without thought</td></tr>
<tr><td>OkMethodDiscard</td><td><code>.ok()</code> result not used</td><td>Misunderstanding of .ok() semantics</td></tr>
<tr><td>MatchIgnoredErr</td><td><code>Err(_) =&gt; ...</code> with no logging</td><td>Generic error handling</td></tr>
<tr><td>UnwrapOrNoLog</td><td><code>.unwrap_or()</code> without logging</td><td>Convenience over observability</td></tr>
<tr><td>UnwrapOrDefaultNoLog</td><td><code>.unwrap_or_default()</code> without logging</td><td>Default fallback without visibility</td></tr>
</tbody>
</table>
</div>
<p>All these patterns are detected at <strong>Medium to High priority</strong> depending on context, as they represent lost error information that makes debugging difficult.</p>
<p><strong>Source:</strong> Error swallowing patterns are defined in <code>src/debt/error_swallowing.rs:229-238</code> and comprehensively tested in <code>tests/error_swallowing_test.rs</code>.</p>
<h2 id="python-error-handling-analysis-planned"><a class="header" href="#python-error-handling-analysis-planned">Python Error Handling Analysis (Planned)</a></h2>
<p>⚠️ <strong>Python error handling detection is planned but not yet implemented. Currently only Rust error patterns are fully supported.</strong></p>
<p>The patterns described below represent the intended future behavior once Python analysis is implemented.</p>
<h3 id="bare-except-clause-detection-planned"><a class="header" href="#bare-except-clause-detection-planned">Bare Except Clause Detection (Planned)</a></h3>
<p>Python’s bare <code>except:</code> catches all exceptions, including system exits and keyboard interrupts:</p>
<pre><code class="language-python"># ❌ CRITICAL: Bare except catches everything
def process_file(path):
    try:
        with open(path) as f:
            return f.read()
    except:  # Detected: BareExceptClause
        return None  # Catches SystemExit, KeyboardInterrupt, etc.

# ❌ HIGH: Catching Exception is too broad
def load_config(path):
    try:
        return yaml.load(open(path))
    except Exception:  # Detected: OverlyBroadException
        return {}  # Silent failure loses error information

# ✅ GOOD: Specific exception types
def process_file(path):
    try:
        with open(path) as f:
            return f.read()
    except FileNotFoundError:
        log.error(f"File not found: {path}")
        return None
    except PermissionError:
        log.error(f"Permission denied: {path}")
        return None
</code></pre>
<p><strong>Why bare except is dangerous:</strong></p>
<ul>
<li>Catches <code>SystemExit</code> (prevents clean shutdown)</li>
<li>Catches <code>KeyboardInterrupt</code> (prevents Ctrl+C)</li>
<li>Catches <code>GeneratorExit</code> (breaks generator protocol)</li>
<li>Masks programming errors like <code>NameError</code>, <code>AttributeError</code></li>
</ul>
<p><strong>Best practices:</strong></p>
<ul>
<li>Always specify exception types: <code>except ValueError</code>, <code>except (TypeError, KeyError)</code></li>
<li>Use <code>except Exception</code> only when truly catching all application errors</li>
<li>Never use bare <code>except:</code> in production code</li>
<li>Log exceptions with full context before suppressing</li>
</ul>
<h3 id="silent-exception-handling-planned"><a class="header" href="#silent-exception-handling-planned">Silent Exception Handling (Planned)</a></h3>
<pre><code class="language-python"># ❌ Silent exception handling
def get_user_age(user_id):
    try:
        user = db.get_user(user_id)
        return user.age
    except:  # Detected: SilentException (no logging, no re-raise)
        pass

# ✅ GOOD: Log and provide meaningful default
def get_user_age(user_id):
    try:
        user = db.get_user(user_id)
        return user.age
    except UserNotFound:
        logger.warning(f"User {user_id} not found")
        return None
    except DatabaseError as e:
        logger.error(f"Database error fetching user {user_id}: {e}")
        raise  # Re-raise for caller to handle
</code></pre>
<h3 id="contextlib-suppress-detection-planned"><a class="header" href="#contextlib-suppress-detection-planned">Contextlib Suppress Detection (Planned)</a></h3>
<p>Python’s <code>contextlib.suppress()</code> intentionally silences exceptions, which can hide errors:</p>
<pre><code class="language-python">from contextlib import suppress

# ❌ MEDIUM: contextlib.suppress hides errors
def cleanup_temp_files(paths):
    for path in paths:
        with suppress(FileNotFoundError, PermissionError):
            os.remove(path)  # Detected: ContextlibSuppress
            # Errors silently suppressed - no visibility into failures

# ✅ GOOD: Log suppressed errors
def cleanup_temp_files(paths):
    for path in paths:
        try:
            os.remove(path)
        except FileNotFoundError:
            logger.debug(f"File already deleted: {path}")
        except PermissionError as e:
            logger.warning(f"Permission denied removing {path}: {e}")
        except Exception as e:
            logger.error(f"Unexpected error removing {path}: {e}")

# ✅ ACCEPTABLE: Use suppress only for truly ignorable cases
def best_effort_cleanup(paths):
    """Best-effort cleanup - failures are expected and acceptable."""
    for path in paths:
        with suppress(OSError):  # OK if documented and intentional
            os.remove(path)
</code></pre>
<p><strong>When contextlib.suppress is acceptable:</strong></p>
<ul>
<li>Cleanup operations where failures are genuinely unimportant</li>
<li>Operations explicitly documented as “best effort”</li>
<li>Code where logging would create noise without value</li>
</ul>
<p><strong>When to avoid contextlib.suppress:</strong></p>
<ul>
<li>Production code where error visibility matters</li>
<li>Operations where partial failure should be noticed</li>
<li>Any case where debugging might be needed later</li>
</ul>
<h3 id="exception-flow-analysis-planned"><a class="header" href="#exception-flow-analysis-planned">Exception Flow Analysis (Planned)</a></h3>
<p>Python exception flow analysis is planned for future implementation. This would track exception propagation through Python codebases to identify functions that can raise exceptions without proper handling.</p>
<pre><code class="language-python"># Potential issue: Exceptions may propagate unhandled
def process_batch(items):
    for item in items:
        validate_item(item)  # Can raise ValueError
        transform_item(item)  # Can raise TransformError
        save_item(item)  # Can raise DatabaseError

# ✅ GOOD: Handle exceptions appropriately
def process_batch(items):
    results = {"success": 0, "failed": 0}
    for item in items:
        try:
            validate_item(item)
            transform_item(item)
            save_item(item)
            results["success"] += 1
        except ValueError as e:
            logger.warning(f"Invalid item {item.id}: {e}")
            results["failed"] += 1
        except (TransformError, DatabaseError) as e:
            logger.error(f"Failed to process item {item.id}: {e}")
            results["failed"] += 1
            # Optionally re-raise critical errors
            if isinstance(e, DatabaseError):
                raise
    return results
</code></pre>
<h2 id="async-error-handling"><a class="header" href="#async-error-handling">Async Error Handling</a></h2>
<h3 id="javascripttypescript-async-patterns-planned"><a class="header" href="#javascripttypescript-async-patterns-planned">JavaScript/TypeScript Async Patterns (Planned)</a></h3>
<p>⚠️ <strong>JavaScript and TypeScript async error detection is planned but not yet fully implemented.</strong></p>
<p><strong>Current Status:</strong></p>
<ul>
<li>JavaScript/TypeScript support focuses on complexity analysis and basic error patterns</li>
<li>Async error handling detection (unhandled promise rejections, missing await) is <strong>fully implemented for Rust only</strong></li>
<li>Enhanced JavaScript/TypeScript async error detection is planned for future releases</li>
</ul>
<p>The examples below show intended future behavior:</p>
<pre><code class="language-javascript">// ❌ CRITICAL: Unhandled promise rejection
async function loadUserData(userId) {
    const response = await fetch(`/api/users/${userId}`);
    // If fetch rejects, promise is unhandled
    return response.json();
}

loadUserData(123);  // Detected: UnhandledPromiseRejection

// ✅ GOOD: Handle rejections
async function loadUserData(userId) {
    try {
        const response = await fetch(`/api/users/${userId}`);
        if (!response.ok) {
            throw new Error(`HTTP ${response.status}: ${response.statusText}`);
        }
        return await response.json();
    } catch (error) {
        console.error(`Failed to load user ${userId}:`, error);
        throw error;  // Re-throw or return default
    }
}

loadUserData(123).catch(err =&gt; {
    console.error("Top-level error handler:", err);
});
</code></pre>
<h3 id="missing-await-detection-planned"><a class="header" href="#missing-await-detection-planned">Missing Await Detection (Planned)</a></h3>
<pre><code class="language-javascript">// ❌ HIGH: Missing await - promise dropped
async function saveAndNotify(data) {
    await saveToDatabase(data);
    sendNotification(data.userId);  // Detected: MissingAwait
    // Function returns before notification completes
}

// ✅ GOOD: Await all async operations
async function saveAndNotify(data) {
    await saveToDatabase(data);
    await sendNotification(data.userId);
}
</code></pre>
<h3 id="async-rust-error-handling"><a class="header" href="#async-rust-error-handling">Async Rust Error Handling</a></h3>
<p>Debtmap detects five async-specific error handling patterns in Rust.</p>
<p><strong>Note:</strong> Async error detection uses pattern matching on tokio APIs and AST-based heuristics. Some patterns (particularly <code>select!</code> macro handling and future dropping) may require manual review, as full semantic analysis would require type information.</p>
<h4 id="1-droppedfuture---future-dropped-without-awaiting"><a class="header" href="#1-droppedfuture---future-dropped-without-awaiting">1. DroppedFuture - Future dropped without awaiting</a></h4>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ❌ HIGH: Dropped future without error handling
async fn process_requests(requests: Vec&lt;Request&gt;) {
    for req in requests {
        tokio::spawn(async move {
            handle_request(req).await  // Detected: DroppedFuture
            // Errors silently dropped
        });
    }
}

// ✅ GOOD: Join handles and propagate errors
async fn process_requests(requests: Vec&lt;Request&gt;) -&gt; Result&lt;()&gt; {
    let handles: Vec&lt;_&gt; = requests.into_iter()
        .map(|req| {
            tokio::spawn(async move {
                handle_request(req).await
            })
        })
        .collect();

    for handle in handles {
        handle.await??;  // Propagate both JoinError and handler errors
    }
    Ok(())
}
<span class="boring">}</span></code></pre>
<h4 id="2-unhandledjoinhandle---spawned-task-without-join"><a class="header" href="#2-unhandledjoinhandle---spawned-task-without-join">2. UnhandledJoinHandle - Spawned task without join</a></h4>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ❌ HIGH: Task spawned but handle never checked
async fn background_sync() {
    tokio::spawn(async {
        sync_to_database().await  // Detected: UnhandledJoinHandle
    });
    // Handle dropped - can't detect if task panicked or failed
}

// ✅ GOOD: Store and check join handle
async fn background_sync() -&gt; Result&lt;()&gt; {
    let handle = tokio::spawn(async {
        sync_to_database().await
    });
    handle.await?  // Wait for completion and check for panic
}
<span class="boring">}</span></code></pre>
<h4 id="3-silenttaskpanic---task-panic-without-monitoring"><a class="header" href="#3-silenttaskpanic---task-panic-without-monitoring">3. SilentTaskPanic - Task panic without monitoring</a></h4>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ❌ HIGH: Task panic silently ignored
tokio::spawn(async {
    panic!("task failed");  // Detected: SilentTaskPanic
});

// ✅ GOOD: Handle task panics
let handle = tokio::spawn(async {
    critical_operation().await
});

match handle.await {
    Ok(Ok(result)) =&gt; println!("Success: {:?}", result),
    Ok(Err(e)) =&gt; eprintln!("Task failed: {}", e),
    Err(e) =&gt; eprintln!("Task panicked: {}", e),
}
<span class="boring">}</span></code></pre>
<h4 id="4-spawnwithoutjoin---spawning-without-storing-handle"><a class="header" href="#4-spawnwithoutjoin---spawning-without-storing-handle">4. SpawnWithoutJoin - Spawning without storing handle</a></h4>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ❌ MEDIUM: Spawn without storing handle
async fn fire_and_forget_tasks(items: Vec&lt;Item&gt;) {
    for item in items {
        tokio::spawn(process_item(item));  // Detected: SpawnWithoutJoin
        // No way to check task completion or errors
    }
}

// ✅ GOOD: Collect handles for later checking
async fn process_tasks_with_monitoring(items: Vec&lt;Item&gt;) -&gt; Result&lt;()&gt; {
    let handles: Vec&lt;_&gt; = items.into_iter()
        .map(|item| tokio::spawn(process_item(item)))
        .collect();

    for handle in handles {
        handle.await??;
    }
    Ok(())
}
<span class="boring">}</span></code></pre>
<h4 id="5-selectbranchignored---select-branch-without-error-handling"><a class="header" href="#5-selectbranchignored---select-branch-without-error-handling">5. SelectBranchIgnored - Select branch without error handling</a></h4>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ❌ MEDIUM: tokio::select! branch error ignored
async fn process_with_timeout(data: Data) {
    tokio::select! {
        result = process_data(data) =&gt; {
            // Detected: SelectBranchIgnored
            // result could be Err but not checked
        }
        _ = tokio::time::sleep(Duration::from_secs(5)) =&gt; {
            println!("Timeout");
        }
    }
}

// ✅ GOOD: Handle errors in select branches
async fn process_with_timeout(data: Data) -&gt; Result&lt;()&gt; {
    tokio::select! {
        result = process_data(data) =&gt; {
            result?;  // Propagate error
            Ok(())
        }
        _ = tokio::time::sleep(Duration::from_secs(5)) =&gt; {
            Err(anyhow!("Processing timeout after 5s"))
        }
    }
}
<span class="boring">}</span></code></pre>
<p><strong>Async Error Pattern Summary:</strong></p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Pattern</th><th>Severity</th><th>Description</th><th>Common in</th></tr>
</thead>
<tbody>
<tr><td>DroppedFuture</td><td>High</td><td>Future result ignored</td><td>Fire-and-forget spawns</td></tr>
<tr><td>UnhandledJoinHandle</td><td>High</td><td>JoinHandle never checked</td><td>Background tasks</td></tr>
<tr><td>SilentTaskPanic</td><td>High</td><td>Task panic not monitored</td><td>Unmonitored spawns</td></tr>
<tr><td>SpawnWithoutJoin</td><td>Medium</td><td>Handle not stored</td><td>Quick prototypes</td></tr>
<tr><td>SelectBranchIgnored</td><td>Medium</td><td>select! branch error ignored</td><td>Concurrent operations</td></tr>
</tbody>
</table>
</div>
<p>All async error patterns emphasize the importance of properly handling errors in concurrent Rust code, where failures can easily go unnoticed.</p>
<p><strong>Source:</strong> Async error patterns are defined in <code>src/debt/async_errors.rs:205-212</code> and tested with tokio-specific patterns.</p>
<h2 id="severity-levels-and-prioritization"><a class="header" href="#severity-levels-and-prioritization">Severity Levels and Prioritization</a></h2>
<p>Error handling issues are assigned severity based on their impact:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Pattern</th><th>Severity</th><th>Weight</th><th>Priority</th><th>Rationale</th></tr>
</thead>
<tbody>
<tr><td>Panic in production</td><td>CRITICAL</td><td>4</td><td>Critical</td><td>Crashes the process</td></tr>
<tr><td>Bare except clause</td><td>CRITICAL</td><td>4</td><td>Critical</td><td>Masks system signals</td></tr>
<tr><td>Silent task panic</td><td>CRITICAL</td><td>4</td><td>Critical</td><td>Hidden failures</td></tr>
<tr><td>Unwrap on Result/Option</td><td>HIGH</td><td>4</td><td>High</td><td>Likely to panic</td></tr>
<tr><td>Dropped future</td><td>HIGH</td><td>4</td><td>High</td><td>Lost error information</td></tr>
<tr><td>Unhandled promise rejection</td><td>HIGH</td><td>4</td><td>High</td><td>Silently fails</td></tr>
<tr><td>Error swallowing</td><td>MEDIUM</td><td>4</td><td>Medium</td><td>Loses debugging context</td></tr>
<tr><td>Missing error context</td><td>MEDIUM</td><td>4</td><td>Medium</td><td>Hard to debug</td></tr>
<tr><td>Expect with generic message</td><td>MEDIUM</td><td>4</td><td>Medium</td><td>Uninformative errors</td></tr>
<tr><td>TODO in production</td><td>MEDIUM</td><td>4</td><td>Medium</td><td>Incomplete implementation</td></tr>
</tbody>
</table>
</div>
<p>All ErrorSwallowing debt has <strong>weight 4</strong> (Major severity), but individual patterns receive different priorities based on production impact.</p>
<h3 id="integration-with-risk-scoring"><a class="header" href="#integration-with-risk-scoring">Integration with Risk Scoring</a></h3>
<p>Error handling issues contribute to the <code>debt_factor</code> in Debtmap’s risk scoring formula:</p>
<pre><code>risk_score = (complexity_factor * 0.4) + (debt_factor * 0.3) + (coverage_factor * 0.3)

where debt_factor includes:
- ErrorSwallowing count * weight (4)
- Combined with other debt types
</code></pre>
<p><strong>Compound risk example:</strong></p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// HIGH RISK: High complexity + error swallowing + low coverage
fn process_transaction(tx: Transaction) -&gt; bool {  // Cyclomatic: 12, Cognitive: 18
    if tx.amount &gt; 1000 {
        if tx.verified {
            if validate_funds(&amp;tx).unwrap() {  // ❌ Panic pattern
                if tx.user_type == "premium" {
                    match apply_premium_discount(&amp;tx) {
                        Ok(_) =&gt; {},
                        Err(_) =&gt; return false,  // ❌ Error swallowed
                    }
                }
                charge_account(&amp;tx).unwrap();  // ❌ Another panic
                return true;
            }
        }
    }
    false
}
// Coverage: 45% (untested error paths)
// Risk Score: Very High (complexity + error handling + coverage gaps)
<span class="boring">}</span></code></pre>
<p>This function would be flagged as <strong>Priority 1</strong> in Debtmap’s output due to:</p>
<ul>
<li>High cyclomatic complexity (12)</li>
<li>Multiple panic patterns (unwrap calls)</li>
<li>Error swallowing (ignored Result)</li>
<li>Coverage gaps in error handling paths</li>
</ul>
<h2 id="configuration-11"><a class="header" href="#configuration-11">Configuration</a></h2>
<h3 id="error-handling-configuration-options"><a class="header" href="#error-handling-configuration-options">Error Handling Configuration Options</a></h3>
<p><strong>All Rust error handling detection is enabled by default.</strong> You typically don’t need to configure anything - Debtmap will automatically detect all error patterns in your Rust code.</p>
<p><strong>When to Use Configuration:</strong></p>
<ul>
<li><strong>Gradual adoption</strong>: Disable some patterns while fixing others</li>
<li><strong>Project-specific needs</strong>: Turn off patterns that don’t apply to your codebase</li>
<li><strong>Performance tuning</strong>: Disable expensive analyzers if not needed</li>
</ul>
<p>Configure error handling analysis in <code>.debtmap.toml</code>:</p>
<pre><code class="language-toml">[error_handling]
# All patterns enabled by default - only add config to DISABLE patterns
# detect_panic_patterns = true     # Default: enabled
# detect_swallowing = true          # Default: enabled
# detect_async_errors = true        # Default: enabled
# detect_context_loss = true        # Default: enabled
# detect_propagation = true         # Default: enabled

# Example: Gradual adoption - start with just panic patterns
detect_panic_patterns = true       # Keep enabled
detect_swallowing = false          # Disable initially
detect_async_errors = false        # Disable initially
detect_context_loss = false        # Disable initially
</code></pre>
<p><strong>Default Behavior (No Configuration):</strong>
All error handling patterns are detected with the <code>ErrorSwallowing</code> debt category (weight 4). Test code automatically receives lower priority.</p>
<h2 id="detection-examples"><a class="header" href="#detection-examples">Detection Examples</a></h2>
<h3 id="what-gets-detected-vs-not-detected"><a class="header" href="#what-gets-detected-vs-not-detected">What Gets Detected vs. Not Detected</a></h3>
<h4 id="rust-examples-fully-implemented"><a class="header" href="#rust-examples-fully-implemented">Rust Examples (Fully Implemented)</a></h4>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ❌ Detected: unwrap() in production code
pub fn get_config() -&gt; Config {
    load_config().unwrap()
}

// ✅ Not detected: ? operator (proper error propagation)
pub fn get_config() -&gt; Result&lt;Config&gt; {
    load_config()?
}

// ✅ Not detected: unwrap() in test
#[test]
fn test_config() {
    let config = load_config().unwrap();  // OK in tests
    assert_eq!(config.port, 8080);
}

// ❌ Detected: expect() with generic message
let value = map.get("key").expect("missing");

// ✅ Not detected: expect() with descriptive context
let value = map.get("key")
    .expect("Configuration must contain 'key' field");
<span class="boring">}</span></code></pre>
<h4 id="python-examples-planned---not-yet-implemented"><a class="header" href="#python-examples-planned---not-yet-implemented">Python Examples (Planned - Not Yet Implemented)</a></h4>
<pre><code class="language-python"># ❌ Detected: bare except
try:
    risky_operation()
except:
    pass

# ✅ Not detected: specific exception
try:
    risky_operation()
except ValueError:
    handle_value_error()

# ❌ Detected: silent exception (no logging/re-raise)
try:
    db.save(record)
except DatabaseError:
    pass  # Silent failure

# ✅ Not detected: logged exception
try:
    db.save(record)
except DatabaseError as e:
    logger.error(f"Failed to save record: {e}")
    raise
</code></pre>
<h2 id="suppression-patterns-1"><a class="header" href="#suppression-patterns-1">Suppression Patterns</a></h2>
<p>For cases where error handling patterns are intentional, use suppression comments:</p>
<p><strong>Rust:</strong></p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// debtmap: ignore - Unwrap is safe here due to prior validation
let value = validated_map.get("key").unwrap();
<span class="boring">}</span></code></pre>
<p><strong>Python:</strong></p>
<pre><code class="language-python">try:
    experimental_feature()
except:  # debtmap: ignore - Intentional catch-all during migration
    use_fallback()
</code></pre>
<p>See <a href="#suppression-patterns">Suppression Patterns</a> for complete syntax and usage.</p>
<h2 id="best-practices-12"><a class="header" href="#best-practices-12">Best Practices</a></h2>
<h3 id="rust-error-handling"><a class="header" href="#rust-error-handling">Rust Error Handling</a></h3>
<ol>
<li>
<p><strong>Prefer <code>?</code> operator over unwrap/expect</strong></p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Instead of: fs::read_to_string(path).unwrap()
// Use: fs::read_to_string(path)?
<span class="boring">}</span></code></pre>
</li>
<li>
<p><strong>Use anyhow for application errors, thiserror for libraries</strong></p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use anyhow::{Context, Result};

fn load_data(path: &amp;Path) -&gt; Result&lt;Data&gt; {
    let content = fs::read_to_string(path)
        .with_context(|| format!("Failed to read {}", path.display()))?;
    parse_data(&amp;content)
        .context("Invalid data format")
}
<span class="boring">}</span></code></pre>
</li>
<li>
<p><strong>Add context at each error boundary</strong></p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>.with_context(|| format!("meaningful message with {}", value))
<span class="boring">}</span></code></pre>
</li>
<li>
<p><strong>Handle Option explicitly</strong></p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>map.get(key).ok_or_else(|| anyhow!("Missing key: {}", key))?
<span class="boring">}</span></code></pre>
</li>
</ol>
<h3 id="python-error-handling-planned---future-implementation"><a class="header" href="#python-error-handling-planned---future-implementation">Python Error Handling (Planned - Future Implementation)</a></h3>
<p><strong>Note:</strong> Python error handling detection is planned. These best practices represent intended future behavior.</p>
<ol>
<li>
<p><strong>Always use specific exception types</strong></p>
<pre><code class="language-python">except (ValueError, KeyError) as e:
</code></pre>
</li>
<li>
<p><strong>Log before suppressing</strong></p>
<pre><code class="language-python">except DatabaseError as e:
    logger.error(f"Database operation failed: {e}", exc_info=True)
    # Then decide: re-raise, return default, or handle
</code></pre>
</li>
<li>
<p><strong>Avoid bare except completely</strong></p>
<pre><code class="language-python"># If you must catch everything:
except Exception as e:  # Not bare except:
    logger.exception("Unexpected error")
    raise
</code></pre>
</li>
<li>
<p><strong>Use context managers for resource cleanup</strong></p>
<pre><code class="language-python">with open(path) as f:  # Ensures cleanup even on exception
    process(f)
</code></pre>
</li>
</ol>
<h3 id="javascripttypescript-error-handling-planned---limited-implementation"><a class="header" href="#javascripttypescript-error-handling-planned---limited-implementation">JavaScript/TypeScript Error Handling (Planned - Limited Implementation)</a></h3>
<p><strong>Note:</strong> JavaScript/TypeScript async error detection is planned. Currently only basic patterns are supported.</p>
<ol>
<li>
<p><strong>Always handle promise rejections</strong></p>
<pre><code class="language-javascript">fetchData().catch(err =&gt; console.error(err));
// Or use try/catch with async/await
</code></pre>
</li>
<li>
<p><strong>Use async/await consistently</strong></p>
<pre><code class="language-javascript">async function process() {
    try {
        const data = await fetchData();
        await saveData(data);
    } catch (error) {
        console.error("Failed:", error);
        throw error;
    }
}
</code></pre>
</li>
<li>
<p><strong>Don’t forget await</strong></p>
<pre><code class="language-javascript">await asyncOperation();  // Don't drop promises
</code></pre>
</li>
</ol>
<h2 id="improving-rust-error-handling-based-on-debtmap-reports"><a class="header" href="#improving-rust-error-handling-based-on-debtmap-reports">Improving Rust Error Handling Based on Debtmap Reports</a></h2>
<h3 id="workflow"><a class="header" href="#workflow">Workflow</a></h3>
<ol>
<li>
<p><strong>Run analysis with error focus</strong></p>
<pre><code class="language-bash">debtmap analyze --filter-categories ErrorSwallowing
</code></pre>
<p>Note: This analyzes Rust error patterns. Python and JavaScript/TypeScript support is limited or planned.</p>
</li>
<li>
<p><strong>Review priority issues first</strong></p>
<ul>
<li>Address CRITICAL (panic in production, bare except) immediately</li>
<li>Schedule HIGH (unwrap, dropped futures) for next sprint</li>
<li>Plan MEDIUM (missing context) for gradual improvement</li>
</ul>
</li>
<li>
<p><strong>Fix systematically</strong></p>
<ul>
<li>One file or module at a time</li>
<li>Add tests as you improve error handling</li>
<li>Run debtmap after each fix to verify</li>
</ul>
</li>
<li>
<p><strong>Validate improvements</strong></p>
<pre><code class="language-bash"># Before fixes
debtmap analyze --output before.json

# After fixes
debtmap analyze --output after.json

# Compare
debtmap compare before.json after.json
</code></pre>
</li>
</ol>
<h3 id="migration-strategy-for-legacy-code"><a class="header" href="#migration-strategy-for-legacy-code">Migration Strategy for Legacy Code</a></h3>
<pre><code class="language-toml"># .debtmap.toml - Gradual adoption
[error_handling]
# Start with just critical panic patterns
detect_panic_patterns = true
detect_swallowing = false      # Add later
detect_async_errors = false    # Add later
detect_context_loss = false    # Add later

# After fixing panic patterns, enable error swallowing detection
# detect_swallowing = true

# Eventually enable all patterns
# detect_swallowing = true
# detect_async_errors = true
# detect_context_loss = true
# detect_propagation = true
</code></pre>
<p>Track progress over time:</p>
<pre><code class="language-bash"># Weekly error handling health check
debtmap analyze --filter-categories ErrorSwallowing | tee weekly-error-health.txt
</code></pre>
<h2 id="troubleshooting-14"><a class="header" href="#troubleshooting-14">Troubleshooting</a></h2>
<h3 id="too-many-false-positives-in-test-code"><a class="header" href="#too-many-false-positives-in-test-code">Too Many False Positives in Test Code</a></h3>
<p><strong>Problem:</strong> Debtmap flagging <code>unwrap()</code> in test functions</p>
<p><strong>Solution:</strong> Debtmap should automatically detect test code via:</p>
<ul>
<li><code>#[cfg(test)]</code> modules in Rust</li>
<li><code>#[test]</code> attributes</li>
<li><code>test_</code> function name prefix in Python</li>
<li><code>*.test.ts</code>, <code>*.spec.js</code> file patterns</li>
</ul>
<p>If false positives persist:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Use suppression comment
let value = result.unwrap();  // debtmap: ignore - Test assertion
<span class="boring">}</span></code></pre>
<h3 id="error-patterns-not-being-detected"><a class="header" href="#error-patterns-not-being-detected">Error Patterns Not Being Detected</a></h3>
<p><strong>Problem:</strong> Known error patterns not appearing in report</p>
<p><strong>Causes and solutions:</strong></p>
<ol>
<li>
<p><strong>Language support not enabled</strong></p>
<pre><code class="language-bash">debtmap analyze --languages rust,python,javascript
</code></pre>
</li>
<li>
<p><strong>Pattern disabled in config</strong></p>
<pre><code class="language-toml">[error_handling]
detect_panic_patterns = true
detect_swallowing = true
detect_async_errors = true  # Ensure relevant detectors are enabled
</code></pre>
</li>
<li>
<p><strong>Suppression comment present</strong></p>
<ul>
<li>Check for <code>debtmap: ignore</code> comments</li>
<li>Review <code>.debtmap.toml</code> ignore patterns</li>
</ul>
</li>
</ol>
<h3 id="disagreement-with-severity-levels"><a class="header" href="#disagreement-with-severity-levels">Disagreement with Severity Levels</a></h3>
<p><strong>Problem:</strong> Severity feels too high/low for your codebase</p>
<p><strong>Solution:</strong> Customize in <code>.debtmap.toml</code>:</p>
<pre><code class="language-toml">[debt_categories.ErrorSwallowing]
weight = 2  # Reduce from default 4 to Warning level
severity = "Warning"

# Or increase for stricter enforcement
# weight = 5
# severity = "Critical"
</code></pre>
<h3 id="cant-find-which-line-has-the-issue"><a class="header" href="#cant-find-which-line-has-the-issue">Can’t Find Which Line Has the Issue</a></h3>
<p><strong>Problem:</strong> Debtmap reports error at wrong line number</p>
<p><strong>Causes:</strong></p>
<ul>
<li>Source code changed since analysis</li>
<li>Parser approximation for line numbers</li>
</ul>
<p><strong>Solutions:</strong></p>
<ol>
<li>Re-run analysis: <code>debtmap analyze</code></li>
<li>Search for pattern: <code>rg "\.unwrap\(\)" src/</code></li>
<li>Enable debug logging: <code>debtmap analyze --log-level debug</code></li>
</ol>
<h3 id="validating-error-handling-improvements"><a class="header" href="#validating-error-handling-improvements">Validating Error Handling Improvements</a></h3>
<p><strong>Problem:</strong> Unsure if fixes actually improved code quality</p>
<p><strong>Solution:</strong> Use compare workflow:</p>
<pre><code class="language-bash"># Baseline before fixes
git checkout main
debtmap analyze --output baseline.json

# After fixes
git checkout feature/improve-errors
debtmap analyze --output improved.json

# Compare reports
debtmap compare baseline.json improved.json
</code></pre>
<p>Look for:</p>
<ul>
<li>Reduced ErrorSwallowing debt count</li>
<li>Lower risk scores for affected functions</li>
<li>Improved coverage of error paths (if running with coverage)</li>
</ul>
<h2 id="related-topics-4"><a class="header" href="#related-topics-4">Related Topics</a></h2>
<ul>
<li><a href="#configuration-2">Configuration</a> - Complete <code>.debtmap.toml</code> reference</li>
<li><a href="#suppression-patterns">Suppression Patterns</a> - Suppress false positives</li>
<li><a href="#scoring-strategies">Scoring Strategies</a> - How error handling affects risk scores</li>
<li><a href="#coverage-integration-1">Coverage Integration</a> - Detect untested error paths</li>
<li><a href="#cli-reference">CLI Reference</a> - Command-line options for error analysis</li>
<li><a href="#troubleshooting-23">Troubleshooting</a> - General debugging guide</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="functional-composition-analysis"><a class="header" href="#functional-composition-analysis">Functional Composition Analysis</a></h1>
<p>Debtmap provides deep AST-based analysis to detect and evaluate functional programming patterns in Rust code. This feature helps you understand how effectively your codebase uses functional composition patterns like iterator pipelines, identify opportunities for refactoring imperative code to functional style, and rewards pure, side-effect-free functions in complexity scoring.</p>
<h2 id="overview-14"><a class="header" href="#overview-14">Overview</a></h2>
<p>Functional analysis examines your code at the AST level to detect:</p>
<ul>
<li><strong>Iterator pipelines</strong> - Chains like <code>.iter().map().filter().collect()</code></li>
<li><strong>Purity analysis</strong> - Functions with no mutable state or side effects</li>
<li><strong>Composition quality metrics</strong> - Overall functional programming quality scores</li>
<li><strong>Side effect classification</strong> - Categorization of Pure, Benign, and Impure side effects</li>
</ul>
<p>This analysis integrates with debtmap’s scoring system, providing score bonuses for high-quality functional code and reducing god object warnings for codebases with many small pure helper functions.</p>
<p><strong>Specification</strong>: This feature implements <a href="https://github.com/yourusername/debtmap/specs/111">Specification 111: AST-Based Functional Pattern Detection</a> with accuracy targets of precision ≥90%, recall ≥85%, F1 ≥0.87, and performance overhead &lt;10%.</p>
<h2 id="configuration-profiles"><a class="header" href="#configuration-profiles">Configuration Profiles</a></h2>
<p>Debtmap provides three pre-configured analysis profiles to match different codebases:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Profile</th><th>Use Case</th><th>Min Pipeline Depth</th><th>Max Closure Complexity</th><th>Purity Threshold</th><th>Quality Threshold</th></tr>
</thead>
<tbody>
<tr><td><strong>Strict</strong></td><td>Functional-first codebases</td><td>3</td><td>3</td><td>0.9</td><td>0.7</td></tr>
<tr><td><strong>Balanced</strong> (default)</td><td>Typical Rust projects</td><td>2</td><td>5</td><td>0.8</td><td>0.6</td></tr>
<tr><td><strong>Lenient</strong></td><td>Imperative-heavy legacy code</td><td>2</td><td>10</td><td>0.5</td><td>0.4</td></tr>
</tbody>
</table>
</div>
<h3 id="choosing-a-profile"><a class="header" href="#choosing-a-profile">Choosing a Profile</a></h3>
<p><strong>Use Strict</strong> when:</p>
<ul>
<li>Your codebase emphasizes functional programming patterns</li>
<li>You want to enforce high purity standards</li>
<li>You’re building a new project with functional-first principles</li>
<li>You want to detect even simple pipelines (3+ stages)</li>
</ul>
<p><strong>Use Balanced</strong> (default) when:</p>
<ul>
<li>You have a typical Rust codebase mixing functional and imperative styles</li>
<li>You want reasonable detection without being overly strict</li>
<li>You’re working on a mature project with mixed patterns</li>
<li>You want to reward functional patterns without penalizing pragmatic imperative code</li>
</ul>
<p><strong>Use Lenient</strong> when:</p>
<ul>
<li>You’re analyzing legacy code with heavy imperative patterns</li>
<li>You want to identify only the most obviously functional code</li>
<li>You’re migrating from an imperative codebase and want gradual improvement</li>
<li>You have complex closures that are still fundamentally functional</li>
</ul>
<h3 id="cli-usage"><a class="header" href="#cli-usage">CLI Usage</a></h3>
<p>Enable functional analysis with the <code>--ast-functional-analysis</code> flag and select a profile with <code>--functional-analysis-profile</code>:</p>
<pre><code class="language-bash"># Enable with balanced profile (default)
debtmap analyze . --ast-functional-analysis --functional-analysis-profile balanced

# Use strict profile for functional-first codebases
debtmap analyze . --ast-functional-analysis --functional-analysis-profile strict

# Use lenient profile for legacy code
debtmap analyze . --ast-functional-analysis --functional-analysis-profile lenient
</code></pre>
<p><strong>Note:</strong> The <code>--ast-functional-analysis</code> flag enables the feature, while <code>--functional-analysis-profile</code> selects the configuration profile (strict/balanced/lenient).</p>
<h2 id="pure-function-detection"><a class="header" href="#pure-function-detection">Pure Function Detection</a></h2>
<p>A function is considered pure when it:</p>
<ol>
<li>Returns same output for same input (deterministic)</li>
<li>Has no observable side effects</li>
<li>Doesn’t mutate external state</li>
<li>Doesn’t perform I/O</li>
</ol>
<h3 id="examples-3"><a class="header" href="#examples-3">Examples</a></h3>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Pure function
fn add(a: i32, b: i32) -&gt; i32 {
    a + b
}

// Pure function with internal iteration
fn factorial(n: u32) -&gt; u32 {
    (1..=n).product()  // Pure despite internal iteration
}

// Not pure: I/O side effect
fn log_and_add(a: i32, b: i32) -&gt; i32 {
    println!("Adding {} and {}", a, b);  // Side effect!
    a + b
}

// Not pure: mutates external state
fn increment_counter(counter: &amp;mut i32) -&gt; i32 {
    *counter += 1;  // Side effect!
    *counter
}
<span class="boring">}</span></code></pre>
<h2 id="pipeline-detection"><a class="header" href="#pipeline-detection">Pipeline Detection</a></h2>
<p>Debtmap detects functional pipelines through deep AST analysis, identifying iterator chains and their transformations.</p>
<h3 id="pipeline-stages"><a class="header" href="#pipeline-stages">Pipeline Stages</a></h3>
<p>The analyzer recognizes these pipeline stage types:</p>
<h4 id="1-iterator-initialization"><a class="header" href="#1-iterator-initialization">1. Iterator Initialization</a></h4>
<p>Methods that start an iterator chain:</p>
<ul>
<li><code>.iter()</code> - Immutable iteration</li>
<li><code>.into_iter()</code> - Consuming iteration</li>
<li><code>.iter_mut()</code> - Mutable iteration</li>
</ul>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Detected iterator initialization
let results = collection.iter()
    .map(|x| x * 2)
    .collect();
<span class="boring">}</span></code></pre>
<h4 id="2-map-transformations"><a class="header" href="#2-map-transformations">2. Map Transformations</a></h4>
<p>Applies a transformation function to each element:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Detected Map stage
items.iter()
    .map(|x| x * 2)          // Simple closure (low complexity)
    .map(|x| {                // Complex closure (higher complexity)
        let doubled = x * 2;
        doubled + 1
    })
    .collect()
<span class="boring">}</span></code></pre>
<p>The analyzer tracks <strong>closure complexity</strong> for each map operation. Complex closures may indicate code smells and affect quality scoring based on your <code>max_closure_complexity</code> threshold.</p>
<h4 id="3-filter-predicates"><a class="header" href="#3-filter-predicates">3. Filter Predicates</a></h4>
<p>Selects elements based on a predicate:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Detected Filter stage
items.iter()
    .filter(|x| *x &gt; 0)      // Simple predicate
    .filter(|x| {             // Complex predicate
        x.is_positive() &amp;&amp; x &lt; 100
    })
    .collect()
<span class="boring">}</span></code></pre>
<h4 id="4-foldreduce-aggregation"><a class="header" href="#4-foldreduce-aggregation">4. Fold/Reduce Aggregation</a></h4>
<p>Combines elements into a single value:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Detected Fold stage
items.iter()
    .fold(0, |acc, x| acc + x)

// Or using reduce
items.iter()
    .reduce(|a, b| a + b)
<span class="boring">}</span></code></pre>
<h4 id="5-flatmap-transformations"><a class="header" href="#5-flatmap-transformations">5. FlatMap Transformations</a></h4>
<p>Maps and flattens nested structures:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Detected FlatMap stage
items.iter()
    .flat_map(|x| vec![x, x * 2])
    .collect()
<span class="boring">}</span></code></pre>
<h4 id="6-inspect-side-effect-aware"><a class="header" href="#6-inspect-side-effect-aware">6. Inspect (Side-Effect Aware)</a></h4>
<p>Performs side effects while passing through values:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Detected Inspect stage (affects purity scoring)
items.iter()
    .inspect(|x| println!("Processing: {}", x))
    .map(|x| x * 2)
    .collect()
<span class="boring">}</span></code></pre>
<h4 id="7-resultoption-chaining"><a class="header" href="#7-resultoption-chaining">7. Result/Option Chaining</a></h4>
<p>Specialized stages for error handling:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Detected AndThen stage
results.iter()
    .and_then(|x| try_process(x))
    .collect()

// Detected MapErr stage
results.iter()
    .map_err(|e| format!("Error: {}", e))
    .collect()
<span class="boring">}</span></code></pre>
<h3 id="terminal-operations"><a class="header" href="#terminal-operations">Terminal Operations</a></h3>
<p>Pipelines typically end with a terminal operation that consumes the iterator:</p>
<ul>
<li><strong><code>collect()</code></strong> - Gather elements into a collection</li>
<li><strong><code>sum()</code></strong> - Sum numeric values</li>
<li><strong><code>count()</code></strong> - Count elements</li>
<li><strong><code>any()</code></strong> - Check if any element matches</li>
<li><strong><code>all()</code></strong> - Check if all elements match</li>
<li><strong><code>find()</code></strong> - Find first matching element</li>
<li><strong><code>reduce()</code></strong> - Reduce to single value</li>
<li><strong><code>for_each()</code></strong> - Execute side effects for each element</li>
</ul>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Complete pipeline with terminal operation
let total: i32 = items.iter()
    .filter(|x| **x &gt; 0)
    .map(|x| x * 2)
    .sum();  // Terminal operation: sum
<span class="boring">}</span></code></pre>
<h3 id="nested-pipelines"><a class="header" href="#nested-pipelines">Nested Pipelines</a></h3>
<p>Debtmap detects pipelines nested within closures, indicating highly functional code patterns:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Nested pipeline detected
let results = outer_items.iter()
    .map(|item| {
        // Inner pipeline (nesting_level = 1)
        item.values.iter()
            .filter(|v| **v &gt; 0)
            .collect()
    })
    .collect();
<span class="boring">}</span></code></pre>
<p><strong>Nesting level</strong> tracking helps identify sophisticated functional composition patterns.</p>
<h3 id="parallel-pipelines"><a class="header" href="#parallel-pipelines">Parallel Pipelines</a></h3>
<p>Parallel iteration using Rayon is automatically detected:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use rayon::prelude::*;

// Detected as parallel pipeline (is_parallel = true)
let results: Vec&lt;_&gt; = items.par_iter()
    .filter(|x| **x &gt; 0)
    .map(|x| x * 2)
    .collect();
<span class="boring">}</span></code></pre>
<p>Parallel pipelines indicate high-performance functional patterns and receive positive quality scoring.</p>
<h3 id="builder-pattern-filtering"><a class="header" href="#builder-pattern-filtering">Builder Pattern Filtering</a></h3>
<p>To avoid false positives, debtmap distinguishes builder patterns from functional pipelines:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// This is a builder pattern, NOT counted as a functional pipeline
let config = ConfigBuilder::new()
    .with_host("localhost")
    .with_port(8080)
    .build();

// This IS a functional pipeline
let values = items.iter()
    .map(|x| x * 2)
    .collect();
<span class="boring">}</span></code></pre>
<p>Builder patterns are filtered out to ensure accurate functional composition metrics.</p>
<h2 id="purity-analysis-1"><a class="header" href="#purity-analysis-1">Purity Analysis</a></h2>
<p>Debtmap analyzes functions to determine their purity level - whether they have side effects and mutable state.</p>
<h3 id="purity-levels"><a class="header" href="#purity-levels">Purity Levels</a></h3>
<p>Functions are classified into three purity levels for god object weighting (defined in <code>src/organization/purity_analyzer.rs:19-26</code>):</p>
<blockquote>
<p><strong>Note:</strong> Debtmap has two purity analysis systems serving different purposes:</p>
<ol>
<li>
<p><strong>Three-level system</strong> (this section) - Used for god object scoring with weight multipliers:</p>
<ul>
<li>Defined in <code>src/organization/purity_analyzer.rs</code></li>
<li>Categories: Pure, ProbablyPure, Impure</li>
<li>Purpose: Dampen god object scores for pure functions via weight multipliers (0.3, 0.5, 1.0)</li>
</ul>
</li>
<li>
<p><strong>Four-level system</strong> - Used for detailed responsibility classification:</p>
<ul>
<li>Defined in <code>src/core/mod.rs:47-60</code> and <code>src/analysis/purity_analysis.rs:34-43</code></li>
<li>Categories: StrictlyPure, LocallyPure, ReadOnly, Impure</li>
<li>Purpose: Fine-grained analysis of function behavior and side effects</li>
<li>Used in responsibility analysis and technical debt assessment</li>
</ul>
</li>
</ol>
<p>The three-level system provides a simplified classification optimized for god object detection. The four-level system offers more granular distinctions (e.g., builder patterns with local mutations vs. read-only accessors).</p>
<p>This chapter focuses on the three-level system used for god object integration. For detailed responsibility classification, see <a href="#responsibility-analysis">Responsibility Analysis</a>.</p>
</blockquote>
<h4 id="pure-weight-03"><a class="header" href="#pure-weight-03">Pure (Weight 0.3)</a></h4>
<p>Guaranteed no side effects:</p>
<ul>
<li>No mutable parameters (<code>&amp;mut</code>, <code>mut self</code>)</li>
<li>No I/O operations</li>
<li>No global mutations</li>
<li>No <code>unsafe</code> blocks</li>
<li>Only immutable bindings</li>
</ul>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Pure function
fn calculate_total(items: &amp;[i32]) -&gt; i32 {
    items.iter().sum()
}

// Pure function with immutable bindings
fn process_value(x: i32) -&gt; i32 {
    let doubled = x * 2;  // Immutable binding
    let result = doubled + 10;
    result
}
<span class="boring">}</span></code></pre>
<h4 id="probably-pure-weight-05"><a class="header" href="#probably-pure-weight-05">Probably Pure (Weight 0.5)</a></h4>
<p>Likely no side effects:</p>
<ul>
<li>Static functions (<code>fn</code> items, not methods)</li>
<li>Associated functions (no <code>self</code>)</li>
<li>No obvious side effects detected</li>
</ul>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Probably pure - static function
fn transform(value: i32) -&gt; i32 {
    value * 2
}

// Probably pure - associated function
impl MyType {
    fn create_default() -&gt; Self {
        MyType { value: 0 }
    }
}
<span class="boring">}</span></code></pre>
<h4 id="impure-weight-10"><a class="header" href="#impure-weight-10">Impure (Weight 1.0)</a></h4>
<p>Has side effects:</p>
<ul>
<li>Uses mutable references (<code>&amp;mut</code>, <code>mut self</code>)</li>
<li>Performs I/O operations (<code>println!</code>, file I/O, network)</li>
<li>Uses <code>async</code> (potential side effects)</li>
<li>Mutates global state</li>
<li>Uses <code>unsafe</code></li>
</ul>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Impure - mutable reference
fn increment(value: &amp;mut i32) {
    *value += 1;
}

// Impure - I/O operation
fn log_value(value: i32) {
    println!("Value: {}", value);
}

// Impure - mutation
fn process_items(items: &amp;mut Vec&lt;i32&gt;) {
    items.push(42);
}
<span class="boring">}</span></code></pre>
<h3 id="purity-weight-multipliers"><a class="header" href="#purity-weight-multipliers">Purity Weight Multipliers</a></h3>
<p>Purity levels affect god object detection through weight multipliers (implemented in <code>src/organization/purity_analyzer.rs:29-39</code>). Pure functions contribute <strong>less</strong> to god object scores, rewarding codebases with many small pure helper functions:</p>
<ul>
<li><strong>Pure (0.3)</strong>: A pure function counts as 30% of a regular function in god object method count calculations</li>
<li><strong>Probably Pure (0.5)</strong>: Counts as 50%</li>
<li><strong>Impure (1.0)</strong>: Full weight</li>
</ul>
<p>The <code>purity_score</code> dampens god object scores via the <code>weight_multiplier</code> calculation. For example, pure functions with weight 0.3 count as only 30% of a regular function when calculating method counts for god object detection.</p>
<p><strong>Example</strong>: A module with 20 pure helper functions (20 × 0.3 = 6.0 effective) is less likely to trigger god object warnings than a module with 10 impure functions (10 × 1.0 = 10.0 effective).</p>
<h2 id="side-effect-detection"><a class="header" href="#side-effect-detection">Side Effect Detection</a></h2>
<h3 id="detected-side-effects"><a class="header" href="#detected-side-effects">Detected Side Effects</a></h3>
<p><strong>I/O Operations:</strong></p>
<ul>
<li>File reading/writing</li>
<li>Network calls</li>
<li>Console output</li>
<li>Database queries</li>
</ul>
<p><strong>State Mutation:</strong></p>
<ul>
<li>Mutable global variables</li>
<li>Shared mutable state</li>
<li>Reference mutations</li>
</ul>
<p><strong>Randomness:</strong></p>
<ul>
<li>Random number generation</li>
<li>Time-dependent behavior</li>
</ul>
<p><strong>System Interaction:</strong></p>
<ul>
<li>Environment variable access</li>
<li>System calls</li>
<li>Thread spawning</li>
</ul>
<h3 id="rust-specific-detection"><a class="header" href="#rust-specific-detection">Rust-Specific Detection</a></h3>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Interior mutability detection
use std::cell::RefCell;

fn has_side_effect() {
    let data = RefCell::new(vec![]);
    data.borrow_mut().push(1);  // Detected as mutation
}

// Unsafe code detection
fn unsafe_side_effect() {
    unsafe {
        // Automatically flagged as potentially impure
    }
}
<span class="boring">}</span></code></pre>
<h3 id="side-effect-classification"><a class="header" href="#side-effect-classification">Side Effect Classification</a></h3>
<p>Side effects are categorized by severity:</p>
<h4 id="pure---no-side-effects"><a class="header" href="#pure---no-side-effects">Pure - No Side Effects</a></h4>
<p>No mutations, I/O, or global state changes:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Pure - only computation
fn fibonacci(n: u32) -&gt; u32 {
    match n {
        0 =&gt; 0,
        1 =&gt; 1,
        _ =&gt; fibonacci(n - 1) + fibonacci(n - 2),
    }
}
<span class="boring">}</span></code></pre>
<h4 id="benign---small-penalty"><a class="header" href="#benign---small-penalty">Benign - Small Penalty</a></h4>
<p>Only logging, tracing, or metrics:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use tracing::debug;

// Benign - logging side effect
fn process(value: i32) -&gt; i32 {
    debug!("Processing value: {}", value);
    value * 2
}
<span class="boring">}</span></code></pre>
<p>Benign side effects receive a <strong>small penalty</strong> in purity scoring. Logging and observability are recognized as practical necessities.</p>
<h4 id="impure---large-penalty"><a class="header" href="#impure---large-penalty">Impure - Large Penalty</a></h4>
<p>I/O, mutations, network operations:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Impure - file I/O
fn save_to_file(data: &amp;str) -&gt; std::io::Result&lt;()&gt; {
    std::fs::write("output.txt", data)
}

// Impure - network operation
async fn fetch_data(url: &amp;str) -&gt; Result&lt;String, reqwest::Error&gt; {
    reqwest::get(url).await?.text().await
}
<span class="boring">}</span></code></pre>
<p>Impure side effects receive a <strong>large penalty</strong> in purity scoring.</p>
<h3 id="purity-metrics"><a class="header" href="#purity-metrics">Purity Metrics</a></h3>
<p>For each function, debtmap calculates purity metrics through the functional composition analysis (<code>src/analysis/functional_composition.rs</code>). These metrics are computed by <code>analyze_composition()</code> and returned in <code>CompositionMetrics</code> and <code>PurityMetrics</code>:</p>
<ul>
<li><strong><code>has_mutable_state</code></strong> - Whether the function uses mutable bindings</li>
<li><strong><code>has_side_effects</code></strong> - Whether I/O or global mutations are detected</li>
<li><strong><code>immutability_ratio</code></strong> - Ratio of immutable to total bindings (0.0-1.0)</li>
<li><strong><code>is_const_fn</code></strong> - Whether declared as <code>const fn</code></li>
<li><strong><code>side_effect_kind</code></strong> - Classification: Pure, Benign, or Impure</li>
<li><strong><code>purity_score</code></strong> - Overall purity score (0.0 impure to 1.0 pure)</li>
</ul>
<h4 id="immutability-ratio"><a class="header" href="#immutability-ratio">Immutability Ratio</a></h4>
<p>The immutability ratio measures how much of a function’s local state is immutable:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn example() {
    let x = 10;         // Immutable
    let y = 20;         // Immutable
    let mut z = 30;     // Mutable
    z += 1;
    // immutability_ratio = 2/3 = 0.67
}
<span class="boring">}</span></code></pre>
<p>Higher immutability ratios contribute to better purity scores.</p>
<h2 id="composition-pattern-recognition"><a class="header" href="#composition-pattern-recognition">Composition Pattern Recognition</a></h2>
<h3 id="function-composition"><a class="header" href="#function-composition">Function Composition</a></h3>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Detected composition pattern
fn process_data(input: String) -&gt; Result&lt;Output&gt; {
    input
        .parse()
        .map(validate)
        .and_then(transform)
        .map(normalize)
}
<span class="boring">}</span></code></pre>
<h3 id="higher-order-functions"><a class="header" href="#higher-order-functions">Higher-Order Functions</a></h3>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Detected HOF pattern
fn apply_twice&lt;F&gt;(f: F, x: i32) -&gt; i32
where
    F: Fn(i32) -&gt; i32,
{
    f(f(x))
}
<span class="boring">}</span></code></pre>
<h3 id="mapfilterfold-chains"><a class="header" href="#mapfilterfold-chains">Map/Filter/Fold Chains</a></h3>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Detected functional pipeline
let result = items
    .iter()
    .filter(|x| x.is_valid())
    .map(|x| x.transform())
    .fold(0, |acc, x| acc + x);
<span class="boring">}</span></code></pre>
<h2 id="composition-quality-scoring"><a class="header" href="#composition-quality-scoring">Composition Quality Scoring</a></h2>
<p>Debtmap combines pipeline metrics and purity analysis into an overall <strong>composition quality score</strong> (0.0-1.0).</p>
<h3 id="scoring-factors"><a class="header" href="#scoring-factors">Scoring Factors</a></h3>
<p>The composition quality score considers:</p>
<ol>
<li><strong>Pipeline depth</strong> - Longer pipelines indicate more functional composition</li>
<li><strong>Purity score</strong> - Higher purity means better functional programming</li>
<li><strong>Immutability ratio</strong> - More immutable bindings improve the score</li>
<li><strong>Closure complexity</strong> - Simpler closures score better</li>
<li><strong>Parallel execution</strong> - Parallel pipelines receive bonuses</li>
<li><strong>Nested pipelines</strong> - Sophisticated composition patterns score higher</li>
</ol>
<h3 id="quality-thresholds"><a class="header" href="#quality-thresholds">Quality Thresholds</a></h3>
<p>Based on your configuration profile, functions with composition quality above the threshold receive <strong>score boosts</strong> in debtmap’s overall analysis:</p>
<ul>
<li><strong>Strict</strong>: Quality ≥ 0.7 required for boost</li>
<li><strong>Balanced</strong>: Quality ≥ 0.6 required for boost</li>
<li><strong>Lenient</strong>: Quality ≥ 0.4 required for boost</li>
</ul>
<p>High-quality functional code can offset complexity in other areas of your codebase.</p>
<h3 id="purity-scoring"><a class="header" href="#purity-scoring">Purity Scoring</a></h3>
<h4 id="distribution-analysis"><a class="header" href="#distribution-analysis">Distribution Analysis</a></h4>
<p>Debtmap calculates purity distribution:</p>
<ul>
<li><strong>Pure functions</strong>: 0 side effects detected</li>
<li><strong>Mostly pure</strong>: Minor side effects (e.g., logging)</li>
<li><strong>Impure</strong>: Multiple side effects</li>
<li><strong>Highly impure</strong>: Extensive state mutation and I/O</li>
</ul>
<h4 id="scoring-formula-1"><a class="header" href="#scoring-formula-1">Scoring Formula</a></h4>
<pre><code>Purity Score = (pure_functions / total_functions) × 100
Side Effect Density = total_side_effects / total_functions
</code></pre>
<h4 id="codebase-health-metrics"><a class="header" href="#codebase-health-metrics">Codebase Health Metrics</a></h4>
<pre><code>Target Purity Levels:
- Core business logic: 80%+ pure
- Utilities: 70%+ pure
- I/O layer: 20-30% pure (expected)
- Overall: 50%+ pure
</code></pre>
<h3 id="integration-with-risk-scoring-1"><a class="header" href="#integration-with-risk-scoring-1">Integration with Risk Scoring</a></h3>
<p>Functional composition quality integrates with debtmap’s risk scoring system and multi-signal aggregation framework:</p>
<ul>
<li><strong>High composition quality</strong> → Lower risk scores (functions with quality above threshold receive score boosts)</li>
<li><strong>Pure functions</strong> → Reduced god object penalties (via weight multipliers in <code>purity_analyzer.rs</code>)</li>
<li><strong>Deep pipelines</strong> → Bonus for functional patterns</li>
<li><strong>Impure side effects</strong> → Risk penalties applied</li>
</ul>
<p><strong>Multi-Signal Integration</strong>: Functional composition analysis is one of several signals aggregated in the unified analysis system (<code>src/builders/unified_analysis.rs</code> and <code>src/analysis/multi_signal_aggregation.rs</code>) alongside complexity metrics, god object detection, and risk assessment. This ensures that functional programming quality contributes to the comprehensive technical debt assessment across multiple dimensions.</p>
<p>This integration ensures that well-written functional code is properly rewarded in the overall technical debt assessment.</p>
<h2 id="practical-examples-3"><a class="header" href="#practical-examples-3">Practical Examples</a></h2>
<h3 id="example-1-detecting-imperative-vs-functional-code"><a class="header" href="#example-1-detecting-imperative-vs-functional-code">Example 1: Detecting Imperative vs Functional Code</a></h3>
<p><strong>Imperative style</strong> (lower composition quality):</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn process_items_imperative(items: Vec&lt;i32&gt;) -&gt; Vec&lt;i32&gt; {
    let mut results = Vec::new();
    for item in items {
        if item &gt; 0 {
            results.push(item * 2);
        }
    }
    results
}
// Detected: No pipelines, mutable state, lower purity score
<span class="boring">}</span></code></pre>
<p><strong>Functional style</strong> (higher composition quality):</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn process_items_functional(items: Vec&lt;i32&gt;) -&gt; Vec&lt;i32&gt; {
    items.iter()
        .filter(|x| **x &gt; 0)
        .map(|x| x * 2)
        .collect()
}
// Detected: Pipeline depth 3, pure function, high composition quality
<span class="boring">}</span></code></pre>
<h3 id="example-2-identifying-refactoring-opportunities"><a class="header" href="#example-2-identifying-refactoring-opportunities">Example 2: Identifying Refactoring Opportunities</a></h3>
<p>When debtmap detects low composition quality, it suggests refactoring:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Original: Imperative with mutations
fn calculate_statistics(data: &amp;[f64]) -&gt; (f64, f64, f64) {
    let mut sum = 0.0;
    let mut min = f64::MAX;
    let mut max = f64::MIN;

    for &amp;value in data {
        sum += value;
        if value &lt; min { min = value; }
        if value &gt; max { max = value; }
    }

    (sum / data.len() as f64, min, max)
}

// Refactored: Functional style
fn calculate_statistics_functional(data: &amp;[f64]) -&gt; (f64, f64, f64) {
    let sum: f64 = data.iter().sum();
    let min = data.iter().min_by(|a, b| a.partial_cmp(b).unwrap()).unwrap();
    let max = data.iter().max_by(|a, b| a.partial_cmp(b).unwrap()).unwrap();

    (sum / data.len() as f64, *min, *max)
}
// Higher purity score, multiple pipelines detected
<span class="boring">}</span></code></pre>
<h3 id="example-3-using-profiles-for-different-codebases"><a class="header" href="#example-3-using-profiles-for-different-codebases">Example 3: Using Profiles for Different Codebases</a></h3>
<p><strong>Strict profile</strong> - Catches subtle functional patterns:</p>
<pre><code class="language-bash">$ debtmap analyze --ast-functional-analysis --functional-analysis-profile strict src/
# Detects pipelines with 3+ stages
# Requires purity ≥ 0.9 for "pure" classification
# Flags closures with complexity &gt; 3
</code></pre>
<p><strong>Balanced profile</strong> - Default for most projects:</p>
<pre><code class="language-bash">$ debtmap analyze --ast-functional-analysis --functional-analysis-profile balanced src/
# Detects pipelines with 2+ stages
# Requires purity ≥ 0.8 for "pure" classification
# Flags closures with complexity &gt; 5
</code></pre>
<p><strong>Lenient profile</strong> - For legacy code:</p>
<pre><code class="language-bash">$ debtmap analyze --ast-functional-analysis --functional-analysis-profile lenient src/
# Detects pipelines with 2+ stages
# Requires purity ≥ 0.5 for "pure" classification
# Flags closures with complexity &gt; 10
</code></pre>
<h3 id="example-4-interpreting-purity-scores"><a class="header" href="#example-4-interpreting-purity-scores">Example 4: Interpreting Purity Scores</a></h3>
<p><strong>Pure function</strong> (score: 1.0):</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn add(a: i32, b: i32) -&gt; i32 {
    a + b
}
// Purity: 1.0 (perfect)
// Immutability ratio: 1.0 (no bindings)
// Side effects: None
<span class="boring">}</span></code></pre>
<p><strong>Mostly pure</strong> (score: 0.8):</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn process(values: &amp;[i32]) -&gt; i32 {
    let doubled: Vec&lt;_&gt; = values.iter().map(|x| x * 2).collect();
    let sum: i32 = doubled.iter().sum();
    sum
}
// Purity: 0.8 (high)
// Immutability ratio: 1.0 (both bindings immutable)
// Side effects: None
// Pipelines: 2 detected
<span class="boring">}</span></code></pre>
<p><strong>Impure function</strong> (score: 0.2):</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn log_and_process(values: &amp;mut Vec&lt;i32&gt;) {
    println!("Processing {} items", values.len());
    values.iter_mut().for_each(|x| *x *= 2);
}
// Purity: 0.2 (low)
// Immutability ratio: 0.0 (mutable parameter)
// Side effects: I/O (println), mutation
<span class="boring">}</span></code></pre>
<h2 id="best-practices-13"><a class="header" href="#best-practices-13">Best Practices</a></h2>
<h3 id="writing-functional-rust-code"><a class="header" href="#writing-functional-rust-code">Writing Functional Rust Code</a></h3>
<p>To achieve high composition quality scores:</p>
<ol>
<li>
<p><strong>Prefer iterator chains over manual loops</strong></p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Good
let evens: Vec&lt;_&gt; = items.iter().filter(|x| *x % 2 == 0).collect();

// Avoid
let mut evens = Vec::new();
for item in &amp;items {
    if item % 2 == 0 { evens.push(item); }
}
<span class="boring">}</span></code></pre>
</li>
<li>
<p><strong>Minimize mutable state</strong></p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Good
let result = calculate(input);

// Avoid
let mut result = 0;
result = calculate(input);
<span class="boring">}</span></code></pre>
</li>
<li>
<p><strong>Separate pure logic from side effects</strong></p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Good - pure computation
fn calculate_price(quantity: u32, unit_price: f64) -&gt; f64 {
    quantity as f64 * unit_price
}

// Good - I/O at the boundary
fn display_price(price: f64) {
    println!("Total: ${:.2}", price);
}
<span class="boring">}</span></code></pre>
</li>
<li>
<p><strong>Keep closures simple</strong></p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Good - simple closure
items.map(|x| x * 2)

// Consider extracting - complex closure
items.map(|x| {
    let temp = expensive_operation(x);
    transform(temp)
})

// Better
fn transform_item(x: i32) -&gt; i32 {
    let temp = expensive_operation(x);
    transform(temp)
}
items.map(transform_item)
<span class="boring">}</span></code></pre>
</li>
<li>
<p><strong>Use parallel iteration for CPU-intensive work</strong></p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use rayon::prelude::*;

let results: Vec&lt;_&gt; = large_dataset.par_iter()
    .map(|item| expensive_computation(item))
    .collect();
<span class="boring">}</span></code></pre>
</li>
</ol>
<h3 id="code-organization"><a class="header" href="#code-organization">Code Organization</a></h3>
<p><strong>Separate pure from impure:</strong></p>
<ul>
<li>Keep pure logic in core modules</li>
<li>Isolate I/O at boundaries</li>
<li>Use dependency injection for testability</li>
</ul>
<p><strong>Maximize purity in:</strong></p>
<ul>
<li>Business logic</li>
<li>Calculations and transformations</li>
<li>Validation functions</li>
<li>Data structure operations</li>
</ul>
<p><strong>Accept impurity in:</strong></p>
<ul>
<li>I/O layers</li>
<li>Logging and monitoring</li>
<li>External system integration</li>
<li>Application boundaries</li>
</ul>
<p><strong>Refactoring strategy:</strong></p>
<ol>
<li>Identify impure functions</li>
<li>Extract pure logic</li>
<li>Push side effects to boundaries</li>
<li>Test pure functions exhaustively</li>
</ol>
<h3 id="migration-guide"><a class="header" href="#migration-guide">Migration Guide</a></h3>
<p>To enable functional analysis on existing projects:</p>
<ol>
<li>
<p><strong>Start with lenient profile</strong> to understand current state:</p>
<pre><code class="language-bash">debtmap analyze --ast-functional-analysis --functional-analysis-profile lenient .
</code></pre>
</li>
<li>
<p><strong>Identify quick wins</strong> - functions that are almost functional:</p>
<ul>
<li>Look for loops that can become iterator chains</li>
<li>Find mutable variables that can be immutable</li>
<li>Spot side effects that can be extracted</li>
</ul>
</li>
<li>
<p><strong>Gradually refactor</strong> to functional patterns:</p>
<ul>
<li>Convert one function at a time</li>
<li>Run tests after each change</li>
<li>Measure improvements with debtmap</li>
</ul>
</li>
<li>
<p><strong>Tighten profile</strong> as codebase improves:</p>
<pre><code class="language-bash"># After refactoring
debtmap analyze --ast-functional-analysis --functional-analysis-profile balanced .

# For new modules
debtmap analyze --ast-functional-analysis --functional-analysis-profile strict src/new_module/
</code></pre>
</li>
<li>
<p><strong>Monitor composition quality trends</strong> over time</p>
</li>
</ol>
<h2 id="use-cases-4"><a class="header" href="#use-cases-4">Use Cases</a></h2>
<h3 id="code-quality-audit"><a class="header" href="#code-quality-audit">Code Quality Audit</a></h3>
<pre><code class="language-bash"># Assess functional purity
debtmap analyze . --ast-functional-analysis --functional-analysis-profile balanced --format markdown
</code></pre>
<h3 id="refactoring-targets"><a class="header" href="#refactoring-targets">Refactoring Targets</a></h3>
<pre><code class="language-bash"># Find impure functions in core logic
debtmap analyze src/core/ --ast-functional-analysis --functional-analysis-profile strict
</code></pre>
<h3 id="onboarding-guide"><a class="header" href="#onboarding-guide">Onboarding Guide</a></h3>
<pre><code class="language-bash"># Show functional patterns in codebase
debtmap analyze . --ast-functional-analysis --functional-analysis-profile balanced --summary
</code></pre>
<h2 id="troubleshooting-15"><a class="header" href="#troubleshooting-15">Troubleshooting</a></h2>
<h3 id="no-pipelines-detected-but-i-have-iterator-chains"><a class="header" href="#no-pipelines-detected-but-i-have-iterator-chains">“No pipelines detected” but I have iterator chains</a></h3>
<ul>
<li><strong>Check pipeline depth</strong>: Your chains may be too short for the profile
<ul>
<li>Strict requires 3+ stages</li>
<li>Balanced/Lenient require 2+ stages</li>
</ul>
</li>
<li><strong>Check for builder patterns</strong>: Method chaining for construction is filtered out</li>
<li><strong>Verify terminal operation</strong>: Ensure the chain ends with <code>collect()</code>, <code>sum()</code>, etc.</li>
</ul>
<h3 id="low-purity-score-for-seemingly-pure-functions"><a class="header" href="#low-purity-score-for-seemingly-pure-functions">“Low purity score” for seemingly pure functions</a></h3>
<ul>
<li><strong>Check for hidden side effects</strong>:
<ul>
<li><code>println!</code> or logging statements</li>
<li>Calls to impure helper functions</li>
<li><code>unsafe</code> blocks</li>
</ul>
</li>
<li><strong>Review immutability ratio</strong>: Unnecessary <code>mut</code> bindings lower the score</li>
<li><strong>Verify no I/O operations</strong>: File access, network calls affect purity</li>
</ul>
<h3 id="high-complexity-closures-flagged"><a class="header" href="#high-complexity-closures-flagged">“High complexity closures flagged”</a></h3>
<ul>
<li><strong>Extract complex closures</strong> into named functions:
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Instead of
items.map(|x| { /* 10 lines */ })

// Use
fn process_item(x: Item) -&gt; Result { /* 10 lines */ }
items.map(process_item)
<span class="boring">}</span></code></pre>
</li>
<li><strong>Adjust <code>max_closure_complexity</code></strong>: Consider lenient profile if needed</li>
<li><strong>Refactor closure logic</strong>: Break down complex operations</li>
</ul>
<h3 id="too-many-false-positives"><a class="header" href="#too-many-false-positives">Too Many False Positives</a></h3>
<p><strong>Issue:</strong> Pure functions flagged as impure</p>
<p><strong>Solution:</strong></p>
<ul>
<li>Use lenient profile</li>
<li>Suppress known patterns</li>
<li>Review detection criteria</li>
<li>Report false positives</li>
</ul>
<h3 id="missing-side-effects"><a class="header" href="#missing-side-effects">Missing Side Effects</a></h3>
<p><strong>Issue:</strong> Known impure functions not detected</p>
<p><strong>Solution:</strong></p>
<ul>
<li>Use strict profile</li>
<li>Check for exotic side effect patterns</li>
<li>Enable comprehensive analysis</li>
</ul>
<h3 id="performance-impact-concerns"><a class="header" href="#performance-impact-concerns">Performance impact concerns</a></h3>
<ul>
<li><strong>Spec 111 targets &lt;10% overhead</strong>: Performance impact should be minimal</li>
<li><strong>Disable for hot paths</strong>: Analyze functional patterns in separate runs if needed</li>
<li><strong>Use parallel processing</strong>: Leverage multi-core parallelism for faster analysis</li>
</ul>
<h2 id="related-chapters-1"><a class="header" href="#related-chapters-1">Related Chapters</a></h2>
<ul>
<li><a href="#analysis-guide">Analysis Guide</a> - Understanding analysis types and methodologies</li>
<li><a href="#complexity-metrics-1">Complexity Metrics</a> - How functional patterns affect complexity metrics</li>
<li><a href="#scoring-strategies">Scoring Strategies</a> - Integration with overall technical debt scoring</li>
<li><a href="#god-object-detection-1">God Object Detection</a> - How purity weights reduce false positives</li>
<li><a href="#configuration-2">Configuration</a> - Advanced functional analysis configuration options</li>
</ul>
<h2 id="summary-5"><a class="header" href="#summary-5">Summary</a></h2>
<p>Functional composition analysis helps you:</p>
<ul>
<li><strong>Identify functional patterns</strong> in your Rust codebase through AST-based pipeline detection</li>
<li><strong>Measure purity</strong> with side effect detection and immutability analysis</li>
<li><strong>Improve code quality</strong> by refactoring imperative code to functional style</li>
<li><strong>Get scoring benefits</strong> for high-quality functional programming patterns</li>
<li><strong>Choose appropriate profiles</strong> (strict/balanced/lenient) for different codebases</li>
</ul>
<p>Enable it with <code>--functional-analysis-profile</code> to start benefiting from functional programming insights in your technical debt analysis.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="god-object-detection-1"><a class="header" href="#god-object-detection-1">God Object Detection</a></h1>
<h2 id="overview-15"><a class="header" href="#overview-15">Overview</a></h2>
<p>Debtmap includes sophisticated god object detection that identifies files and types that have grown too large and taken on too many responsibilities. God objects (also called “god classes” or “god modules”) are a significant source of technical debt as they:</p>
<ul>
<li>Violate the Single Responsibility Principle</li>
<li>Become difficult to maintain and test</li>
<li>Create bottlenecks in development</li>
<li>Increase the risk of bugs due to high coupling</li>
<li>Have high coupling with many other modules</li>
<li>Are hard to test effectively</li>
</ul>
<p>This chapter explains how Debtmap identifies god objects, calculates their scores, and provides actionable refactoring recommendations.</p>
<h2 id="detection-criteria-1"><a class="header" href="#detection-criteria-1">Detection Criteria</a></h2>
<p>Debtmap uses two distinct detection strategies depending on the file structure:</p>
<h3 id="god-class-criteria"><a class="header" href="#god-class-criteria">God Class Criteria</a></h3>
<p>A struct/class is classified as a god class when it violates multiple thresholds:</p>
<ol>
<li><strong>Method Count</strong> - Number of impl methods on the struct</li>
<li><strong>Field Count</strong> - Number of struct/class fields</li>
<li><strong>Responsibility Count</strong> - Distinct responsibilities inferred from method names (max_traits in config)</li>
<li><strong>Lines of Code</strong> - Estimated lines for the struct and its impl blocks</li>
<li><strong>Complexity Sum</strong> - Combined cyclomatic complexity of struct methods</li>
</ol>
<p><strong>Note:</strong> All five criteria are evaluated by the <code>determine_confidence</code> function to calculate confidence levels. Each criterion that exceeds its threshold contributes to the violation count.</p>
<h3 id="god-module-criteria"><a class="header" href="#god-module-criteria">God Module Criteria</a></h3>
<p>A file is classified as a god module when it has excessive standalone functions:</p>
<ol>
<li><strong>Standalone Function Count</strong> - Total standalone functions (not in impl blocks)</li>
<li><strong>Responsibility Count</strong> - Distinct responsibilities across all functions</li>
<li><strong>Lines of Code</strong> - Total lines in the file</li>
<li><strong>Complexity Sum</strong> - Combined cyclomatic complexity (estimated as <code>function_count × 5</code>)</li>
</ol>
<p><strong>Key Difference:</strong> God class detection focuses on a single struct’s methods, while god module detection counts standalone functions across the entire file.</p>
<h3 id="language-specific-thresholds"><a class="header" href="#language-specific-thresholds">Language-Specific Thresholds</a></h3>
<h4 id="rust"><a class="header" href="#rust">Rust</a></h4>
<ul>
<li><strong>Max Methods</strong>: 20 (includes both impl methods and standalone functions)</li>
<li><strong>Max Fields</strong>: 15</li>
<li><strong>Max Responsibilities</strong>: 5</li>
<li><strong>Max Lines</strong>: 1000</li>
<li><strong>Max Complexity</strong>: 200</li>
</ul>
<h4 id="python"><a class="header" href="#python">Python</a></h4>
<ul>
<li><strong>Max Methods</strong>: 15</li>
<li><strong>Max Fields</strong>: 10</li>
<li><strong>Max Responsibilities</strong>: 3</li>
<li><strong>Max Lines</strong>: 500</li>
<li><strong>Max Complexity</strong>: 150</li>
</ul>
<h4 id="javascripttypescript"><a class="header" href="#javascripttypescript">JavaScript/TypeScript</a></h4>
<ul>
<li><strong>Max Methods</strong>: 15</li>
<li><strong>Max Fields</strong>: 20</li>
<li><strong>Max Responsibilities</strong>: 3</li>
<li><strong>Max Lines</strong>: 500</li>
<li><strong>Max Complexity</strong>: 150</li>
</ul>
<p><strong>Note:</strong> TypeScript uses the same thresholds as JavaScript since both languages have similar structural patterns. The implementation treats them identically for god object detection purposes.</p>
<p>These thresholds can be customized per-language in your <code>.debtmap.toml</code> configuration file.</p>
<h2 id="god-class-vs-god-module-detection"><a class="header" href="#god-class-vs-god-module-detection">God Class vs God Module Detection</a></h2>
<p>Debtmap distinguishes between two distinct types of god objects:</p>
<h3 id="god-class-detection"><a class="header" href="#god-class-detection">God Class Detection</a></h3>
<p>A <strong>god class</strong> is a single struct/class with excessive methods and fields. Debtmap analyzes the largest type in a file using:</p>
<ol>
<li>Find the largest type (struct/class) by <code>method_count + field_count × 2</code></li>
<li>Count <strong>only the impl methods</strong> for that struct</li>
<li>Check against thresholds:
<ul>
<li>Rust: &gt;20 methods, &gt;15 fields</li>
<li>Python: &gt;15 methods, &gt;10 fields</li>
<li>JavaScript/TypeScript: &gt;15 methods, &gt;20 fields</li>
</ul>
</li>
</ol>
<p><strong>Example:</strong> A struct with 25 methods and 18 fields would be flagged as a god class.</p>
<h3 id="god-module-detection"><a class="header" href="#god-module-detection">God Module Detection</a></h3>
<p>A <strong>god module</strong> is a file with excessive standalone functions (no dominant struct). Debtmap counts standalone functions when:</p>
<ol>
<li>No struct/class is found, OR</li>
<li>The file has many standalone functions outside of any impl blocks</li>
</ol>
<p><strong>Implementation Detail:</strong> All three detection types (GodClass, GodFile, GodModule) share a single unified <code>DebtType::GodObject</code> variant. The specific detection type is distinguished using the <code>detection_type</code> field within the variant:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>DebtType::GodObject {
    methods: u32,
    fields: Option&lt;u32&gt;,
    responsibilities: u32,
    god_object_score: Score0To100,
    lines: u32,
}
<span class="boring">}</span></code></pre>
<p>The <code>detection_type</code> field (tracked separately in the analysis) indicates whether this god object was detected as:</p>
<ul>
<li><code>GodClass</code> - Single struct with excessive methods/fields (has <code>fields: Some(count)</code>)</li>
<li><code>GodFile</code> - File with excessive functions or lines of code (has <code>fields: None</code>)</li>
<li><code>GodModule</code> - Same as <code>GodFile</code>, used conceptually for files with many standalone functions (has <code>fields: None</code>)</li>
</ul>
<p>This unified structure ensures consistent handling across all god object types while the <code>fields</code> value (Some vs None) provides the key distinction between class-based and file-based detection.</p>
<p><strong>Example:</strong> A file like <code>rust_call_graph.rs</code> with 270 standalone functions would be flagged as a god module (using the <code>GodFile</code>/<code>GodModule</code> detection type).</p>
<h3 id="why-separate-analysis"><a class="header" href="#why-separate-analysis">Why Separate Analysis?</a></h3>
<p>Previously, Debtmap combined standalone functions with struct methods, causing <strong>false positives</strong> for functional/procedural modules. The current implementation analyzes them separately to:</p>
<ul>
<li>Avoid penalizing pure functional modules</li>
<li>Distinguish between architectural issues (god class) and organizational issues (god module)</li>
<li>Provide more accurate refactoring recommendations</li>
</ul>
<p><strong>Key Distinction:</strong> A file containing a struct with 15 methods plus 20 standalone functions is analyzed as:</p>
<ul>
<li><strong>God Class:</strong> No (15 methods &lt; 20 threshold)</li>
<li><strong>God Module:</strong> Possibly (20 standalone functions, approaching threshold)</li>
</ul>
<p>See <code>src/organization/god_object_detector.rs:449-505</code> for implementation details.</p>
<h2 id="confidence-levels"><a class="header" href="#confidence-levels">Confidence Levels</a></h2>
<p>Debtmap assigns confidence levels based <strong>solely on the number of thresholds violated</strong>:</p>
<ul>
<li><strong>Definite</strong> (5 violations) - All five metrics exceed thresholds - clear god object requiring immediate refactoring</li>
<li><strong>Probable</strong> (3-4 violations) - Most metrics exceed thresholds - likely god object that should be refactored</li>
<li><strong>Possible</strong> (1-2 violations) - Some metrics exceed thresholds - potential god object worth reviewing</li>
<li><strong>NotGodObject</strong> (0 violations) - All metrics within acceptable limits</li>
</ul>
<p><strong>Note:</strong> The confidence level is determined by violation count alone. The god object score (calculated separately) is used for prioritization and ranking, but does not affect the confidence classification.</p>
<p><strong>Example:</strong> Consider two files both with <code>violation_count=2</code> (Possible confidence):</p>
<ul>
<li>File A: 21 methods, 16 fields (just over the threshold)</li>
<li>File B: 100 methods, 50 fields (severely over the threshold)</li>
</ul>
<p>Both receive the same “Possible” confidence level, but File B will have a much higher god object score for prioritization purposes. This separation ensures consistent confidence classification while still allowing scores to reflect severity.</p>
<p>See <code>src/organization/god_object_analysis.rs:236-268</code> for the <code>determine_confidence</code> function.</p>
<h2 id="scoring-algorithms"><a class="header" href="#scoring-algorithms">Scoring Algorithms</a></h2>
<p>Debtmap provides three scoring algorithms to accommodate different analysis needs.</p>
<h3 id="simple-scoring"><a class="header" href="#simple-scoring">Simple Scoring</a></h3>
<p>The base scoring algorithm calculates god object score using four factors:</p>
<pre><code>method_factor = min(method_count / max_methods, 3.0)
field_factor = min(field_count / max_fields, 3.0)
responsibility_factor = min(responsibility_count / 3, 3.0)
size_factor = min(lines_of_code / max_lines, 3.0)

base_score = method_factor × field_factor × responsibility_factor × size_factor
</code></pre>
<p><strong>Score Enforcement:</strong></p>
<ul>
<li>If <code>violation_count &gt; 0</code>: <code>final_score = max(base_score × 20 × violation_count, min_score)</code>
<ul>
<li>Where <code>min_score</code> is: 1 violation → 30.0, 2 violations → 50.0, 3+ violations → 70.0</li>
</ul>
</li>
<li>Else: <code>final_score = base_score × 10</code></li>
</ul>
<p><strong>Source</strong>: src/organization/god_object/scoring.rs:44-90</p>
<p>The graduated minimum scores ensure that severity matches the number of violations while preventing over-flagging of moderate files.</p>
<h3 id="complexity-weighted-scoring"><a class="header" href="#complexity-weighted-scoring">Complexity-Weighted Scoring</a></h3>
<p>Unlike raw method counting, this algorithm weights each method by its cyclomatic complexity. This ensures that 100 simple functions (complexity 1-3) score better than 10 highly complex functions (complexity 17+).</p>
<p>The formula is similar to simple scoring, but uses <code>weighted_method_count</code> (sum of complexity weights) instead of raw counts:</p>
<pre><code>method_factor = min(weighted_method_count / max_methods, 3.0)
</code></pre>
<p>Additionally, a <strong>complexity factor</strong> is applied:</p>
<ul>
<li>Average complexity &lt; 3.0: <code>0.7</code> (reward simple functions)</li>
<li>Average complexity &gt; 10.0: <code>1.5</code> (penalize complex functions)</li>
<li>Otherwise: <code>1.0</code></li>
</ul>
<p>The final score becomes:</p>
<pre><code>final_score = max(base_score × 20 × complexity_factor × violation_count, min_score)
</code></pre>
<p>Where <code>min_score</code> is: 1 violation → 30.0, 2 violations → 50.0, 3+ violations → 70.0</p>
<p>This approach better reflects the true maintainability burden of a large module while using conservative scaling to prevent small files from being over-flagged.</p>
<p><strong>Source</strong>: src/organization/god_object/scoring.rs:118-178</p>
<h3 id="purity-weighted-scoring-advanced"><a class="header" href="#purity-weighted-scoring-advanced">Purity-Weighted Scoring (Advanced)</a></h3>
<p><strong>Available for Rust only</strong> (requires <code>syn::ItemFn</code> analysis)</p>
<p>This advanced scoring variant combines both <strong>complexity weighting</strong> and <strong>purity analysis</strong>, building on top of complexity-weighted scoring to further reduce the impact of pure functions. This prevents pure functional modules from being unfairly penalized. The algorithm:</p>
<ol>
<li>
<p>Analyzes each function for purity using three levels:</p>
<ul>
<li>
<p><strong>Pure</strong> (no side effects): Functions with read-only operations, no I/O, no mutation</p>
<ul>
<li>Weight multiplier: <code>0.3</code></li>
<li>Examples: <code>calculate_sum()</code>, <code>format_string()</code>, <code>is_valid()</code></li>
</ul>
</li>
<li>
<p><strong>Probably Pure</strong> (likely no side effects): Functions that appear pure but may have hidden side effects</p>
<ul>
<li>Weight multiplier: <code>0.5</code></li>
<li>Examples: Functions using trait methods (could have side effects), generic operations</li>
</ul>
</li>
<li>
<p><strong>Impure</strong> (has side effects): Functions with clear side effects like I/O, mutation, external calls</p>
<ul>
<li>Weight multiplier: <code>1.0</code></li>
<li>Examples: <code>save_to_file()</code>, <code>update_state()</code>, <code>send_request()</code></li>
</ul>
</li>
</ul>
</li>
<li>
<p>Purity Detection Heuristics:</p>
<ul>
<li><strong>Pure indicators</strong>: No <code>mut</code> references, no I/O operations, no external function calls</li>
<li><strong>Impure indicators</strong>: File/network operations, mutable state, database access, logging</li>
<li><strong>Probably Pure</strong>: Generic functions, trait method calls, or ambiguous patterns</li>
</ul>
</li>
<li>
<p>Combines complexity and purity weights to calculate the total contribution:</p>
<pre><code>total_weight = complexity_weight × purity_multiplier
</code></pre>
<p>This means pure functions get both the complexity-based weight AND the purity multiplier applied together.</p>
<p><strong>Example:</strong> A pure function with complexity 5 contributes only <code>5 × 0.3 = 1.5</code> to the weighted count (compared to 5.0 for an impure function of the same complexity).</p>
</li>
<li>
<p>Tracks the <code>PurityDistribution</code>:</p>
<ul>
<li><code>pure_count</code>, <code>probably_pure_count</code>, <code>impure_count</code></li>
<li><code>pure_weight_contribution</code>, <code>probably_pure_weight_contribution</code>, <code>impure_weight_contribution</code></li>
</ul>
</li>
</ol>
<p><strong>Impact:</strong> A file with 100 pure helper functions (total complexity 150) might have a weighted method count of only <code>150 × 0.3 = 45</code>, avoiding false positives while still catching stateful god objects with many impure methods.</p>
<p>See <code>src/organization/god_object_detector.rs:196-258</code> and <code>src/organization/purity_analyzer.rs</code>.</p>
<h2 id="responsibility-detection"><a class="header" href="#responsibility-detection">Responsibility Detection</a></h2>
<p>Responsibilities are inferred from method names using behavioral heuristics. Debtmap recognizes the following categories based on the <code>BehaviorCategory</code> enum (src/organization/behavioral_decomposition.rs:10-41):</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Category</th><th>Method Patterns</th><th>Examples</th></tr>
</thead>
<tbody>
<tr><td><strong>Lifecycle</strong></td><td><code>new</code>, <code>init</code>, <code>setup</code>, <code>destroy</code>, <code>cleanup</code></td><td>Initialization and teardown</td></tr>
<tr><td><strong>State Management</strong></td><td><code>update_*</code>, <code>mutate_*</code>, <code>*_state</code></td><td>State transitions and mutations</td></tr>
<tr><td><strong>Rendering</strong></td><td><code>render</code>, <code>draw</code>, <code>paint</code>, <code>display</code>, <code>format</code></td><td>Display and formatting</td></tr>
<tr><td><strong>Event Handling</strong></td><td><code>handle_*</code>, <code>on_*</code></td><td>Event dispatchers and handlers</td></tr>
<tr><td><strong>Persistence</strong></td><td><code>save</code>, <code>load</code>, <code>serialize</code>, <code>deserialize</code></td><td>Data storage and retrieval</td></tr>
<tr><td><strong>Validation</strong></td><td><code>validate_*</code>, <code>check_*</code>, <code>verify_*</code>, <code>ensure_*</code>, <code>is_*</code></td><td>Data validation</td></tr>
<tr><td><strong>Computation</strong></td><td><code>calculate</code>, <code>compute</code>, <code>evaluate</code></td><td>Pure deterministic calculations</td></tr>
<tr><td><strong>Parsing</strong></td><td><code>parse</code>, <code>read</code>, <code>extract</code>, <code>decode</code>, <code>unmarshal</code>, <code>scan</code></td><td>Data parsing and reading</td></tr>
<tr><td><strong>Filtering</strong></td><td><code>filter</code>, <code>select</code>, <code>find</code>, <code>search</code>, <code>query</code>, <code>lookup</code>, <code>match</code></td><td>Search and filtering</td></tr>
<tr><td><strong>Transformation</strong></td><td><code>transform</code>, <code>convert</code>, <code>map</code>, <code>apply</code>, <code>adapt</code></td><td>Data transformation</td></tr>
<tr><td><strong>Data Access</strong></td><td><code>get</code>, <code>set</code>, <code>fetch</code>, <code>retrieve</code>, <code>access</code></td><td>Getter/setter methods</td></tr>
<tr><td><strong>Construction</strong></td><td><code>create</code>, <code>build</code>, <code>make</code>, <code>construct</code></td><td>Object construction</td></tr>
<tr><td><strong>Processing</strong></td><td><code>process</code>, <code>handle</code>, <code>execute</code>, <code>run</code></td><td>General processing</td></tr>
<tr><td><strong>Communication</strong></td><td><code>send</code>, <code>receive</code>, <code>transmit</code>, <code>broadcast</code>, <code>notify</code></td><td>Inter-process communication</td></tr>
<tr><td><strong>Domain</strong></td><td><em>(no match)</em></td><td>Domain-specific with custom name</td></tr>
</tbody>
</table>
</div>
<p><strong>Note:</strong> The <code>Domain</code> category serves as the fallback when no behavioral pattern matches. It extracts a domain name from the method’s first word to provide context-specific categorization.</p>
<p><strong>Classification Order</strong>: More specific categories are checked first (Construction before Lifecycle, Validation before Rendering) to ensure accurate categorization. See <code>BehavioralCategorizer::categorize_method()</code> in src/organization/behavioral_decomposition.rs:151-249.</p>
<p><strong>Distinct Responsibility Counting:</strong> Debtmap counts the number of <strong>unique</strong> responsibility categories used by a struct/module’s methods. A high responsibility count (e.g., &gt;5) indicates the module is handling too many different concerns, violating the Single Responsibility Principle.</p>
<p>Responsibility count directly affects:</p>
<ul>
<li>God object scoring (via <code>responsibility_factor</code>)</li>
<li>Refactoring recommendations (methods grouped by responsibility for suggested splits)</li>
<li>Detection confidence (counted as one of the five violation criteria)</li>
</ul>
<p>See <code>BehavioralCategorizer::categorize_method()</code> in src/organization/behavioral_decomposition.rs:151-249 and <code>infer_responsibility_with_confidence()</code> in src/organization/god_object/classifier.rs:128-163.</p>
<h2 id="examples-and-case-studies"><a class="header" href="#examples-and-case-studies">Examples and Case Studies</a></h2>
<h3 id="example-1-large-rust-module"><a class="header" href="#example-1-large-rust-module">Example 1: Large Rust Module</a></h3>
<p><strong>File:</strong> <code>rust_call_graph.rs</code> with 270 standalone functions</p>
<p><strong>Detection:</strong></p>
<ul>
<li><strong>Is God Object:</strong> Yes</li>
<li><strong>Method Count:</strong> 270</li>
<li><strong>Field Count:</strong> 0 (no struct)</li>
<li><strong>Responsibilities:</strong> 8</li>
<li><strong>Confidence:</strong> Definite</li>
<li><strong>Score:</strong> &gt;1000 (severe violation)</li>
</ul>
<p><strong>Recommendation:</strong> Break into multiple focused modules:</p>
<ul>
<li><code>CallGraphBuilder</code> (construction methods)</li>
<li><code>CallGraphAnalyzer</code> (analysis methods)</li>
<li><code>CallGraphFormatter</code> (output methods)</li>
</ul>
<h3 id="example-2-complex-python-class"><a class="header" href="#example-2-complex-python-class">Example 2: Complex Python Class</a></h3>
<p><strong>File:</strong> <code>data_manager.py</code> with class containing 25 methods and 12 fields</p>
<p><strong>Detection:</strong></p>
<ul>
<li><strong>Is God Object:</strong> Yes</li>
<li><strong>Method Count:</strong> 25</li>
<li><strong>Field Count:</strong> 12</li>
<li><strong>Responsibilities:</strong> 6 (Data Access, Validation, Persistence, etc.)</li>
<li><strong>Confidence:</strong> Probable</li>
<li><strong>Score:</strong> ~150-200</li>
</ul>
<p><strong>Recommendation:</strong> Split by responsibility:</p>
<ul>
<li><code>DataAccessLayer</code> (get/set methods)</li>
<li><code>DataValidator</code> (validate/check methods)</li>
<li><code>DataPersistence</code> (save/load methods)</li>
</ul>
<h3 id="example-3-mixed-paradigm-file-god-module"><a class="header" href="#example-3-mixed-paradigm-file-god-module">Example 3: Mixed Paradigm File (God Module)</a></h3>
<p><strong>File:</strong> <code>utils.rs</code> with small struct (5 methods, 3 fields) + 60 standalone functions</p>
<p><strong>Detection:</strong></p>
<ul>
<li><strong>God Class (struct):</strong> No (5 methods &lt; 20 threshold, 3 fields &lt; 15 threshold)</li>
<li><strong>God Module (file):</strong> Yes (60 standalone functions &gt; 50 threshold)</li>
<li><strong>Confidence:</strong> Probable</li>
<li><strong>Score:</strong> ~120</li>
</ul>
<p><strong>Analysis:</strong> The struct and standalone functions are analyzed separately. The struct is not a god class, but the file is a god module due to the excessive standalone functions. This indicates an overgrown utility module that should be split into smaller, focused modules.</p>
<p><strong>Recommendation:</strong> Split standalone functions into focused utility modules:</p>
<ul>
<li><code>StringUtils</code> (formatting, parsing)</li>
<li><code>FileUtils</code> (file operations)</li>
<li><code>MathUtils</code> (calculations)</li>
</ul>
<h2 id="refactoring-recommendations-5"><a class="header" href="#refactoring-recommendations-5">Refactoring Recommendations</a></h2>
<p>When <code>is_god_object = true</code>, Debtmap generates <strong>recommended module splits</strong> using the <code>recommend_module_splits</code> function. This feature:</p>
<ol>
<li>
<p>Groups methods by their inferred responsibilities</p>
</li>
<li>
<p>Creates a <code>ModuleSplit</code> for each responsibility group containing:</p>
<ul>
<li><code>suggested_name</code> (e.g., “DataAccessManager”, “ValidationManager”)</li>
<li><code>methods_to_move</code> (list of method names)</li>
<li><code>responsibility</code> (category name)</li>
<li><code>estimated_lines</code> (approximate LOC for the new module)</li>
</ul>
</li>
<li>
<p>Orders splits by cohesion (most focused responsibility groups first)</p>
</li>
</ol>
<p><strong>Example output:</strong></p>
<pre><code>Recommended Splits:
  1. DataAccessManager (12 methods, ~150 lines)
  2. ValidationManager (8 methods, ~100 lines)
  3. PersistenceManager (5 methods, ~75 lines)
</code></pre>
<p>This provides an actionable roadmap for breaking down god objects into focused, single-responsibility modules.</p>
<p>See <code>src/organization/god_object_detector.rs:165-177</code> and <code>src/organization/god_object_analysis.rs:40-45</code>.</p>
<h3 id="code-examples"><a class="header" href="#code-examples">Code Examples</a></h3>
<h4 id="split-by-responsibility"><a class="header" href="#split-by-responsibility">Split by Responsibility</a></h4>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Before: UserManager (god object)
struct UserManager { ... }

// After: Split into focused modules
struct AuthService { ... }
struct ProfileService { ... }
struct PermissionService { ... }
struct NotificationService { ... }
<span class="boring">}</span></code></pre>
<h4 id="extract-common-functionality"><a class="header" href="#extract-common-functionality">Extract Common Functionality</a></h4>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Extract shared dependencies
struct ServiceContext {
    db: Database,
    cache: Cache,
    logger: Logger,
}

// Each service gets a reference
struct AuthService&lt;'a&gt; {
    context: &amp;'a ServiceContext,
}
<span class="boring">}</span></code></pre>
<h4 id="use-composition"><a class="header" href="#use-composition">Use Composition</a></h4>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Compose services instead of inheriting
struct UserFacade {
    auth: AuthService,
    profile: ProfileService,
    permissions: PermissionService,
}

impl UserFacade {
    fn login(&amp;mut self, credentials: Credentials) -&gt; Result&lt;Session&gt; {
        self.auth.login(credentials)
    }
}
<span class="boring">}</span></code></pre>
<h2 id="configuration-12"><a class="header" href="#configuration-12">Configuration</a></h2>
<h3 id="toml-configuration-1"><a class="header" href="#toml-configuration-1">TOML Configuration</a></h3>
<p>Add a <code>[god_object_detection]</code> section to your <code>.debtmap.toml</code>:</p>
<pre><code class="language-toml">[god_object_detection]
enabled = true

[god_object_detection.rust]
max_methods = 20
max_fields = 15
max_traits = 5      # max_traits = max responsibilities
max_lines = 1000
max_complexity = 200

# Note: The configuration field is named 'max_traits' for historical reasons,
# but it controls the maximum number of responsibilities/concerns, not Rust traits.
# This is a legacy naming issue from early development.

[god_object_detection.python]
max_methods = 15
max_fields = 10
max_traits = 3
max_lines = 500
max_complexity = 150

[god_object_detection.javascript]
max_methods = 15
max_fields = 20
max_traits = 3
max_lines = 500
max_complexity = 150
</code></pre>
<p><strong>Note:</strong> <code>enabled</code> defaults to <code>true</code>. Set to <code>false</code> to disable god object detection entirely (equivalent to <code>--no-god-object</code> CLI flag).</p>
<p>See <code>src/config.rs:500-582</code>.</p>
<h3 id="tuning-for-your-project-1"><a class="header" href="#tuning-for-your-project-1">Tuning for Your Project</a></h3>
<p><strong>Strict mode (smaller modules):</strong></p>
<pre><code class="language-toml">[god_object_detection.rust]
max_methods = 15
max_fields = 10
max_traits = 3
</code></pre>
<p><strong>Lenient mode (larger modules acceptable):</strong></p>
<pre><code class="language-toml">[god_object_detection.rust]
max_methods = 30
max_fields = 20
max_traits = 7
</code></pre>
<h3 id="cli-options"><a class="header" href="#cli-options">CLI Options</a></h3>
<p>Debtmap provides several CLI flags to control god object detection behavior:</p>
<h4 id="--no-god-object"><a class="header" href="#--no-god-object"><code>--no-god-object</code></a></h4>
<p>Disables god object detection entirely.</p>
<pre><code class="language-bash">debtmap analyze . --no-god-object
</code></pre>
<p><strong>Use case:</strong> When you only want function-level complexity analysis without file-level aggregation.</p>
<h4 id="--aggregate-only"><a class="header" href="#--aggregate-only"><code>--aggregate-only</code></a></h4>
<p>Shows only file-level god object scores, hiding individual function details.</p>
<pre><code class="language-bash">debtmap analyze . --aggregate-only
</code></pre>
<p><strong>Use case:</strong> High-level overview of which files are god objects without function-by-function breakdowns.</p>
<h4 id="--no-aggregation"><a class="header" href="#--no-aggregation"><code>--no-aggregation</code></a></h4>
<p>Disables file-level aggregation, showing only individual function metrics.</p>
<pre><code class="language-bash">debtmap analyze . --no-aggregation
</code></pre>
<p><strong>Use case:</strong> Detailed function-level analysis without combining into file scores.</p>
<h4 id="--aggregation-method-method"><a class="header" href="#--aggregation-method-method"><code>--aggregation-method &lt;METHOD&gt;</code></a></h4>
<p>Chooses how to combine function scores into file-level scores:</p>
<ul>
<li><code>sum</code> - Add all function scores</li>
<li><code>weighted_sum</code> - Weight by complexity (default)</li>
<li><code>logarithmic_sum</code> - Logarithmic scaling for large files</li>
<li><code>max_plus_average</code> - Max score + average of others</li>
</ul>
<pre><code class="language-bash">debtmap analyze . --aggregation-method logarithmic_sum
</code></pre>
<h4 id="--min-problematic-n"><a class="header" href="#--min-problematic-n"><code>--min-problematic &lt;N&gt;</code></a></h4>
<p>Sets minimum number of problematic functions required for file-level aggregation.</p>
<pre><code class="language-bash">debtmap analyze . --min-problematic 3
</code></pre>
<p><strong>Use case:</strong> Avoid flagging files with only 1-2 complex functions as god objects.</p>
<p>See <code>features.json:65-71</code> and <code>features.json:507-512</code>.</p>
<h2 id="output-display-1"><a class="header" href="#output-display-1">Output Display</a></h2>
<h3 id="file-level-display"><a class="header" href="#file-level-display">File-Level Display</a></h3>
<p>When a god object is detected, Debtmap displays:</p>
<pre><code>⚠️ God Object: 270 methods, 0 fields, 8 responsibilities
Score: 1350 (Confidence: Definite)
</code></pre>
<h3 id="function-level-display"><a class="header" href="#function-level-display">Function-Level Display</a></h3>
<p>Within a god object file, individual functions show:</p>
<pre><code>├─ ⚠️ God Object: 45 methods, 20 fields, 5 responsibilities
│      Score: 250 (Confidence: Probable)
</code></pre>
<p>The <code>⚠️ God Object</code> indicator makes it immediately clear which files need architectural refactoring.</p>
<h2 id="integration-with-file-level-scoring"><a class="header" href="#integration-with-file-level-scoring">Integration with File-Level Scoring</a></h2>
<p>God object detection affects the overall technical debt prioritization through a <strong>god object multiplier</strong>:</p>
<pre><code>god_object_multiplier = 2.0 + normalized_god_object_score
</code></pre>
<h3 id="normalization-1"><a class="header" href="#normalization-1">Normalization</a></h3>
<p>The <code>normalized_god_object_score</code> is scaled to the 0-1 range using:</p>
<pre><code>normalized_score = min(god_object_score / max_expected_score, 1.0)
</code></pre>
<p>Where <code>max_expected_score</code> is typically based on the maximum score in the analysis (e.g., 1000 for severe violations).</p>
<h3 id="impact-on-prioritization"><a class="header" href="#impact-on-prioritization">Impact on Prioritization</a></h3>
<p>This multiplier means:</p>
<ol>
<li><strong>Non-god objects</strong> (score = 0): multiplier = 2.0 (baseline)</li>
<li><strong>Moderate god objects</strong> (score = 200): multiplier ≈ 2.2-2.5</li>
<li><strong>Severe god objects</strong> (score = 1000+): multiplier ≈ 3.0 (maximum)</li>
</ol>
<p><strong>Result:</strong> God objects receive <strong>2-3× higher priority</strong> in debt rankings, ensuring that:</p>
<ul>
<li>Functions within god objects inherit elevated scores due to architectural concerns</li>
<li>God objects surface in the “top 10 most problematic” lists</li>
<li>Architectural debt is weighted appropriately alongside function-level complexity</li>
</ul>
<p>See the <a href="#file-level-scoring">Scoring Strategies</a> documentation for complete details on how this multiplier integrates into the overall debt calculation.</p>
<h2 id="metrics-tracking-advanced"><a class="header" href="#metrics-tracking-advanced">Metrics Tracking (Advanced)</a></h2>
<p>For teams tracking god object evolution over time, Debtmap provides <code>GodObjectMetrics</code> with:</p>
<ul>
<li><strong>Snapshots</strong> - Historical god object data per file</li>
<li><strong>Trends</strong> - Improving/Stable/Worsening classification (based on ±10 point score changes)</li>
<li><strong>New God Objects</strong> - Files that crossed the threshold</li>
<li><strong>Resolved God Objects</strong> - Files that were refactored below thresholds</li>
</ul>
<p>This enables longitudinal analysis: “Are we reducing god objects sprint-over-sprint?”</p>
<p>See <code>src/organization/god_object_metrics.rs:1-228</code>.</p>
<h2 id="troubleshooting-16"><a class="header" href="#troubleshooting-16">Troubleshooting</a></h2>
<h3 id="why-is-my-functional-module-flagged-as-a-god-object"><a class="header" href="#why-is-my-functional-module-flagged-as-a-god-object">“Why is my functional module flagged as a god object?”</a></h3>
<p><strong>Answer:</strong> Debtmap now analyzes god classes (structs) separately from god modules (standalone functions). If your functional module with 100 pure helper functions is flagged, it’s being detected as a <strong>god module</strong> (not a god class), which indicates the file has grown too large and should be split for better organization.</p>
<p><strong>Solutions:</strong></p>
<ol>
<li><strong>Accept the finding</strong>: 100+ functions in one file is difficult to navigate and maintain, even if each function is simple</li>
<li><strong>Split by responsibility</strong>: Organize functions into smaller, focused modules (e.g., <code>string_utils.rs</code>, <code>file_utils.rs</code>, <code>math_utils.rs</code>)</li>
<li><strong>Use purity-weighted scoring</strong> (Rust only): Pure functions contribute only 0.3× weight, dramatically reducing scores for functional modules</li>
<li><strong>Adjust thresholds</strong>: Increase <code>max_methods</code> in <code>.debtmap.toml</code> if your project standards allow larger modules</li>
</ol>
<h3 id="my-god-object-score-seems-too-high"><a class="header" href="#my-god-object-score-seems-too-high">“My god object score seems too high”</a></h3>
<p><strong>Answer:</strong> The scoring algorithm uses exponential scaling (<code>base_score × 50 × violation_count</code>) to ensure god objects are prioritized.</p>
<p><strong>Solutions:</strong></p>
<ol>
<li>Check the violation count - 5 violations means severe issues</li>
<li>Review each metric - are method count, field count, responsibilities, LOC, and complexity all high?</li>
<li>Consider if the score accurately reflects maintainability burden</li>
</ol>
<h3 id="why-does-my-test-file-show-as-a-god-object"><a class="header" href="#why-does-my-test-file-show-as-a-god-object">“Why does my test file show as a god object?”</a></h3>
<p><strong>Answer:</strong> Test files often have many test functions, which can trigger god module detection. However, this is usually expected for comprehensive test suites.</p>
<p><strong>Solutions:</strong></p>
<ol>
<li><strong>Accept the finding</strong>: Large test files (100+ test functions) can be difficult to navigate and maintain</li>
<li><strong>Split by feature</strong>: Organize tests into smaller files grouped by the feature they test (e.g., <code>user_auth_tests.rs</code>, <code>user_profile_tests.rs</code>)</li>
<li><strong>Adjust thresholds</strong>: If your project standards accept large test files, increase <code>max_methods</code> in <code>.debtmap.toml</code></li>
<li><strong>Use test organization</strong>: Group related tests in modules within the test file for better structure</li>
</ol>
<p><strong>Note:</strong> Debtmap does not automatically exclude test files from god object detection. Consider the trade-offs between comprehensive test coverage in one file versus better organization across multiple test files.</p>
<h3 id="can-i-disable-god-object-detection-for-specific-files"><a class="header" href="#can-i-disable-god-object-detection-for-specific-files">“Can I disable god object detection for specific files?”</a></h3>
<p><strong>Answer:</strong> Currently, god object detection is global. However, you can:</p>
<ol>
<li>Use <code>--no-god-object</code> to disable entirely</li>
<li>Use <code>--no-aggregation</code> to skip file-level analysis</li>
<li>Adjust thresholds in <code>.debtmap.toml</code> to be more lenient</li>
</ol>
<h2 id="best-practices-14"><a class="header" href="#best-practices-14">Best Practices</a></h2>
<p>To avoid god objects:</p>
<ol>
<li><strong>Follow Single Responsibility Principle</strong> - Each module should have one clear purpose</li>
<li><strong>Regular Refactoring</strong> - Split modules before they reach thresholds</li>
<li><strong>Monitor Growth</strong> - Track method and field counts as modules evolve</li>
<li><strong>Use Composition</strong> - Prefer smaller, composable units over large monoliths</li>
<li><strong>Clear Boundaries</strong> - Define clear module interfaces and responsibilities</li>
<li><strong>Leverage Purity</strong> - Keep pure functions separate from stateful logic (reduces scores in Rust)</li>
<li><strong>Set Project Thresholds</strong> - Customize <code>.debtmap.toml</code> to match your team’s standards</li>
</ol>
<h2 id="configuration-tradeoffs"><a class="header" href="#configuration-tradeoffs">Configuration Tradeoffs</a></h2>
<p><strong>Strict Thresholds</strong> (e.g., Rust: 10 methods):</p>
<ul>
<li>✅ Catch problems early</li>
<li>✅ Enforce strong modularity</li>
<li>❌ May flag legitimate large modules</li>
<li>❌ More noise in reports</li>
</ul>
<p><strong>Lenient Thresholds</strong> (e.g., Rust: 50 methods):</p>
<ul>
<li>✅ Reduce false positives</li>
<li>✅ Focus on egregious violations</li>
<li>❌ Miss real god objects</li>
<li>❌ Allow technical debt to grow</li>
</ul>
<p><strong>Recommended:</strong> Start with defaults, then adjust based on your codebase’s characteristics. Use metrics tracking to monitor trends over time.</p>
<h2 id="related-documentation-2"><a class="header" href="#related-documentation-2">Related Documentation</a></h2>
<ul>
<li><a href="#scoring-strategies">Scoring Strategies</a> - How god objects affect overall file scores</li>
<li><a href="#configuration-2">Configuration</a> - Complete <code>.debtmap.toml</code> reference</li>
<li><a href="#cli-reference">CLI Reference</a> - All command-line options</li>
<li><a href="#tiered-prioritization-3">Tiered Prioritization</a> - How god objects are prioritized</li>
</ul>
<h2 id="summary-6"><a class="header" href="#summary-6">Summary</a></h2>
<p>God object detection is a powerful architectural analysis feature that:</p>
<ul>
<li>Identifies files/types violating single responsibility principle</li>
<li>Provides multiple scoring algorithms (simple, complexity-weighted, purity-weighted)</li>
<li>Generates actionable refactoring recommendations</li>
<li>Integrates with file-level scoring for holistic debt prioritization</li>
<li>Supports customization via TOML config and CLI flags</li>
</ul>
<p>By combining quantitative metrics (method count, LOC, complexity) with qualitative analysis (responsibility detection, purity), Debtmap helps teams systematically address architectural debt.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="multi-pass-analysis"><a class="header" href="#multi-pass-analysis">Multi-Pass Analysis</a></h1>
<p>Multi-pass analysis is enabled by default in debtmap. It performs two separate complexity analyses on your code to distinguish between genuine logical complexity and complexity artifacts introduced by code formatting. By comparing raw and normalized versions of your code, debtmap can attribute complexity to specific sources and provide actionable insights for refactoring.</p>
<h2 id="overview-16"><a class="header" href="#overview-16">Overview</a></h2>
<p>Traditional complexity analysis treats all code as-is, which means formatting choices like multiline expressions, whitespace, and indentation can artificially inflate complexity metrics. Multi-pass analysis solves this problem by:</p>
<ol>
<li><strong>Raw Analysis</strong> - Measures complexity of code exactly as written</li>
<li><strong>Normalized Analysis</strong> - Measures complexity after removing formatting artifacts</li>
<li><strong>Attribution</strong> - Compares the two analyses to identify complexity sources</li>
</ol>
<p>The difference between raw and normalized complexity reveals how much “complexity” comes from formatting versus genuine logical complexity from control flow, branching, and nesting.</p>
<h2 id="how-it-works-6"><a class="header" href="#how-it-works-6">How It Works</a></h2>
<h3 id="two-pass-analysis-process"><a class="header" href="#two-pass-analysis-process">Two-Pass Analysis Process</a></h3>
<pre><code>┌─────────────┐
│  Raw Code   │
└──────┬──────┘
       │
       ├─────────────────────┐
       │                     │
       ▼                     ▼
┌──────────────┐    ┌────────────────────┐
│ Raw Analysis │    │ Normalize Formatting│
└──────┬───────┘    └─────────┬──────────┘
       │                      │
       │                      ▼
       │            ┌──────────────────────┐
       │            │ Normalized Analysis  │
       │            └─────────┬────────────┘
       │                      │
       └──────────┬───────────┘
                  ▼
         ┌──────────────────┐
         │ Attribution      │
         │ Engine           │
         └─────────┬────────┘
                   │
         ┌─────────┴──────────┐
         │                    │
         ▼                    ▼
┌─────────────────┐  ┌─────────────────┐
│ Insights        │  │ Recommendations │
└─────────────────┘  └─────────────────┘
</code></pre>
<p><strong>Raw Analysis</strong> examines your code as-is, capturing all complexity including:</p>
<ul>
<li>Logical control flow (if, loops, match, try/catch)</li>
<li>Function calls and closures</li>
<li>Formatting artifacts (multiline expressions, whitespace, indentation)</li>
</ul>
<p><strong>Normalized Analysis</strong> processes semantically equivalent code with standardized formatting:</p>
<ul>
<li>Removes excessive whitespace</li>
<li>Normalizes multiline expressions to single lines where appropriate</li>
<li>Standardizes indentation</li>
<li>Preserves logical structure</li>
</ul>
<p><strong>Attribution Engine</strong> compares the results to categorize complexity sources:</p>
<ul>
<li><strong>Logical Complexity</strong> - From control flow and branching (normalized result)</li>
<li><strong>Formatting Artifacts</strong> - From code formatting choices (difference between raw and normalized)</li>
<li><strong>Pattern Complexity</strong> - From recognized code patterns (error handling, validation, etc.)</li>
</ul>
<blockquote>
<p><strong>Note</strong>: Pattern complexity analysis is part of the standard multi-pass analysis. No additional configuration is required to enable pattern detection.</p>
</blockquote>
<h2 id="cli-usage-1"><a class="header" href="#cli-usage-1">CLI Usage</a></h2>
<p>Multi-pass analysis runs by default. You can disable it if needed for performance-constrained scenarios:</p>
<pre><code class="language-bash"># Basic analysis (multi-pass enabled by default)
debtmap analyze .

# Multi-pass with detailed attribution breakdown
debtmap analyze . --attribution

# Control detail level
debtmap analyze . --attribution --detail-level comprehensive

# Output as JSON for tooling integration
debtmap analyze . --attribution --json

# Disable multi-pass for faster single-pass analysis
debtmap analyze . --no-multi-pass
</code></pre>
<h3 id="available-flags"><a class="header" href="#available-flags">Available Flags</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Flag</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>--no-multi-pass</code></td><td>Disable multi-pass analysis (use single-pass for performance)</td></tr>
<tr><td><code>--attribution</code></td><td>Show detailed complexity attribution breakdown (requires multi-pass)</td></tr>
<tr><td><code>--detail-level &lt;level&gt;</code></td><td>Set output detail: <code>summary</code>, <code>standard</code>, <code>comprehensive</code>, <code>debug</code> (CLI accepts lowercase values)</td></tr>
<tr><td><code>--json</code></td><td>Output results in JSON format</td></tr>
</tbody>
</table>
</div>
<blockquote>
<p><strong>Note</strong>: The <code>--attribution</code> flag requires multi-pass analysis to be enabled (the default), as attribution depends on comparing raw and normalized analyses. Use <code>--no-multi-pass</code> only when performance is critical.</p>
</blockquote>
<h2 id="attribution-engine"><a class="header" href="#attribution-engine">Attribution Engine</a></h2>
<p>The attribution engine breaks down complexity into three main categories, each with detailed tracking and suggestions.</p>
<h3 id="logical-complexity"><a class="header" href="#logical-complexity">Logical Complexity</a></h3>
<p>Represents inherent complexity from your code’s control flow and structure:</p>
<ul>
<li><strong>Function complexity</strong> - Cyclomatic and cognitive complexity per function</li>
<li><strong>Control flow</strong> - If statements, loops, match expressions</li>
<li><strong>Error handling</strong> - Try/catch blocks, Result/Option handling</li>
<li><strong>Closures and callbacks</strong> - Anonymous functions and callbacks</li>
<li><strong>Nesting levels</strong> - Depth of nested control structures</li>
</ul>
<p>Each logical complexity component includes:</p>
<ul>
<li><strong>Contribution</strong> - Complexity points from this construct</li>
<li><strong>Location</strong> - File, line, column, and span information</li>
<li><strong>Suggestions</strong> - Specific refactoring recommendations</li>
</ul>
<p>Example:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Function with high logical complexity
fn process_data(items: Vec&lt;Item&gt;) -&gt; Result&lt;Vec&lt;Output&gt;&gt; {
    let mut results = Vec::new();

    for item in items {                          // +1 (loop)
        if item.is_valid() {                     // +1 (if)
            match item.category {                // +1 (match)
                Category::A =&gt; {
                    if item.value &gt; 100 {        // +2 (nested if)
                        results.push(transform_a(&amp;item)?);
                    }
                }
                Category::B =&gt; {
                    results.push(transform_b(&amp;item)?);
                }
                _ =&gt; continue,                   // +1 (match arm)
            }
        }
    }

    Ok(results)
}
// Logical complexity: ~7 points
<span class="boring">}</span></code></pre>
<h3 id="formatting-artifacts"><a class="header" href="#formatting-artifacts">Formatting Artifacts</a></h3>
<p>Identifies complexity introduced by code formatting choices:</p>
<ul>
<li><strong>Multiline expressions</strong> - Long expressions split across multiple lines</li>
<li><strong>Excessive whitespace</strong> - Blank lines within code blocks</li>
<li><strong>Inconsistent indentation</strong> - Mixed tabs/spaces or irregular indentation</li>
<li><strong>Line breaks in chains</strong> - Method chains split across many lines</li>
</ul>
<p>Formatting artifacts are categorized by severity:</p>
<ul>
<li><strong>Low</strong> - Minor formatting inconsistencies (&lt;10% impact)</li>
<li><strong>Medium</strong> - Noticeable formatting impact (10-25% impact)</li>
<li><strong>High</strong> - Significant complexity inflation (&gt;25% impact)</li>
</ul>
<p>Example:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Same function with formatting that inflates complexity
fn process_data(
    items: Vec&lt;Item&gt;
) -&gt; Result&lt;Vec&lt;Output&gt;&gt; {
    let mut results =
        Vec::new();

    for item in
        items
    {
        if item
            .is_valid()
        {
            match item
                .category
            {
                Category::A =&gt;
                {
                    if item
                        .value
                        &gt; 100
                    {
                        results
                            .push(
                                transform_a(
                                    &amp;item
                                )?
                            );
                    }
                }
                Category::B =&gt;
                {
                    results
                        .push(
                            transform_b(
                                &amp;item
                            )?
                        );
                }
                _ =&gt; continue,
            }
        }
    }

    Ok(results)
}
// Raw complexity: ~12 points (formatting adds ~5 points)
// Normalized complexity: ~7 points (true logical complexity)
<span class="boring">}</span></code></pre>
<h3 id="pattern-complexity"><a class="header" href="#pattern-complexity">Pattern Complexity</a></h3>
<p>Recognizes common code patterns and their complexity characteristics:</p>
<ul>
<li><strong>Error handling patterns</strong> - Result/Option propagation, error conversion</li>
<li><strong>Validation patterns</strong> - Input validation, constraint checking</li>
<li><strong>Data transformation</strong> - Map/filter/fold chains, data conversions</li>
<li><strong>Builder patterns</strong> - Fluent interfaces and builders</li>
<li><strong>State machines</strong> - Explicit state management</li>
</ul>
<p>Each pattern includes:</p>
<ul>
<li><strong>Confidence score</strong> (0.0-1.0) - How certain the pattern recognition is</li>
<li><strong>Opportunities</strong> - Suggestions for pattern extraction or improvement</li>
</ul>
<p>Example:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Error handling pattern (confidence: 0.85)
fn load_config(path: &amp;Path) -&gt; Result&lt;Config&gt; {
    let contents = fs::read_to_string(path)
        .context("Failed to read config file")?;

    let config: Config = serde_json::from_str(&amp;contents)
        .context("Failed to parse config JSON")?;

    config.validate()
        .context("Config validation failed")?;

    Ok(config)
}
// Pattern complexity: moderate error handling overhead
// Suggestion: Consider error enum for better type safety
<span class="boring">}</span></code></pre>
<h2 id="understanding-attribution-output"><a class="header" href="#understanding-attribution-output">Understanding Attribution Output</a></h2>
<p>When you run with <code>--attribution</code>, you’ll see a detailed breakdown:</p>
<pre><code class="language-bash">$ debtmap analyze src/main.rs --multi-pass --attribution --detail-level comprehensive
</code></pre>
<h3 id="sample-output"><a class="header" href="#sample-output">Sample Output</a></h3>
<pre><code>Multi-Pass Analysis Results
============================

File: src/main.rs
Raw Complexity: 45
Normalized Complexity: 32
Formatting Impact: 28.9%

Attribution Breakdown
---------------------

Logical Complexity: 32 points
├─ Function 'main' (line 10): 8 points
│  ├─ Control flow: 5 points (2 if, 1 match, 2 loops)
│  ├─ Nesting: 3 points (max depth: 3)
│  └─ Suggestions:
│     - Break down into smaller functions
│     - Extract complex conditions into named variables
│
├─ Function 'process_request' (line 45): 12 points
│  ├─ Control flow: 8 points (4 if, 1 match, 3 early returns)
│  ├─ Nesting: 4 points (max depth: 4)
│  └─ Suggestions:
│     - Consider using early returns to reduce nesting
│     - Extract validation logic into separate function
│
└─ Function 'handle_error' (line 89): 12 points
   ├─ Control flow: 9 points (5 match arms, 4 if conditions)
   ├─ Pattern: Error handling (confidence: 0.90)
   └─ Suggestions:
      - Consider error enum instead of multiple match arms

Formatting Artifacts: 13 points (28.9% of raw complexity)
├─ Multiline expressions: 8 points (Medium severity)
│  └─ Locations: lines 23, 45, 67, 89
├─ Excessive whitespace: 3 points (Low severity)
│  └─ Locations: lines 12-14, 56-58
└─ Inconsistent indentation: 2 points (Low severity)
   └─ Locations: lines 34, 78

Pattern Complexity: 3 recognized patterns
├─ Error handling (confidence: 0.85): 8 occurrences
│  └─ Opportunity: Consider centralizing error handling
├─ Validation (confidence: 0.72): 5 occurrences
│  └─ Opportunity: Extract validation to separate module
└─ Data transformation (confidence: 0.68): 3 occurrences
   └─ Opportunity: Review for functional composition
</code></pre>
<h3 id="interpreting-the-results"><a class="header" href="#interpreting-the-results">Interpreting the Results</a></h3>
<p><strong>Logical Complexity Breakdown</strong></p>
<ul>
<li>Each function is listed with its complexity contribution</li>
<li>Control flow elements are itemized (if, loops, match, etc.)</li>
<li>Nesting depth shows how deeply structures are nested</li>
<li>Suggestions are specific to that function’s complexity patterns</li>
</ul>
<p><strong>Formatting Artifacts</strong></p>
<ul>
<li>Shows percentage of “false” complexity from formatting</li>
<li>Severity indicates impact on metrics</li>
<li>Locations help you find the formatting issues</li>
<li>High formatting impact (&gt;25%) suggests inconsistent style</li>
</ul>
<p><strong>Pattern Analysis</strong></p>
<ul>
<li>Confidence score shows pattern recognition certainty</li>
<li>High confidence (&gt;0.7) means reliable pattern detection</li>
<li>Low confidence (&lt;0.5) suggests unique code structure</li>
<li>Opportunities highlight potential refactoring</li>
</ul>
<h2 id="insights-and-recommendations"><a class="header" href="#insights-and-recommendations">Insights and Recommendations</a></h2>
<p>Multi-pass analysis automatically generates insights and recommendations based on the attribution results.</p>
<h3 id="insight-types"><a class="header" href="#insight-types">Insight Types</a></h3>
<p><strong>FormattingImpact</strong></p>
<ul>
<li>Triggered when formatting contributes &gt;20% of measured complexity</li>
<li>Suggests using automated formatting tools</li>
<li>Recommends standardizing team coding style</li>
</ul>
<p><strong>PatternOpportunity</strong></p>
<ul>
<li>Triggered when pattern confidence is low (&lt;0.5)</li>
<li>Suggests extracting common patterns</li>
<li>Recommends reviewing for code duplication</li>
</ul>
<p><strong>RefactoringCandidate</strong></p>
<ul>
<li>Triggered when logical complexity exceeds threshold (&gt;20)</li>
<li>Identifies functions needing breakdown</li>
<li>Provides specific refactoring strategies</li>
</ul>
<p><strong>ComplexityHotspot</strong></p>
<ul>
<li>Identifies areas of concentrated complexity</li>
<li>Highlights files or modules needing attention</li>
<li>Suggests architectural improvements</li>
</ul>
<h3 id="recommendation-structure"><a class="header" href="#recommendation-structure">Recommendation Structure</a></h3>
<p>Each recommendation includes:</p>
<ul>
<li><strong>Priority</strong>: Low, Medium, High</li>
<li><strong>Category</strong>: Refactoring, Pattern, Formatting, General</li>
<li><strong>Title</strong>: Brief description of the issue</li>
<li><strong>Description</strong>: Detailed explanation</li>
<li><strong>Estimated Impact</strong>: Expected complexity reduction (in points)</li>
<li><strong>Suggested Actions</strong>: Specific steps to take</li>
</ul>
<h3 id="example-recommendations"><a class="header" href="#example-recommendations">Example Recommendations</a></h3>
<pre><code class="language-json">{
  "recommendations": [
    {
      "priority": "High",
      "category": "Refactoring",
      "title": "Simplify control flow in 'process_request'",
      "description": "This function contributes 12 complexity points with deeply nested conditions",
      "estimated_impact": 6,
      "suggested_actions": [
        "Extract validation logic into separate function",
        "Use early returns to reduce nesting depth",
        "Consider state pattern for complex branching"
      ]
    },
    {
      "priority": "Medium",
      "category": "Formatting",
      "title": "Formatting contributes 29% of measured complexity",
      "description": "Code formatting choices are inflating complexity metrics",
      "estimated_impact": 13,
      "suggested_actions": [
        "Use automated formatting tools (rustfmt, prettier)",
        "Standardize code formatting across the team",
        "Configure editor to format on save"
      ]
    },
    {
      "priority": "Low",
      "category": "Pattern",
      "title": "Low pattern recognition suggests unique code structure",
      "description": "Pattern confidence score of 0.45 indicates non-standard patterns",
      "estimated_impact": 3,
      "suggested_actions": [
        "Consider extracting common patterns into utilities",
        "Review for code duplication opportunities",
        "Document unique patterns for team understanding"
      ]
    }
  ]
}
</code></pre>
<h2 id="performance-considerations-2"><a class="header" href="#performance-considerations-2">Performance Considerations</a></h2>
<p>Multi-pass analysis adds overhead compared to single-pass analysis, but debtmap monitors and limits this overhead.</p>
<h3 id="performance-metrics"><a class="header" href="#performance-metrics">Performance Metrics</a></h3>
<p>When performance tracking is enabled, you’ll see:</p>
<pre><code>Performance Metrics
-------------------
Raw analysis: 145ms
Normalized analysis: 132ms
Attribution: 45ms
Total time: 322ms
Memory used: 12.3 MB

Overhead: 121.7% vs single-pass (145ms baseline)
⚠️  Warning: Overhead exceeds 25% target
</code></pre>
<blockquote>
<p><strong>Note</strong>: Memory usage values are estimates based on parallelism level, not precise heap measurements.</p>
</blockquote>
<p><strong>Tracked Metrics:</strong></p>
<ul>
<li><strong>Raw analysis time</strong> - Time to analyze original code</li>
<li><strong>Normalized analysis time</strong> - Time to analyze normalized code</li>
<li><strong>Attribution time</strong> - Time to compute attribution breakdown</li>
<li><strong>Total time</strong> - Complete multi-pass analysis duration</li>
<li><strong>Memory used</strong> - Estimated additional memory for two-pass analysis</li>
</ul>
<h3 id="performance-overhead-1"><a class="header" href="#performance-overhead-1">Performance Overhead</a></h3>
<p><strong>Target Overhead</strong>: ≤25% compared to single-pass analysis</p>
<p>Multi-pass analysis aims to add no more than 25% overhead versus standard single-pass analysis. If overhead exceeds this threshold, a warning is issued.</p>
<p><strong>Typical Overhead:</strong></p>
<ul>
<li>Attribution adds ~10-15% on average</li>
<li>Normalization adds ~5-10% on average</li>
<li>Total overhead usually 15-25%</li>
</ul>
<p><strong>Factors Affecting Performance:</strong></p>
<ul>
<li><strong>File size</strong> - Larger files take proportionally longer</li>
<li><strong>Complexity</strong> - More complex code requires more analysis time</li>
<li><strong>Language</strong> - Some languages (TypeScript) are slower to parse</li>
<li><strong>Parallel processing</strong> - Overhead is per-file, parallel reduces impact</li>
</ul>
<h3 id="optimization-tips-1"><a class="header" href="#optimization-tips-1">Optimization Tips</a></h3>
<p><strong>Disable Performance Tracking in Production</strong></p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>MultiPassOptions {
    performance_tracking: false,  // Reduces overhead slightly
    ..Default::default()
}
<span class="boring">}</span></code></pre>
<p><strong>Use Parallel Processing</strong></p>
<pre><code class="language-bash"># Parallel analysis amortizes overhead across cores
# Note: --jobs is a general debtmap flag controlling parallelism for all analysis
debtmap analyze . --multi-pass --jobs 8
</code></pre>
<p><strong>Target Specific Files</strong></p>
<pre><code class="language-bash"># Analyze only files that need detailed attribution
debtmap analyze src/complex_module.rs --multi-pass --attribution
</code></pre>
<h2 id="comparative-analysis"><a class="header" href="#comparative-analysis">Comparative Analysis</a></h2>
<p>Multi-pass analysis supports comparing code changes to validate refactoring efforts.</p>
<h3 id="basic-comparison"><a class="header" href="#basic-comparison">Basic Comparison</a></h3>
<p>The <code>compare_complexity</code> function is a standalone convenience function that performs complete multi-pass analysis on both code versions and returns the computed differences:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use debtmap::analysis::multi_pass::compare_complexity;
use debtmap::core::Language;

let before_code = r#"
fn process(items: Vec&lt;i32&gt;) -&gt; i32 {
    let mut sum = 0;
    for item in items {
        if item &gt; 0 {
            if item % 2 == 0 {
                sum += item * 2;
            } else {
                sum += item;
            }
        }
    }
    sum
}
"#;

let after_code = r#"
fn process(items: Vec&lt;i32&gt;) -&gt; i32 {
    items
        .into_iter()
        .filter(|&amp;item| item &gt; 0)
        .map(|item| if item % 2 == 0 { item * 2 } else { item })
        .sum()
}
"#;

let comparison = compare_complexity(before_code, after_code, Language::Rust)?;

println!("Complexity change: {}", comparison.complexity_change);
println!("Cognitive complexity change: {}", comparison.cognitive_change);
println!("Formatting impact change: {}", comparison.formatting_impact_change);
<span class="boring">}</span></code></pre>
<h3 id="comparison-results"><a class="header" href="#comparison-results">Comparison Results</a></h3>
<p>The <code>ComparativeAnalysis</code> struct contains the computed differences between before and after analyses:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct ComparativeAnalysis {
    pub complexity_change: i32,        // Negative = improvement
    pub cognitive_change: i32,         // Negative = improvement
    pub formatting_impact_change: f32, // Negative = less formatting noise
    pub improvements: Vec&lt;String&gt;,
    pub regressions: Vec&lt;String&gt;,
}
<span class="boring">}</span></code></pre>
<blockquote>
<p><strong>Note</strong>: The <code>compare_complexity</code> function performs both analyses internally and returns only the change metrics. To access the full before/after results, perform separate analyses using <code>MultiPassAnalyzer</code>.</p>
</blockquote>
<p><strong>Interpreting Changes:</strong></p>
<ul>
<li><strong>Negative complexity change</strong> - Refactoring reduced complexity ✓</li>
<li><strong>Positive complexity change</strong> - Refactoring increased complexity ✗</li>
<li><strong>Improvements</strong> - List of detected improvements (reduced nesting, extracted functions, etc.)</li>
<li><strong>Regressions</strong> - List of detected regressions (increased complexity, new anti-patterns, etc.)</li>
</ul>
<h3 id="example-output-4"><a class="header" href="#example-output-4">Example Output</a></h3>
<pre><code>Comparative Analysis
====================

Complexity Changes:
├─ Cyclomatic: 8 → 4 (-4, -50%)
├─ Cognitive: 12 → 5 (-7, -58.3%)
└─ Formatting Impact: 25% → 10% (-15%, -60%)

Improvements Detected:
✓ Reduced nesting depth (3 → 1)
✓ Eliminated mutable state
✓ Replaced imperative loop with functional chain
✓ Improved formatting consistency

No regressions detected.

Verdict: Refactoring reduced complexity by 50% and improved code clarity.
</code></pre>
<h2 id="configuration-options"><a class="header" href="#configuration-options">Configuration Options</a></h2>
<p>Configure multi-pass analysis programmatically:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use debtmap::analysis::multi_pass::{MultiPassAnalyzer, MultiPassOptions};
use debtmap::analysis::diagnostics::{DetailLevel, OutputFormat};
use debtmap::core::Language;

let options = MultiPassOptions {
    language: Language::Rust,
    detail_level: DetailLevel::Comprehensive,
    enable_recommendations: true,
    track_source_locations: true,
    generate_insights: true,
    output_format: OutputFormat::Json, // Also available: Yaml, Markdown, Html, Text
    performance_tracking: true,
};

let analyzer = MultiPassAnalyzer::new(options);
<span class="boring">}</span></code></pre>
<h3 id="configuration-fields"><a class="header" href="#configuration-fields">Configuration Fields</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Field</th><th>Type</th><th>Default</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>language</code></td><td><code>Language</code></td><td><code>Rust</code></td><td>Target programming language</td></tr>
<tr><td><code>detail_level</code></td><td><code>DetailLevel</code></td><td><code>Standard</code></td><td>Output detail: Summary, Standard, Comprehensive, Debug (CLI uses lowercase: <code>--detail-level standard</code>)</td></tr>
<tr><td><code>enable_recommendations</code></td><td><code>bool</code></td><td><code>true</code></td><td>Generate actionable recommendations</td></tr>
<tr><td><code>track_source_locations</code></td><td><code>bool</code></td><td><code>true</code></td><td>Include file/line/column in attribution</td></tr>
<tr><td><code>generate_insights</code></td><td><code>bool</code></td><td><code>true</code></td><td>Automatically generate insights</td></tr>
<tr><td><code>output_format</code></td><td><code>OutputFormat</code></td><td><code>Json</code></td><td>Output format: Json, Yaml, Markdown, Html, Text</td></tr>
<tr><td><code>performance_tracking</code></td><td><code>bool</code></td><td><code>false</code></td><td>Track and report performance metrics</td></tr>
</tbody>
</table>
</div>
<h2 id="use-cases-5"><a class="header" href="#use-cases-5">Use Cases</a></h2>
<h3 id="when-to-use-multi-pass-analysis-default"><a class="header" href="#when-to-use-multi-pass-analysis-default">When to Use Multi-Pass Analysis (Default)</a></h3>
<p>Multi-pass analysis is the default because it provides the most valuable insights:</p>
<p><strong>Refactoring Validation</strong></p>
<ul>
<li>Compare before/after complexity to validate refactoring</li>
<li>Ensure complexity actually decreased</li>
<li>Identify unintended complexity increases</li>
</ul>
<p><strong>Formatting Impact Assessment</strong></p>
<ul>
<li>Determine how much formatting affects your metrics</li>
<li>Justify automated formatting tool adoption</li>
<li>Identify formatting inconsistencies</li>
</ul>
<p><strong>Targeted Refactoring</strong></p>
<ul>
<li>Use attribution to find highest-impact refactoring targets</li>
<li>Focus on logical complexity, not formatting artifacts</li>
<li>Prioritize functions with actionable suggestions</li>
</ul>
<p><strong>Code Review</strong></p>
<ul>
<li>Provide objective complexity data in pull requests</li>
<li>Identify genuine complexity increases vs formatting changes</li>
<li>Guide refactoring discussions with data</li>
</ul>
<p><strong>Codebase Health Monitoring</strong></p>
<ul>
<li>Track logical complexity trends over time</li>
<li>Separate signal (logic) from noise (formatting)</li>
<li>Identify complexity hotspots for architectural review</li>
</ul>
<h3 id="when-to-disable-multi-pass-no-multi-pass"><a class="header" href="#when-to-disable-multi-pass-no-multi-pass">When to Disable Multi-Pass (–no-multi-pass)</a></h3>
<p>Use <code>--no-multi-pass</code> for single-pass analysis only when:</p>
<p><strong>Performance is Critical</strong></p>
<ul>
<li>Fast complexity checks during development</li>
<li>CI/CD gates where every second matters</li>
<li>Very large codebases (&gt;100k LOC) where overhead is significant</li>
</ul>
<p><strong>Simple Use Cases</strong></p>
<ul>
<li>When overall complexity trends are enough</li>
<li>No need for detailed attribution</li>
<li>Formatting is already standardized</li>
</ul>
<p><strong>Resource Constraints</strong></p>
<ul>
<li>Limited CPU or memory available</li>
<li>Running on CI infrastructure with strict time limits</li>
</ul>
<h2 id="future-enhancements-1"><a class="header" href="#future-enhancements-1">Future Enhancements</a></h2>
<h3 id="spec-84-detailed-ast-based-source-mapping"><a class="header" href="#spec-84-detailed-ast-based-source-mapping">Spec 84: Detailed AST-Based Source Mapping</a></h3>
<p>The current implementation uses estimated complexity locations based on function metrics. <a href="https://github.com/yourusername/debtmap/blob/master/specs/84-detailed-ast-source-mapping.md">Spec 84</a> will enhance attribution with precise AST-based source mapping:</p>
<p><strong>Planned Improvements:</strong></p>
<ul>
<li><strong>Exact AST node locations</strong> - Precise line, column, and span for each complexity point</li>
<li><strong>100% accurate mapping</strong> - No estimation, direct AST-to-source mapping</li>
<li><strong>IDE integration</strong> - Jump from complexity reports directly to source code</li>
<li><strong>Inline visualization</strong> - Show complexity heat maps in your editor</li>
<li><strong>Statement-level tracking</strong> - Complexity attribution at statement granularity</li>
</ul>
<p><strong>Current vs Future:</strong></p>
<p>Current (estimated):</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>ComplexityComponent {
    location: CodeLocation {
        line: 45,      // Function start line
        column: 0,     // Estimated
        span: None,    // Not available
    },
    description: "Function: process_request",
}
<span class="boring">}</span></code></pre>
<p>Future (precise):</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>ComplexityComponent {
    location: SourceLocation {
        line: 47,           // Exact if statement line
        column: 8,          // Exact column
        span: Some(47, 52), // Exact span of construct
        ast_path: "fn::process_request::body::if[0]",
    },
    description: "If condition: item.is_valid()",
}
<span class="boring">}</span></code></pre>
<p>This will enable:</p>
<ul>
<li>Click-to-navigate from reports to exact code locations</li>
<li>Visual Studio Code / IntelliJ integration for inline complexity display</li>
<li>More precise refactoring suggestions</li>
<li>Better complexity trend tracking at fine granularity</li>
</ul>
<h2 id="summary-7"><a class="header" href="#summary-7">Summary</a></h2>
<p>Multi-pass analysis (enabled by default) provides deep insights into your code’s complexity by:</p>
<ol>
<li><strong>Separating signal from noise</strong> - Distinguishing logical complexity from formatting artifacts</li>
<li><strong>Attributing complexity sources</strong> - Identifying what contributes to complexity and why</li>
<li><strong>Generating actionable insights</strong> - Providing specific refactoring recommendations</li>
<li><strong>Validating refactoring</strong> - Comparing before/after to prove complexity reduction</li>
<li><strong>Monitoring performance</strong> - Ensuring overhead stays within acceptable bounds</li>
</ol>
<p>Multi-pass analysis runs by default, providing the most valuable insights out of the box. The overhead (typically 15-25%) is worthwhile for understanding <em>why</em> code is complex and <em>how</em> to improve it.</p>
<p>For performance-critical scenarios or very large codebases, use <code>--no-multi-pass</code> to disable multi-pass analysis and run faster single-pass analysis instead. You can also use the <code>DEBTMAP_SINGLE_PASS=1</code> environment variable to disable multi-pass analysis globally.</p>
<hr>
<p><strong>See Also:</strong></p>
<ul>
<li><a href="#analysis-guide">Analysis Guide</a> - General analysis capabilities</li>
<li><a href="#scoring-strategies">Scoring Strategies</a> - How complexity affects debt scores</li>
<li><a href="#coverage-integration-1">Coverage Integration</a> - Combining complexity with coverage</li>
<li><a href="#examples-5">Examples</a> - Real-world multi-pass analysis examples</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="parallel-processing"><a class="header" href="#parallel-processing">Parallel Processing</a></h1>
<p>Debtmap leverages Rust’s powerful parallel processing capabilities to analyze large codebases efficiently. Built on Rayon for data parallelism and DashMap for lock-free concurrent data structures, debtmap achieves 10-100x faster performance than Java/Python-based competitors.</p>
<h2 id="overview-17"><a class="header" href="#overview-17">Overview</a></h2>
<p>Debtmap’s parallel processing architecture uses a three-phase approach:</p>
<ol>
<li><strong>Parallel File Parsing</strong> - Parse source files concurrently across all available CPU cores</li>
<li><strong>Parallel Multi-File Extraction</strong> - Extract call graphs from parsed files in parallel</li>
<li><strong>Parallel Enhanced Analysis</strong> - Analyze trait dispatch, function pointers, and framework patterns</li>
</ol>
<p>This parallel pipeline is controlled by CLI flags that let you tune performance for your environment.</p>
<h2 id="performance-characteristics-3"><a class="header" href="#performance-characteristics-3">Performance Characteristics</a></h2>
<p><strong>Typical analysis times:</strong></p>
<ul>
<li>Small project (1k-5k LOC): &lt;1 second</li>
<li>Medium project (10k-50k LOC): 2-8 seconds</li>
<li>Large project (100k-500k LOC): 10-45 seconds</li>
</ul>
<p><strong>Comparison with other tools (medium-sized Rust project, ~50k LOC):</strong></p>
<ul>
<li>SonarQube: 3-4 minutes</li>
<li>CodeClimate: 2-3 minutes</li>
<li>Debtmap: 5-8 seconds</li>
</ul>
<h2 id="cli-flags-for-parallelization"><a class="header" href="#cli-flags-for-parallelization">CLI Flags for Parallelization</a></h2>
<p>Debtmap provides two flags to control parallel processing behavior:</p>
<h3 id="jobs---j"><a class="header" href="#jobs---j">–jobs / -j</a></h3>
<p>Control the number of worker threads for parallel processing:</p>
<pre><code class="language-bash"># Use all available CPU cores (default)
debtmap analyze --jobs 0

# Limit to 4 threads
debtmap analyze --jobs 4
debtmap analyze -j 4
</code></pre>
<p><strong>Behavior:</strong></p>
<ul>
<li><code>--jobs 0</code> (default): Auto-detects available CPU cores using <code>std::thread::available_parallelism()</code>. Falls back to 4 threads if detection fails.</li>
<li><code>--jobs N</code>: Explicitly sets the thread pool to N threads.</li>
</ul>
<p><strong>When to use:</strong></p>
<ul>
<li>Use <code>--jobs 0</code> for maximum performance on developer workstations</li>
<li>Use <code>--jobs 1-4</code> in memory-constrained environments like CI/CD</li>
<li>Use <code>--jobs 1</code> for deterministic analysis order during debugging</li>
</ul>
<p><strong>Environment Variables:</strong></p>
<p>You can also set the default via environment variables:</p>
<p><strong><code>DEBTMAP_JOBS</code></strong> - Set the default thread count:</p>
<pre><code class="language-bash">export DEBTMAP_JOBS=4
debtmap analyze  # Uses 4 threads
</code></pre>
<p><strong><code>DEBTMAP_PARALLEL</code></strong> - Enable/disable parallel processing programmatically:</p>
<pre><code class="language-bash">export DEBTMAP_PARALLEL=true
debtmap analyze  # Parallel processing enabled

export DEBTMAP_PARALLEL=1
debtmap analyze  # Parallel processing enabled (also accepts '1')
</code></pre>
<p>The <code>DEBTMAP_PARALLEL</code> variable accepts <code>true</code> or <code>1</code> to enable parallel processing. This is useful for programmatic control in scripts or CI environments.</p>
<p>The CLI flags (<code>--jobs</code>, <code>--no-parallel</code>) take precedence over environment variables.</p>
<h3 id="no-parallel"><a class="header" href="#no-parallel">–no-parallel</a></h3>
<p>Disable parallel call graph construction entirely:</p>
<pre><code class="language-bash">debtmap analyze --no-parallel
</code></pre>
<p><strong>When to use:</strong></p>
<ul>
<li><strong>Debugging concurrency issues</strong>: Isolate whether a problem is parallelism-related</li>
<li><strong>Memory-constrained environments</strong>: Parallel processing increases memory usage</li>
<li><strong>Deterministic analysis</strong>: Ensures consistent ordering for reproducibility</li>
</ul>
<p><strong>Performance Impact:</strong></p>
<p>Disabling parallelization significantly increases analysis time:</p>
<ul>
<li>Small projects (&lt; 100 files): 2-3x slower</li>
<li>Medium projects (100-1000 files): 5-10x slower</li>
<li>Large projects (&gt; 1000 files): 10-50x slower</li>
</ul>
<p>For more details on both flags, see the <a href="#performance--caching">CLI Reference</a>.</p>
<h2 id="rayon-parallel-iterators"><a class="header" href="#rayon-parallel-iterators">Rayon Parallel Iterators</a></h2>
<p>Debtmap uses <a href="https://docs.rs/rayon">Rayon</a>, a data parallelism library for Rust, to parallelize file processing operations.</p>
<h3 id="thread-pool-configuration"><a class="header" href="#thread-pool-configuration">Thread Pool Configuration</a></h3>
<p>The global Rayon thread pool is configured at startup based on the <code>--jobs</code> parameter:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// From src/builders/parallel_call_graph.rs:48-53
if self.config.num_threads &gt; 0 {
    rayon::ThreadPoolBuilder::new()
        .num_threads(self.config.num_threads)
        .build_global()
        .ok(); // Ignore if already configured
}
<span class="boring">}</span></code></pre>
<p>This configures Rayon to use a specific number of worker threads for all parallel operations throughout the analysis.</p>
<h3 id="worker-thread-selection"><a class="header" href="#worker-thread-selection">Worker Thread Selection</a></h3>
<p>The <code>get_worker_count()</code> function determines how many threads to use:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// From src/main.rs:828-836
fn get_worker_count(jobs: usize) -&gt; usize {
    if jobs == 0 {
        std::thread::available_parallelism()
            .map(|n| n.get())
            .unwrap_or(4)  // Fallback if detection fails
    } else {
        jobs  // Use explicit value
    }
}
<span class="boring">}</span></code></pre>
<p><strong>Auto-detection behavior:</strong></p>
<ul>
<li>Queries the OS for available parallelism (CPU cores)</li>
<li>Respects cgroup limits in containers (Docker, Kubernetes)</li>
<li>Falls back to 4 threads if detection fails (rare)</li>
</ul>
<p><strong>Manual configuration:</strong></p>
<ul>
<li>Useful in shared environments (CI/CD, shared build servers)</li>
<li>Prevents resource contention with other processes</li>
<li>Enables reproducible benchmarking</li>
</ul>
<h3 id="parallel-file-processing"><a class="header" href="#parallel-file-processing">Parallel File Processing</a></h3>
<p><strong>Phase 1: Parallel File I/O and Sequential Parsing</strong></p>
<p>File reading is parallelized, but AST parsing is sequential due to <code>syn::File</code> not being <code>Send</code>:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// From src/builders/parallel_call_graph.rs:115-143
// Step 1: Read file contents in parallel (I/O bound)
let file_contents: Vec&lt;_&gt; = rust_files
    .par_iter()  // Parallel I/O operations
    .filter_map(|file_path| {
        let content = io::read_file(file_path).ok()?;
        Some((file_path.clone(), content))
    })
    .collect();

// Step 2: Parse files to AST (sequential - syn::File not Send)
let parsed_files: Vec&lt;_&gt; = file_contents
    .iter()
    .enumerate()
    .filter_map(|(idx, (file_path, content))| {
        let parsed = syn::parse_file(content).ok()?;
        parallel_graph.stats().increment_files();
        Some((file_path.clone(), parsed))
    })
    .collect();
<span class="boring">}</span></code></pre>
<p><strong>Key features:</strong></p>
<ul>
<li><strong>Parallel I/O</strong>: File reading uses <code>.par_iter()</code> to maximize disk throughput</li>
<li><strong>Sequential parsing</strong>: AST parsing is sequential because <code>syn::File</code> lacks <code>Send</code> bound</li>
<li><strong>Why this works</strong>: I/O operations dominate analysis time, so parallelizing file reads provides most of the speedup</li>
<li>Progress tracking uses atomic counters and unified progress system (see <a href="#parallel-call-graph-statistics">Parallel Call Graph Statistics</a>)</li>
</ul>
<p><strong>Phase 2: All-Files-At-Once Extraction</strong></p>
<p>All files are processed together without chunking to enable optimal cross-file call resolution:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// From src/builders/parallel_call_graph.rs:152-176
// Process ALL files at once (no chunking)
// This enables optimal cross-file call resolution with a single PathResolver
let files_for_extraction: Vec&lt;_&gt; = parsed_files
    .iter()
    .map(|(path, parsed)| (parsed.clone(), path.clone()))
    .collect();

// Extract call graph for all files with full cross-file resolution
// Internal implementation may use Rayon parallel iterators
let graph = extract_call_graph_multi_file(&amp;files_for_extraction);

// Merge into main graph
parallel_graph.merge_concurrent(graph);
<span class="boring">}</span></code></pre>
<p><strong>Design rationale:</strong></p>
<ul>
<li><strong>No chunking</strong>: All files processed together for complete visibility</li>
<li><strong>Optimal cross-file resolution</strong>: Single <code>PathResolver</code> sees all functions across entire codebase</li>
<li><strong>Internal parallelism</strong>: The <code>extract_call_graph_multi_file</code> function may parallelize internally using Rayon</li>
<li><strong>Simplified merging</strong>: One merge operation instead of per-chunk merges</li>
</ul>
<p><strong>AST Parsing Optimization (Spec 132)</strong></p>
<p>Prior to spec 132, files were parsed twice during call graph construction:</p>
<ol>
<li>Phase 1: Read files and store content as strings</li>
<li>Phase 2: <strong>Re-parse the same content</strong> to extract call graphs</li>
</ol>
<p>This redundant parsing was eliminated by parsing each file exactly once and reusing the parsed <code>syn::File</code> AST:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Optimized: Parse once in Phase 1
let parsed_files: Vec&lt;(PathBuf, syn::File)&gt; = rust_files
    .par_iter()
    .filter_map(|file_path| {
        let content = io::read_file(file_path).ok()?;
        let parsed = syn::parse_file(&amp;content).ok()?;  // Parse ONCE
        Some((file_path.clone(), parsed))
    })
    .collect();

// Phase 2: Reuse parsed ASTs (no re-parsing)
for chunk in parsed_files.chunks(chunk_size) {
    let chunk_for_extraction: Vec&lt;_&gt; = chunk
        .iter()
        .map(|(path, parsed)| (parsed.clone(), path.clone()))  // Clone AST
        .collect();
    // Extract call graph...
}
<span class="boring">}</span></code></pre>
<p><strong>Performance Impact:</strong></p>
<ul>
<li><strong>Before</strong>: 2N parse operations (404 files × 2 = 808 parses)</li>
<li><strong>After</strong>: N parse operations (404 files × 1 = 404 parses)</li>
<li><strong>Speedup</strong>: Cloning a parsed AST is <strong>44% faster</strong> than re-parsing</li>
<li><strong>Time saved</strong>: ~432ms per analysis run on 400-file projects</li>
<li><strong>Memory overhead</strong>: &lt;100MB for parsed AST storage</li>
</ul>
<p><strong>Why Clone Instead of Borrow?</strong></p>
<ul>
<li><code>syn::File</code> is not <code>Send + Sync</code> (cannot be shared across threads)</li>
<li>Call graph extraction requires owned AST values</li>
<li>Cloning is still significantly faster than re-parsing (1.33ms vs 2.40ms per file)</li>
</ul>
<p>See <code>docs/spec-132-benchmark-results.md</code> for detailed benchmarks validating these improvements.</p>
<p><strong>Phase 3: Enhanced Analysis</strong></p>
<p>The third phase analyzes trait dispatch, function pointers, and framework patterns. This phase is currently sequential due to complex shared state requirements, but benefits from the parallel foundation built in phases 1-2.</p>
<h3 id="parallel-architecture"><a class="header" href="#parallel-architecture">Parallel Architecture</a></h3>
<p>Debtmap processes files in parallel using Rayon’s parallel iterators:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>files.par_iter()
    .map(|file| analyze_file(file))
    .collect()
<span class="boring">}</span></code></pre>
<p>Each file is:</p>
<ol>
<li>Parsed independently</li>
<li>Analyzed for complexity</li>
<li>Scored and prioritized</li>
</ol>
<h2 id="dashmap-for-lock-free-concurrency"><a class="header" href="#dashmap-for-lock-free-concurrency">DashMap for Lock-Free Concurrency</a></h2>
<p>Debtmap uses <a href="https://docs.rs/dashmap">DashMap</a>, a concurrent hash map implementation, for lock-free data structures during parallel call graph construction.</p>
<h3 id="why-dashmap"><a class="header" href="#why-dashmap">Why DashMap?</a></h3>
<p>Traditional approaches to concurrent hash maps use a single <code>Mutex&lt;HashMap&gt;</code>, which creates contention:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ❌ Traditional approach - serializes all access
let map = Arc&lt;Mutex&lt;HashMap&lt;K, V&gt;&gt;&gt;;

// Thread 1 blocks Thread 2, even for reads
let val = map.lock().unwrap().get(&amp;key);
<span class="boring">}</span></code></pre>
<p>DashMap provides <strong>lock-free reads</strong> and <strong>fine-grained write locking</strong> through internal sharding:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// ✅ DashMap approach - concurrent reads, fine-grained writes
let map = Arc&lt;DashMap&lt;K, V&gt;&gt;;

// Multiple threads can read concurrently without blocking
let val = map.get(&amp;key);

// Writes only lock the specific shard, not the whole map
map.insert(key, value);
<span class="boring">}</span></code></pre>
<h3 id="parallelcallgraph-implementation"><a class="header" href="#parallelcallgraph-implementation">ParallelCallGraph Implementation</a></h3>
<p>The <code>ParallelCallGraph</code> uses DashMap for all concurrent data structures:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// From src/priority/parallel_call_graph.rs:50-56
pub struct ParallelCallGraph {
    nodes: Arc&lt;DashMap&lt;FunctionId, NodeInfo&gt;&gt;,      // Functions
    edges: Arc&lt;DashSet&lt;FunctionCall&gt;&gt;,              // Calls
    caller_index: Arc&lt;DashMap&lt;FunctionId, DashSet&lt;FunctionId&gt;&gt;&gt;,  // Who calls this?
    callee_index: Arc&lt;DashMap&lt;FunctionId, DashSet&lt;FunctionId&gt;&gt;&gt;,  // Who does this call?
    stats: Arc&lt;ParallelStats&gt;,                      // Atomic counters
}
<span class="boring">}</span></code></pre>
<p><strong>Key components:</strong></p>
<ol>
<li><strong>nodes</strong>: Maps function identifiers to metadata (complexity, lines, flags)</li>
<li><strong>edges</strong>: Set of all function calls (deduplicated automatically)</li>
<li><strong>caller_index</strong>: Reverse index for “who calls this function?”</li>
<li><strong>callee_index</strong>: Forward index for “what does this function call?”</li>
<li><strong>stats</strong>: Atomic counters for progress tracking</li>
</ol>
<h3 id="concurrent-operations"><a class="header" href="#concurrent-operations">Concurrent Operations</a></h3>
<p><strong>Adding Functions Concurrently</strong></p>
<p>Multiple analyzer threads can add functions simultaneously:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// From src/priority/parallel_call_graph.rs:79-96
pub fn add_function(
    &amp;self,
    id: FunctionId,
    is_entry_point: bool,
    is_test: bool,
    complexity: u32,
    lines: usize,
) {
    let node_info = NodeInfo {
        id: id.clone(),
        is_entry_point,
        is_test,
        complexity,
        lines,
    };
    self.nodes.insert(id, node_info);
    self.stats.add_nodes(1);  // Atomic increment
}
<span class="boring">}</span></code></pre>
<p><strong>Atomicity guarantees:</strong></p>
<ul>
<li><code>DashMap::insert()</code> is atomic - no data races</li>
<li><code>AtomicUsize</code> counters can be incremented from multiple threads safely</li>
<li>No locks required for reading existing nodes</li>
</ul>
<p><strong>Adding Calls Concurrently</strong></p>
<p>Function calls are added with automatic deduplication:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// From src/priority/parallel_call_graph.rs:99-117
pub fn add_call(&amp;self, caller: FunctionId, callee: FunctionId, call_type: CallType) {
    let call = FunctionCall {
        caller: caller.clone(),
        callee: callee.clone(),
        call_type,
    };

    if self.edges.insert(call) {  // DashSet deduplicates automatically
        // Update indices concurrently
        self.caller_index
            .entry(caller.clone())
            .or_default()
            .insert(callee.clone());

        self.callee_index.entry(callee).or_default().insert(caller);

        self.stats.add_edges(1);  // Only increment if actually inserted
    }
}
<span class="boring">}</span></code></pre>
<p><strong>Deduplication:</strong></p>
<ul>
<li><code>DashSet::insert()</code> returns <code>true</code> only for new items</li>
<li>Duplicate calls from multiple threads are safely ignored</li>
<li>Indices are updated atomically using <code>entry()</code> API</li>
</ul>
<h3 id="shared-read-only-data"><a class="header" href="#shared-read-only-data">Shared Read-Only Data</a></h3>
<p>Analysis configuration and indexes are shared across threads:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let coverage_index = Arc::new(build_coverage_index());

// All threads share the same index
files.par_iter()
    .map(|file| analyze_with_coverage(file, &amp;coverage_index))
<span class="boring">}</span></code></pre>
<h3 id="memory-overhead"><a class="header" href="#memory-overhead">Memory Overhead</a></h3>
<p>DashMap uses internal sharding for parallelism, which has a memory overhead:</p>
<ul>
<li><strong>DashMap overhead</strong>: ~2x the memory of a regular <code>HashMap</code> due to sharding</li>
<li><strong>DashSet overhead</strong>: Similar to DashMap</li>
<li><strong>Benefit</strong>: Enables concurrent access without contention</li>
<li><strong>Trade-off</strong>: Debtmap prioritizes speed over memory for large codebases</li>
</ul>
<p>For memory-constrained environments, use <code>--jobs 2-4</code> or <code>--no-parallel</code> to reduce parallel overhead.</p>
<h2 id="parallel-call-graph-statistics"><a class="header" href="#parallel-call-graph-statistics">Parallel Call Graph Statistics</a></h2>
<p>Debtmap tracks parallel processing progress using atomic counters that can be safely updated from multiple threads. These statistics are integrated with a unified progress system that provides consolidated reporting across all analysis phases.</p>
<h3 id="unified-progress-system-integration"><a class="header" href="#unified-progress-system-integration">Unified Progress System Integration</a></h3>
<p>Parallel statistics now integrate with <code>crate::io::progress::AnalysisProgress</code> (src/builders/parallel_call_graph.rs:134-139), which replaced older per-phase progress bars. This provides:</p>
<ul>
<li><strong>Consolidated progress reporting</strong>: Single progress view showing “3/4 Building call graph”</li>
<li><strong>Cross-phase coordination</strong>: Progress updates coordinated across parsing, extraction, and analysis phases</li>
<li><strong>Reduced visual clutter</strong>: Replaces multiple progress bars with unified display</li>
</ul>
<h3 id="parallelstats-structure"><a class="header" href="#parallelstats-structure">ParallelStats Structure</a></h3>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// From src/priority/parallel_call_graph.rs:7-14
pub struct ParallelStats {
    pub total_nodes: AtomicUsize,      // Functions processed
    pub total_edges: AtomicUsize,      // Calls discovered
    pub files_processed: AtomicUsize,  // Files completed
    pub total_files: AtomicUsize,      // Total files to process
}
<span class="boring">}</span></code></pre>
<p><strong>Atomic operations:</strong></p>
<ul>
<li><code>fetch_add()</code> - Atomically increment counters from any thread</li>
<li><code>load()</code> - Read current value without blocking</li>
<li><code>Ordering::Relaxed</code> - Sufficient for statistics (no synchronization needed)</li>
</ul>
<h3 id="progress-tracking"><a class="header" href="#progress-tracking">Progress Tracking</a></h3>
<p>Progress ratio calculation for long-running analysis:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// From src/priority/parallel_call_graph.rs:38-46
pub fn progress_ratio(&amp;self) -&gt; f64 {
    let processed = self.files_processed.load(Ordering::Relaxed) as f64;
    let total = self.total_files.load(Ordering::Relaxed) as f64;
    if total &gt; 0.0 {
        processed / total
    } else {
        0.0
    }
}
<span class="boring">}</span></code></pre>
<p>This enables progress callbacks during analysis:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// From src/builders/parallel_call_graph.rs:110-121
parallel_graph.stats().increment_files();
if let Some(ref callback) = self.config.progress_callback {
    let processed = parallel_graph
        .stats()
        .files_processed
        .load(std::sync::atomic::Ordering::Relaxed);
    let total = parallel_graph
        .stats()
        .total_files
        .load(std::sync::atomic::Ordering::Relaxed);
    callback(processed, total);
}
<span class="boring">}</span></code></pre>
<h3 id="log-output-format"><a class="header" href="#log-output-format">Log Output Format</a></h3>
<p>After analysis completes, debtmap reports final statistics:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// From src/builders/parallel_call_graph.rs:85-93
log::info!(
    "Parallel call graph complete: {} nodes, {} edges, {} files processed",
    stats.total_nodes.load(std::sync::atomic::Ordering::Relaxed),
    stats.total_edges.load(std::sync::atomic::Ordering::Relaxed),
    stats
        .files_processed
        .load(std::sync::atomic::Ordering::Relaxed),
);
<span class="boring">}</span></code></pre>
<p><strong>Example output:</strong></p>
<pre><code>INFO - Processing 1247 Rust files in parallel
INFO - Progress: 100/1247 files processed
INFO - Progress: 500/1247 files processed
INFO - Progress: 1000/1247 files processed
INFO - Parallel call graph complete: 8942 nodes, 23451 edges, 1247 files processed
</code></pre>
<h2 id="cross-file-call-resolution"><a class="header" href="#cross-file-call-resolution">Cross-File Call Resolution</a></h2>
<p>Debtmap uses a two-phase parallel resolution approach for resolving cross-file function calls, achieving 10-15% faster call graph construction on multi-core systems.</p>
<h3 id="two-phase-architecture"><a class="header" href="#two-phase-architecture">Two-Phase Architecture</a></h3>
<p><strong>Phase 1: Parallel Resolution (Read-Only)</strong></p>
<p>The first phase processes unresolved calls concurrently using Rayon’s parallel iterators:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// From src/priority/call_graph/cross_file.rs
let resolutions: Vec&lt;(FunctionCall, FunctionId)&gt; = calls_to_resolve
    .par_iter()  // Parallel iteration
    .filter_map(|call| {
        // Pure function - safe for parallel execution
        Self::resolve_call_with_advanced_matching(
            &amp;all_functions,
            &amp;call.callee.name,
            &amp;call.caller.file,
        ).map(|resolved_callee| {
            (call.clone(), resolved_callee)
        })
    })
    .collect();
<span class="boring">}</span></code></pre>
<p><strong>Key benefits:</strong></p>
<ul>
<li><strong>Pure functional resolution</strong>: No side effects, safe for concurrent execution</li>
<li><strong>Immutable data</strong>: All inputs are read-only during the parallel phase</li>
<li><strong>Independent operations</strong>: Each call resolution is independent of others</li>
<li><strong>Parallel efficiency</strong>: Utilizes all available CPU cores</li>
<li><strong>Sophisticated matching</strong>: <code>resolve_call_with_advanced_matching</code> delegates to <code>CallResolver</code> (src/analyzers/call_graph/call_resolution.rs:44-58) which handles associated functions, qualified paths, and type hints</li>
</ul>
<p><strong>Phase 2: Sequential Updates (Mutation)</strong></p>
<p>The second phase applies all resolutions to the graph sequentially:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Apply resolutions to graph in sequence
for (original_call, resolved_callee) in resolutions {
    self.apply_call_resolution(&amp;original_call, &amp;resolved_callee);
}
<span class="boring">}</span></code></pre>
<p><strong>Key benefits:</strong></p>
<ul>
<li><strong>Batch updates</strong>: All resolutions processed together</li>
<li><strong>Data consistency</strong>: Sequential updates maintain index synchronization</li>
<li><strong>Deterministic</strong>: Same results regardless of parallel execution order</li>
</ul>
<h3 id="performance-impact"><a class="header" href="#performance-impact">Performance Impact</a></h3>
<p>The two-phase approach provides significant speedups on multi-core systems:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>CPU Cores</th><th>Speedup</th><th>Example Time (1500 calls)</th></tr>
</thead>
<tbody>
<tr><td>1</td><td>0%</td><td>100ms (baseline)</td></tr>
<tr><td>2</td><td>~8%</td><td>92ms</td></tr>
<tr><td>4</td><td>~12%</td><td>88ms</td></tr>
<tr><td>8</td><td>~15%</td><td>85ms</td></tr>
</tbody>
</table>
</div>
<p><strong>Performance characteristics:</strong></p>
<ul>
<li><strong>Best case</strong>: 10-15% reduction in call graph construction time</li>
<li><strong>Scaling</strong>: Diminishing returns beyond 8 cores due to batching overhead</li>
<li><strong>Memory overhead</strong>: &lt;10MB for resolutions vector, even for large projects</li>
</ul>
<h3 id="thread-safety"><a class="header" href="#thread-safety">Thread Safety</a></h3>
<p>The parallel resolution phase is thread-safe without locks because:</p>
<ol>
<li><strong>Pure resolution logic</strong>: <code>resolve_call_with_advanced_matching()</code> is a static method with no side effects</li>
<li><strong>Immutable inputs</strong>: All function data is read-only during parallel phase</li>
<li><strong>Independent resolutions</strong>: No dependencies between different call resolutions</li>
<li><strong>Safe collection</strong>: Rayon handles thread synchronization for result collection</li>
</ol>
<p>The sequential update phase requires no synchronization since it runs single-threaded.</p>
<h3 id="memory-efficiency"><a class="header" href="#memory-efficiency">Memory Efficiency</a></h3>
<p><strong>Resolutions vector overhead:</strong></p>
<ul>
<li>Per-resolution size: ~200 bytes (FunctionCall + FunctionId)</li>
<li>For 1000 resolutions: ~200KB</li>
<li>For 2000 resolutions: ~400KB</li>
<li>Maximum overhead: &lt;10MB even for very large projects</li>
</ul>
<p><strong>Total memory footprint:</strong></p>
<pre><code>Total Memory = Base Graph + Resolutions Vector
             ≈ 5-10MB + 0.2-0.4MB
             ≈ 5-10MB (negligible overhead)
</code></pre>
<h3 id="integration-with-call-graph-construction"><a class="header" href="#integration-with-call-graph-construction">Integration with Call Graph Construction</a></h3>
<p>The two-phase resolution integrates seamlessly into the existing call graph construction pipeline:</p>
<pre><code>File Parsing (Parallel)
    ↓
Function Extraction (Parallel)
    ↓
Build Initial Call Graph
    ↓
[NEW] Parallel Cross-File Resolution
    ├─ Phase 1: Parallel resolution → collect resolutions
    └─ Phase 2: Sequential updates → apply to graph
    ↓
Call Graph Complete
</code></pre>
<h3 id="configuration-13"><a class="header" href="#configuration-13">Configuration</a></h3>
<p>Cross-file resolution respects the <code>--jobs</code> flag for thread pool sizing:</p>
<pre><code class="language-bash"># Use all cores for maximum speedup
debtmap analyze --jobs 0

# Limit to 4 threads
debtmap analyze --jobs 4

# Disable parallelism (debugging)
debtmap analyze --no-parallel
</code></pre>
<p>The <code>--no-parallel</code> flag disables parallel call graph construction entirely, including cross-file resolution parallelization.</p>
<h3 id="debugging"><a class="header" href="#debugging">Debugging</a></h3>
<p>To verify parallel resolution is working:</p>
<pre><code class="language-bash"># Enable verbose logging
debtmap analyze -vv

# Look for messages like:
# "Resolving 1523 cross-file calls in parallel"
# "Parallel resolution complete: 1423 resolved in 87ms"
</code></pre>
<p>To compare parallel vs sequential performance:</p>
<pre><code class="language-bash"># Parallel (default)
time debtmap analyze .

# Sequential (for comparison)
time debtmap analyze . --no-parallel
</code></pre>
<p>Expected difference: 10-15% faster with parallel resolution on 4-8 core systems.</p>
<h2 id="concurrent-merging"><a class="header" href="#concurrent-merging">Concurrent Merging</a></h2>
<p>The <code>merge_concurrent()</code> method combines call graphs from different analysis phases using parallel iteration.</p>
<h3 id="implementation"><a class="header" href="#implementation">Implementation</a></h3>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// From src/priority/parallel_call_graph.rs:120-138
pub fn merge_concurrent(&amp;self, other: CallGraph) {
    // Parallelize node merging
    let nodes_vec: Vec&lt;_&gt; = other.get_all_functions().collect();
    nodes_vec.par_iter().for_each(|func_id| {
        if let Some((is_entry, is_test, complexity, lines)) = other.get_function_info(func_id) {
            self.add_function((*func_id).clone(), is_entry, is_test, complexity, lines);
        }
    });

    // Parallelize edge merging
    let calls_vec: Vec&lt;_&gt; = other.get_all_calls();
    calls_vec.par_iter().for_each(|call| {
        self.add_call(
            call.caller.clone(),
            call.callee.clone(),
            call.call_type.clone(),
        );
    });
}
<span class="boring">}</span></code></pre>
<p><strong>How it works:</strong></p>
<ol>
<li>Extract all nodes and edges from the source <code>CallGraph</code></li>
<li>Use <code>par_iter()</code> to merge nodes in parallel</li>
<li>Use <code>par_iter()</code> to merge edges in parallel</li>
<li>DashMap/DashSet automatically handle concurrent insertions</li>
</ol>
<h3 id="converting-between-representations"><a class="header" href="#converting-between-representations">Converting Between Representations</a></h3>
<p>Debtmap uses two call graph representations:</p>
<ul>
<li><strong>ParallelCallGraph</strong>: Concurrent data structures (DashMap/DashSet) for parallel construction</li>
<li><strong>CallGraph</strong>: Sequential data structures (HashMap/HashSet) for analysis algorithms</li>
</ul>
<p>Conversion happens at phase boundaries:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// From src/priority/parallel_call_graph.rs:141-162
pub fn to_call_graph(&amp;self) -&gt; CallGraph {
    let mut call_graph = CallGraph::new();

    // Add all nodes
    for entry in self.nodes.iter() {
        let node = entry.value();
        call_graph.add_function(
            node.id.clone(),
            node.is_entry_point,
            node.is_test,
            node.complexity,
            node.lines,
        );
    }

    // Add all edges
    for call in self.edges.iter() {
        call_graph.add_call(call.clone());
    }

    call_graph
}
<span class="boring">}</span></code></pre>
<p><strong>Why two representations?</strong></p>
<ul>
<li><strong>ParallelCallGraph</strong>: Optimized for concurrent writes during construction</li>
<li><strong>CallGraph</strong>: Optimized for graph algorithms (PageRank, connectivity, transitive reduction)</li>
<li>Conversion overhead is negligible compared to analysis time</li>
</ul>
<h2 id="coverage-index-optimization"><a class="header" href="#coverage-index-optimization">Coverage Index Optimization</a></h2>
<p>Debtmap uses an optimized nested HashMap structure for coverage data lookups, providing significant performance improvements for coverage-enabled analysis.</p>
<h3 id="nested-hashmap-architecture"><a class="header" href="#nested-hashmap-architecture">Nested HashMap Architecture</a></h3>
<p>The <code>CoverageIndex</code> structure uses a two-level nested HashMap instead of a flat structure:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Optimized structure (nested)
pub struct CoverageIndex {
    /// Outer map: file path → inner map of functions
    by_file: HashMap&lt;PathBuf, HashMap&lt;String, FunctionCoverage&gt;&gt;,

    /// Line-based index for range queries
    by_line: HashMap&lt;PathBuf, BTreeMap&lt;usize, FunctionCoverage&gt;&gt;,

    /// Pre-computed file paths for efficient iteration
    file_paths: Vec&lt;PathBuf&gt;,
}

// OLD structure (flat) - no longer used
HashMap&lt;(PathBuf, String), FunctionCoverage&gt;
<span class="boring">}</span></code></pre>
<h3 id="performance-characteristics-1-1"><a class="header" href="#performance-characteristics-1-1">Performance Characteristics</a></h3>
<p>The nested structure provides dramatic performance improvements:</p>
<p><strong>Lookup Complexity:</strong></p>
<ul>
<li><strong>Exact match</strong>: O(1) file hash + O(1) function hash</li>
<li><strong>Path strategies</strong>: O(files) instead of O(functions)</li>
<li><strong>Line-based</strong>: O(log functions_in_file) binary search</li>
</ul>
<p><strong>Real-World Performance:</strong></p>
<ul>
<li>Exact match lookups: ~100 nanoseconds</li>
<li>Path matching fallback: ~10 microseconds (375 file checks vs 1,500 function checks)</li>
<li>Overall speedup: <strong>50-100x faster</strong> coverage lookups</li>
</ul>
<h3 id="why-this-matters-1"><a class="header" href="#why-this-matters-1">Why This Matters</a></h3>
<p>When analyzing a typical Rust project with coverage enabled:</p>
<ul>
<li><strong>Function count</strong>: ~1,500 functions (after demangling)</li>
<li><strong>File count</strong>: ~375 files</li>
<li><strong>Lookups per analysis</strong>: ~19,600</li>
<li><strong>Average functions per file</strong>: ~4</li>
</ul>
<p><strong>OLD flat structure (O(n) scans):</strong></p>
<ul>
<li>19,600 lookups × 4,500 comparisons = 88 million operations</li>
<li>Estimated time: ~1 minute</li>
</ul>
<p><strong>NEW nested structure (O(1) lookups):</strong></p>
<ul>
<li>19,600 lookups × 1-3 operations = ~60,000 operations</li>
<li>Estimated time: ~3 seconds</li>
</ul>
<p><strong>Speedup</strong>: ~20x faster just from index structure optimization</p>
<h3 id="combined-with-function-demangling"><a class="header" href="#combined-with-function-demangling">Combined with Function Demangling</a></h3>
<p>This optimization works synergistically with LLVM coverage function name demangling (Spec 134):</p>
<p><strong>Original (no demangling, flat structure):</strong></p>
<ul>
<li>18,631 mangled functions</li>
<li>O(n) linear scans</li>
<li>Total time: 10+ minutes</li>
</ul>
<p><strong>After demangling (Spec 134):</strong></p>
<ul>
<li>1,500 demangled functions</li>
<li>O(n) linear scans (still)</li>
<li>Total time: ~1 minute</li>
</ul>
<p><strong>After nested structure (Spec 135):</strong></p>
<ul>
<li>1,500 demangled functions</li>
<li>O(1) hash lookups</li>
<li>Total time: ~3 seconds</li>
</ul>
<p><strong>Combined speedup: ~50,000x</strong> (10+ minutes → 3 seconds)</p>
<h3 id="implementation-details"><a class="header" href="#implementation-details">Implementation Details</a></h3>
<p><strong>Exact Match Lookup (O(1)):</strong></p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub fn get_function_coverage(&amp;self, file: &amp;Path, function_name: &amp;str) -&gt; Option&lt;f64&gt; {
    // Two O(1) hash lookups
    if let Some(file_functions) = self.by_file.get(file) {
        if let Some(coverage) = file_functions.get(function_name) {
            return Some(coverage.coverage_percentage / 100.0);
        }
    }
    // Fallback to path strategies (rare)
    self.find_by_path_strategies(file, function_name)
}
<span class="boring">}</span></code></pre>
<p><strong>Path Strategy Fallback (O(files)):</strong></p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn find_by_path_strategies(&amp;self, query_path: &amp;Path, function_name: &amp;str) -&gt; Option&lt;f64&gt; {
    // Iterate over FILES not FUNCTIONS (375 vs 1,500 = 4x faster)
    for file_path in &amp;self.file_paths {
        if query_path.ends_with(file_path) {
            // O(1) lookup once we find the right file
            if let Some(file_functions) = self.by_file.get(file_path) {
                if let Some(coverage) = file_functions.get(function_name) {
                    return Some(coverage.coverage_percentage / 100.0);
                }
            }
        }
    }
    None
}
<span class="boring">}</span></code></pre>
<h3 id="memory-overhead-1"><a class="header" href="#memory-overhead-1">Memory Overhead</a></h3>
<p>The nested structure has minimal memory overhead:</p>
<p><strong>Flat structure:</strong></p>
<ul>
<li>1,500 entries × ~200 bytes = 300KB</li>
</ul>
<p><strong>Nested structure:</strong></p>
<ul>
<li>Outer HashMap: 375 entries × ~50 bytes = 18.75KB</li>
<li>Inner HashMaps: 375 × ~4 functions × ~200 bytes = 300KB</li>
<li>File paths vector: 375 × ~100 bytes = 37.5KB</li>
<li><strong>Total: ~356KB</strong></li>
</ul>
<p><strong>Memory increase: ~56KB (18%)</strong> - negligible cost for 50-100x speedup</p>
<h3 id="benchmarking-coverage-performance"><a class="header" href="#benchmarking-coverage-performance">Benchmarking Coverage Performance</a></h3>
<p>Debtmap includes benchmarks to validate coverage index performance:</p>
<pre><code class="language-bash"># Run coverage performance benchmarks
cargo bench --bench coverage_performance

# Compare old flat structure vs new nested structure
# Expected results:
#   old_flat_structure:    450ms
#   new_nested_structure:  8ms
#   Speedup: ~56x
</code></pre>
<p>The <code>flat_vs_nested_comparison</code> benchmark simulates the old O(n) scan behavior and compares it with the new nested structure, demonstrating the 50-100x improvement.</p>
<h3 id="impact-on-analysis-time"><a class="header" href="#impact-on-analysis-time">Impact on Analysis Time</a></h3>
<p>Coverage lookups are now negligible overhead:</p>
<p><strong>Without coverage optimization:</strong></p>
<ul>
<li>Analysis overhead from coverage: ~1 minute</li>
<li>Percentage of total time: 60-80%</li>
</ul>
<p><strong>With coverage optimization:</strong></p>
<ul>
<li>Analysis overhead from coverage: ~3 seconds</li>
<li>Percentage of total time: 5-10%</li>
</ul>
<p>This makes coverage-enabled analysis practical for CI/CD pipelines and real-time feedback during development.</p>
<h2 id="performance-tuning-3"><a class="header" href="#performance-tuning-3">Performance Tuning</a></h2>
<h3 id="optimal-thread-count"><a class="header" href="#optimal-thread-count">Optimal Thread Count</a></h3>
<p><strong>General rule:</strong> Use physical core count, not logical cores.</p>
<pre><code class="language-bash"># Check physical core count
lscpu | grep "Core(s) per socket"

# macOS
sysctl hw.physicalcpu
</code></pre>
<p><strong>Recommended settings:</strong></p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>System</th><th>Cores</th><th>Recommended –jobs</th></tr>
</thead>
<tbody>
<tr><td>Laptop</td><td>4</td><td>Default or 4</td></tr>
<tr><td>Desktop</td><td>8</td><td>Default</td></tr>
<tr><td>Workstation</td><td>16+</td><td>Default</td></tr>
<tr><td>CI/CD</td><td>Varies</td><td>2-4 (shared resources)</td></tr>
</tbody>
</table>
</div>
<h3 id="memory-considerations"><a class="header" href="#memory-considerations">Memory Considerations</a></h3>
<p>Each thread requires memory for:</p>
<ul>
<li>AST parsing (~1-5 MB per file)</li>
<li>Analysis state (~500 KB per file)</li>
<li>Temporary buffers</li>
</ul>
<p><strong>Memory usage estimate:</strong></p>
<pre><code>Total Memory ≈ (Thread Count) × (Average File Size) × 2-3
</code></pre>
<p><strong>Example (50 files, average 10 KB each, 8 threads):</strong></p>
<pre><code>Memory ≈ 8 × 10 KB × 3 = 240 KB (negligible)
</code></pre>
<p>For very large files (&gt;1 MB), consider reducing thread count.</p>
<h3 id="memory-vs-speed-tradeoffs"><a class="header" href="#memory-vs-speed-tradeoffs">Memory vs Speed Tradeoffs</a></h3>
<p>Parallel processing uses more memory:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Configuration</th><th>Memory Overhead</th><th>Speed Benefit</th></tr>
</thead>
<tbody>
<tr><td><code>--no-parallel</code></td><td>Baseline</td><td>Baseline</td></tr>
<tr><td><code>--jobs 1</code></td><td>+10% (data structures)</td><td>1x</td></tr>
<tr><td><code>--jobs 4</code></td><td>+30% (+ worker buffers)</td><td>4-6x</td></tr>
<tr><td><code>--jobs 8</code></td><td>+50% (+ worker buffers)</td><td>6-10x</td></tr>
<tr><td><code>--jobs 16</code></td><td>+80% (+ worker buffers)</td><td>10-15x</td></tr>
</tbody>
</table>
</div>
<p><strong>Memory overhead sources:</strong></p>
<ul>
<li>DashMap internal sharding (~2x HashMap)</li>
<li>Per-worker thread stacks and buffers</li>
<li>Parallel iterator intermediates</li>
</ul>
<h3 id="io-bound-vs-cpu-bound"><a class="header" href="#io-bound-vs-cpu-bound">I/O Bound vs CPU Bound</a></h3>
<p><strong>CPU-bound analysis (default):</strong></p>
<ul>
<li>Complexity calculations</li>
<li>Pattern detection</li>
<li>Risk scoring</li>
</ul>
<p>Parallel processing provides 4-8x speedup.</p>
<p><strong>I/O-bound operations:</strong></p>
<ul>
<li>Reading files from disk</li>
<li>Loading coverage data</li>
</ul>
<p>Limited speedup from parallelism (1.5-2x).</p>
<p><strong>If analysis is I/O-bound:</strong></p>
<ol>
<li>Use SSD storage</li>
<li>Reduce thread count (less I/O contention)</li>
<li>Use <code>--max-files</code> to limit scope</li>
</ol>
<h2 id="scaling-strategies"><a class="header" href="#scaling-strategies">Scaling Strategies</a></h2>
<h3 id="small-projects-10k-loc"><a class="header" href="#small-projects-10k-loc">Small Projects (&lt;10k LOC)</a></h3>
<pre><code class="language-bash"># Default settings are fine
debtmap analyze .
</code></pre>
<p>Parallel overhead may exceed benefits. Consider <code>--no-parallel</code> if analysis is &lt;1 second.</p>
<h3 id="medium-projects-10k-100k-loc"><a class="header" href="#medium-projects-10k-100k-loc">Medium Projects (10k-100k LOC)</a></h3>
<pre><code class="language-bash"># Use all cores
debtmap analyze .
</code></pre>
<p>Optimal parallel efficiency. Expect 4-8x speedup from parallelism.</p>
<h3 id="large-projects-100k-loc"><a class="header" href="#large-projects-100k-loc">Large Projects (&gt;100k LOC)</a></h3>
<pre><code class="language-bash"># Use all cores
debtmap analyze . --jobs 0  # 0 = all cores
</code></pre>
<p>Maximize parallel processing for large codebases.</p>
<h3 id="cicd-environments"><a class="header" href="#cicd-environments">CI/CD Environments</a></h3>
<pre><code class="language-bash"># Limit threads to avoid resource contention
debtmap analyze . --jobs 2
</code></pre>
<p>CI environments often limit CPU cores per job.</p>
<h3 id="scaling-behavior"><a class="header" href="#scaling-behavior">Scaling Behavior</a></h3>
<p>Debtmap’s parallel processing scales with CPU core count:</p>
<p><strong>Strong Scaling (Fixed Problem Size):</strong></p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>CPU Cores</th><th>Speedup</th><th>Efficiency</th></tr>
</thead>
<tbody>
<tr><td>1</td><td>1x</td><td>100%</td></tr>
<tr><td>2</td><td>1.8x</td><td>90%</td></tr>
<tr><td>4</td><td>3.4x</td><td>85%</td></tr>
<tr><td>8</td><td>6.2x</td><td>78%</td></tr>
<tr><td>16</td><td>10.5x</td><td>66%</td></tr>
<tr><td>32</td><td>16.8x</td><td>53%</td></tr>
</tbody>
</table>
</div>
<p>Efficiency decreases at higher core counts due to:</p>
<ul>
<li>Synchronization overhead (atomic operations, DashMap locking)</li>
<li>Memory bandwidth saturation</li>
<li>Diminishing returns from Amdahl’s law (sequential portions)</li>
</ul>
<p><strong>Weak Scaling (Problem Size Grows with Cores):</strong></p>
<p>Debtmap maintains high efficiency when problem size scales with core count, making it ideal for analyzing larger codebases on more powerful machines.</p>
<h2 id="tuning-guidelines-1"><a class="header" href="#tuning-guidelines-1">Tuning Guidelines</a></h2>
<p><strong>Development Workstations:</strong></p>
<pre><code class="language-bash"># Use all cores for maximum speed
debtmap analyze --jobs 0
</code></pre>
<p><strong>CI/CD Environments:</strong></p>
<pre><code class="language-bash"># Limit threads to avoid resource contention
debtmap analyze --jobs 2

# Or disable parallelism on very constrained runners
debtmap analyze --no-parallel
</code></pre>
<p><strong>Containers:</strong></p>
<pre><code class="language-bash"># Auto-detection respects cgroup limits
debtmap analyze --jobs 0

# Or explicitly match container CPU allocation
debtmap analyze --jobs 4
</code></pre>
<p><strong>Benchmarking:</strong></p>
<pre><code class="language-bash"># Use fixed thread count for reproducible results
debtmap analyze --jobs 8
</code></pre>
<h2 id="profiling-and-debugging"><a class="header" href="#profiling-and-debugging">Profiling and Debugging</a></h2>
<h3 id="measure-analysis-time"><a class="header" href="#measure-analysis-time">Measure Analysis Time</a></h3>
<pre><code class="language-bash">time debtmap analyze .
</code></pre>
<h3 id="disable-parallelism-for-debugging"><a class="header" href="#disable-parallelism-for-debugging">Disable Parallelism for Debugging</a></h3>
<pre><code class="language-bash">debtmap analyze . --no-parallel -vv
</code></pre>
<p>Single-threaded mode with verbose output for debugging.</p>
<h3 id="profile-thread-usage"><a class="header" href="#profile-thread-usage">Profile Thread Usage</a></h3>
<p>Use system tools to monitor thread usage:</p>
<pre><code class="language-bash"># Linux
htop

# macOS
Activity Monitor (View &gt; CPU Usage &gt; Show Threads)
</code></pre>
<p>Look for:</p>
<ul>
<li>All cores at ~100% utilization (optimal)</li>
<li>Some cores idle (I/O bound or insufficient work)</li>
<li>Excessive context switching (too many threads)</li>
</ul>
<h3 id="finding-optimal-settings"><a class="header" href="#finding-optimal-settings">Finding Optimal Settings</a></h3>
<p><strong>Finding the optimal setting:</strong></p>
<pre><code class="language-bash"># Benchmark different configurations
time debtmap analyze --jobs 0  # Auto
time debtmap analyze --jobs 4  # 4 threads
time debtmap analyze --jobs 8  # 8 threads
time debtmap analyze --no-parallel  # Sequential
</code></pre>
<p>Monitor memory usage during analysis:</p>
<pre><code class="language-bash"># Monitor peak memory usage
/usr/bin/time -v debtmap analyze --jobs 8
</code></pre>
<h2 id="best-practices-15"><a class="header" href="#best-practices-15">Best Practices</a></h2>
<ol>
<li><strong>Use default settings</strong> - Debtmap auto-detects optimal thread count</li>
<li><strong>Limit threads in CI</strong> - Use <code>--jobs 2</code> or <code>--jobs 4</code> in shared environments</li>
<li><strong>Profile before tuning</strong> - Measure actual performance impact</li>
<li><strong>Consider I/O</strong> - If using slow storage, reduce thread count</li>
</ol>
<h2 id="troubleshooting-17"><a class="header" href="#troubleshooting-17">Troubleshooting</a></h2>
<h3 id="analysis-is-slow-despite-parallelism"><a class="header" href="#analysis-is-slow-despite-parallelism">Analysis is Slow Despite Parallelism</a></h3>
<p><strong>Possible causes:</strong></p>
<ol>
<li>I/O bottleneck (slow disk)</li>
<li>Memory pressure (swapping)</li>
<li>Thread contention</li>
</ol>
<p><strong>Solutions:</strong></p>
<ul>
<li>Use faster storage (SSD)</li>
<li>Reduce thread count to avoid memory pressure</li>
<li>Limit analysis scope with <code>--max-files</code></li>
</ul>
<h3 id="slow-analysis-performance"><a class="header" href="#slow-analysis-performance">Slow Analysis Performance</a></h3>
<p>If analysis is slower than expected:</p>
<ol>
<li>
<p><strong>Check thread count:</strong></p>
<pre><code class="language-bash"># Ensure you're using all cores
debtmap analyze --jobs 0 -vv | grep "threads"
</code></pre>
</li>
<li>
<p><strong>Check I/O bottleneck:</strong></p>
<pre><code class="language-bash"># Use iotop or similar to check disk saturation
# SSD storage significantly improves performance
</code></pre>
</li>
<li>
<p><strong>Check memory pressure:</strong></p>
<pre><code class="language-bash"># Monitor memory usage during analysis
top -p $(pgrep debtmap)
</code></pre>
</li>
<li>
<p><strong>Try different thread counts:</strong></p>
<pre><code class="language-bash"># Sometimes less threads = less contention
debtmap analyze --jobs 4
</code></pre>
</li>
</ol>
<h3 id="high-cpu-usage-but-no-progress"><a class="header" href="#high-cpu-usage-but-no-progress">High CPU Usage But No Progress</a></h3>
<p><strong>Possible cause:</strong> Analyzing very complex files (large ASTs)</p>
<p><strong>Solution:</strong></p>
<pre><code class="language-bash"># Reduce thread count to avoid memory thrashing
debtmap analyze . --jobs 2
</code></pre>
<h3 id="high-memory-usage"><a class="header" href="#high-memory-usage">High Memory Usage</a></h3>
<p>If debtmap uses too much memory:</p>
<ol>
<li>
<p><strong>Reduce parallelism:</strong></p>
<pre><code class="language-bash">debtmap analyze --jobs 2
</code></pre>
</li>
<li>
<p><strong>Disable parallel call graph:</strong></p>
<pre><code class="language-bash">debtmap analyze --no-parallel
</code></pre>
</li>
<li>
<p><strong>Analyze subdirectories separately:</strong></p>
<pre><code class="language-bash"># Process codebase in chunks
debtmap analyze src/module1
debtmap analyze src/module2
</code></pre>
</li>
</ol>
<h3 id="inconsistent-results-between-runs"><a class="header" href="#inconsistent-results-between-runs">Inconsistent Results Between Runs</a></h3>
<p><strong>Possible cause:</strong> Non-deterministic parallel aggregation (rare)</p>
<p><strong>Solution:</strong></p>
<pre><code class="language-bash"># Use single-threaded mode
debtmap analyze . --no-parallel
</code></pre>
<p>If results differ, report as a bug.</p>
<h3 id="debugging-concurrency-issues"><a class="header" href="#debugging-concurrency-issues">Debugging Concurrency Issues</a></h3>
<p>If you suspect a concurrency bug:</p>
<ol>
<li>
<p><strong>Run sequentially to isolate:</strong></p>
<pre><code class="language-bash">debtmap analyze --no-parallel
</code></pre>
</li>
<li>
<p><strong>Use deterministic mode:</strong></p>
<pre><code class="language-bash"># Single-threaded = deterministic order
debtmap analyze --jobs 1
</code></pre>
</li>
<li>
<p><strong>Enable verbose logging:</strong></p>
<pre><code class="language-bash">debtmap analyze -vvv --no-parallel &gt; debug.log 2&gt;&amp;1
</code></pre>
</li>
<li>
<p><strong>Report the issue:</strong>
If behavior differs between <code>--no-parallel</code> and parallel mode, please <a href="https://github.com/yourusername/debtmap/issues">report it</a> with:</p>
<ul>
<li>Command used</li>
<li>Platform (OS, CPU core count)</li>
<li>Debtmap version</li>
<li>Minimal reproduction case</li>
</ul>
</li>
</ol>
<h3 id="thread-contention-warning"><a class="header" href="#thread-contention-warning">Thread Contention Warning</a></h3>
<p>If you see warnings about thread contention:</p>
<pre><code>WARN - High contention detected on parallel call graph
</code></pre>
<p>This indicates too many threads competing for locks. Try:</p>
<pre><code class="language-bash"># Reduce thread count
debtmap analyze --jobs 4
</code></pre>
<h2 id="see-also-7"><a class="header" href="#see-also-7">See Also</a></h2>
<ul>
<li><a href="#performance--caching">CLI Reference - Performance &amp; Caching</a> - Complete flag documentation</li>
<li><a href="#configuration-2">Configuration</a> - Project-specific settings</li>
<li><a href="#troubleshooting-23">Troubleshooting</a> - General troubleshooting guide</li>
<li><a href="#slow-analysis-performance">Troubleshooting - Slow Analysis</a> - Performance debugging guide</li>
<li><a href="#high-memory-usage">Troubleshooting - High Memory Usage</a> - Memory optimization tips</li>
<li><a href="#frequently-asked-questions-1">FAQ - Reducing Parallelism</a> - Common questions about parallel processing</li>
<li><a href="#architecture">Architecture</a> - High-level system design</li>
</ul>
<h2 id="summary-8"><a class="header" href="#summary-8">Summary</a></h2>
<p>Debtmap’s parallel processing architecture provides:</p>
<ul>
<li><strong>10-100x speedup</strong> over sequential analysis using Rayon parallel iterators</li>
<li><strong>Lock-free concurrency</strong> with DashMap for minimal contention</li>
<li><strong>Flexible configuration</strong> via <code>--jobs</code> and <code>--no-parallel</code> flags</li>
<li><strong>Automatic thread pool tuning</strong> that respects system resources</li>
<li><strong>Production-grade reliability</strong> with atomic progress tracking and concurrent merging</li>
</ul>
<p>The three-phase parallel pipeline (parse → extract → analyze) maximizes parallelism while maintaining correctness through carefully designed concurrent data structures.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="prodigy-integration"><a class="header" href="#prodigy-integration">Prodigy Integration</a></h1>
<p>Debtmap integrates with <a href="https://github.com/iepathos/prodigy">Prodigy</a> to provide fully automated technical debt reduction through AI-driven workflows. This chapter explains how to set up and use Prodigy workflows to automatically refactor code, add tests, and improve codebase quality.</p>
<h2 id="prerequisites-checklist"><a class="header" href="#prerequisites-checklist">Prerequisites Checklist</a></h2>
<p>Before using Prodigy with Debtmap, ensure you have:</p>
<ul>
<li><input disabled="" type="checkbox"> Rust 1.70 or later installed</li>
<li><input disabled="" type="checkbox"> Debtmap installed (<code>cargo install debtmap</code>)</li>
<li><input disabled="" type="checkbox"> Prodigy installed (<code>cargo install --git https://github.com/iepathos/prodigy prodigy</code>)</li>
<li><input disabled="" type="checkbox"> Anthropic API key for Claude access</li>
<li><input disabled="" type="checkbox"> Git (for worktree management)</li>
<li><input disabled="" type="checkbox"> Optional: <code>just</code> command runner (a command runner like make), or use direct <code>cargo</code> commands as alternatives</li>
</ul>
<h2 id="what-is-prodigy"><a class="header" href="#what-is-prodigy">What is Prodigy?</a></h2>
<blockquote>
<p><strong>Note</strong>: Prodigy is a separate open-source tool (https://github.com/iepathos/prodigy). You need to install both Debtmap and Prodigy to use this integration.</p>
</blockquote>
<p>Prodigy is an AI-powered workflow automation system that uses Claude to execute complex multi-step tasks. When integrated with Debtmap, it can:</p>
<ul>
<li><strong>Automatically refactor</strong> high-complexity functions identified by Debtmap</li>
<li><strong>Add unit tests</strong> for untested code</li>
<li><strong>Fix code duplication</strong> by extracting shared logic</li>
<li><strong>Improve code organization</strong> by addressing architectural issues</li>
<li><strong>Validate improvements</strong> with automated testing</li>
</ul>
<p>All changes are made in isolated git worktrees, validated with tests and linting, and only committed if all checks pass.</p>
<h2 id="benefits"><a class="header" href="#benefits">Benefits</a></h2>
<h3 id="automated-debt-reduction"><a class="header" href="#automated-debt-reduction">Automated Debt Reduction</a></h3>
<p>Instead of manually addressing each technical debt item, Prodigy can:</p>
<ol>
<li>Analyze Debtmap’s output</li>
<li>Select high-priority items</li>
<li>Generate refactoring plans</li>
<li>Execute refactorings automatically</li>
<li>Validate with tests</li>
<li>Commit clean changes</li>
</ol>
<h3 id="iterative-improvement"><a class="header" href="#iterative-improvement">Iterative Improvement</a></h3>
<p>Prodigy supports <strong>iterative workflows</strong>:</p>
<ul>
<li>Run analysis → fix top items → re-analyze → fix more</li>
<li>Configurable iteration count (default: 5 iterations)</li>
<li>Each iteration focuses on highest-priority remaining items</li>
</ul>
<h3 id="safe-experimentation"><a class="header" href="#safe-experimentation">Safe Experimentation</a></h3>
<p>All changes happen in <strong>isolated git worktrees</strong>:</p>
<ul>
<li>Original branch remains untouched</li>
<li>Failed attempts don’t affect main codebase</li>
<li>Easy to review before merging</li>
<li>Automatic cleanup after workflow</li>
</ul>
<h2 id="prerequisites-1"><a class="header" href="#prerequisites-1">Prerequisites</a></h2>
<h3 id="install-prodigy"><a class="header" href="#install-prodigy">Install Prodigy</a></h3>
<pre><code class="language-bash"># Install Prodigy from GitHub repository
cargo install --git https://github.com/iepathos/prodigy prodigy

# Verify installation
prodigy --version
</code></pre>
<blockquote>
<p><strong>Note</strong>: Currently, Prodigy must be installed from GitHub. Check the <a href="https://github.com/iepathos/prodigy">Prodigy repository</a> for the latest installation instructions.</p>
</blockquote>
<p><strong>Requirements:</strong></p>
<ul>
<li>Rust 1.70 or later</li>
<li>Git (for worktree management)</li>
<li>Anthropic API key for Claude access</li>
</ul>
<h3 id="configure-claude-api"><a class="header" href="#configure-claude-api">Configure Claude API</a></h3>
<pre><code class="language-bash"># Set Claude API key
export ANTHROPIC_API_KEY="your-api-key-here"

# Or in ~/.prodigy/config.toml:
[api]
anthropic_key = "your-api-key-here"
</code></pre>
<h3 id="ensure-debtmap-is-installed"><a class="header" href="#ensure-debtmap-is-installed">Ensure Debtmap is Installed</a></h3>
<pre><code class="language-bash"># Install Debtmap
cargo install debtmap

# Verify installation
debtmap --version
</code></pre>
<h2 id="quick-start-3"><a class="header" href="#quick-start-3">Quick Start</a></h2>
<h3 id="1-initialize-workflow"><a class="header" href="#1-initialize-workflow">1. Initialize Workflow</a></h3>
<p>Create a workflow file <code>workflows/debtmap.yml</code>:</p>
<pre><code class="language-yaml"># Sequential workflow. Fix top technical debt item

# Phase 1: Generate coverage data
- shell: "just coverage-lcov"  # or: cargo tarpaulin --out lcov --output-dir target/coverage

# Phase 2: Analyze tech debt and capture baseline
- shell: "debtmap analyze . --lcov target/coverage/lcov.info --output .prodigy/debtmap-before.json --format json"

# Phase 3: Create implementation plan (PLANNING PHASE)
- claude: "/prodigy-debtmap-plan --before .prodigy/debtmap-before.json --output .prodigy/IMPLEMENTATION_PLAN.md"
  capture_output: true
  validate:
    commands:
      - claude: "/prodigy-validate-debtmap-plan --before .prodigy/debtmap-before.json --plan .prodigy/IMPLEMENTATION_PLAN.md --output .prodigy/plan-validation.json"
    result_file: ".prodigy/plan-validation.json"
    threshold: 75
    on_incomplete:
      commands:
        - claude: "/prodigy-revise-debtmap-plan --gaps ${validation.gaps} --plan .prodigy/IMPLEMENTATION_PLAN.md"
      max_attempts: 3
      fail_workflow: false

# Phase 4: Execute the plan (IMPLEMENTATION PHASE)
- claude: "/prodigy-debtmap-implement --plan .prodigy/IMPLEMENTATION_PLAN.md"
  commit_required: true
  validate:
    commands:
      - shell: "debtmap analyze . --lcov target/coverage/lcov.info --output .prodigy/debtmap-after.json --format json"
      - shell: "debtmap compare --before .prodigy/debtmap-before.json --after .prodigy/debtmap-after.json --plan .prodigy/IMPLEMENTATION_PLAN.md --output .prodigy/comparison.json --format json"
      - shell: "debtmap validate-improvement --comparison .prodigy/comparison.json --output .prodigy/debtmap-validation.json"
    result_file: ".prodigy/debtmap-validation.json"
    threshold: 75
    on_incomplete:
      commands:
        - claude: "/prodigy-complete-debtmap-fix --gaps ${validation.gaps} --plan .prodigy/IMPLEMENTATION_PLAN.md --attempt ${validation.attempt_number}"
          commit_required: true
        - shell: "just coverage-lcov"
        - shell: "debtmap analyze . --lcov target/coverage/lcov.info --output .prodigy/debtmap-after.json --format json"
        - shell: "debtmap compare --before .prodigy/debtmap-before.json --after .prodigy/debtmap-after.json --plan .prodigy/IMPLEMENTATION_PLAN.md --output .prodigy/comparison.json --format json"
      max_attempts: 5
      fail_workflow: true

# Phase 5: Run tests with automatic fixing
- shell: "just test"
  on_failure:
    claude: "/prodigy-debug-test-failure --output ${shell.output}"
    max_attempts: 5
    fail_workflow: true

# Phase 6: Run linting and formatting
- shell: "just fmt-check &amp;&amp; just lint"
  on_failure:
    claude: "/prodigy-lint ${shell.output}"
    max_attempts: 5
    fail_workflow: true
</code></pre>
<blockquote>
<p><strong>Note about <code>just</code></strong>: This example uses <code>just</code> (a command runner like <code>make</code>). If you don’t have a <code>justfile</code>, replace <code>just coverage-lcov</code> with <code>cargo tarpaulin --out lcov --output-dir target/coverage</code>, <code>just test</code> with <code>cargo test</code>, and <code>just fmt-check &amp;&amp; just lint</code> with <code>cargo fmt --check &amp;&amp; cargo clippy -- -D warnings</code>.</p>
</blockquote>
<h3 id="2-run-workflow"><a class="header" href="#2-run-workflow">2. Run Workflow</a></h3>
<pre><code class="language-bash"># Run with auto-confirm, 5 iterations
prodigy run workflows/debtmap.yml -yn 5

# Run with custom iteration count
prodigy run workflows/debtmap.yml -yn 10

# Run single iteration for testing
prodigy run workflows/debtmap.yml -yn 1
</code></pre>
<p><strong>Command Flags:</strong></p>
<ul>
<li><code>-y</code> (<code>--yes</code>) - Auto-confirm workflow steps (skip prompts)</li>
<li><code>-n 5</code> (<code>--max-iterations 5</code>) - Run workflow for up to 5 iterations</li>
</ul>
<p><strong>Note</strong>: Worktrees are managed separately via the <code>prodigy worktree</code> command. In MapReduce mode, Prodigy automatically creates isolated worktrees for each parallel agent.</p>
<h3 id="3-review-results"><a class="header" href="#3-review-results">3. Review Results</a></h3>
<p>Prodigy creates a detailed report:</p>
<pre><code>📊 WORKFLOW SUMMARY
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

Iterations: 5
Items Fixed: 12
Tests Added: 8
Complexity Reduced: 145 → 78 (-46%)
Coverage Improved: 45% → 72% (+27%)

✅ All validations passed
</code></pre>
<h2 id="useful-prodigy-commands"><a class="header" href="#useful-prodigy-commands">Useful Prodigy Commands</a></h2>
<p>Beyond <code>prodigy run</code>, several commands help manage workflows and sessions:</p>
<h3 id="resume-interrupted-workflows"><a class="header" href="#resume-interrupted-workflows">Resume Interrupted Workflows</a></h3>
<pre><code class="language-bash"># Resume an interrupted sequential workflow
prodigy resume &lt;SESSION_ID&gt;

# Resume an interrupted MapReduce job
prodigy resume-job &lt;JOB_ID&gt;

# List all sessions to find the SESSION_ID
prodigy sessions
</code></pre>
<p><strong>When to use</strong>: If a workflow is interrupted (Ctrl-C, system crash, network issues), you can resume from the last checkpoint rather than starting over.</p>
<h3 id="view-checkpoints"><a class="header" href="#view-checkpoints">View Checkpoints</a></h3>
<pre><code class="language-bash"># List all available checkpoints
prodigy checkpoints

# List checkpoints for specific session
prodigy checkpoints --session &lt;SESSION_ID&gt;
</code></pre>
<p><strong>When to use</strong>: To see available restore points for interrupted workflows.</p>
<h3 id="manage-worktrees"><a class="header" href="#manage-worktrees">Manage Worktrees</a></h3>
<pre><code class="language-bash"># List all Prodigy worktrees
prodigy worktree list

# Clean up old worktrees
prodigy worktree clean

# Remove specific worktree
prodigy worktree remove &lt;SESSION_ID&gt;
</code></pre>
<p><strong>When to use</strong>: MapReduce workflows create many worktrees. Clean them up periodically to save disk space.</p>
<h3 id="monitor-mapreduce-progress"><a class="header" href="#monitor-mapreduce-progress">Monitor MapReduce Progress</a></h3>
<pre><code class="language-bash"># View progress of running MapReduce job
prodigy progress &lt;JOB_ID&gt;

# View events and logs from MapReduce job
prodigy events &lt;JOB_ID&gt;

# Filter events by type
prodigy events &lt;JOB_ID&gt; --type agent_started
prodigy events &lt;JOB_ID&gt; --type agent_completed
prodigy events &lt;JOB_ID&gt; --type agent_failed
</code></pre>
<p><strong>When to use</strong>: Monitor long-running MapReduce jobs to see how many agents have completed, which are still running, and which have failed.</p>
<h3 id="manage-dead-letter-queue"><a class="header" href="#manage-dead-letter-queue">Manage Dead Letter Queue</a></h3>
<pre><code class="language-bash"># View failed MapReduce items in DLQ
prodigy dlq list &lt;JOB_ID&gt;

# Retry failed items from DLQ
prodigy dlq retry &lt;JOB_ID&gt; &lt;ITEM_ID&gt;

# Remove items from DLQ
prodigy dlq remove &lt;JOB_ID&gt; &lt;ITEM_ID&gt;
</code></pre>
<p><strong>When to use</strong>: When some MapReduce agents fail, their items go to the Dead Letter Queue. You can retry them individually or investigate why they failed.</p>
<h3 id="session-management"><a class="header" href="#session-management">Session Management</a></h3>
<pre><code class="language-bash"># List all workflow sessions
prodigy sessions

# Clean up old sessions
prodigy clean
</code></pre>
<p><strong>When to use</strong>: View history of workflow runs and clean up old data.</p>
<h2 id="workflow-configuration"><a class="header" href="#workflow-configuration">Workflow Configuration</a></h2>
<p>Prodigy workflows are defined as YAML lists of steps. Each step can be either a <code>shell</code> command or a <code>claude</code> slash command.</p>
<h3 id="workflow-step-types"><a class="header" href="#workflow-step-types">Workflow Step Types</a></h3>
<h4 id="shell-commands"><a class="header" href="#shell-commands">Shell Commands</a></h4>
<p>Execute shell commands directly:</p>
<pre><code class="language-yaml"># Simple shell command
- shell: "cargo test"

# With timeout (in seconds)
- shell: "just coverage-lcov"
  timeout: 900  # 15 minutes

# With error handling
- shell: "just test"
  on_failure:
    claude: "/prodigy-debug-test-failure --output ${shell.output}"
    max_attempts: 5
    fail_workflow: true
</code></pre>
<p><strong>Shell Command Fields:</strong></p>
<ul>
<li><code>shell</code>: Command to execute (string)</li>
<li><code>timeout</code>: Maximum execution time in seconds (optional)</li>
<li><code>on_failure</code>: Error handler configuration (optional)
<ul>
<li><code>claude</code>: Slash command to run on failure</li>
<li><code>max_attempts</code>: Maximum retry attempts</li>
<li><code>fail_workflow</code>: If true, fail entire workflow after max attempts</li>
</ul>
</li>
</ul>
<h4 id="claude-commands"><a class="header" href="#claude-commands">Claude Commands</a></h4>
<p>Execute Claude Code slash commands:</p>
<pre><code class="language-yaml"># Simple Claude command
- claude: "/prodigy-debtmap-plan --before .prodigy/debtmap-before.json --output .prodigy/IMPLEMENTATION_PLAN.md"

# With output capture (makes command output available in ${shell.output})
- claude: "/prodigy-debtmap-plan --before .prodigy/debtmap-before.json --output .prodigy/IMPLEMENTATION_PLAN.md"
  capture_output: true

# With commit requirement (workflow fails if no git commit made)
- claude: "/prodigy-debtmap-implement --plan .prodigy/IMPLEMENTATION_PLAN.md"
  commit_required: true

# With timeout and validation
- claude: "/prodigy-debtmap-implement --plan .prodigy/IMPLEMENTATION_PLAN.md"
  commit_required: true
  timeout: 1800  # 30 minutes
  validate:
    commands:
      - shell: "cargo test"
    result_file: ".prodigy/validation.json"
    threshold: 75
</code></pre>
<p><strong>Claude Command Fields:</strong></p>
<ul>
<li><code>claude</code>: Slash command to execute (string)</li>
<li><code>capture_output</code>: If true, command output is available in <code>${shell.output}</code> variable (optional)</li>
<li><code>commit_required</code>: If true, workflow fails if command doesn’t create a git commit (optional)</li>
<li><code>timeout</code>: Maximum execution time in seconds (optional)</li>
<li><code>validate</code>: Validation configuration (optional, see Step-Level Validation below)</li>
</ul>
<h3 id="step-level-validation"><a class="header" href="#step-level-validation">Step-Level Validation</a></h3>
<p>Steps can include validation that must pass:</p>
<pre><code class="language-yaml">- claude: "/prodigy-debtmap-implement --plan .prodigy/IMPLEMENTATION_PLAN.md"
  commit_required: true
  validate:
    commands:
      - shell: "cargo test"
      - shell: "cargo clippy -- -D warnings"
    result_file: ".prodigy/validation.json"
    threshold: 75
    on_incomplete:
      commands:
        - claude: "/prodigy-complete-debtmap-fix --gaps ${validation.gaps} --plan .prodigy/IMPLEMENTATION_PLAN.md"
          commit_required: true
      max_attempts: 5
      fail_workflow: true
</code></pre>
<p><strong>Validation Options:</strong></p>
<ul>
<li><code>commands</code>: List of commands to run for validation</li>
<li><code>result_file</code>: JSON file containing validation results</li>
<li><code>threshold</code>: Minimum score (0-100) required to pass</li>
<li><code>on_incomplete</code>: Actions to take if validation score &lt; threshold</li>
<li><code>max_attempts</code>: Maximum retry attempts</li>
<li><code>fail_workflow</code>: Whether to fail entire workflow if validation never passes</li>
</ul>
<h3 id="error-handling"><a class="header" href="#error-handling">Error Handling</a></h3>
<p>Use <code>on_failure</code> to handle command failures:</p>
<pre><code class="language-yaml">- shell: "just fmt-check &amp;&amp; just lint"
  on_failure:
    claude: "/prodigy-lint ${shell.output}"
    max_attempts: 5
    fail_workflow: true
</code></pre>
<p><strong>Error Handling Options:</strong></p>
<ul>
<li><code>claude</code>: Slash command to fix the failure</li>
<li><code>max_attempts</code>: Maximum fix attempts</li>
<li><code>fail_workflow</code>: If true, workflow fails after max_attempts; if false, continues to next step</li>
</ul>
<h3 id="coverage-integration-3"><a class="header" href="#coverage-integration-3">Coverage Integration</a></h3>
<p>Generate and use coverage data in workflows. See <a href="#coverage-integration-1">Coverage Integration</a> for details on generating LCOV files and understanding coverage metrics.</p>
<pre><code class="language-yaml"># Generate coverage
- shell: "just coverage-lcov"

# Use coverage in analysis
- shell: "debtmap analyze . --lcov target/coverage/lcov.info --output .prodigy/debtmap-before.json --format json"
</code></pre>
<h2 id="claude-slash-commands"><a class="header" href="#claude-slash-commands">Claude Slash Commands</a></h2>
<blockquote>
<p><strong>Important</strong>: The slash commands documented below are custom commands provided in Debtmap’s <code>.claude/commands/</code> directory. They are included in the Debtmap repository as working examples. You can use them as-is or create your own based on these patterns.</p>
<p><strong>Note on workflow styles</strong>: The sequential workflow (workflows/debtmap.yml) uses <code>shell:</code> commands directly, while the MapReduce workflow (workflows/debtmap-reduce.yml) uses <code>claude:</code> wrapper commands for some operations like <code>validate-improvement</code>. Both approaches are valid - use whichever fits your workflow style.</p>
</blockquote>
<p>Prodigy workflows use Claude Code slash commands to perform analysis, planning, and implementation. The key commands used in the debtmap workflow are:</p>
<h3 id="planning-commands"><a class="header" href="#planning-commands">Planning Commands</a></h3>
<h4 id="prodigy-debtmap-plan"><a class="header" href="#prodigy-debtmap-plan"><code>/prodigy-debtmap-plan</code></a></h4>
<p>Creates an implementation plan for the top priority debt item.</p>
<pre><code class="language-yaml">- claude: "/prodigy-debtmap-plan --before .prodigy/debtmap-before.json --output .prodigy/IMPLEMENTATION_PLAN.md"
  capture_output: true
</code></pre>
<p><strong>Parameters:</strong></p>
<ul>
<li><code>--before</code>: Path to debtmap analysis JSON file</li>
<li><code>--output</code>: Path to write implementation plan</li>
</ul>
<h4 id="prodigy-validate-debtmap-plan"><a class="header" href="#prodigy-validate-debtmap-plan"><code>/prodigy-validate-debtmap-plan</code></a></h4>
<p>Validates that the implementation plan is complete and addresses the debt item.</p>
<pre><code class="language-yaml">- claude: "/prodigy-validate-debtmap-plan --before .prodigy/debtmap-before.json --plan .prodigy/IMPLEMENTATION_PLAN.md --output .prodigy/plan-validation.json"
</code></pre>
<p><strong>Parameters:</strong></p>
<ul>
<li><code>--before</code>: Original debtmap analysis</li>
<li><code>--plan</code>: Implementation plan to validate</li>
<li><code>--output</code>: Validation results JSON (with score 0-100)</li>
</ul>
<h4 id="prodigy-revise-debtmap-plan"><a class="header" href="#prodigy-revise-debtmap-plan"><code>/prodigy-revise-debtmap-plan</code></a></h4>
<p>Revises an incomplete plan based on validation gaps.</p>
<pre><code class="language-yaml">- claude: "/prodigy-revise-debtmap-plan --gaps ${validation.gaps} --plan .prodigy/IMPLEMENTATION_PLAN.md"
</code></pre>
<p><strong>Parameters:</strong></p>
<ul>
<li><code>--gaps</code>: List of missing items from validation</li>
<li><code>--plan</code>: Plan file to update</li>
</ul>
<h3 id="implementation-commands"><a class="header" href="#implementation-commands">Implementation Commands</a></h3>
<h4 id="prodigy-debtmap-implement"><a class="header" href="#prodigy-debtmap-implement"><code>/prodigy-debtmap-implement</code></a></h4>
<p>Executes the implementation plan.</p>
<pre><code class="language-yaml">- claude: "/prodigy-debtmap-implement --plan .prodigy/IMPLEMENTATION_PLAN.md"
  commit_required: true
</code></pre>
<p><strong>Parameters:</strong></p>
<ul>
<li><code>--plan</code>: Path to implementation plan</li>
</ul>
<h4 id="prodigy-validate-debtmap-improvement-1"><a class="header" href="#prodigy-validate-debtmap-improvement-1"><code>/prodigy-validate-debtmap-improvement</code></a></h4>
<p>Validates that the implementation successfully addressed the debt item.</p>
<blockquote>
<p><strong>Note</strong>: In practice, workflows use the <code>debtmap validate-improvement</code> shell command directly. While a Claude slash command wrapper exists, both the sequential workflow (workflows/debtmap.yml:31) and MapReduce workflow (workflows/debtmap-reduce.yml:44) use the shell command for consistency.</p>
</blockquote>
<pre><code class="language-yaml"># Standard approach (used in actual workflows)
- shell: "debtmap validate-improvement --comparison .prodigy/comparison.json --output .prodigy/debtmap-validation.json"
</code></pre>
<p><strong>Parameters:</strong></p>
<ul>
<li><code>--comparison</code>: Debtmap comparison results (before vs after)</li>
<li><code>--output</code>: Validation results JSON (with score 0-100)</li>
<li><code>--previous-validation</code>: (Optional) Previous validation result for trend tracking</li>
<li><code>--threshold</code>: (Optional) Improvement threshold percentage (default: 75.0)</li>
</ul>
<h4 id="prodigy-complete-debtmap-fix"><a class="header" href="#prodigy-complete-debtmap-fix"><code>/prodigy-complete-debtmap-fix</code></a></h4>
<p>Completes a partial fix based on validation gaps.</p>
<pre><code class="language-yaml">- claude: "/prodigy-complete-debtmap-fix --gaps ${validation.gaps} --plan .prodigy/IMPLEMENTATION_PLAN.md --attempt ${validation.attempt_number}"
  commit_required: true
</code></pre>
<p><strong>Parameters:</strong></p>
<ul>
<li><code>--gaps</code>: Validation gaps to address</li>
<li><code>--plan</code>: Original implementation plan</li>
<li><code>--attempt</code>: Current retry attempt number (from ${validation.attempt_number})</li>
</ul>
<h3 id="testing-and-quality-commands"><a class="header" href="#testing-and-quality-commands">Testing and Quality Commands</a></h3>
<h4 id="prodigy-debug-test-failure"><a class="header" href="#prodigy-debug-test-failure"><code>/prodigy-debug-test-failure</code></a></h4>
<p>Automatically fixes failing tests.</p>
<pre><code class="language-yaml">- shell: "just test"
  on_failure:
    claude: "/prodigy-debug-test-failure --output ${shell.output}"
    max_attempts: 5
</code></pre>
<p><strong>Parameters:</strong></p>
<ul>
<li><code>--output</code>: Test failure output from shell command</li>
</ul>
<h4 id="prodigy-lint"><a class="header" href="#prodigy-lint"><code>/prodigy-lint</code></a></h4>
<p>Fixes linting and formatting issues.</p>
<pre><code class="language-yaml">- shell: "just fmt-check &amp;&amp; just lint"
  on_failure:
    claude: "/prodigy-lint ${shell.output}"
    max_attempts: 5
</code></pre>
<p><strong>Parameters:</strong></p>
<ul>
<li>Shell output with linting errors</li>
</ul>
<h2 id="target-selection"><a class="header" href="#target-selection">Target Selection</a></h2>
<p>Target selection happens through the debtmap analysis and slash commands, not through workflow configuration:</p>
<h3 id="how-targets-are-selected"><a class="header" href="#how-targets-are-selected">How Targets Are Selected</a></h3>
<ol>
<li><strong>Debtmap analyzes</strong> the codebase and scores all items by complexity, coverage, and risk</li>
<li><strong>Planning command</strong> (<code>/prodigy-debtmap-plan</code>) selects the highest priority item</li>
<li><strong>Implementation command</strong> (<code>/prodigy-debtmap-implement</code>) fixes that specific item</li>
<li><strong>Next iteration</strong> re-analyzes and selects the next highest priority item</li>
</ol>
<h3 id="factors-in-prioritization"><a class="header" href="#factors-in-prioritization">Factors in Prioritization</a></h3>
<ul>
<li><strong>Complexity score</strong>: Functions with cyclomatic complexity &gt; 10</li>
<li><strong>Coverage percentage</strong>: Lower coverage increases priority</li>
<li><strong>Risk score</strong>: Complexity × (100 - coverage%)</li>
<li><strong>Debt type</strong>: Complexity, TestGap, Duplication, GodObject, DeepNesting</li>
</ul>
<h3 id="customizing-target-selection"><a class="header" href="#customizing-target-selection">Customizing Target Selection</a></h3>
<p>To focus on specific debt types or modules, modify the slash commands or create custom commands in <code>.claude/commands/</code></p>
<h2 id="mapreduce-workflows"><a class="header" href="#mapreduce-workflows">MapReduce Workflows</a></h2>
<p>Prodigy supports MapReduce workflows for processing multiple items in parallel. This is powerful for large-scale refactoring where you want to fix many debt items simultaneously.</p>
<h3 id="when-to-use-mapreduce"><a class="header" href="#when-to-use-mapreduce">When to Use MapReduce</a></h3>
<ul>
<li>Processing multiple independent debt items simultaneously (e.g., refactor 10 high-complexity functions in parallel)</li>
<li>Applying the same fix pattern across many files</li>
<li>Large-scale codebase cleanup tasks</li>
<li>Situations where sequential iteration would be too slow</li>
</ul>
<h3 id="mapreduce-vs-sequential-workflows"><a class="header" href="#mapreduce-vs-sequential-workflows">MapReduce vs Sequential Workflows</a></h3>
<p><strong>Sequential Workflow</strong> (<code>-n 5</code>):</p>
<ul>
<li>Runs entire workflow N times in sequence</li>
<li>Fixes one item per iteration</li>
<li>Each iteration re-analyzes the codebase</li>
<li>Total time: N × workflow_duration</li>
</ul>
<p><strong>MapReduce Workflow</strong>:</p>
<ul>
<li>Processes multiple items in parallel in a single run</li>
<li>Setup phase runs once</li>
<li>Map phase spawns N parallel agents (each in isolated worktree)</li>
<li>Reduce phase aggregates results</li>
<li>Total time: setup + max(map_agent_durations) + reduce</li>
</ul>
<h3 id="complete-mapreduce-example"><a class="header" href="#complete-mapreduce-example">Complete MapReduce Example</a></h3>
<p>Create <code>workflows/debtmap-reduce.yml</code>:</p>
<pre><code class="language-yaml">name: debtmap-parallel-elimination
mode: mapreduce

# Setup phase: Analyze the codebase and generate debt items
setup:
  timeout: 900  # 15 minutes for coverage generation
  commands:
    # Generate coverage data with tarpaulin
    - shell: "just coverage-lcov"

    # Run debtmap with coverage data to establish baseline
    - shell: "debtmap analyze src --lcov target/coverage/lcov.info --output .prodigy/debtmap-before.json --format json"

# Map phase: Process each debt item in parallel with planning and validation
map:
  # Input configuration - debtmap-before.json contains items array
  input: .prodigy/debtmap-before.json
  json_path: "$.items[*]"

  # Commands to execute for each debt item
  agent_template:
    # Phase 1: Create implementation plan
    - claude: "/prodigy-debtmap-plan --item '${item}' --output .prodigy/plan-${item_id}.md"
      capture_output: true
      validate:
        commands:
          - claude: "/prodigy-validate-debtmap-plan --item '${item}' --plan .prodigy/plan-${item_id}.md --output .prodigy/validation-${item_id}.json"
        result_file: ".prodigy/validation-${item_id}.json"
        threshold: 75
        on_incomplete:
          commands:
            - claude: "/prodigy-revise-debtmap-plan --gaps ${validation.gaps} --plan .prodigy/plan-${item_id}.md"
          max_attempts: 3
          fail_workflow: false

    # Phase 2: Execute the plan
    - claude: "/prodigy-debtmap-implement --plan .prodigy/plan-${item_id}.md"
      commit_required: true
      validate:
        commands:
          - shell: "just coverage-lcov"
          - shell: "debtmap analyze src --lcov target/coverage/lcov.info --output .prodigy/debtmap-after-${item_id}.json --format json"
          - shell: "debtmap compare --before .prodigy/debtmap-before.json --after .prodigy/debtmap-after-${item_id}.json --plan .prodigy/plan-${item_id}.md --output .prodigy/comparison-${item_id}.json --format json"
          - shell: "debtmap validate-improvement --comparison .prodigy/comparison-${item_id}.json --output .prodigy/debtmap-validation-${item_id}.json"
        result_file: ".prodigy/debtmap-validation-${item_id}.json"
        threshold: 75
        on_incomplete:
          commands:
            - claude: "/prodigy-complete-debtmap-fix --gaps ${validation.gaps} --plan .prodigy/plan-${item_id}.md --attempt ${validation.attempt_number}"
              commit_required: true
            - shell: "just coverage-lcov"
            - shell: "debtmap analyze src --lcov target/coverage/lcov.info --output .prodigy/debtmap-after-${item_id}.json --format json"
            - shell: "debtmap compare --before .prodigy/debtmap-before.json --after .prodigy/debtmap-after-${item_id}.json --plan .prodigy/plan-${item_id}.md --output .prodigy/comparison-${item_id}.json --format json"
          max_attempts: 5
          fail_workflow: true

    # Phase 3: Verify tests pass
    - shell: "just test"
      on_failure:
        claude: "/prodigy-debug-test-failure --output ${shell.output}"
        max_attempts: 5
        fail_workflow: true

    # Phase 4: Check formatting and linting
    - shell: "just fmt-check &amp;&amp; just lint"
      on_failure:
        claude: "/prodigy-lint ${shell.output}"
        max_attempts: 5
        fail_workflow: true

  # Parallelization settings
  max_parallel: 5  # Run up to 5 agents in parallel

  # Filter and sort items
  # Note: NULLS LAST ensures File items (with null Function.unified_score.final_score)
  # and Function items (with null File.score) sort correctly
  filter: "File.score &gt;= 10 OR Function.unified_score.final_score &gt;= 10"
  sort_by: "File.score DESC NULLS LAST, Function.unified_score.final_score DESC NULLS LAST"
  max_items: 10  # Limit to 10 items per run

# Reduce phase: Aggregate results and verify overall improvements
reduce:
  # Phase 1: Run final tests across all changes
  - shell: "just test"
    on_failure:
      claude: "/prodigy-debug-test-failure --output ${shell.output}"
      max_attempts: 5
      fail_workflow: true

  # Phase 2: Check formatting and linting
  - shell: "just fmt-check &amp;&amp; just lint"
    on_failure:
      claude: "/prodigy-lint ${shell.output}"
      max_attempts: 5
      fail_workflow: true

  # Phase 3: Re-run debtmap to measure cumulative improvements
  - shell: "just coverage-lcov"
  - shell: "debtmap analyze src --lcov target/coverage/lcov.info --output .prodigy/debtmap-after.json --format json"

  # Phase 4: Create final commit with summary
  - write_file:
      path: ".prodigy/map-results.json"
      content: "${map.results}"
      format: json
      create_dirs: true

  - claude: |
      /prodigy-compare-debt-results \
        --before .prodigy/debtmap-before.json \
        --after .prodigy/debtmap-after.json \
        --map-results-file .prodigy/map-results.json \
        --successful ${map.successful} \
        --failed ${map.failed} \
        --total ${map.total}
    commit_required: true
</code></pre>
<h3 id="running-mapreduce-workflows"><a class="header" href="#running-mapreduce-workflows">Running MapReduce Workflows</a></h3>
<pre><code class="language-bash"># Run MapReduce workflow (single execution processes multiple items in parallel)
prodigy run workflows/debtmap-reduce.yml

# Run with auto-confirm
prodigy run workflows/debtmap-reduce.yml -y
</code></pre>
<p><strong>Note</strong>: MapReduce workflows don’t typically use <code>-n</code> for iterations. Instead, they process multiple items in a single run through parallel map agents.</p>
<h3 id="mapreduce-configuration-options"><a class="header" href="#mapreduce-configuration-options">MapReduce Configuration Options</a></h3>
<h4 id="top-level-fields"><a class="header" href="#top-level-fields">Top-Level Fields</a></h4>
<ul>
<li><code>name</code>: Workflow name (string)</li>
<li><code>mode: mapreduce</code>: Enables MapReduce mode (required)</li>
<li><code>setup</code>: Commands to run once before map phase</li>
<li><code>map</code>: Map phase configuration</li>
<li><code>reduce</code>: Commands to run after all map agents complete</li>
</ul>
<h4 id="setup-phase-fields"><a class="header" href="#setup-phase-fields">Setup Phase Fields</a></h4>
<ul>
<li><code>timeout</code>: Maximum time in seconds for setup phase</li>
<li><code>commands</code>: List of shell or claude commands to run</li>
</ul>
<h4 id="map-phase-fields"><a class="header" href="#map-phase-fields">Map Phase Fields</a></h4>
<ul>
<li><code>input</code>: Path to JSON file containing items to process</li>
<li><code>json_path</code>: JSONPath expression to extract items array (e.g., <code>$.items[*]</code>)</li>
<li><code>agent_template</code>: List of commands to run for each item (each item gets its own agent in an isolated worktree)</li>
<li><code>max_parallel</code>: Maximum number of agents to run concurrently</li>
<li><code>filter</code>: Expression to filter which items to process (e.g., <code>"score &gt;= 10"</code>)</li>
<li><code>sort_by</code>: Expression to sort items (e.g., <code>"score DESC"</code>)</li>
<li><code>max_items</code>: Limit total items processed</li>
</ul>
<h4 id="mapreduce-specific-variables"><a class="header" href="#mapreduce-specific-variables">MapReduce-Specific Variables</a></h4>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Variable</th><th>Available In</th><th>Type</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>${item}</code></td><td>map phase</td><td>JSON</td><td>The full JSON object for current item (File or Function type)</td></tr>
<tr><td><code>${item_id}</code></td><td>map phase</td><td>string</td><td>Unique ID for current item (auto-generated)</td></tr>
<tr><td><code>${validation.gaps}</code></td><td>map phase</td><td>array</td><td>List of validation gaps from failed validation</td></tr>
<tr><td><code>${validation.attempt_number}</code></td><td>map phase</td><td>number</td><td>Current retry attempt number (1, 2, 3, etc.)</td></tr>
<tr><td><code>${shell.output}</code></td><td>both phases</td><td>string</td><td>Output from previous shell command</td></tr>
<tr><td><code>${map.results}</code></td><td>reduce phase</td><td>array</td><td>All map agent results as JSON</td></tr>
<tr><td><code>${map.successful}</code></td><td>reduce phase</td><td>number</td><td>Count of successful map agents</td></tr>
<tr><td><code>${map.failed}</code></td><td>reduce phase</td><td>number</td><td>Count of failed map agents</td></tr>
<tr><td><code>${map.total}</code></td><td>reduce phase</td><td>number</td><td>Total number of map agents</td></tr>
</tbody>
</table>
</div>
<p><strong>Understanding ${item} Structure:</strong></p>
<p>The <code>${item}</code> variable contains different fields depending on whether it’s a File or Function debt item:</p>
<ul>
<li><strong>File items</strong>: Have a <code>File.score</code> field (non-null) and <code>Function.unified_score.final_score</code> is null</li>
<li><strong>Function items</strong>: Have a <code>Function.unified_score.final_score</code> field (non-null) and <code>File.score</code> is null</li>
</ul>
<p>This distinction matters when filtering and sorting items in MapReduce workflows. See the filter/sort_by examples below for proper handling of both types.</p>
<h3 id="mapreduce-architecture"><a class="header" href="#mapreduce-architecture">MapReduce Architecture</a></h3>
<pre><code>┌─────────────────────────────────────────────────────────┐
│ Setup Phase (main worktree)                            │
│ - Generate coverage data                               │
│ - Run debtmap analysis                                 │
│ - Output: .prodigy/debtmap-before.json                 │
└─────────────────────────────────────────────────────────┘
                           │
                           ▼
┌─────────────────────────────────────────────────────────┐
│ Map Phase (parallel worktrees)                         │
│                                                         │
│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐ │
│  │ Agent 1      │  │ Agent 2      │  │ Agent 3      │ │
│  │ Item #1      │  │ Item #2      │  │ Item #3      │ │
│  │ Worktree A   │  │ Worktree B   │  │ Worktree C   │ │
│  │              │  │              │  │              │ │
│  │ Plan → Fix   │  │ Plan → Fix   │  │ Plan → Fix   │ │
│  │ → Validate   │  │ → Validate   │  │ → Validate   │ │
│  │ → Test       │  │ → Test       │  │ → Test       │ │
│  │ → Commit     │  │ → Commit     │  │ → Commit     │ │
│  └──────────────┘  └──────────────┘  └──────────────┘ │
│                                                         │
│  ┌──────────────┐  ┌──────────────┐                   │
│  │ Agent 4      │  │ Agent 5      │                   │
│  │ Item #4      │  │ Item #5      │                   │
│  │ Worktree D   │  │ Worktree E   │                   │
│  │              │  │              │                   │
│  │ Plan → Fix   │  │ Plan → Fix   │                   │
│  │ → Validate   │  │ → Validate   │                   │
│  │ → Test       │  │ → Test       │                   │
│  │ → Commit     │  │ → Commit     │                   │
│  └──────────────┘  └──────────────┘                   │
└─────────────────────────────────────────────────────────┘
                           │
                           ▼
┌─────────────────────────────────────────────────────────┐
│ Reduce Phase (main worktree)                           │
│ - Merge all agent worktrees                            │
│ - Run final tests on merged code                       │
│ - Run final linting                                    │
│ - Re-analyze with debtmap                              │
│ - Generate summary commit                              │
└─────────────────────────────────────────────────────────┘
</code></pre>
<p><strong>Key Concepts:</strong></p>
<ul>
<li><strong>Isolation</strong>: Each map agent works in its own git worktree</li>
<li><strong>Parallelism</strong>: Multiple agents process different items simultaneously</li>
<li><strong>Validation</strong>: Each agent validates its changes independently</li>
<li><strong>Merging</strong>: Reduce phase merges all successful agent worktrees</li>
<li><strong>Final Validation</strong>: Reduce phase ensures merged code passes all tests</li>
</ul>
<h2 id="iteration-strategy"><a class="header" href="#iteration-strategy">Iteration Strategy</a></h2>
<h3 id="how-iterations-work"><a class="header" href="#how-iterations-work">How Iterations Work</a></h3>
<p>When you run <code>prodigy run workflows/debtmap.yml -yn 5</code>, the workflow executes up to 5 times:</p>
<ol>
<li>
<p><strong>Iteration 1</strong>:</p>
<ul>
<li>Analyze codebase with debtmap</li>
<li>Select highest priority item</li>
<li>Create implementation plan</li>
<li>Execute plan and validate</li>
<li>Run tests and linting</li>
</ul>
</li>
<li>
<p><strong>Iteration 2</strong>:</p>
<ul>
<li>Re-analyze codebase (scores updated based on Iteration 1 changes)</li>
<li>Select next highest priority item</li>
<li>Repeat plan/implement/validate cycle</li>
</ul>
</li>
<li>
<p><strong>Continue</strong> until iteration limit reached or workflow completes without finding issues</p>
</li>
</ol>
<h3 id="controlling-iterations"><a class="header" href="#controlling-iterations">Controlling Iterations</a></h3>
<p>Iterations are controlled via the <code>-n</code> flag:</p>
<pre><code class="language-bash"># Single iteration (testing)
prodigy run workflows/debtmap.yml -yn 1

# Standard run (5 iterations)
prodigy run workflows/debtmap.yml -yn 5

# Deep cleanup (10+ iterations)
prodigy run workflows/debtmap.yml -yn 20
</code></pre>
<h3 id="what-happens-each-iteration"><a class="header" href="#what-happens-each-iteration">What Happens Each Iteration</a></h3>
<p>Each iteration runs the <strong>entire workflow from start to finish</strong>:</p>
<ol>
<li>Generate coverage data</li>
<li>Analyze technical debt</li>
<li>Create implementation plan</li>
<li>Execute plan</li>
<li>Validate improvement</li>
<li>Run tests (with auto-fixing)</li>
<li>Run linting (with auto-fixing)</li>
</ol>
<p>The workflow continues to the next iteration automatically if all steps succeed.</p>
<h3 id="example-output-5"><a class="header" href="#example-output-5">Example Output</a></h3>
<pre><code>Iteration 1:
  - Fixed: parse_expression() (9.2 → 5.1)
  - Fixed: calculate_score() (8.8 → 4.2)
  - Fixed: apply_weights() (8.5 → 5.8)
  ✓ Tests pass

Iteration 2:
  - Fixed: normalize_results() (7.5 → 3.9)
  - Fixed: aggregate_data() (7.2 → 4.1)
  ✓ Tests pass

Iteration 3:
  - No items above threshold (6.0)
  ✓ Early stop

Final Results:
  Items fixed: 5
  Average complexity: 15.2 → 8.6
</code></pre>
<h2 id="validation-1"><a class="header" href="#validation-1">Validation</a></h2>
<p>Prodigy validates changes at the workflow step level, not as a standalone configuration.</p>
<h3 id="step-level-validation-1"><a class="header" href="#step-level-validation-1">Step-Level Validation</a></h3>
<p>Validation is attached to specific workflow steps:</p>
<pre><code class="language-yaml">- claude: "/prodigy-debtmap-implement --plan .prodigy/IMPLEMENTATION_PLAN.md"
  commit_required: true
  validate:
    commands:
      - shell: "debtmap analyze . --lcov target/coverage/lcov.info --output .prodigy/debtmap-after.json --format json"
      - shell: "debtmap compare --before .prodigy/debtmap-before.json --after .prodigy/debtmap-after.json --plan .prodigy/IMPLEMENTATION_PLAN.md --output .prodigy/comparison.json --format json"
      - shell: "debtmap validate-improvement --comparison .prodigy/comparison.json --output .prodigy/debtmap-validation.json"
    result_file: ".prodigy/debtmap-validation.json"
    threshold: 75
    on_incomplete:
      commands:
        - claude: "/prodigy-complete-debtmap-fix --gaps ${validation.gaps} --plan .prodigy/IMPLEMENTATION_PLAN.md --attempt ${validation.attempt_number}"
          commit_required: true
        - shell: "just coverage-lcov"
        - shell: "debtmap analyze . --lcov target/coverage/lcov.info --output .prodigy/debtmap-after.json --format json"
        - shell: "debtmap compare --before .prodigy/debtmap-before.json --after .prodigy/debtmap-after.json --plan .prodigy/IMPLEMENTATION_PLAN.md --output .prodigy/comparison.json --format json"
      max_attempts: 5
      fail_workflow: true
</code></pre>
<h3 id="validation-process"><a class="header" href="#validation-process">Validation Process</a></h3>
<ol>
<li><strong>Commands run</strong>: Execute validation commands (shell or claude)</li>
<li><strong>Check result file</strong>: Read JSON file specified in <code>result_file</code></li>
<li><strong>Compare to threshold</strong>: Score must be &gt;= threshold (0-100 scale)</li>
<li><strong>On incomplete</strong>: If score &lt; threshold, run <code>on_incomplete</code> commands</li>
<li><strong>Retry</strong>: Repeat up to <code>max_attempts</code> times</li>
<li><strong>Fail or continue</strong>: If <code>fail_workflow: true</code>, stop workflow; otherwise continue</li>
</ol>
<h3 id="validation-result-format"><a class="header" href="#validation-result-format">Validation Result Format</a></h3>
<p>The <code>result_file</code> JSON should contain:</p>
<pre><code class="language-json">{
  "score": 85,
  "passed": true,
  "gaps": [],
  "details": "All debt improvement criteria met"
}
</code></pre>
<h3 id="test-validation-with-auto-fix"><a class="header" href="#test-validation-with-auto-fix">Test Validation with Auto-Fix</a></h3>
<p>Tests are validated with automatic fixing on failure:</p>
<pre><code class="language-yaml">- shell: "just test"
  on_failure:
    claude: "/prodigy-debug-test-failure --output ${shell.output}"
    max_attempts: 5
    fail_workflow: true
</code></pre>
<p>If tests fail, Prodigy automatically attempts to fix them up to 5 times before failing the workflow.</p>
<h2 id="output-and-metrics"><a class="header" href="#output-and-metrics">Output and Metrics</a></h2>
<h3 id="workflow-report"><a class="header" href="#workflow-report">Workflow Report</a></h3>
<pre><code class="language-json">{
  "workflow": "debtmap-debt-reduction",
  "iterations": 5,
  "items_processed": 12,
  "items_fixed": 10,
  "items_failed": 2,
  "metrics": {
    "complexity_before": 145,
    "complexity_after": 78,
    "complexity_reduction": -46.2,
    "coverage_before": 45.3,
    "coverage_after": 72.1,
    "coverage_improvement": 26.8
  },
  "changes": [
    {
      "file": "src/parser.rs",
      "function": "parse_expression",
      "before_score": 9.2,
      "after_score": 5.1,
      "improvements": ["Reduced complexity", "Added tests"]
    }
  ]
}
</code></pre>
<h3 id="commit-messages"><a class="header" href="#commit-messages">Commit Messages</a></h3>
<p>Prodigy generates descriptive commit messages:</p>
<pre><code>refactor(parser): reduce complexity in parse_expression

- Extract nested conditionals to helper functions
- Add unit tests for edge cases
- Coverage: 0% → 85%
- Complexity: 22 → 8

Generated by Prodigy workflow: debtmap-debt-reduction
Iteration: 1/5
</code></pre>
<h2 id="integration-with-cicd"><a class="header" href="#integration-with-cicd">Integration with CI/CD</a></h2>
<h3 id="github-actions-1"><a class="header" href="#github-actions-1">GitHub Actions</a></h3>
<pre><code class="language-yaml">name: Prodigy Debt Reduction

on:
  schedule:
    - cron: '0 0 * * 0'  # Weekly on Sunday
  workflow_dispatch:

jobs:
  reduce-debt:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Setup Rust
        uses: actions-rs/toolchain@v1
        with:
          toolchain: stable

      - name: Install Prodigy
        run: cargo install prodigy

      - name: Install dependencies
        run: |
          cargo install debtmap
          cargo install just

      - name: Run Prodigy workflow
        env:
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
        run: prodigy run workflows/debtmap.yml -yn 5

      - name: Create PR
        uses: peter-evans/create-pull-request@v5
        with:
          title: "chore: automated debt reduction via Prodigy"
          body: |
            Automated technical debt reduction using Prodigy workflow.

            This PR was generated by the weekly debt reduction workflow.
            Review changes carefully before merging.
          branch: prodigy-debt-reduction
</code></pre>
<h3 id="gitlab-ci-1"><a class="header" href="#gitlab-ci-1">GitLab CI</a></h3>
<pre><code class="language-yaml">prodigy-debt-reduction:
  stage: quality
  rules:
    - if: '$CI_PIPELINE_SOURCE == "schedule"'
  script:
    - cargo install prodigy
    - cargo install debtmap
    - cargo install just
    - prodigy run workflows/debtmap.yml -yn 5
  artifacts:
    paths:
      - .prodigy/debtmap-*.json
      - .prodigy/comparison.json
</code></pre>
<h3 id="important-ci-considerations"><a class="header" href="#important-ci-considerations">Important CI Considerations</a></h3>
<ul>
<li><strong>API Keys</strong>: Store <code>ANTHROPIC_API_KEY</code> as a secret</li>
<li><strong>Worktrees</strong>: MapReduce mode creates isolated worktrees automatically for parallel processing</li>
<li><strong>Dependencies</strong>: Install <code>prodigy</code>, <code>debtmap</code>, and <code>just</code> (or your build tool)</li>
<li><strong>Timeout</strong>: CI jobs may need extended timeout for multiple iterations</li>
<li><strong>Review</strong>: Always create a PR for human review before merging automated changes</li>
</ul>
<h2 id="best-practices-16"><a class="header" href="#best-practices-16">Best Practices</a></h2>
<h3 id="1-start-small"><a class="header" href="#1-start-small">1. Start Small</a></h3>
<p>Begin with low iteration counts:</p>
<pre><code class="language-bash"># First run: 1 iteration to test workflow
prodigy run workflows/debtmap.yml -yn 1

# Standard run: 3-5 iterations
prodigy run workflows/debtmap.yml -yn 5
</code></pre>
<h3 id="2-focus-on-high-priority-items"><a class="header" href="#2-focus-on-high-priority-items">2. Focus on High-Priority Items</a></h3>
<p>The debtmap analysis automatically prioritizes by:</p>
<ul>
<li>Complexity score (cyclomatic complexity)</li>
<li>Coverage percentage (lower coverage = higher priority)</li>
<li>Risk score (complexity × (100 - coverage%))</li>
</ul>
<p>To focus on specific areas, create custom slash commands in <code>.claude/commands/</code> that filter by:</p>
<ul>
<li>Module/file patterns</li>
<li>Specific debt types (Complexity, TestGap, Duplication)</li>
<li>Score thresholds</li>
</ul>
<h3 id="3-validate-thoroughly"><a class="header" href="#3-validate-thoroughly">3. Validate Thoroughly</a></h3>
<p>Use comprehensive validation in your workflow:</p>
<pre><code class="language-yaml">- shell: "just test"
  on_failure:
    claude: "/prodigy-debug-test-failure --output ${shell.output}"
    max_attempts: 5
    fail_workflow: true

- shell: "just fmt-check &amp;&amp; just lint"
  on_failure:
    claude: "/prodigy-lint ${shell.output}"
    max_attempts: 5
    fail_workflow: true
</code></pre>
<h3 id="4-review-before-merging"><a class="header" href="#4-review-before-merging">4. Review Before Merging</a></h3>
<p>Always review Prodigy’s changes:</p>
<pre><code class="language-bash"># Find your worktree
ls ~/.prodigy/worktrees/

# Check changes
cd ~/.prodigy/worktrees/session-xxx
git diff main

# Review commit history
git log --oneline

# Run full test suite
cargo test --all-features
</code></pre>
<h3 id="5-monitor-progress"><a class="header" href="#5-monitor-progress">5. Monitor Progress</a></h3>
<p>Track debt reduction over iterations:</p>
<pre><code class="language-bash"># Compare before and after
debtmap compare --before .prodigy/debtmap-before.json --after .prodigy/debtmap-after.json

# View detailed metrics
cat .prodigy/comparison.json | jq
</code></pre>
<h2 id="troubleshooting-18"><a class="header" href="#troubleshooting-18">Troubleshooting</a></h2>
<h3 id="workflow-fails-to-start"><a class="header" href="#workflow-fails-to-start">Workflow Fails to Start</a></h3>
<p><strong>Issue</strong>: “Prodigy not found” or “API key missing”</p>
<p><strong>Solution</strong>:</p>
<pre><code class="language-bash"># Install Prodigy
cargo install prodigy

# Set API key
export ANTHROPIC_API_KEY="your-key"

# Verify installation
prodigy --version
</code></pre>
<h3 id="validation-failures"><a class="header" href="#validation-failures">Validation Failures</a></h3>
<p><strong>Issue</strong>: Validation score below threshold</p>
<p><strong>Solution</strong>: Check validation results:</p>
<pre><code class="language-bash"># View validation details
cat .prodigy/debtmap-validation.json

# Check what gaps remain
cat .prodigy/debtmap-validation.json | jq '.gaps'

# Review comparison results
cat .prodigy/comparison.json
</code></pre>
<p>The workflow will automatically retry up to <code>max_attempts</code> times with <code>/prodigy-complete-debtmap-fix</code>.</p>
<h3 id="test-failures"><a class="header" href="#test-failures">Test Failures</a></h3>
<p><strong>Issue</strong>: Tests fail after implementation</p>
<p><strong>Solution</strong>: The workflow includes automatic test fixing:</p>
<pre><code class="language-yaml">- shell: "just test"
  on_failure:
    claude: "/prodigy-debug-test-failure --output ${shell.output}"
    max_attempts: 5
    fail_workflow: true
</code></pre>
<p>If tests still fail after 5 attempts, review manually:</p>
<pre><code class="language-bash"># Check test output
just test

# Review recent changes
git diff HEAD~1
</code></pre>
<h3 id="no-items-processed"><a class="header" href="#no-items-processed">No Items Processed</a></h3>
<p><strong>Issue</strong>: Workflow completes but doesn’t find debt to fix</p>
<p><strong>Possible Causes</strong>:</p>
<ol>
<li>Codebase has very low debt scores (below selection threshold)</li>
<li>Coverage data not generated properly</li>
<li>Debtmap analysis found no high-priority items</li>
</ol>
<p><strong>Solution</strong>:</p>
<pre><code class="language-bash"># Check debtmap analysis results
cat .prodigy/debtmap-before.json | jq '.items | sort_by(-.unified_score.final_score) | .[0:5]'

# Verify coverage was generated
ls -lh target/coverage/lcov.info

# Run debtmap manually to see what's detected
debtmap analyze . --lcov target/coverage/lcov.info
</code></pre>
<h3 id="workflow-hangs-or-times-out"><a class="header" href="#workflow-hangs-or-times-out">Workflow Hangs or Times Out</a></h3>
<p><strong>Issue</strong>: Workflow takes too long or appears stuck</p>
<p><strong>Possible Causes</strong>:</p>
<ul>
<li>Large codebase with many files</li>
<li>Complex refactoring requiring extensive analysis</li>
<li>Network issues with Claude API</li>
</ul>
<p><strong>Solution</strong>:</p>
<ul>
<li>Reduce iteration count for testing (<code>-n 1</code>)</li>
<li>Check Claude API connectivity</li>
<li>Monitor worktree for progress: <code>cd ~/.prodigy/worktrees/session-xxx &amp;&amp; git log</code></li>
</ul>
<h3 id="mapreduce-specific-troubleshooting"><a class="header" href="#mapreduce-specific-troubleshooting">MapReduce-Specific Troubleshooting</a></h3>
<h4 id="resuming-failed-mapreduce-jobs"><a class="header" href="#resuming-failed-mapreduce-jobs">Resuming Failed MapReduce Jobs</a></h4>
<p><strong>Issue</strong>: MapReduce job was interrupted or failed</p>
<p><strong>Solution</strong>:</p>
<pre><code class="language-bash"># Find the job ID from recent sessions
prodigy sessions

# Resume the MapReduce job from checkpoint
prodigy resume-job &lt;JOB_ID&gt;
</code></pre>
<p>The job will resume from where it left off, skipping already-completed items.</p>
<h4 id="checking-mapreduce-progress"><a class="header" href="#checking-mapreduce-progress">Checking MapReduce Progress</a></h4>
<p><strong>Issue</strong>: Want to monitor long-running MapReduce job</p>
<p><strong>Solution</strong>:</p>
<pre><code class="language-bash"># View overall progress
prodigy progress &lt;JOB_ID&gt;

# View detailed events
prodigy events &lt;JOB_ID&gt;

# Filter for specific event types
prodigy events &lt;JOB_ID&gt; --type agent_completed
prodigy events &lt;JOB_ID&gt; --type agent_failed
</code></pre>
<p><strong>Output example</strong>:</p>
<pre><code>MapReduce Job: job-abc123
Status: running
Progress: 7/10 items (70%)
- Completed: 5
- Running: 2
- Failed: 3
</code></pre>
<h4 id="managing-failed-mapreduce-items"><a class="header" href="#managing-failed-mapreduce-items">Managing Failed MapReduce Items</a></h4>
<p><strong>Issue</strong>: Some agents failed, items in Dead Letter Queue</p>
<p><strong>Solution</strong>:</p>
<pre><code class="language-bash"># View failed items
prodigy dlq list &lt;JOB_ID&gt;

# Review why an item failed (check events)
prodigy events &lt;JOB_ID&gt; --item &lt;ITEM_ID&gt;

# Retry specific failed item
prodigy dlq retry &lt;JOB_ID&gt; &lt;ITEM_ID&gt;

# Remove unfixable items from DLQ
prodigy dlq remove &lt;JOB_ID&gt; &lt;ITEM_ID&gt;
</code></pre>
<p><strong>Common failure reasons</strong>:</p>
<ul>
<li>Validation threshold not met after max_attempts</li>
<li>Tests fail and can’t be fixed automatically</li>
<li>Merge conflicts with other agents’ changes</li>
<li>Timeout exceeded for complex refactoring</li>
</ul>
<h4 id="cleaning-up-mapreduce-worktrees"><a class="header" href="#cleaning-up-mapreduce-worktrees">Cleaning Up MapReduce Worktrees</a></h4>
<p><strong>Issue</strong>: Disk space consumed by many MapReduce worktrees</p>
<p><strong>Solution</strong>:</p>
<pre><code class="language-bash"># List all worktrees
prodigy worktree list

# Clean up completed job worktrees
prodigy worktree clean

# Remove specific session's worktrees
prodigy worktree remove &lt;SESSION_ID&gt;

# Manual cleanup (if Prodigy commands don't work)
rm -rf ~/.prodigy/worktrees/session-xxx
</code></pre>
<p><strong>When to clean</strong>:</p>
<ul>
<li>After successful job completion and merge</li>
<li>When disk space is low</li>
<li>After abandoned or failed jobs</li>
</ul>
<h4 id="mapreduce-merge-conflicts"><a class="header" href="#mapreduce-merge-conflicts">MapReduce Merge Conflicts</a></h4>
<p><strong>Issue</strong>: Reduce phase fails due to merge conflicts between agent worktrees</p>
<p><strong>Possible Causes</strong>:</p>
<ul>
<li>Multiple agents modified overlapping code</li>
<li>Agents made conflicting architectural changes</li>
<li>Shared dependencies updated differently</li>
</ul>
<p><strong>Solution</strong>:</p>
<pre><code class="language-bash"># Review which agents succeeded
prodigy events &lt;JOB_ID&gt; --type agent_completed

# Check merge conflicts
cd ~/.prodigy/worktrees/session-xxx
git status

# Manually resolve conflicts
# Edit conflicting files
git add .
git commit -m "Resolve MapReduce merge conflicts"

# Resume the job
prodigy resume-job &lt;JOB_ID&gt;
</code></pre>
<p><strong>Prevention</strong>:</p>
<ul>
<li>Use <code>filter</code> to ensure agents work on independent items</li>
<li>Reduce <code>max_parallel</code> to minimize conflicts</li>
<li>Design debt items to be truly independent</li>
</ul>
<h4 id="understanding-mapreduce-variables"><a class="header" href="#understanding-mapreduce-variables">Understanding MapReduce Variables</a></h4>
<p>If you’re debugging workflow files, these variables are available:</p>
<p><strong>In map phase (agent_template)</strong>:</p>
<ul>
<li><code>${item}</code>: Full JSON of current item being processed</li>
<li><code>${item_id}</code>: Unique ID for current item</li>
<li><code>${validation.gaps}</code>: Validation gaps from validation result</li>
<li><code>${validation.attempt_number}</code>: Current retry attempt (1, 2, 3…)</li>
<li><code>${shell.output}</code>: Output from previous shell command</li>
</ul>
<p><strong>In reduce phase</strong>:</p>
<ul>
<li><code>${map.results}</code>: All map agent results as JSON</li>
<li><code>${map.successful}</code>: Count of successful agents</li>
<li><code>${map.failed}</code>: Count of failed agents</li>
<li><code>${map.total}</code>: Total number of agents</li>
</ul>
<p><strong>Example debug command</strong>:</p>
<pre><code class="language-yaml"># In agent_template, log the item being processed
- shell: "echo 'Processing item: ${item_id}' &gt;&gt; .prodigy/debug.log"
</code></pre>
<h2 id="example-workflows-1"><a class="header" href="#example-workflows-1">Example Workflows</a></h2>
<h3 id="full-repository-cleanup"><a class="header" href="#full-repository-cleanup">Full Repository Cleanup</a></h3>
<p>For comprehensive debt reduction, use a higher iteration count:</p>
<pre><code class="language-bash"># Run 10 iterations for deeper cleanup
prodigy run workflows/debtmap.yml -yn 10

# Run 20 iterations for major refactoring
prodigy run workflows/debtmap.yml -yn 20
</code></pre>
<p>The workflow automatically:</p>
<ol>
<li>Selects highest priority items each iteration</li>
<li>Addresses different debt types (Complexity, TestGap, Duplication)</li>
<li>Validates all changes with tests and linting</li>
<li>Commits only successful improvements</li>
</ol>
<h3 id="custom-workflow-for-specific-focus"><a class="header" href="#custom-workflow-for-specific-focus">Custom Workflow for Specific Focus</a></h3>
<p>Create a custom workflow file for focused improvements:</p>
<p><strong><code>workflows/add-tests.yml</code></strong> - Focus on test coverage:</p>
<pre><code class="language-yaml"># Generate coverage
- shell: "just coverage-lcov"

# Analyze with focus on test gaps
- shell: "debtmap analyze . --lcov target/coverage/lcov.info --output .prodigy/debtmap-before.json --format json"

# Create plan (slash command will prioritize TestGap items)
- claude: "/prodigy-debtmap-plan --before .prodigy/debtmap-before.json --output .prodigy/IMPLEMENTATION_PLAN.md"

# ... rest of standard workflow steps
</code></pre>
<p>Run with:</p>
<pre><code class="language-bash">prodigy run workflows/add-tests.yml -yn 5
</code></pre>
<h3 id="targeted-module-cleanup"><a class="header" href="#targeted-module-cleanup">Targeted Module Cleanup</a></h3>
<p>Create a custom slash command to focus on specific modules:</p>
<p><strong><code>.claude/commands/refactor-module.md</code></strong>:</p>
<pre><code class="language-markdown"># /refactor-module

Refactor the highest complexity item in the specified module.

Arguments: --module &lt;module_name&gt;

... implementation details ...
</code></pre>
<p>Then create a workflow using this command for targeted refactoring.</p>
<h2 id="see-also-8"><a class="header" href="#see-also-8">See Also</a></h2>
<ul>
<li><a href="#cli-reference">Debtmap CLI Reference</a> - All Debtmap command options including <code>analyze</code>, <code>compare</code>, and <code>validate</code></li>
<li><a href="#coverage-integration-1">Coverage Integration</a> - Generating and using LCOV coverage data with Debtmap</li>
<li><a href="#configuration-2">Configuration</a> - Debtmap configuration file options</li>
<li><a href="#tiered-prioritization-3">Tiered Prioritization</a> - Understanding how Debtmap scores and prioritizes debt items</li>
<li><a href="https://github.com/iepathos/prodigy">Prodigy Documentation</a> - Full Prodigy reference and advanced features</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="responsibility-analysis"><a class="header" href="#responsibility-analysis">Responsibility Analysis</a></h1>
<h2 id="overview-18"><a class="header" href="#overview-18">Overview</a></h2>
<p>Responsibility analysis is a core feature of Debtmap that helps identify violations of the <strong>Single Responsibility Principle (SRP)</strong>, one of the fundamental SOLID design principles. By analyzing function and method names, Debtmap automatically infers the distinct functional responsibilities within a code unit and detects when a single module, struct, or class has taken on too many concerns.</p>
<p>This chapter provides an in-depth look at how Debtmap determines responsibilities, categorizes them, and uses this information to guide refactoring decisions.</p>
<h2 id="what-are-responsibilities"><a class="header" href="#what-are-responsibilities">What Are Responsibilities?</a></h2>
<p>In the context of Debtmap, a <strong>responsibility</strong> is a distinct functional domain or concern that a code unit handles. Examples include:</p>
<ul>
<li><strong>Data Access</strong> - Getting and setting values from data structures</li>
<li><strong>Validation</strong> - Checking inputs, verifying constraints, ensuring correctness</li>
<li><strong>Persistence</strong> - Saving and loading data to/from storage</li>
<li><strong>Computation</strong> - Performing calculations and transformations</li>
<li><strong>Communication</strong> - Sending and receiving messages or events</li>
</ul>
<p>According to the Single Responsibility Principle, each module should have <strong>one and only one reason to change</strong>. When a module handles multiple unrelated responsibilities (e.g., validation, persistence, AND computation), it becomes:</p>
<ul>
<li><strong>Harder to understand</strong> - Developers must mentally juggle multiple concerns</li>
<li><strong>More fragile</strong> - Changes to one responsibility can break others</li>
<li><strong>Difficult to test</strong> - Testing requires complex setup across multiple domains</li>
<li><strong>Prone to coupling</strong> - Dependencies from different domains become entangled</li>
</ul>
<p>Debtmap’s responsibility analysis automatically identifies these violations and provides concrete recommendations for splitting modules along responsibility boundaries.</p>
<h2 id="how-responsibilities-are-detected"><a class="header" href="#how-responsibilities-are-detected">How Responsibilities Are Detected</a></h2>
<h3 id="pattern-based-inference"><a class="header" href="#pattern-based-inference">Pattern-Based Inference</a></h3>
<p>Debtmap uses <strong>prefix-based pattern matching</strong> to infer responsibilities from function and method names. This approach is both simple and effective because well-named functions naturally express their intent through conventional prefixes.</p>
<p><strong>Implementation Location:</strong> <code>src/organization/god_object_analysis.rs:316-386</code></p>
<p>The <code>infer_responsibility_from_method()</code> function performs case-insensitive prefix matching:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub fn infer_responsibility_from_method(method_name: &amp;str) -&gt; String {
    let lower_name = method_name.to_lowercase();

    if lower_name.starts_with("format_") || lower_name.starts_with("render_") {
        return "Formatting &amp; Output".to_string();
    }
    if lower_name.starts_with("parse_") || lower_name.starts_with("read_") {
        return "Parsing &amp; Input".to_string();
    }
    // ... additional patterns
}
<span class="boring">}</span></code></pre>
<p>This approach works across languages (Rust, Python, JavaScript/TypeScript) because naming conventions are relatively consistent in modern codebases.</p>
<h3 id="responsibility-categories"><a class="header" href="#responsibility-categories">Responsibility Categories</a></h3>
<p>Debtmap recognizes <strong>11 built-in responsibility categories</strong> plus a generic “Utilities” fallback:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Category</th><th>Prefixes</th><th>Examples</th></tr>
</thead>
<tbody>
<tr><td><strong>Formatting &amp; Output</strong></td><td><code>format_</code>, <code>render_</code>, <code>write_</code>, <code>print_</code></td><td><code>format_json()</code>, <code>render_table()</code>, <code>write_report()</code></td></tr>
<tr><td><strong>Parsing &amp; Input</strong></td><td><code>parse_</code>, <code>read_</code>, <code>extract_</code></td><td><code>parse_config()</code>, <code>read_file()</code>, <code>extract_fields()</code></td></tr>
<tr><td><strong>Filtering &amp; Selection</strong></td><td><code>filter_</code>, <code>select_</code>, <code>find_</code></td><td><code>filter_results()</code>, <code>select_top()</code>, <code>find_item()</code></td></tr>
<tr><td><strong>Transformation</strong></td><td><code>transform_</code>, <code>convert_</code>, <code>map_</code>, <code>apply_</code></td><td><code>transform_data()</code>, <code>convert_type()</code>, <code>map_fields()</code></td></tr>
<tr><td><strong>Data Access</strong></td><td><code>get_</code>, <code>set_</code></td><td><code>get_value()</code>, <code>set_name()</code></td></tr>
<tr><td><strong>Validation</strong></td><td><code>validate_</code>, <code>check_</code>, <code>verify_</code>, <code>is_*</code></td><td><code>validate_input()</code>, <code>check_bounds()</code>, <code>is_valid()</code></td></tr>
<tr><td><strong>Computation</strong></td><td><code>calculate_</code>, <code>compute_</code></td><td><code>calculate_score()</code>, <code>compute_sum()</code></td></tr>
<tr><td><strong>Construction</strong></td><td><code>create_</code>, <code>build_</code>, <code>new_*</code>, <code>make_</code></td><td><code>create_instance()</code>, <code>build_config()</code>, <code>new_user()</code></td></tr>
<tr><td><strong>Persistence</strong></td><td><code>save_</code>, <code>load_</code>, <code>store_</code></td><td><code>save_data()</code>, <code>load_cache()</code>, <code>store_result()</code></td></tr>
<tr><td><strong>Processing</strong></td><td><code>process_</code>, <code>handle_</code></td><td><code>process_request()</code>, <code>handle_error()</code></td></tr>
<tr><td><strong>Communication</strong></td><td><code>send_</code>, <code>receive_</code></td><td><code>send_message()</code>, <code>receive_data()</code></td></tr>
<tr><td><strong>Utilities</strong></td><td><em>(all others)</em></td><td><code>helper()</code>, <code>do_work()</code>, <code>utility_fn()</code></td></tr>
</tbody>
</table>
</div>
<h3 id="grouping-methods-by-responsibility"><a class="header" href="#grouping-methods-by-responsibility">Grouping Methods by Responsibility</a></h3>
<p>Once individual methods are categorized, Debtmap groups them using <code>group_methods_by_responsibility()</code>:</p>
<p><strong>Implementation Location:</strong> <code>src/organization/god_object_analysis.rs:268-280</code></p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub fn group_methods_by_responsibility(methods: &amp;[String]) -&gt; HashMap&lt;String, Vec&lt;String&gt;&gt; {
    let mut groups: HashMap&lt;String, Vec&lt;String&gt;&gt; = HashMap::new();
    for method in methods {
        let responsibility = infer_responsibility_from_method(method);
        groups.entry(responsibility).or_default().push(method.clone());
    }
    groups
}
<span class="boring">}</span></code></pre>
<p><strong>Output Structure:</strong></p>
<ul>
<li><strong>Keys</strong>: Responsibility category names (e.g., “Data Access”, “Validation”)</li>
<li><strong>Values</strong>: Lists of method names belonging to each category</li>
</ul>
<p>The <strong>responsibility count</strong> is simply the number of unique keys in this HashMap.</p>
<h3 id="example-analysis"><a class="header" href="#example-analysis">Example Analysis</a></h3>
<p>Consider a Rust struct with these methods:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>impl UserManager {
    fn get_user(&amp;self, id: UserId) -&gt; Option&lt;User&gt; { }
    fn set_password(&amp;mut self, id: UserId, password: &amp;str) { }
    fn validate_email(&amp;self, email: &amp;str) -&gt; bool { }
    fn validate_password(&amp;self, password: &amp;str) -&gt; bool { }
    fn save_user(&amp;self, user: &amp;User) -&gt; Result&lt;()&gt; { }
    fn load_user(&amp;self, id: UserId) -&gt; Result&lt;User&gt; { }
    fn send_notification(&amp;self, user_id: UserId, msg: &amp;str) { }
    fn format_user_profile(&amp;self, user: &amp;User) -&gt; String { }
}
<span class="boring">}</span></code></pre>
<p><strong>Debtmap’s Analysis:</strong></p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Method</th><th>Inferred Responsibility</th></tr>
</thead>
<tbody>
<tr><td><code>get_user</code></td><td>Data Access</td></tr>
<tr><td><code>set_password</code></td><td>Data Access</td></tr>
<tr><td><code>validate_email</code></td><td>Validation</td></tr>
<tr><td><code>validate_password</code></td><td>Validation</td></tr>
<tr><td><code>save_user</code></td><td>Persistence</td></tr>
<tr><td><code>load_user</code></td><td>Persistence</td></tr>
<tr><td><code>send_notification</code></td><td>Communication</td></tr>
<tr><td><code>format_user_profile</code></td><td>Formatting &amp; Output</td></tr>
</tbody>
</table>
</div>
<p><strong>Result:</strong></p>
<ul>
<li><strong>Responsibility Count</strong>: 5 (Data Access, Validation, Persistence, Communication, Formatting)</li>
<li><strong>Assessment</strong>: This violates SRP - <code>UserManager</code> has too many distinct concerns</li>
</ul>
<h2 id="responsibility-scoring"><a class="header" href="#responsibility-scoring">Responsibility Scoring</a></h2>
<h3 id="integration-with-god-object-detection-1"><a class="header" href="#integration-with-god-object-detection-1">Integration with God Object Detection</a></h3>
<p>Responsibility count is a critical factor in <a href="#god-object-detection-1">God Object Detection</a>. The scoring algorithm includes:</p>
<pre><code>responsibility_factor = min(responsibility_count / 3.0, 3.0)
god_object_score = method_factor × field_factor × responsibility_factor × size_factor
</code></pre>
<p><strong>Why divide by 3.0?</strong></p>
<ul>
<li><strong>1-3 responsibilities</strong>: Normal, well-scoped module</li>
<li><strong>4-6 responsibilities</strong>: Warning signs, approaching problematic territory</li>
<li><strong>7+ responsibilities</strong>: Severe violation, likely a god object</li>
</ul>
<h3 id="language-specific-thresholds-1"><a class="header" href="#language-specific-thresholds-1">Language-Specific Thresholds</a></h3>
<p>Different languages have different expectations for responsibility counts:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Language</th><th>Max Responsibilities</th><th>Rationale</th></tr>
</thead>
<tbody>
<tr><td><strong>Rust</strong></td><td>5</td><td>Strong module system encourages tight boundaries</td></tr>
<tr><td><strong>Python</strong></td><td>3</td><td>Duck typing makes mixing concerns more dangerous</td></tr>
<tr><td><strong>JavaScript/TypeScript</strong></td><td>3</td><td>Prototype-based, benefits from focused classes</td></tr>
</tbody>
</table>
</div>
<p>These thresholds can be customized in <code>.debtmap.toml</code>:</p>
<pre><code class="language-toml">[god_object_detection.rust]
max_traits = 5      # max_traits = max responsibilities

[god_object_detection.python]
max_traits = 3
</code></pre>
<h3 id="confidence-determination"><a class="header" href="#confidence-determination">Confidence Determination</a></h3>
<p>Responsibility count contributes to overall confidence levels:</p>
<p><strong>Implementation Location:</strong> <code>src/organization/god_object_analysis.rs:234-266</code></p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub fn determine_confidence(
    method_count: usize,
    field_count: usize,
    responsibility_count: usize,
    lines_of_code: usize,
    complexity_sum: u32,
    thresholds: &amp;GodObjectThresholds,
) -&gt; GodObjectConfidence {
    let mut violations = 0;

    if responsibility_count &gt; thresholds.max_traits {
        violations += 1;
    }
    // ... check other metrics

    match violations {
        5 =&gt; GodObjectConfidence::Definite,
        3..=4 =&gt; GodObjectConfidence::Probable,
        1..=2 =&gt; GodObjectConfidence::Possible,
        _ =&gt; GodObjectConfidence::NotGodObject,
    }
}
<span class="boring">}</span></code></pre>
<h2 id="advanced-responsibility-detection"><a class="header" href="#advanced-responsibility-detection">Advanced Responsibility Detection</a></h2>
<h3 id="module-level-analysis"><a class="header" href="#module-level-analysis">Module-Level Analysis</a></h3>
<p>For large modules without a single dominant struct, Debtmap performs <strong>module-level responsibility detection</strong>:</p>
<p><strong>Implementation Location:</strong> <code>src/organization/god_object_detector.rs:682-697</code></p>
<p>The <code>classify_responsibility()</code> function provides extended categorization:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn classify_responsibility(prefix: &amp;str) -&gt; String {
    match prefix {
        "get" | "set" =&gt; "Data Access",
        "calculate" | "compute" =&gt; "Computation",
        "validate" | "check" | "verify" | "ensure" =&gt; "Validation",
        "save" | "load" | "store" | "retrieve" | "fetch" =&gt; "Persistence",
        "create" | "build" | "new" | "make" | "init" =&gt; "Construction",
        "send" | "receive" | "handle" | "manage" =&gt; "Communication",
        "update" | "modify" | "change" | "edit" =&gt; "Modification",
        "delete" | "remove" | "clear" | "reset" =&gt; "Deletion",
        "is" | "has" | "can" | "should" | "will" =&gt; "State Query",
        "process" | "transform" =&gt; "Processing",
        _ =&gt; format!("{} Operations", capitalize_first(prefix)),
    }
}
<span class="boring">}</span></code></pre>
<p>This extended mapping covers 10 core categories plus dynamic fallback for custom prefixes.</p>
<h3 id="responsibility-groups"><a class="header" href="#responsibility-groups">Responsibility Groups</a></h3>
<p>The <code>ResponsibilityGroup</code> data structure tracks detailed information about each responsibility:</p>
<p><strong>Implementation Location:</strong> <code>src/organization/mod.rs:156-161</code></p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[derive(Debug, Clone, PartialEq)]
pub struct ResponsibilityGroup {
    pub name: String,           // e.g., "DataAccessManager"
    pub methods: Vec&lt;String&gt;,   // Methods in this group
    pub fields: Vec&lt;String&gt;,    // Associated fields
    pub responsibility: String, // e.g., "Data Access"
}
<span class="boring">}</span></code></pre>
<p>This structure enables:</p>
<ul>
<li><strong>Refactoring recommendations</strong> - Suggest splitting by responsibility group</li>
<li><strong>Cohesion analysis</strong> - Measure how tightly methods are related</li>
<li><strong>Field-method correlation</strong> - Identify which fields belong to which responsibilities</li>
</ul>
<h2 id="refactoring-based-on-responsibilities"><a class="header" href="#refactoring-based-on-responsibilities">Refactoring Based on Responsibilities</a></h2>
<h3 id="recommended-module-splits"><a class="header" href="#recommended-module-splits">Recommended Module Splits</a></h3>
<p>When Debtmap detects a module with multiple responsibilities, it generates actionable refactoring recommendations using <code>recommend_module_splits()</code>:</p>
<p><strong>Implementation Location:</strong> <code>src/organization/god_object_detector.rs:165-177</code></p>
<p><strong>Process:</strong></p>
<ol>
<li>Group all methods by their inferred responsibilities</li>
<li>Create a <code>ModuleSplit</code> for each responsibility group</li>
<li>Suggest module names (e.g., “DataAccessManager”, “ValidationManager”)</li>
<li>Estimate lines of code for each new module</li>
<li>Order by cohesion (most focused groups first)</li>
</ol>
<p><strong>Example Output:</strong></p>
<pre><code>Recommended Splits for UserManager:
  1. DataAccessManager (5 methods, ~80 lines)
     - get_user, set_password, get_email, set_email, update_profile

  2. ValidationManager (4 methods, ~60 lines)
     - validate_email, validate_password, check_permissions, verify_token

  3. PersistenceManager (3 methods, ~50 lines)
     - save_user, load_user, delete_user

  4. NotificationManager (2 methods, ~30 lines)
     - send_notification, send_bulk_notifications
</code></pre>
<h3 id="practical-refactoring-patterns"><a class="header" href="#practical-refactoring-patterns">Practical Refactoring Patterns</a></h3>
<h4 id="pattern-1-extract-service-classes"><a class="header" href="#pattern-1-extract-service-classes">Pattern 1: Extract Service Classes</a></h4>
<p><strong>Before (God Object):</strong></p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>struct UserManager {
    db: Database,
    cache: Cache,
    notifier: Notifier,
}

impl UserManager {
    fn get_user(&amp;self, id: UserId) -&gt; Option&lt;User&gt; { }
    fn validate_email(&amp;self, email: &amp;str) -&gt; bool { }
    fn save_user(&amp;self, user: &amp;User) -&gt; Result&lt;()&gt; { }
    fn send_notification(&amp;self, id: UserId, msg: &amp;str) { }
}
<span class="boring">}</span></code></pre>
<p><strong>After (Split by Responsibility):</strong></p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Data Access
struct UserRepository {
    db: Database,
    cache: Cache,
}

// Validation
struct UserValidator;

// Persistence
struct UserPersistence {
    db: Database,
}

// Communication
struct NotificationService {
    notifier: Notifier,
}
<span class="boring">}</span></code></pre>
<h4 id="pattern-2-use-facade-for-composition"><a class="header" href="#pattern-2-use-facade-for-composition">Pattern 2: Use Facade for Composition</a></h4>
<p>After splitting, create a facade to coordinate:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>struct UserFacade {
    repository: UserRepository,
    validator: UserValidator,
    persistence: UserPersistence,
    notifier: NotificationService,
}

impl UserFacade {
    fn register_user(&amp;mut self, user: User) -&gt; Result&lt;()&gt; {
        self.validator.validate_email(&amp;user.email)?;
        self.persistence.save_user(&amp;user)?;
        self.notifier.send_welcome(&amp;user.id)?;
        Ok(())
    }
}
<span class="boring">}</span></code></pre>
<h4 id="pattern-3-trait-based-separation-rust"><a class="header" href="#pattern-3-trait-based-separation-rust">Pattern 3: Trait-Based Separation (Rust)</a></h4>
<p>Use traits to define responsibility boundaries:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>trait DataAccess {
    fn get_user(&amp;self, id: UserId) -&gt; Option&lt;User&gt;;
}

trait Validation {
    fn validate_email(&amp;self, email: &amp;str) -&gt; bool;
}

trait Persistence {
    fn save_user(&amp;self, user: &amp;User) -&gt; Result&lt;()&gt;;
}

// Implement only the needed traits per struct
impl DataAccess for UserRepository { }
impl Validation for UserValidator { }
impl Persistence for UserPersistence { }
<span class="boring">}</span></code></pre>
<h2 id="data-structures"><a class="header" href="#data-structures">Data Structures</a></h2>
<h3 id="godobjectanalysis"><a class="header" href="#godobjectanalysis">GodObjectAnalysis</a></h3>
<p>The main result structure includes responsibility information:</p>
<p><strong>Implementation Location:</strong> <code>src/organization/god_object_analysis.rs:5-18</code></p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct GodObjectAnalysis {
    pub is_god_object: bool,
    pub method_count: usize,
    pub field_count: usize,
    pub responsibility_count: usize,      // Number of distinct responsibilities
    pub lines_of_code: usize,
    pub complexity_sum: u32,
    pub god_object_score: f64,
    pub recommended_splits: Vec&lt;ModuleSplit&gt;,
    pub confidence: GodObjectConfidence,
    pub responsibilities: Vec&lt;String&gt;,    // List of responsibility names
    pub purity_distribution: Option&lt;PurityDistribution&gt;,
}
<span class="boring">}</span></code></pre>
<h3 id="modulesplit"><a class="header" href="#modulesplit">ModuleSplit</a></h3>
<p>Recommendations for splitting modules:</p>
<p><strong>Implementation Location:</strong> <code>src/organization/god_object_analysis.rs:40-45</code></p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct ModuleSplit {
    pub suggested_name: String,         // e.g., "ValidationManager"
    pub methods_to_move: Vec&lt;String&gt;,   // Methods for this module
    pub responsibility: String,          // Responsibility category
    pub estimated_lines: usize,         // Approximate LOC
}
<span class="boring">}</span></code></pre>
<h2 id="testing-responsibility-detection"><a class="header" href="#testing-responsibility-detection">Testing Responsibility Detection</a></h2>
<p>Debtmap includes comprehensive tests for responsibility detection:</p>
<p><strong>Implementation Location:</strong> <code>src/organization/god_object_analysis.rs:623-838</code></p>
<h3 id="test-coverage"><a class="header" href="#test-coverage">Test Coverage</a></h3>
<p><strong>Key test cases:</strong></p>
<ul>
<li><strong>Prefix recognition</strong> - Each of the 11 categories is tested individually</li>
<li><strong>Case insensitivity</strong> - <code>Format_Output</code> and <code>format_output</code> both map to “Formatting &amp; Output”</li>
<li><strong>Multiple responsibilities</strong> - Grouping diverse methods correctly</li>
<li><strong>Empty input handling</strong> - Graceful handling of empty method lists</li>
<li><strong>Edge cases</strong> - Methods without recognized prefixes default to “Utilities”</li>
</ul>
<p><strong>Example Test:</strong></p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[test]
fn test_multiple_responsibility_groups() {
    let methods = vec![
        "format_output".to_string(),
        "parse_input".to_string(),
        "get_value".to_string(),
        "validate_data".to_string(),
    ];

    let groups = group_methods_by_responsibility(&amp;methods);

    assert_eq!(groups.len(), 4); // 4 distinct responsibilities
    assert!(groups.contains_key("Formatting &amp; Output"));
    assert!(groups.contains_key("Parsing &amp; Input"));
    assert!(groups.contains_key("Data Access"));
    assert!(groups.contains_key("Validation"));
}
<span class="boring">}</span></code></pre>
<h2 id="configuration-14"><a class="header" href="#configuration-14">Configuration</a></h2>
<h3 id="toml-configuration-2"><a class="header" href="#toml-configuration-2">TOML Configuration</a></h3>
<p>Customize responsibility thresholds in <code>.debtmap.toml</code>:</p>
<pre><code class="language-toml">[god_object_detection]
enabled = true

[god_object_detection.rust]
max_traits = 5      # Max responsibilities for Rust

[god_object_detection.python]
max_traits = 3      # Max responsibilities for Python

[god_object_detection.javascript]
max_traits = 3      # Max responsibilities for JavaScript/TypeScript
</code></pre>
<h3 id="tuning-guidelines-2"><a class="header" href="#tuning-guidelines-2">Tuning Guidelines</a></h3>
<p><strong>Strict SRP Enforcement:</strong></p>
<pre><code class="language-toml">[god_object_detection.rust]
max_traits = 3
</code></pre>
<ul>
<li>Enforces very tight single responsibility</li>
<li>Suitable for greenfield projects or strict refactoring efforts</li>
</ul>
<p><strong>Balanced Approach (Default):</strong></p>
<pre><code class="language-toml">[god_object_detection.rust]
max_traits = 5
</code></pre>
<ul>
<li>Allows some flexibility while catching major violations</li>
<li>Works well for most projects</li>
</ul>
<p><strong>Lenient Mode:</strong></p>
<pre><code class="language-toml">[god_object_detection.rust]
max_traits = 7
</code></pre>
<ul>
<li>Only flags severe SRP violations</li>
<li>Useful for large legacy codebases during initial assessment</li>
</ul>
<h2 id="output-and-reporting"><a class="header" href="#output-and-reporting">Output and Reporting</a></h2>
<h3 id="console-output"><a class="header" href="#console-output">Console Output</a></h3>
<p>When analyzing a file with multiple responsibilities:</p>
<pre><code>src/services/user_manager.rs
  ⚠️ God Object: 18 methods, 8 fields, 5 responsibilities
     Score: 185 (Confidence: Probable)

     Responsibilities:
       - Data Access (5 methods)
       - Validation (4 methods)
       - Persistence (3 methods)
       - Communication (3 methods)
       - Formatting &amp; Output (3 methods)

     Recommended Splits:
       1. DataAccessManager (5 methods, ~75 lines)
       2. ValidationManager (4 methods, ~60 lines)
       3. PersistenceManager (3 methods, ~45 lines)
</code></pre>
<h3 id="json-output-1"><a class="header" href="#json-output-1">JSON Output</a></h3>
<p>For programmatic analysis, use <code>--format json</code>:</p>
<pre><code class="language-json">{
  "file": "src/services/user_manager.rs",
  "is_god_object": true,
  "responsibility_count": 5,
  "responsibilities": [
    "Data Access",
    "Validation",
    "Persistence",
    "Communication",
    "Formatting &amp; Output"
  ],
  "recommended_splits": [
    {
      "suggested_name": "DataAccessManager",
      "methods_to_move": ["get_user", "set_password", "get_email"],
      "responsibility": "Data Access",
      "estimated_lines": 75
    }
  ]
}
</code></pre>
<h2 id="best-practices-17"><a class="header" href="#best-practices-17">Best Practices</a></h2>
<h3 id="writing-srp-compliant-code"><a class="header" href="#writing-srp-compliant-code">Writing SRP-Compliant Code</a></h3>
<ol>
<li><strong>Name functions descriptively</strong> - Use standard prefixes (<code>get_</code>, <code>validate_</code>, etc.)</li>
<li><strong>Group related functions</strong> - Keep similar responsibilities together</li>
<li><strong>Limit responsibility count</strong> - Aim for 1-3 responsibilities per module</li>
<li><strong>Review regularly</strong> - Run Debtmap periodically to catch responsibility creep</li>
<li><strong>Refactor early</strong> - Split modules before they hit thresholds</li>
</ol>
<h3 id="code-review-guidelines"><a class="header" href="#code-review-guidelines">Code Review Guidelines</a></h3>
<p>When reviewing responsibility analysis results:</p>
<ol>
<li><strong>Check responsibility boundaries</strong> - Are they logically distinct?</li>
<li><strong>Validate groupings</strong> - Do the recommended splits make sense?</li>
<li><strong>Consider dependencies</strong> - Will splitting introduce more coupling?</li>
<li><strong>Estimate refactoring cost</strong> - Is the improvement worth the effort?</li>
<li><strong>Prioritize by score</strong> - Focus on high-scoring god objects first</li>
</ol>
<h3 id="team-adoption"><a class="header" href="#team-adoption">Team Adoption</a></h3>
<p><strong>Phase 1: Assessment</strong></p>
<ul>
<li>Run Debtmap on codebase</li>
<li>Review responsibility violations</li>
<li>Identify top 10 problematic modules</li>
</ul>
<p><strong>Phase 2: Education</strong></p>
<ul>
<li>Share responsibility analysis results with team</li>
<li>Discuss SRP and its benefits</li>
<li>Agree on responsibility threshold standards</li>
</ul>
<p><strong>Phase 3: Incremental Refactoring</strong></p>
<ul>
<li>Start with highest-scoring modules</li>
<li>Apply recommended splits</li>
<li>Measure improvement with follow-up analysis</li>
</ul>
<p><strong>Phase 4: Continuous Monitoring</strong></p>
<ul>
<li>Integrate Debtmap into CI/CD</li>
<li>Track responsibility counts over time</li>
<li>Prevent new SRP violations from merging</li>
</ul>
<h2 id="limitations-and-edge-cases"><a class="header" href="#limitations-and-edge-cases">Limitations and Edge Cases</a></h2>
<h3 id="false-positives"><a class="header" href="#false-positives">False Positives</a></h3>
<p><strong>Scenario 1: Utilities Module</strong></p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// utilities.rs - 15 helper functions with different prefixes
fn format_date() { }
fn parse_config() { }
fn validate_email() { }
// ... 12 more diverse utilities
<span class="boring">}</span></code></pre>
<p><strong>Issue:</strong> Flagged as having multiple responsibilities, but it’s intentionally a utility collection.</p>
<p><strong>Solution:</strong> Either accept the flagging (utilities should perhaps be split) or increase <code>max_traits</code> threshold.</p>
<h3 id="false-negatives"><a class="header" href="#false-negatives">False Negatives</a></h3>
<p><strong>Scenario 2: Poor Naming</strong></p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>impl DataProcessor {
    fn process_data(&amp;mut self) { /* does everything */ }
    fn handle_stuff(&amp;mut self) { /* also does everything */ }
    fn do_work(&amp;mut self) { /* yet more mixed concerns */ }
}
<span class="boring">}</span></code></pre>
<p><strong>Issue:</strong> All methods map to “Processing” or “Utilities”, so responsibility count is low despite clear SRP violations.</p>
<p><strong>Solution:</strong> Encourage better naming conventions in your team. Debtmap relies on descriptive function names.</p>
<h3 id="language-specific-challenges"><a class="header" href="#language-specific-challenges">Language-Specific Challenges</a></h3>
<p><strong>Rust:</strong> Trait implementations may group methods by trait rather than responsibility, artificially inflating counts.</p>
<p><strong>Python:</strong> Dynamic typing and duck typing make responsibility boundaries less clear from signatures alone.</p>
<p><strong>JavaScript:</strong> Prototype methods and closures may not follow conventional naming patterns.</p>
<h2 id="integration-with-other-features"><a class="header" href="#integration-with-other-features">Integration with Other Features</a></h2>
<h3 id="god-object-detection-2"><a class="header" href="#god-object-detection-2">God Object Detection</a></h3>
<p>Responsibility analysis is a core component of <a href="#god-object-detection-1">God Object Detection</a>. The responsibility count contributes to:</p>
<ul>
<li>God object scoring</li>
<li>Confidence level determination</li>
<li>Refactoring recommendations</li>
</ul>
<h3 id="tiered-prioritization-2"><a class="header" href="#tiered-prioritization-2">Tiered Prioritization</a></h3>
<p>High responsibility counts increase priority in <a href="#tiered-prioritization-3">Tiered Prioritization</a> through the god object multiplier.</p>
<h3 id="risk-assessment"><a class="header" href="#risk-assessment">Risk Assessment</a></h3>
<p>Modules with multiple responsibilities receive higher risk scores in risk assessment, as they are more prone to bugs and harder to maintain.</p>
<h2 id="related-documentation-3"><a class="header" href="#related-documentation-3">Related Documentation</a></h2>
<ul>
<li><a href="#god-object-detection-1">God Object Detection</a> - Full god object analysis including responsibility detection</li>
<li><a href="#configuration-2">Configuration</a> - TOML configuration reference</li>
<li><a href="#metrics-reference">Metrics Reference</a> - All metrics including responsibility count</li>
<li><a href="#architecture">Architecture</a> - High-level design including analysis pipelines</li>
</ul>
<h2 id="summary-9"><a class="header" href="#summary-9">Summary</a></h2>
<p>Responsibility analysis in Debtmap:</p>
<ul>
<li><strong>Automatically detects SRP violations</strong> through pattern-based method name analysis</li>
<li><strong>Categorizes methods</strong> into 11 built-in responsibility types</li>
<li><strong>Provides actionable refactoring recommendations</strong> with suggested module splits</li>
<li><strong>Integrates with god object detection</strong> for holistic architectural analysis</li>
<li><strong>Supports language-specific thresholds</strong> for Rust, Python, and JavaScript/TypeScript</li>
<li><strong>Is fully configurable</strong> via <code>.debtmap.toml</code> and CLI flags</li>
</ul>
<p>By surfacing responsibility violations early and suggesting concrete refactoring paths, Debtmap helps teams maintain clean, modular architectures that follow the Single Responsibility Principle.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="scoring-strategies"><a class="header" href="#scoring-strategies">Scoring Strategies</a></h1>
<p>Debtmap provides two complementary scoring approaches: <strong>file-level</strong> and <strong>function-level</strong>. Understanding when to use each approach helps you make better refactoring decisions and prioritize work effectively.</p>
<h2 id="overview-19"><a class="header" href="#overview-19">Overview</a></h2>
<p>Different refactoring scenarios require different levels of granularity:</p>
<ul>
<li><strong>File-level scoring</strong>: Identifies architectural issues and planning major refactoring initiatives</li>
<li><strong>Function-level scoring</strong>: Pinpoints specific hot spots for targeted improvements</li>
</ul>
<p>This chapter explains both approaches, when to use each, and how to interpret the results.</p>
<h2 id="file-level-scoring"><a class="header" href="#file-level-scoring">File-Level Scoring</a></h2>
<p>File-level scoring aggregates metrics across all functions in a file to identify architectural problems and module-level refactoring opportunities.</p>
<h3 id="formula-1"><a class="header" href="#formula-1">Formula</a></h3>
<pre><code>File Score = Size × Complexity × Coverage Factor × Density × GodObject × FunctionScores
</code></pre>
<p><strong>Note</strong>: This is a conceptual formula showing the multiplicative relationship between factors. The actual implementation in <code>src/priority/file_metrics.rs</code> includes additional normalization steps and conditional adjustments. See source code for exact calculation details.</p>
<p>Where each factor is calculated as:</p>
<ul>
<li><strong>Size</strong> = <code>sqrt(total_lines / 100)</code></li>
<li><strong>Complexity</strong> = <code>(avg_complexity / 5.0) × sqrt(total_complexity / 50.0)</code></li>
<li><strong>Coverage Factor</strong> = <code>((1.0 - coverage_percent) × 2.0) + 1.0</code></li>
<li><strong>Density</strong> = <code>1.0 + ((function_count - 50) × 0.02)</code> if function_count &gt; 50, else 1.0</li>
<li><strong>GodObject</strong> = <code>2.0 + god_object_score</code> if detected</li>
<li><strong>FunctionScores</strong> = <code>sum(function_scores) / 10</code></li>
</ul>
<h3 id="factors"><a class="header" href="#factors">Factors</a></h3>
<p><strong>Size Factor</strong>: <code>sqrt(total_lines / 100)</code></p>
<ul>
<li>Larger files have higher impact</li>
<li>Square root dampens the effect to avoid over-penalizing large files</li>
<li>Rationale: Refactoring a 1000-line file affects more code than a 100-line file</li>
</ul>
<p><strong>Complexity Factor</strong>: Combines average and total complexity</p>
<ul>
<li><code>(average_cyclomatic + total_cyclomatic / function_count) / 2</code></li>
<li>Balances per-function and aggregate complexity</li>
<li>Rationale: Both concentrated complexity and spread-out complexity matter</li>
</ul>
<p><strong>Coverage Factor</strong>: <code>(coverage_gap × 2.0) + 1.0</code> where <code>coverage_gap = 1.0 - coverage_percent</code></p>
<ul>
<li>Lower coverage increases score multiplicatively</li>
<li>Range: 1.0 (100% coverage) to 3.0 (0% coverage)</li>
<li>Formula expands to: <code>((1.0 - coverage_percent) × 2.0) + 1.0</code></li>
<li>Example: 50% coverage → gap=0.5 → factor=(0.5×2.0)+1.0 = 2.0x</li>
<li>Rationale: Untested files amplify existing complexity and risk through a multiplicative factor greater than 1.0</li>
<li>Note: Earlier versions used <code>1.0 - coverage_percent</code> (range 0-1); current implementation uses expanded range 1-3 for stronger emphasis</li>
</ul>
<p><strong>Density Factor</strong>: Penalizes files with excessive function count</p>
<ul>
<li>Triggers when function count &gt; 50</li>
<li>Formula: <code>1.0 + ((function_count - 50) * 0.02)</code> if function_count &gt; 50, else 1.0</li>
<li>Creates a gradual linear increase: 51 functions = 1.02x, 75 functions = 1.50x, 100 functions = 2.0x</li>
<li>Example: A file with 75 functions gets 1.0 + ((75 - 50) * 0.02) = 1.0 + 0.50 = 1.50x multiplier</li>
<li>Rationale: Files with many functions likely violate single responsibility</li>
</ul>
<p><strong>God Object Multiplier</strong>: <code>2.0 + god_object_score</code> when detected</p>
<ul>
<li>Applies when god object detection flags the file</li>
<li>Range: 2.0 (borderline) to 3.0 (severe god object)</li>
<li>Rationale: God objects need immediate architectural attention</li>
</ul>
<p><strong>Function Scores</strong>: <code>sum(all_function_scores) / 10</code></p>
<ul>
<li>Normalized sum of individual function debt scores</li>
<li>Provides baseline before modifiers</li>
</ul>
<h3 id="use-cases-6"><a class="header" href="#use-cases-6">Use Cases</a></h3>
<p><strong>1. Planning Major Refactoring Initiatives</strong></p>
<pre><code class="language-bash"># Show top 10 files needing architectural refactoring
debtmap analyze . --aggregate-only --top 10
</code></pre>
<p>Use when:</p>
<ul>
<li>Planning sprint or quarterly refactoring work</li>
<li>Deciding which modules to split</li>
<li>Prioritizing architectural improvements</li>
<li>Allocating team resources</li>
</ul>
<p><strong>Note</strong>: File-level scoring is enabled with the <code>--aggregate-only</code> flag (no argument required: use <code>debtmap analyze . --aggregate-only</code>), which changes output to show only file-level metrics instead of function-level details.</p>
<p><strong>2. Identifying Architectural Issues</strong></p>
<p>File-level scoring excels at finding:</p>
<ul>
<li>God objects with too many responsibilities</li>
<li>Files with poor cohesion</li>
<li>Modules that should be split</li>
<li>Files with too many functions</li>
</ul>
<pre><code class="language-bash"># Focus on architectural problems
debtmap analyze . --aggregate-only --filter Architecture
</code></pre>
<p><strong>3. Breaking Up Monolithic Modules</strong></p>
<pre><code class="language-bash"># Find files with excessive function counts
debtmap analyze . --aggregate-only --min-problematic 50
</code></pre>
<p><strong>4. Evaluating Overall Codebase Health</strong></p>
<pre><code class="language-bash"># Generate file-level report for executive summary
debtmap analyze . --aggregate-only --format markdown -o report.md
</code></pre>
<h3 id="aggregation-methods"><a class="header" href="#aggregation-methods">Aggregation Methods</a></h3>
<p>Debtmap supports multiple aggregation methods for file-level scores, configurable via CLI or configuration file.</p>
<h4 id="weighted-sum-default"><a class="header" href="#weighted-sum-default">Weighted Sum (Default)</a></h4>
<p><strong>Formula</strong>: <code>Σ(function_score × complexity_weight × coverage_weight)</code></p>
<pre><code class="language-bash">debtmap analyze . --aggregation-method weighted_sum
</code></pre>
<p>Or via configuration file:</p>
<pre><code class="language-toml">[aggregation]
method = "weighted_sum"
</code></pre>
<p><strong>Characteristics</strong>:</p>
<ul>
<li>Weights functions by their complexity and coverage gaps</li>
<li>Emphasizes high-impact functions over trivial ones</li>
<li>Best for most use cases where you want to focus on significant issues</li>
</ul>
<p><strong>Best for</strong>: Standard codebases where you want proportional emphasis on complex, untested code</p>
<h4 id="simple-sum"><a class="header" href="#simple-sum">Simple Sum</a></h4>
<p><strong>Formula</strong>: <code>Σ(function_scores)</code></p>
<pre><code class="language-toml">[aggregation]
method = "sum"
</code></pre>
<p><strong>Characteristics</strong>:</p>
<ul>
<li>Adds all function scores directly without weighting</li>
<li>Treats all functions equally regardless of complexity</li>
<li>Useful for broad overview and trend analysis</li>
</ul>
<p><strong>Best for</strong>: Getting a raw count-based view of technical debt across all functions</p>
<h4 id="logarithmic-sum"><a class="header" href="#logarithmic-sum">Logarithmic Sum</a></h4>
<p><strong>Formula</strong>: <code>log(1 + Σ(function_scores))</code></p>
<pre><code class="language-toml">[aggregation]
method = "logarithmic_sum"
</code></pre>
<p><strong>Characteristics</strong>:</p>
<ul>
<li>Dampens impact of many small issues to prevent score explosion</li>
<li>Prevents files with hundreds of minor issues from dominating</li>
<li>Creates more balanced comparisons across files of different sizes</li>
</ul>
<p><strong>Best for</strong>: Legacy codebases with many small issues where you want to avoid extreme scores</p>
<h4 id="max-plus-average"><a class="header" href="#max-plus-average">Max Plus Average</a></h4>
<p><strong>Formula</strong>: <code>max_score × 0.6 + avg_score × 0.4</code></p>
<pre><code class="language-toml">[aggregation]
method = "max_plus_average"
</code></pre>
<p><strong>Characteristics</strong>:</p>
<ul>
<li>Considers worst function (60%) plus average of all functions (40%)</li>
<li>Balances worst-case and typical-case scenarios</li>
<li>Highlights files with both a critical hot spot and general issues</li>
</ul>
<p><strong>Best for</strong>: Identifying files with concentrated complexity alongside general code quality concerns</p>
<h4 id="choosing-an-aggregation-method"><a class="header" href="#choosing-an-aggregation-method">Choosing an Aggregation Method</a></h4>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Codebase Type</th><th>Recommended Method</th><th>Rationale</th></tr>
</thead>
<tbody>
<tr><td>New/Modern</td><td><code>weighted_sum</code></td><td>Proportional emphasis on real issues</td></tr>
<tr><td>Legacy with many small issues</td><td><code>logarithmic_sum</code></td><td>Prevents score explosion</td></tr>
<tr><td>Mixed quality</td><td><code>max_plus_average</code></td><td>Balances hot spots with overall quality</td></tr>
<tr><td>Trend analysis</td><td><code>sum</code></td><td>Simple, consistent metric over time</td></tr>
</tbody>
</table>
</div>
<p><strong>Performance Note</strong>: All aggregation methods have O(n) complexity where n = number of functions. Performance differences are negligible for typical codebases (&lt;100k functions). Choose based on prioritization strategy, not performance concerns.</p>
<h3 id="configuration-15"><a class="header" href="#configuration-15">Configuration</a></h3>
<blockquote>
<p><strong>IMPORTANT</strong>: The configuration file must be named <strong><code>.debtmap.toml</code></strong> (not <code>debtmap.yml</code> or other variants) and placed in your project root directory. Debtmap searches for <code>.debtmap.toml</code> in the current directory, or you can specify an alternate path with <code>--config path/to/config.toml</code>.</p>
</blockquote>
<pre><code class="language-toml">[aggregation]
method = "weighted_sum"
min_problematic = 3              # Need 3+ problematic functions for file-level score

[god_object_detection]
enabled = true
max_methods = 20
max_fields = 15
max_responsibilities = 5
</code></pre>
<h2 id="function-level-scoring"><a class="header" href="#function-level-scoring">Function-Level Scoring</a></h2>
<p>Function-level scoring identifies specific functions needing attention for targeted improvements.</p>
<h3 id="formula-1-1"><a class="header" href="#formula-1-1">Formula</a></h3>
<pre><code>Base Score = (Complexity Factor × 10 × 0.50) + (Dependency Factor × 10 × 0.25)
Coverage Multiplier = 1.0 - coverage_percent
Final Score = Base Score × Coverage Multiplier × Role Multiplier
</code></pre>
<p><strong>Formula Breakdown:</strong></p>
<ol>
<li><strong>Complexity Factor</strong>: Raw complexity / 2.0, clamped to 0-10 range (complexity of 20+ maps to 10.0)</li>
<li><strong>Dependency Factor</strong>: Upstream dependency count / 2.0, capped at 10.0 (20+ dependencies map to 10.0)</li>
<li><strong>Base Score</strong>: (Complexity Factor × 10 × 0.50) + (Dependency Factor × 10 × 0.25)
<ul>
<li>50% weight on complexity, 25% weight on dependencies</li>
</ul>
</li>
<li><strong>Coverage Multiplier</strong>: 1.0 - coverage_percent (0% coverage = 1.0, 100% coverage = 0.0)</li>
<li><strong>Final Score</strong>: Base Score × Coverage Multiplier × Role Multiplier</li>
</ol>
<p><strong>Why Hard-Coded Weights?</strong> The base weights (0.50 for complexity, 0.25 for dependencies) are intentionally not configurable to:</p>
<ul>
<li><strong>Ensure consistency</strong>: Scores remain comparable across projects and teams</li>
<li><strong>Prevent instability</strong>: Avoid extreme configurations that break prioritization</li>
<li><strong>Simplify configuration</strong>: Reduce cognitive load for users</li>
<li><strong>Maintain calibration</strong>: Weights are empirically tuned based on analysis of real codebases</li>
</ul>
<p>You can still customize prioritization significantly through configurable <code>role_multipliers</code>, <code>coverage_weights</code>, and normalization settings.</p>
<p><strong>Note</strong>: Coverage acts as a dampening multiplier rather than an additive factor. Lower coverage (higher multiplier) increases the final score, making untested complex code a higher priority. Role multipliers and coverage weights remain configurable to allow customization while maintaining stable base calculations.</p>
<p><strong>Migration Note</strong>: Earlier versions used an additive model with weights (Complexity × 0.35) + (Coverage × 0.50) + (Dependency × 0.15). The current model (spec 122) uses coverage as a multiplicative dampener, which better reflects that testing gaps amplify existing complexity rather than adding to it.</p>
<h3 id="metrics"><a class="header" href="#metrics">Metrics</a></h3>
<p><strong>Cyclomatic Complexity</strong></p>
<ul>
<li>Counts decision points (if, match, loops)</li>
<li>Guides test case count</li>
</ul>
<p><strong>Cognitive Complexity</strong></p>
<ul>
<li>Measures understanding difficulty</li>
<li>Accounts for nesting depth</li>
</ul>
<p><strong>Coverage Percentage</strong></p>
<ul>
<li>Direct line coverage from LCOV</li>
<li>0% coverage = maximum urgency</li>
</ul>
<p><strong>Dependency Count</strong></p>
<ul>
<li>Upstream callers + downstream callees</li>
<li>Higher dependencies = higher impact</li>
</ul>
<p><strong>Role Multiplier</strong></p>
<p>Functions are classified by role, and each role receives a multiplier based on its architectural importance:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Role</th><th>Multiplier</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><strong>Pure logic</strong></td><td>1.2x</td><td>Core business rules and algorithms</td></tr>
<tr><td><strong>Unknown</strong></td><td>1.0x</td><td>Functions without clear classification</td></tr>
<tr><td><strong>Entry point</strong></td><td>0.9x</td><td>Public APIs, main functions, HTTP handlers</td></tr>
<tr><td><strong>Orchestrator</strong></td><td>0.8x</td><td>Functions that coordinate other functions</td></tr>
<tr><td><strong>IO wrapper</strong></td><td>0.7x</td><td>Simple file/network I/O wrappers</td></tr>
<tr><td><strong>Pattern match</strong></td><td>0.6x</td><td>Functions primarily doing pattern matching</td></tr>
</tbody>
</table>
</div>
<p><strong>Note</strong>: Role multipliers are configurable via the <code>[role_multipliers]</code> section in <code>.debtmap.toml</code>. The multipliers have been rebalanced to be less extreme than earlier versions - pure logic was reduced from 1.5x to 1.2x, while orchestrator and IO wrapper were increased to better reflect their importance in modern codebases.</p>
<h3 id="constructor-detection-1"><a class="header" href="#constructor-detection-1">Constructor Detection</a></h3>
<p>Debtmap includes intelligent constructor detection to prevent false positives where trivial initialization functions are misclassified as critical business logic.</p>
<p><strong>Problem</strong>: Simple constructors like <code>new()</code>, <code>default()</code>, or <code>from_config()</code> often have low complexity but were being flagged as high-priority pure logic functions.</p>
<p><strong>Solution</strong>: Constructor detection automatically identifies and classifies these functions as <code>IOWrapper</code> (low priority) instead of <code>PureLogic</code> (high priority).</p>
<p><strong>Detection Criteria</strong>:</p>
<p>A function is considered a simple constructor if it meets ALL of the following:</p>
<ol>
<li>
<p><strong>Name matches a constructor pattern</strong> (configurable):</p>
<ul>
<li>Exact match: <code>new</code>, <code>default</code>, <code>empty</code>, <code>zero</code>, <code>any</code></li>
<li>Prefix match: <code>from_*</code>, <code>with_*</code>, <code>create_*</code>, <code>make_*</code>, <code>build_*</code>, <code>of_*</code></li>
</ul>
</li>
<li>
<p><strong>Low cyclomatic complexity</strong> (≤ 2 by default)</p>
</li>
<li>
<p><strong>Short length</strong> (&lt; 15 lines by default)</p>
</li>
<li>
<p><strong>Minimal nesting</strong> (≤ 1 level by default)</p>
</li>
<li>
<p><strong>Low cognitive complexity</strong> (≤ 3 by default)</p>
</li>
</ol>
<p><strong>Example</strong>:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Simple constructor - detected and classified as IOWrapper
fn new() -&gt; Self {
    Self {
        field1: 0,
        field2: String::new(),
    }
}

// Complex factory - NOT detected as constructor, remains PureLogic
fn create_with_validation(data: Data) -&gt; Result&lt;Self&gt; {
    validate(&amp;data)?;
    // ... 30 lines of logic
    Ok(Self { ... })
}
<span class="boring">}</span></code></pre>
<p><strong>Configuration</strong>:</p>
<p>Constructor detection is fully configurable in <code>.debtmap.toml</code>:</p>
<pre><code class="language-toml">[classification.constructors]
# Enable AST-based constructor detection (default: true)
# When enabled, uses Abstract Syntax Tree analysis for accurate detection
# Disable only if experiencing performance issues with very large codebases
ast_detection = true

# Constructor name patterns
patterns = [
    "new",
    "default",
    "from_",
    "with_",
    "create_",
    "make_",
    "build_",
    "of_",
    "empty",
    "zero",
    "any",
]

# Complexity thresholds
max_cyclomatic = 2     # Maximum cyclomatic complexity
max_cognitive = 3      # Maximum cognitive complexity
max_length = 15        # Maximum lines
max_nesting = 1        # Maximum nesting depth
</code></pre>
<p><strong>Customization Example</strong>:</p>
<p>To add custom constructor patterns or adjust thresholds:</p>
<pre><code class="language-toml">[classification.constructors]
ast_detection = true      # Keep AST detection enabled (recommended)

patterns = [
    "new",
    "default",
    "from_",
    "with_",
    "init_",        # Add custom pattern
    "setup_",       # Add custom pattern
]
max_cyclomatic = 3    # Allow slightly more complex constructors
max_length = 20       # Allow longer constructors
</code></pre>
<p>To disable AST-based detection (if experiencing performance issues):</p>
<pre><code class="language-toml">[classification.constructors]
ast_detection = false     # Fall back to pattern-only matching
# Note: May reduce detection accuracy but improves performance
</code></pre>
<p><strong>Performance and Disabling</strong>:</p>
<p>Constructor detection is <strong>always enabled</strong> and cannot be fully disabled, as it’s integral to accurate priority scoring. However, you can:</p>
<ol>
<li><strong>Disable AST analysis</strong> (shown above): Falls back to pattern-only matching, reducing accuracy but improving performance for very large codebases (100k+ functions)</li>
<li><strong>Adjust thresholds</strong>: Make detection more lenient by increasing <code>max_cyclomatic</code>, <code>max_cognitive</code>, or <code>max_length</code></li>
<li><strong>Remove patterns</strong>: Delete specific patterns from the <code>patterns</code> list to exclude them from detection</li>
</ol>
<p><strong>Performance Impact</strong>:</p>
<ul>
<li>AST-based detection: Negligible impact (&lt;5% overhead) for typical codebases</li>
<li>Pattern-only detection: Near-zero performance impact</li>
<li>Recommendation: Keep <code>ast_detection = true</code> unless profiling shows it’s a bottleneck</li>
</ul>
<p><strong>Accuracy Trade-offs</strong>:</p>
<ul>
<li>With AST: 95%+ accuracy in identifying simple constructors</li>
<li>Without AST: ~70% accuracy, more false negatives</li>
</ul>
<p>This feature is part of spec 117 and helps reduce false positives in priority scoring.</p>
<h3 id="role-based-adjustments"><a class="header" href="#role-based-adjustments">Role-Based Adjustments</a></h3>
<p>DebtMap uses a sophisticated two-stage role adjustment mechanism to ensure that scores accurately reflect both the testing strategy appropriate for each function type and the architectural importance of different roles.</p>
<h4 id="why-role-based-adjustments"><a class="header" href="#why-role-based-adjustments">Why Role-Based Adjustments?</a></h4>
<p><strong>Problem</strong>: Traditional scoring treats all functions equally, leading to false positives:</p>
<ol>
<li>
<p><strong>Entry points</strong> (CLI handlers, HTTP routes, <code>main</code> functions) typically use integration tests rather than unit tests</p>
<ul>
<li>Flagging them for “low unit test coverage” misses that they’re tested differently</li>
<li>They orchestrate other code but contain minimal business logic</li>
</ul>
</li>
<li>
<p><strong>Pure business logic</strong> functions should have comprehensive unit tests</p>
<ul>
<li>Easy to test in isolation with deterministic inputs/outputs</li>
<li>Core value of the application lives here</li>
</ul>
</li>
<li>
<p><strong>I/O wrappers</strong> are often tested implicitly through integration tests</p>
<ul>
<li>Thin abstractions over file system, network, or database operations</li>
<li>Unit testing them provides limited value compared to integration testing</li>
</ul>
</li>
</ol>
<p><strong>Solution</strong>: DebtMap applies role-based adjustments in two stages to address both coverage expectations and architectural importance.</p>
<h4 id="stage-1-role-based-coverage-weighting"><a class="header" href="#stage-1-role-based-coverage-weighting">Stage 1: Role-Based Coverage Weighting</a></h4>
<p>The first stage adjusts coverage penalty expectations based on function role. This prevents functions that use different testing strategies from unfairly dominating the priority list.</p>
<p><strong>How It Works</strong>:</p>
<p>For each function, DebtMap:</p>
<ol>
<li>Detects the function’s role (entry point, pure logic, I/O wrapper, etc.)</li>
<li>Applies a coverage weight multiplier based on that role</li>
<li>Reduces or increases the coverage penalty accordingly</li>
</ol>
<p><strong>Default Coverage Weights</strong> (configurable in <code>.debtmap.toml</code>):</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Function Role</th><th>Coverage Weight</th><th>Impact on Scoring</th></tr>
</thead>
<tbody>
<tr><td>Pure Logic</td><td>1.2</td><td>Higher coverage penalty (should have unit tests)</td></tr>
<tr><td>Unknown</td><td>1.0</td><td>Standard penalty</td></tr>
<tr><td>Pattern Match</td><td>1.0</td><td>Standard penalty</td></tr>
<tr><td>Orchestrator</td><td>0.8</td><td>Reduced penalty (partially integration tested)</td></tr>
<tr><td>I/O Wrapper</td><td>0.7</td><td>Reduced penalty (often integration tested)</td></tr>
<tr><td>Entry Point</td><td>0.6</td><td>Significantly reduced penalty (integration tested)</td></tr>
</tbody>
</table>
</div>
<p><strong>Example Score Changes</strong>:</p>
<p><strong>Before role-based coverage adjustment</strong>:</p>
<pre><code>Function: handle_request (Entry Point)
  Complexity: 5
  Coverage: 0%
  Raw Coverage Penalty: 1.0 (full penalty)
  Score: 8.5 (flagged as high priority)
</code></pre>
<p><strong>After role-based coverage adjustment</strong>:</p>
<pre><code>Function: handle_request (Entry Point)
  Complexity: 5
  Coverage: 0%
  Adjusted Coverage Penalty: 0.4 (60% reduction via 0.6 weight)
  Score: 4.2 (medium priority - more realistic)

  Rationale: Entry points are integration tested, not unit tested.
  This function is likely tested via API/CLI integration tests.
</code></pre>
<p><strong>Comparison with Pure Logic</strong>:</p>
<pre><code>Function: calculate_discount (Pure Logic)
  Complexity: 5
  Coverage: 0%
  Adjusted Coverage Penalty: 1.2 (20% increase via 1.2 weight)
  Score: 9.8 (critical priority)

  Rationale: Pure logic should have unit tests.
  This function needs immediate test coverage.
</code></pre>
<h4 id="stage-2-role-multiplier"><a class="header" href="#stage-2-role-multiplier">Stage 2: Role Multiplier</a></h4>
<p>The second stage applies a final role-based multiplier to reflect architectural importance. This multiplier is <strong>clamped by default</strong> to prevent extreme score swings.</p>
<p><strong>Configuration</strong> (<code>.debtmap.toml</code> under <code>[scoring.role_multiplier]</code>):</p>
<pre><code class="language-toml">[scoring.role_multiplier]
clamp_min = 0.3           # Minimum multiplier (default: 0.3)
clamp_max = 1.8           # Maximum multiplier (default: 1.8)
enable_clamping = true    # Enable clamping (default: true)
</code></pre>
<p><strong>Clamp Range Rationale</strong>:</p>
<ul>
<li><strong>Default [0.3, 1.8]</strong>: Balances differentiation with stability</li>
<li><strong>Lower bound (0.3)</strong>: I/O wrappers still contribute 30% of base score (not invisible)</li>
<li><strong>Upper bound (1.8)</strong>: Critical entry points don’t overwhelm other issues (max 180%)</li>
<li><strong>Configurable</strong>: Adjust based on project priorities</li>
</ul>
<p><strong>Example with Clamping</strong>:</p>
<pre><code>Function: process_data (Complex Pure Logic)
  Base Score: 45.0
  Unclamped Role Multiplier: 2.5
  Clamped Multiplier: 1.8 (clamp_max)
  Final Score: 45.0 × 1.8 = 81.0

  Effect: Prevents one complex function from dominating entire priority list
</code></pre>
<h4 id="why-two-stages"><a class="header" href="#why-two-stages">Why Two Stages?</a></h4>
<p>The separation of coverage weight adjustment and role multiplier ensures they work together without interfering:</p>
<p><strong>Stage 1 (Coverage Weight)</strong>: Adjusts testing expectations</p>
<ul>
<li><strong>Question</strong>: “How much should we penalize missing unit tests for this type of function?”</li>
<li><strong>Example</strong>: Entry points get 60% of normal coverage penalty (they’re integration tested)</li>
</ul>
<p><strong>Stage 2 (Role Multiplier)</strong>: Adjusts architectural importance</p>
<ul>
<li><strong>Question</strong>: “How important is this function relative to others with similar complexity?”</li>
<li><strong>Example</strong>: Critical entry points might get a 1.2x multiplier (clamped), while simple I/O wrappers get 0.5x (clamped)</li>
</ul>
<p><strong>Independent Contributions</strong>:</p>
<pre><code>1. Calculate base score from complexity + dependencies
2. Apply coverage weight by role → adjusted coverage penalty
3. Combine into preliminary score
4. Apply clamped role multiplier → final score
</code></pre>
<p>This approach ensures:</p>
<ul>
<li>Coverage adjustments don’t interfere with role multiplier</li>
<li>Both mechanisms contribute independently</li>
<li>Clamping prevents instability from extreme multipliers</li>
</ul>
<h4 id="how-this-reduces-false-positives"><a class="header" href="#how-this-reduces-false-positives">How This Reduces False Positives</a></h4>
<p><strong>False Positive #1: Entry Points Flagged for Low Coverage</strong></p>
<p><strong>Before</strong>:</p>
<pre><code>Top Priority Items:
1. main() - Score: 9.2 (0% unit test coverage)
2. handle_cli_command() - Score: 8.8 (5% unit test coverage)
3. run_server() - Score: 8.5 (0% unit test coverage)
</code></pre>
<p><strong>After</strong>:</p>
<pre><code>Top Priority Items:
1. calculate_tax() - Score: 9.8 (0% coverage, Pure Logic)
2. validate_payment() - Score: 9.2 (10% coverage, Pure Logic)
3. main() - Score: 4.2 (0% coverage, Entry Point - integration tested)
</code></pre>
<p><strong>Result</strong>: Business logic functions that actually need unit tests rise to the top.</p>
<p><strong>False Positive #2: I/O Wrappers Over-Prioritized</strong></p>
<p><strong>Before</strong>:</p>
<pre><code>Function: read_config_file
  Complexity: 3
  Coverage: 0%
  Score: 7.5 (high priority)

  Issue: This is a thin wrapper over std::fs::read_to_string.
  Unit testing it provides minimal value vs integration tests.
</code></pre>
<p><strong>After</strong>:</p>
<pre><code>Function: read_config_file
  Complexity: 3
  Coverage: 0%
  Adjusted Coverage Weight: 0.7
  Score: 3.2 (low priority)

  Rationale: I/O wrappers are integration tested.
  Focus on business logic instead.
</code></pre>
<h4 id="configuration-examples"><a class="header" href="#configuration-examples">Configuration Examples</a></h4>
<p><strong>Emphasize Pure Logic Testing</strong>:</p>
<pre><code class="language-toml">[scoring.role_coverage_weights]
pure_logic = 1.5        # Strong penalty for untested pure logic
entry_point = 0.5       # Minimal penalty for untested entry points
io_wrapper = 0.5        # Minimal penalty for untested I/O wrappers
</code></pre>
<p><strong>Conservative Approach (Smaller Adjustments)</strong>:</p>
<pre><code class="language-toml">[scoring.role_coverage_weights]
pure_logic = 1.1        # Slight increase
entry_point = 0.9       # Slight decrease
io_wrapper = 0.9        # Slight decrease
</code></pre>
<p><strong>Disable Multiplier Clamping</strong> (not recommended for production):</p>
<pre><code class="language-toml">[scoring.role_multiplier]
enable_clamping = false   # Allow unclamped multipliers
# Warning: May cause unstable prioritization
</code></pre>
<h4 id="verification-1"><a class="header" href="#verification-1">Verification</a></h4>
<p>To see how role-based adjustments affect your codebase:</p>
<pre><code class="language-bash"># Show detailed scoring breakdown
debtmap analyze . --verbose

# Compare with role adjustments disabled
debtmap analyze . --config minimal.toml
</code></pre>
<p><strong>Sample verbose output</strong>:</p>
<pre><code>Function: src/handlers/request.rs:handle_request
  Role: Entry Point
  Complexity: 5
  Coverage: 0%
  Coverage Weight: 0.6 (Entry Point adjustment)
  Adjusted Coverage Penalty: 0.4 (reduced from 1.0)
  Base Score: 15.0
  Role Multiplier: 1.2 (clamped from 1.5)
  Final Score: 18.0

  Interpretation:
    - Entry point gets 60% coverage penalty instead of 100%
    - Likely tested via integration tests
    - Still flagged due to complexity, but not over-penalized for coverage
</code></pre>
<h4 id="benefits-summary"><a class="header" href="#benefits-summary">Benefits Summary</a></h4>
<ul>
<li><strong>Fewer false positives</strong>: Entry points and I/O wrappers no longer dominate priority lists</li>
<li><strong>Better resource allocation</strong>: Testing efforts focus on pure logic where unit tests provide most value</li>
<li><strong>Recognition of testing strategies</strong>: Integration tests are valued equally with unit tests</li>
<li><strong>Stable prioritization</strong>: Clamping prevents extreme multipliers from causing volatile rankings</li>
<li><strong>Configurable</strong>: Adjust weights and clamp ranges to match your project’s testing philosophy</li>
</ul>
<h3 id="use-cases-1-1"><a class="header" href="#use-cases-1-1">Use Cases</a></h3>
<p><strong>1. Identifying Specific Hot Spots</strong></p>
<pre><code class="language-bash"># Show top 20 functions needing attention
debtmap analyze . --top 20
</code></pre>
<p>Use when:</p>
<ul>
<li>Planning individual developer tasks</li>
<li>Assigning specific refactoring work</li>
<li>Identifying functions to test first</li>
<li>Code review focus</li>
</ul>
<p><strong>2. Sprint Planning for Developers</strong></p>
<pre><code class="language-bash"># Get function-level tasks for this sprint
debtmap analyze . --top 10 --format json -o sprint-tasks.json
</code></pre>
<p><strong>3. Writing Unit Tests</strong></p>
<pre><code class="language-bash"># Find untested complex functions
debtmap analyze . --lcov coverage.lcov --filter Testing --top 15
</code></pre>
<p><strong>4. Targeted Performance Optimization</strong></p>
<pre><code class="language-bash"># Find complex hot paths
debtmap analyze . --filter Performance --context --top 10
</code></pre>
<h3 id="configuration-1-1"><a class="header" href="#configuration-1-1">Configuration</a></h3>
<p>Complete configuration file example showing all scoring-related sections.</p>
<p><strong>File name</strong>: <code>.debtmap.toml</code> (must be placed in your project root)</p>
<pre><code class="language-toml"># .debtmap.toml - Complete scoring configuration

# Role multipliers (applied to final score after coverage multiplier)
[role_multipliers]
pure_logic = 1.2             # Core business rules and algorithms
unknown = 1.0                # Functions without clear classification
entry_point = 0.9            # Public APIs, main functions, HTTP handlers
orchestrator = 0.8           # Functions that coordinate other functions
io_wrapper = 0.7             # File/network I/O wrappers
pattern_match = 0.6          # Functions primarily doing pattern matching

# Aggregation settings (for file-level scoring)
[aggregation]
method = "weighted_sum"      # Options: weighted_sum, sum, logarithmic_sum, max_plus_average
min_problematic = 3          # Minimum number of problematic functions to report file

# Normalization settings (for advanced multi-phase normalization)
[normalization]
linear_threshold = 10.0       # Scores below this use linear scaling (1:1 mapping)
logarithmic_threshold = 100.0 # Scores above this use logarithmic dampening
sqrt_multiplier = 3.33        # Applied to scores between linear and log thresholds
log_multiplier = 10.0         # Applied to scores above logarithmic threshold
show_raw_scores = true        # Display both normalized (0-100) and raw scores in output
</code></pre>
<p><strong>Note on Scoring Weights</strong>: The base complexity and dependency weights are hard-coded for consistency across environments. However, you can customize prioritization significantly through configurable options:</p>
<p><strong>What’s Configurable:</strong></p>
<ul>
<li><code>role_multipliers</code> - Adjust importance of different function types (pure logic, entry points, I/O wrappers)</li>
<li><code>coverage_weights</code> - Role-specific coverage penalty adjustments</li>
<li><code>normalization</code> settings - Control score scaling and range</li>
<li><code>aggregation.method</code> - Choose how function scores combine into file scores</li>
</ul>
<p><strong>What’s Hard-Coded:</strong></p>
<ul>
<li>Base complexity weight (50%) and dependency weight (25%)</li>
<li>Coverage multiplier formula: <code>1.0 - coverage_percent</code></li>
</ul>
<p><strong>Impact</strong>: While base weights are fixed, the configurable multipliers and weights provide significant control over final rankings and priorities. A function with <code>role_multiplier = 1.5</code> and <code>coverage_weight = 1.2</code> can have 80% higher priority than the same function with default settings.</p>
<p><strong>Note</strong>: The configuration file must be named <code>.debtmap.toml</code> (not <code>debtmap.yml</code> or other variants) and placed in your project root directory.</p>
<h2 id="when-to-use-each-approach"><a class="header" href="#when-to-use-each-approach">When to Use Each Approach</a></h2>
<h3 id="use-file-level-scoring-when"><a class="header" href="#use-file-level-scoring-when">Use File-Level Scoring When:</a></h3>
<p>✅ Planning architectural refactoring
✅ Quarterly or annual planning
✅ Deciding which modules to split
✅ Executive summaries and high-level reports
✅ Team capacity planning
✅ Identifying god objects
✅ Module reorganization</p>
<p><strong>Command:</strong></p>
<pre><code class="language-bash">debtmap analyze . --aggregate-only
</code></pre>
<h3 id="use-function-level-scoring-when"><a class="header" href="#use-function-level-scoring-when">Use Function-Level Scoring When:</a></h3>
<p>✅ Sprint planning
✅ Individual developer task assignment
✅ Writing specific unit tests
✅ Code review preparation
✅ Pair programming sessions
✅ Daily or weekly development work
✅ Targeted hot spot fixes</p>
<p><strong>Command:</strong></p>
<pre><code class="language-bash">debtmap analyze . --top 20
</code></pre>
<h3 id="use-both-together"><a class="header" href="#use-both-together">Use Both Together:</a></h3>
<p>Many workflows benefit from both views:</p>
<pre><code class="language-bash"># Step 1: Identify problematic files
debtmap analyze . --aggregate-only --top 5 -o files.json

# Step 2: Drill into specific file
debtmap analyze src/problematic/module.rs --format terminal
</code></pre>
<h2 id="comparison-examples"><a class="header" href="#comparison-examples">Comparison Examples</a></h2>
<h3 id="example-1-god-object-detection"><a class="header" href="#example-1-god-object-detection">Example 1: God Object Detection</a></h3>
<p><strong>Command</strong>:</p>
<pre><code class="language-bash">debtmap analyze src/services/user_service.rs --aggregate-only
</code></pre>
<p><strong>File-Level View:</strong></p>
<pre><code>src/services/user_service.rs - Score: 245.8
  - 850 lines, 45 methods
  - God Object: 78% score
  - Action: Split into UserAuth, UserProfile, UserNotifications
</code></pre>
<p><strong>Command</strong>:</p>
<pre><code class="language-bash">debtmap analyze src/services/user_service.rs --top 5
</code></pre>
<p><strong>Function-Level View:</strong></p>
<pre><code>src/services/user_service.rs:142 - authenticate_user() - Score: 8.5
src/services/user_service.rs:298 - update_profile() - Score: 7.2
src/services/user_service.rs:456 - send_notification() - Score: 6.8
</code></pre>
<p><strong>Decision</strong>: File-level score (245.8) correctly identifies architectural issue. Individual functions aren’t exceptionally complex, but the file has too many responsibilities. <strong>Solution</strong>: Split the file.</p>
<h3 id="example-2-targeted-function-fix"><a class="header" href="#example-2-targeted-function-fix">Example 2: Targeted Function Fix</a></h3>
<p><strong>Command</strong>:</p>
<pre><code class="language-bash">debtmap analyze src/parsers/expression.rs --aggregate-only
</code></pre>
<p><strong>File-Level View:</strong></p>
<pre><code>src/parsers/expression.rs - Score: 45.2
  - 320 lines, 12 functions
  - No god object detected
</code></pre>
<p><strong>Command</strong>:</p>
<pre><code class="language-bash">debtmap analyze src/parsers/expression.rs --top 5
</code></pre>
<p><strong>Function-Level View:</strong></p>
<pre><code>src/parsers/expression.rs:89 - parse_complex_expression() - Score: 9.1
  - Cyclomatic: 22, Cognitive: 35
  - Coverage: 0%
  - Action: Add tests and refactor
</code></pre>
<p><strong>Decision</strong>: File as a whole is acceptable, but one function needs attention. <strong>Solution</strong>: Focus on that specific function.</p>
<h3 id="example-3-balanced-refactoring"><a class="header" href="#example-3-balanced-refactoring">Example 3: Balanced Refactoring</a></h3>
<p><strong>Command</strong>:</p>
<pre><code class="language-bash">debtmap analyze src/analysis/scoring.rs --aggregate-only --coverage-file coverage.lcov
</code></pre>
<p><strong>File-Level View:</strong></p>
<pre><code>src/analysis/scoring.rs - Score: 125.6
  - 580 lines, 18 functions
  - High complexity, low coverage
</code></pre>
<p><strong>Command</strong>:</p>
<pre><code class="language-bash">debtmap analyze src/analysis/scoring.rs --coverage-file coverage.lcov --top 5
</code></pre>
<p><strong>Function-Level View:</strong></p>
<pre><code>calculate_score() - Score: 8.8 (15% coverage)
apply_weights() - Score: 8.2 (10% coverage)
normalize_results() - Score: 7.5 (0% coverage)
</code></pre>
<p><strong>Decision</strong>: Both file and functions need work. <strong>Solution</strong>: Add tests first (function-level), then consider splitting if complexity persists (file-level).</p>
<h2 id="score-normalization"><a class="header" href="#score-normalization">Score Normalization</a></h2>
<p>Both scoring approaches normalize to a 0-10 scale for consistency.</p>
<h3 id="normalization-strategies"><a class="header" href="#normalization-strategies">Normalization Strategies</a></h3>
<p><strong>Default: Linear Clamping</strong></p>
<p>The default normalization uses simple linear clamping to the 0-100 range:</p>
<ul>
<li><strong>Formula</strong>: Score is clamped between 0.0 and 100.0</li>
<li><strong>Behavior</strong>: No transformation, just boundary enforcement</li>
<li><strong>Usage</strong>: Production output uses this method</li>
</ul>
<p>This ensures scores stay within the expected range without additional transformations.</p>
<p><strong>Advanced: Multi-Phase Normalization</strong></p>
<p>For more sophisticated normalization, debtmap provides multi-phase scaling with different formulas for different score ranges:</p>
<p><strong>Phase 1 - Linear (scores &lt; 10)</strong>:</p>
<ul>
<li>Formula: <code>normalized = raw_score</code></li>
<li>Behavior: 1:1 mapping, no scaling</li>
<li>Rationale: Preserve low score distinctions</li>
</ul>
<p><strong>Phase 2 - Square Root (scores 10-100)</strong>:</p>
<ul>
<li>Formula: <code>normalized = 10.0 + sqrt(raw_score - 10.0) × 3.33</code></li>
<li>Behavior: Moderate dampening</li>
<li>Rationale: Balance between linear and logarithmic</li>
</ul>
<p><strong>Phase 3 - Logarithmic (scores &gt; 100)</strong>:</p>
<ul>
<li>Formula: <code>normalized = 41.59 + ln(raw_score / 100.0) × 10.0</code></li>
<li>Behavior: Strong dampening of extreme values</li>
<li>Rationale: Prevent outliers from dominating</li>
</ul>
<p>This multi-phase approach dampens extreme values while preserving distinctions in the normal range. Configure via <code>[normalization]</code> section in <code>.debtmap.toml</code>.</p>
<h3 id="configuration-2-1"><a class="header" href="#configuration-2-1">Configuration</a></h3>
<pre><code class="language-toml">[normalization]
linear_threshold = 10.0       # Scores below this use linear scaling (1:1 mapping)
logarithmic_threshold = 100.0 # Scores above this use logarithmic dampening
sqrt_multiplier = 3.33        # Applied to scores between linear and log thresholds
log_multiplier = 10.0         # Applied to scores above logarithmic threshold
show_raw_scores = true        # Display both normalized (0-10) and raw scores in output
</code></pre>
<p><strong>Explanation</strong>:</p>
<ul>
<li><strong>linear_threshold</strong>: Scores below this value are mapped 1:1 (no scaling)</li>
<li><strong>logarithmic_threshold</strong>: Scores above this value are dampened logarithmically to prevent extreme values</li>
<li><strong>sqrt_multiplier</strong>: Square root scaling applied to mid-range scores (between linear and logarithmic thresholds)</li>
<li><strong>log_multiplier</strong>: Logarithmic dampening factor for very high scores</li>
<li><strong>show_raw_scores</strong>: When enabled, output includes both the normalized 0-10 score and the raw calculated score</li>
</ul>
<h2 id="best-practices-18"><a class="header" href="#best-practices-18">Best Practices</a></h2>
<h3 id="workflow-integration-1"><a class="header" href="#workflow-integration-1">Workflow Integration</a></h3>
<p><strong>Week 1: File-Level Assessment</strong></p>
<pre><code class="language-bash"># Identify architectural problems
debtmap analyze . --aggregate-only --top 10
</code></pre>
<p><strong>Week 2-4: Function-Level Work</strong></p>
<pre><code class="language-bash"># Work through specific functions
debtmap analyze src/target/module.rs
</code></pre>
<p><strong>Monthly: Compare Progress</strong></p>
<pre><code class="language-bash">debtmap compare --before baseline.json --after current.json
</code></pre>
<h3 id="team-collaboration"><a class="header" href="#team-collaboration">Team Collaboration</a></h3>
<ul>
<li><strong>Architects</strong>: Use file-level scores for strategic planning</li>
<li><strong>Tech Leads</strong>: Use both for sprint planning</li>
<li><strong>Developers</strong>: Use function-level for daily work</li>
<li><strong>QA</strong>: Use function-level for test prioritization</li>
</ul>
<h3 id="cicd-integration-5"><a class="header" href="#cicd-integration-5">CI/CD Integration</a></h3>
<pre><code class="language-bash"># Gate: No new file-level regressions
debtmap analyze . --aggregate-only --format json -o file-scores.json

# Gate: No new critical function-level issues
debtmap analyze . --min-priority critical --format json -o critical-items.json
</code></pre>
<h2 id="troubleshooting-19"><a class="header" href="#troubleshooting-19">Troubleshooting</a></h2>
<p><strong>Issue</strong>: File-level scores seem too high</p>
<p><strong>Solution</strong>: Check aggregation method:</p>
<pre><code class="language-toml">[aggregation]
method = "logarithmic_sum"  # Dampen scores
</code></pre>
<p><strong>Issue</strong>: Function-level scores all similar</p>
<p><strong>Solution</strong>: Adjust role multipliers to create more differentiation:</p>
<pre><code class="language-toml">[role_multipliers]
pure_logic = 1.5     # Emphasize business logic more
io_wrapper = 0.5     # De-emphasize I/O wrappers more
</code></pre>
<p><strong>Note</strong>: Base scoring weights (complexity 50%, dependency 25%) are hard-coded and cannot be configured.</p>
<p><strong>Issue</strong>: Too many low-priority items</p>
<p><strong>Solution</strong>: Use minimum thresholds:</p>
<pre><code class="language-toml">[thresholds]
minimum_debt_score = 3.0
</code></pre>
<h2 id="rebalanced-debt-scoring-spec-136"><a class="header" href="#rebalanced-debt-scoring-spec-136">Rebalanced Debt Scoring (Spec 136)</a></h2>
<p>Debtmap now includes an advanced <strong>rebalanced scoring algorithm</strong> that prioritizes actual code quality issues—complexity, coverage gaps, and structural problems—over pure file size concerns.</p>
<h3 id="enabling-rebalanced-scoring"><a class="header" href="#enabling-rebalanced-scoring">Enabling Rebalanced Scoring</a></h3>
<blockquote>
<p><strong>IMPORTANT</strong>: Rebalanced scoring is enabled through your <code>.debtmap.toml</code> configuration file by adding the <code>[scoring_rebalanced]</code> section. This activates the rebalanced algorithm described below.</p>
</blockquote>
<p><strong>Default Behavior</strong>: By default, debtmap uses the standard scoring algorithm described earlier in this chapter. To use rebalanced scoring, add the <code>[scoring_rebalanced]</code> section to your config:</p>
<pre><code class="language-toml"># .debtmap.toml
[scoring_rebalanced]
preset = "balanced"  # Activates rebalanced scoring with balanced preset
</code></pre>
<p><strong>Relationship to Standard Scoring</strong>:</p>
<ul>
<li>Rebalanced scoring <strong>supplements</strong> standard scoring, providing an alternative prioritization strategy</li>
<li>Both algorithms can coexist - choose which to use based on your needs</li>
<li>File-level and function-level scoring both work with rebalanced scoring</li>
<li>Output format remains the same, only score calculations differ</li>
</ul>
<p><strong>Migration Path</strong>:</p>
<ol>
<li><strong>Test first</strong>: Add <code>[scoring_rebalanced]</code> section to a test config file</li>
<li><strong>Compare</strong>: Run analysis with both standard and rebalanced scoring on same codebase</li>
<li><strong>Evaluate</strong>: Review how priorities change (large simple files rank lower, complex untested code ranks higher)</li>
<li><strong>Adopt</strong>: Once satisfied, switch your primary config to use rebalanced scoring</li>
<li><strong>Tune</strong>: Adjust preset or custom weights based on your team’s priorities</li>
</ol>
<p><strong>Quick Start</strong>:</p>
<pre><code class="language-bash"># Create test config with rebalanced scoring
cat &gt; .debtmap-rebalanced.toml &lt;&lt;EOF
[scoring_rebalanced]
preset = "balanced"
EOF

# Compare results
debtmap analyze . --format terminal                            # Standard scoring
debtmap analyze . --config .debtmap-rebalanced.toml --format terminal  # Rebalanced scoring
</code></pre>
<h3 id="philosophy"><a class="header" href="#philosophy">Philosophy</a></h3>
<p>Traditional scoring often over-emphasizes file size, causing large but simple files to rank higher than complex, untested code. The rebalanced algorithm fixes this by:</p>
<ol>
<li><strong>De-emphasizing size</strong>: Reduces size weight from ~1.5 to 0.3 (80% reduction)</li>
<li><strong>Emphasizing quality</strong>: Increases weights for complexity (1.0) and coverage gaps (1.0)</li>
<li><strong>Additive bonuses</strong>: Provides +20 bonus for complex + untested code (not multiplicative)</li>
<li><strong>Context-aware thresholds</strong>: Integrates with file type classification from Spec 135</li>
</ol>
<h3 id="multi-dimensional-scoring"><a class="header" href="#multi-dimensional-scoring">Multi-Dimensional Scoring</a></h3>
<p>The rebalanced algorithm computes five scoring components:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Component</th><th>Weight</th><th>Range</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><strong>Complexity</strong></td><td>1.0</td><td>0-100</td><td>Cyclomatic + cognitive complexity</td></tr>
<tr><td><strong>Coverage Gap</strong></td><td>1.0</td><td>0-80</td><td>Testing coverage deficit with complexity bonus</td></tr>
<tr><td><strong>Structural</strong></td><td>0.8</td><td>0-60</td><td>God objects and architectural issues</td></tr>
<tr><td><strong>Size</strong></td><td>0.3</td><td>0-30</td><td>File size (reduced from previous ~1.5)</td></tr>
<tr><td><strong>Code Smells</strong></td><td>0.6</td><td>0-40</td><td>Long functions, deep nesting, impure logic</td></tr>
</tbody>
</table>
</div>
<p><strong>Weighted Total Formula</strong>:</p>
<pre><code>weighted_total = (complexity × 1.0) + (coverage × 1.0) + (structural × 0.8)
                 + (size × 0.3) + (smells × 0.6)

normalized_score = (weighted_total / 237.0) × 200.0  // Normalize to 0-200 range
</code></pre>
<h3 id="scoring-presets"><a class="header" href="#scoring-presets">Scoring Presets</a></h3>
<p>Debtmap provides four presets for different prioritization strategies:</p>
<h4 id="balanced-default"><a class="header" href="#balanced-default">Balanced (Default)</a></h4>
<pre><code class="language-toml">[scoring_rebalanced]
preset = "balanced"
</code></pre>
<p>Weights:</p>
<ul>
<li>Complexity: 1.0, Coverage: 1.0, Structural: 0.8, Size: 0.3, Smells: 0.6</li>
</ul>
<p><strong>Use when</strong>: Standard development with focus on actual code quality</p>
<h4 id="quality-focused"><a class="header" href="#quality-focused">Quality-Focused</a></h4>
<pre><code class="language-toml">[scoring_rebalanced]
preset = "quality-focused"
</code></pre>
<p>Weights:</p>
<ul>
<li>Complexity: 1.2, Coverage: 1.1, Structural: 0.9, Size: 0.2, Smells: 0.7</li>
</ul>
<p><strong>Use when</strong>: Maximum emphasis on code quality, minimal concern for file size</p>
<h4 id="test-coverage-focused"><a class="header" href="#test-coverage-focused">Test-Coverage-Focused</a></h4>
<pre><code class="language-toml">[scoring_rebalanced]
preset = "test-coverage"
</code></pre>
<p>Weights:</p>
<ul>
<li>Complexity: 0.8, Coverage: 1.3, Structural: 0.6, Size: 0.2, Smells: 0.5</li>
</ul>
<p><strong>Use when</strong>: Prioritizing test coverage improvements</p>
<h4 id="size-focused-legacy"><a class="header" href="#size-focused-legacy">Size-Focused (Legacy)</a></h4>
<pre><code class="language-toml">[scoring_rebalanced]
preset = "size-focused"
</code></pre>
<p>Weights:</p>
<ul>
<li>Complexity: 0.5, Coverage: 0.4, Structural: 0.6, Size: 1.5, Smells: 0.3</li>
</ul>
<p><strong>Use when</strong>: Maintaining legacy scoring behavior, file size is primary concern</p>
<h3 id="custom-weights"><a class="header" href="#custom-weights">Custom Weights</a></h3>
<p>You can define custom weights in <code>.debtmap.toml</code>:</p>
<pre><code class="language-toml">[scoring_rebalanced]
complexity_weight = 1.2
coverage_weight = 1.0
structural_weight = 0.8
size_weight = 0.2
smell_weight = 0.7
</code></pre>
<h3 id="severity-levels"><a class="header" href="#severity-levels">Severity Levels</a></h3>
<p>The rebalanced algorithm assigns severity based on normalized score and risk factors:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Severity</th><th>Criteria</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><strong>CRITICAL</strong></td><td>Score &gt; 120 OR (complexity &gt; 60 AND coverage &gt; 40)</td><td>Requires immediate attention</td></tr>
<tr><td><strong>HIGH</strong></td><td>Score &gt; 80 OR (complexity &gt; 40 AND coverage &gt; 20) OR structural &gt; 50</td><td>High priority for next sprint</td></tr>
<tr><td><strong>MEDIUM</strong></td><td>Score &gt; 40 OR single moderate issue</td><td>Plan for future sprint</td></tr>
<tr><td><strong>LOW</strong></td><td>Everything else</td><td>Minor concerns, size-only issues</td></tr>
</tbody>
</table>
</div>
<p><strong>Evaluation Logic</strong>: Severity is assigned based on the <strong>first matching criteria</strong> (logical OR). An item needs to satisfy <strong>only ONE condition</strong> to qualify for that severity level. For example, a function with score=90 is HIGH severity even if complexity and coverage are both low, because it meets the “Score &gt; 80” condition.</p>
<h3 id="example-prioritization"><a class="header" href="#example-prioritization">Example Prioritization</a></h3>
<p><strong>Complex Untested Function</strong> (HIGH priority):</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn process_payment(cart: &amp;Cart, user: &amp;User) -&gt; Result&lt;Receipt&gt; {
    // 150 lines, cyclomatic: 42, cognitive: 77
    // Coverage: 38%

    // Rebalanced Score:
    // - Complexity: 100.0 (very high)
    // - Coverage: 57.2 (gap × 0.6 + 20 bonus for complex+untested)
    // - Structural: 0.0
    // - Size: 0.0 (function-level scoring)
    // - Smells: 25.0 (long function)
    // Total: 95.3 → CRITICAL severity
}
<span class="boring">}</span></code></pre>
<p><strong>Large Simple Function</strong> (LOW priority):</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn format_report(data: &amp;ReportData) -&gt; String {
    // 2000 lines, cyclomatic: 3, cognitive: 5
    // Coverage: 100%

    // Rebalanced Score:
    // - Complexity: 0.0 (trivial)
    // - Coverage: 0.0 (well tested)
    // - Structural: 0.0
    // - Size: 0.0 (function-level scoring)
    // - Smells: 15.0 (long but simple)
    // Total: 3.2 → LOW severity
}
<span class="boring">}</span></code></pre>
<p><strong>Result</strong>: Complex untested code ranks 30× higher than large simple code.</p>
<h3 id="integration-with-file-classification-spec-135"><a class="header" href="#integration-with-file-classification-spec-135">Integration with File Classification (Spec 135)</a></h3>
<p>The rebalanced scoring integrates with context-aware file size thresholds:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use debtmap::organization::file_classifier::{classify_file, get_threshold};

let file_type = classify_file(source, path);
let threshold = get_threshold(&amp;file_type, function_count, lines);

// Apply context-aware scoring:
// - Generated code: 0.1× size multiplier
// - Test code: Lenient thresholds (650 lines)
// - Business logic: Strict thresholds (400 lines)
<span class="boring">}</span></code></pre>
<h3 id="generated-code-detection"><a class="header" href="#generated-code-detection">Generated Code Detection</a></h3>
<p>The rebalanced scoring automatically detects and reduces scores for generated code:</p>
<p><strong>Detection Markers</strong> (first 20 lines):</p>
<ul>
<li>“DO NOT EDIT”</li>
<li>“automatically generated”</li>
<li>“AUTO-GENERATED”</li>
<li>“@generated”</li>
<li>“Code generated by”</li>
</ul>
<p><strong>Generated Code Score Adjustment</strong>:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>if is_generated_code(source) {
    size_score *= 0.1;  // 90% reduction
}
<span class="boring">}</span></code></pre>
<h3 id="scoring-rationale"><a class="header" href="#scoring-rationale">Scoring Rationale</a></h3>
<p>Each debt item includes a detailed rationale explaining the score:</p>
<pre><code>Debt Item: src/payment/processor.rs:142 - process_payment()
Score: 95.3 (CRITICAL)

Primary factors:
  - High cyclomatic complexity (+100.0)
  - Significant coverage gap (+57.2)

Bonuses:
  - Complex + untested: +20 bonus applied
  - Code smells detected (+25.0)

Context adjustments:
  - Size de-emphasized (weight: 0.3)
</code></pre>
<h3 id="migration-from-legacy-scoring"><a class="header" href="#migration-from-legacy-scoring">Migration from Legacy Scoring</a></h3>
<p><strong>Breaking Changes</strong>:</p>
<ul>
<li>Scores will change significantly for all debt items</li>
<li>Large files with low complexity will rank lower</li>
<li>Complex untested code will rank higher</li>
<li>Size-based prioritization reduced by 80%</li>
</ul>
<p><strong>Restoring Legacy Behavior</strong>:</p>
<pre><code class="language-toml">[scoring_rebalanced]
preset = "size-focused"
</code></pre>
<p><strong>Gradual Migration</strong>:</p>
<ol>
<li>Run analysis with standard scoring first (no config changes)</li>
<li>Create a test config with <code>[scoring_rebalanced]</code> section</li>
<li>Compare results between standard and rebalanced scoring</li>
<li>Adjust team priorities based on new rankings</li>
<li>Switch to rebalanced scoring after validation</li>
</ol>
<p><strong>Note</strong>: There is no <code>--legacy-scoring</code> CLI flag. Switch between algorithms by modifying your configuration file (add or remove the <code>[scoring_rebalanced]</code> section).</p>
<h3 id="configuration-reference-1"><a class="header" href="#configuration-reference-1">Configuration Reference</a></h3>
<p>Complete configuration example:</p>
<pre><code class="language-toml"># .debtmap.toml

[scoring_rebalanced]
# Use a preset (balanced, quality-focused, test-coverage, size-focused)
preset = "balanced"

# Or define custom weights
complexity_weight = 1.0
coverage_weight = 1.0
structural_weight = 0.8
size_weight = 0.3
smell_weight = 0.6
</code></pre>
<h3 id="when-to-use-rebalanced-scoring"><a class="header" href="#when-to-use-rebalanced-scoring">When to Use Rebalanced Scoring</a></h3>
<p>✅ <strong>Use rebalanced scoring when</strong>:</p>
<ul>
<li>You want to prioritize code quality over file size</li>
<li>Complex untested code is a concern</li>
<li>You’re building new features and need quality focus</li>
<li>Your team values testability and maintainability</li>
</ul>
<p>❌ <strong>Use legacy/size-focused when</strong>:</p>
<ul>
<li>You’re managing a legacy codebase with large files</li>
<li>File size reduction is the primary concern</li>
<li>You need compatibility with existing workflows</li>
<li>Your team’s priority is file splitting over quality</li>
</ul>
<h3 id="performance-2"><a class="header" href="#performance-2">Performance</a></h3>
<p>The rebalanced scoring algorithm has minimal performance impact:</p>
<ul>
<li>Same O(n) complexity as legacy scoring</li>
<li>No additional file I/O required</li>
<li>Parallel processing compatible</li>
<li>Adds ~5% to analysis time for rationale generation</li>
</ul>
<h2 id="score-based-prioritization-with-exponential-scaling-spec-171"><a class="header" href="#score-based-prioritization-with-exponential-scaling-spec-171">Score-Based Prioritization with Exponential Scaling (Spec 171)</a></h2>
<p>DebtMap uses exponential scaling and risk boosting to amplify high-severity technical debt items, ensuring critical issues stand out clearly in priority lists. This section explains how these mechanisms work and how to configure them for your project.</p>
<h3 id="why-exponential-scaling"><a class="header" href="#why-exponential-scaling">Why Exponential Scaling?</a></h3>
<p>Traditional linear multipliers create uniform gaps between scores:</p>
<ul>
<li>Linear 2x multiplier: Score 50 → 100, Score 100 → 200 (uniform +50 and +100 gaps)</li>
</ul>
<p>Exponential scaling creates growing gaps that make critical issues impossible to miss:</p>
<ul>
<li>Exponential scaling (^1.4): Score 50 → 279, Score 100 → 1000 (gaps grow dramatically)</li>
</ul>
<p><strong>Key Benefits</strong>:</p>
<ul>
<li><strong>Visual Separation</strong>: Critical items have dramatically higher scores than medium items</li>
<li><strong>Natural Clustering</strong>: Similar-severity items cluster together in ranked lists</li>
<li><strong>Actionable Ordering</strong>: Work through the list from top to bottom with confidence</li>
<li><strong>No Arbitrary Thresholds</strong>: Pure score-based ranking eliminates debates about tier boundaries</li>
</ul>
<h3 id="how-exponential-scaling-works"><a class="header" href="#how-exponential-scaling-works">How Exponential Scaling Works</a></h3>
<p>After calculating the base score (complexity + coverage + dependencies), DebtMap applies pattern-specific exponential scaling:</p>
<p><strong>Formula</strong>:</p>
<pre><code>scaled_score = base_score ^ exponent
</code></pre>
<p><strong>Pattern-Specific Exponents</strong> (configurable in <code>.debtmap.toml</code>):</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Pattern Type</th><th>Default Exponent</th><th>Rationale</th></tr>
</thead>
<tbody>
<tr><td>God Objects</td><td>1.4</td><td>Highest amplification - architectural issues deserve top priority</td></tr>
<tr><td>Long Functions</td><td>1.3</td><td>High amplification - major refactoring candidates</td></tr>
<tr><td>Complex Functions</td><td>1.2</td><td>Moderate amplification - complexity issues</td></tr>
<tr><td>Primitive Obsession</td><td>1.1</td><td>Light amplification - design smell but lower urgency</td></tr>
</tbody>
</table>
</div>
<h3 id="example-god-object-scaling-exponent--14"><a class="header" href="#example-god-object-scaling-exponent--14">Example: God Object Scaling (exponent = 1.4)</a></h3>
<p>Comparing three God Objects with different base scores:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Base Score</th><th>Calculation</th><th>Scaled Score</th><th>Amplification</th></tr>
</thead>
<tbody>
<tr><td>10</td><td>10^1.4</td><td>25.1</td><td>2.5x</td></tr>
<tr><td>50</td><td>50^1.4</td><td>279.5</td><td>5.6x</td></tr>
<tr><td>100</td><td>100^1.4</td><td>1000.0</td><td>10x</td></tr>
</tbody>
</table>
</div>
<p><strong>Result</strong>: The highest-severity God Object (score 100) gets 10x amplification, while a minor issue (score 10) only gets 2.5x. This creates clear visual separation in your priority list.</p>
<h3 id="risk-boosting"><a class="header" href="#risk-boosting">Risk Boosting</a></h3>
<p>After exponential scaling, DebtMap applies additional risk multipliers based on architectural position:</p>
<p><strong>Risk Multipliers</strong> (applied multiplicatively):</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>final_score = scaled_score × risk_multiplier
<span class="boring">}</span></code></pre>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Risk Factor</th><th>Multiplier</th><th>Rationale</th></tr>
</thead>
<tbody>
<tr><td>High dependency count (10+ callers)</td><td>1.2x</td><td>Harder to refactor safely, affects more code</td></tr>
<tr><td>Entry point (main, CLI handlers, routes)</td><td>1.15x</td><td>Failures cascade to all downstream code</td></tr>
<tr><td>Low test coverage (&lt;30%)</td><td>1.1x</td><td>Riskier to modify without tests</td></tr>
</tbody>
</table>
</div>
<p><strong>Example</strong>:</p>
<pre><code>Function: process_payment (God Object)
  Base Score: 85.0
  Exponentially Scaled: 85^1.4 = 554.3
  Risk Factors:
    - Entry point: ×1.15
    - Low coverage (15%): ×1.1
  Final Score: 554.3 × 1.15 × 1.1 = 701.7
</code></pre>
<h3 id="complete-scoring-pipeline"><a class="header" href="#complete-scoring-pipeline">Complete Scoring Pipeline</a></h3>
<p>DebtMap processes scores through multiple stages:</p>
<pre><code>1. Base Score Calculation
   ↓
   Weighted sum of:
   - Coverage factor (40% weight)
   - Complexity factor (40% weight)
   - Dependency factor (20% weight)

2. Exponential Scaling
   ↓
   Pattern-specific exponent applied:
   - God Objects: ^1.4
   - Long Functions: ^1.3
   - etc.

3. Risk Boosting
   ↓
   Architectural position multipliers:
   - High dependencies: ×1.2
   - Entry points: ×1.15
   - Low coverage: ×1.1

4. Final Score
   ↓
   Used for ranking (no tier bucketing)

5. Output
   ↓
   Sorted descending by final score
</code></pre>
<h3 id="configuration-3-1"><a class="header" href="#configuration-3-1">Configuration</a></h3>
<p>You can customize exponential scaling parameters in <code>.debtmap.toml</code>:</p>
<pre><code class="language-toml">[priority.scaling.god_object]
exponent = 1.5              # Increase amplification for God Objects
min_threshold = 30.0        # Only scale scores above 30
max_threshold = 500.0       # Cap scaled scores at 500

[priority.scaling.long_function]
exponent = 1.3              # Default amplification
min_threshold = 0.0         # No minimum threshold
max_threshold = 1000.0      # High cap for extreme cases

[priority.scaling.complex_function]
exponent = 1.2              # Moderate amplification
min_threshold = 20.0        # Scale scores above 20
max_threshold = 800.0       # Cap at 800
</code></pre>
<p><strong>Configuration Parameters</strong>:</p>
<ul>
<li><strong>exponent</strong>: The exponential scaling factor (higher = more amplification)</li>
<li><strong>min_threshold</strong>: Minimum base score to apply scaling (prevents amplifying trivial issues)</li>
<li><strong>max_threshold</strong>: Maximum scaled score (prevents extreme outliers)</li>
</ul>
<h3 id="tuning-guidelines-3"><a class="header" href="#tuning-guidelines-3">Tuning Guidelines</a></h3>
<p><strong>Increase amplification when</strong>:</p>
<ul>
<li>Critical issues aren’t standing out enough in your priority list</li>
<li>Team needs stronger signal about what to tackle first</li>
<li>You have many medium-severity items obscuring high-severity ones</li>
</ul>
<p><strong>Decrease amplification when</strong>:</p>
<ul>
<li>Priority list feels too top-heavy (too many “critical” items)</li>
<li>Scores are getting too large (e.g., thousands)</li>
<li>You want more gradual transitions between severity levels</li>
</ul>
<p><strong>Example: More Aggressive God Object Detection</strong></p>
<pre><code class="language-toml">[priority.scaling.god_object]
exponent = 1.6              # Higher amplification
min_threshold = 20.0        # Start scaling earlier
max_threshold = 2000.0      # Allow higher caps
</code></pre>
<h3 id="comparing-with-vs-without-exponential-scaling"><a class="header" href="#comparing-with-vs-without-exponential-scaling">Comparing With vs Without Exponential Scaling</a></h3>
<p><strong>Without Exponential Scaling (Linear Multipliers)</strong>:</p>
<pre><code>Priority List:
1. God Object (base: 85) → final: 170 (2x multiplier)
2. Long Function (base: 80) → final: 160 (2x multiplier)
3. Complex Function (base: 75) → final: 150 (2x multiplier)
4. Medium Issue (base: 70) → final: 140 (2x multiplier)
</code></pre>
<p><strong>Problem</strong>: Gaps are uniform (10 points). Hard to distinguish critical from medium issues.</p>
<p><strong>With Exponential Scaling</strong>:</p>
<pre><code>Priority List:
1. God Object (base: 85) → scaled: 554 → with risk: 701
2. Long Function (base: 80) → scaled: 447 → with risk: 492
3. Complex Function (base: 75) → scaled: 357 → with risk: 357
4. Medium Issue (base: 70) → scaled: 282 → with risk: 282
</code></pre>
<p><strong>Result</strong>: Clear separation. God Object stands out as 2.5x higher than medium issues.</p>
<h3 id="score-based-ranking-vs-tier-based-ranking"><a class="header" href="#score-based-ranking-vs-tier-based-ranking">Score-Based Ranking vs Tier-Based Ranking</a></h3>
<p>DebtMap uses pure score-based ranking (not tier-based) for finer granularity:</p>
<p><strong>Traditional Tier-Based Ranking</strong>:</p>
<pre><code>Critical: Items with score ≥ 200
High: Items with score 100-199
Medium: Items with score 50-99
Low: Items with score &lt; 50
</code></pre>
<p><strong>Problem</strong>: All “Critical” items look equally important, even if one has score 201 and another has score 1000.</p>
<p><strong>Score-Based Ranking</strong>:</p>
<pre><code>1. process_payment - Score: 1247.3
2. UserService.authenticate - Score: 891.2
3. calculate_tax - Score: 654.1
...
</code></pre>
<p><strong>Benefits</strong>:</p>
<ul>
<li>Every item has a unique priority position</li>
<li>Natural ordering - work from highest to lowest</li>
<li>No arbitrary boundaries or threshold debates</li>
<li>Finer-grained decision making</li>
</ul>
<p><strong>Compatibility Note</strong>: For tools expecting Priority enums, scores can be mapped to tiers:</p>
<ul>
<li>Score ≥ 200: Critical</li>
<li>Score ≥ 100: High</li>
<li>Score ≥ 50: Medium</li>
<li>Score &lt; 50: Low</li>
</ul>
<p>However, the primary output uses raw scores for maximum granularity.</p>
<h3 id="practical-examples-4"><a class="header" href="#practical-examples-4">Practical Examples</a></h3>
<p><strong>Example 1: Identifying Architectural Hot Spots</strong></p>
<pre><code class="language-bash">debtmap analyze . --top 10
</code></pre>
<p><strong>Output</strong>:</p>
<pre><code>Top 10 Technical Debt Items (Sorted by Score)

1. src/services/user_service.rs:45 - UserService::authenticate
   Score: 1247.3 | Pattern: God Object | Coverage: 12%
   → 45 methods, 892 lines, high complexity
   → Risk factors: Entry point (×1.15), High dependencies (×1.2)

2. src/payment/processor.rs:142 - process_payment
   Score: 891.2 | Pattern: Complex Function | Coverage: 8%
   → Cyclomatic: 42, Cognitive: 77
   → Risk factors: Entry point (×1.15), Low coverage (×1.1)

3. src/reporting/generator.rs:234 - generate_monthly_report
   Score: 654.1 | Pattern: Long Function | Coverage: 45%
   → 287 lines, moderate complexity
   → Risk factors: High dependencies (×1.2)
</code></pre>
<p><strong>Action</strong>: Focus on top 3 items first - they have dramatically higher scores than items 4-10.</p>
<p><strong>Example 2: Monitoring Exponential Scaling Impact</strong></p>
<pre><code class="language-bash"># Analyze with verbose output to see scaling details
debtmap analyze . --verbose --top 5
</code></pre>
<p><strong>Verbose Output</strong>:</p>
<pre><code>Function: src/services/user_service.rs:45 - UserService::authenticate
  Base Score: 85.0
  Pattern: God Object
  Exponential Scaling (^1.4): 85.0^1.4 = 554.3
  Risk Boosting:
    - Entry point: ×1.15 → 637.4
    - High dependencies (15 callers): ×1.2 → 764.9
    - Low coverage (12%): ×1.1 → 841.4
  Final Score: 841.4
</code></pre>
<p><strong>Insight</strong>: Base score of 85 amplified to 841 through exponential scaling and risk boosting - a 9.9x total amplification.</p>
<h3 id="when-to-use-exponential-scaling"><a class="header" href="#when-to-use-exponential-scaling">When to Use Exponential Scaling</a></h3>
<p>✅ <strong>Use exponential scaling when</strong>:</p>
<ul>
<li>You need clear visual separation between critical and medium issues</li>
<li>Your priority list has too many “high priority” items</li>
<li>You want top issues to stand out dramatically</li>
<li>You prefer score-based ranking over tier-based bucketing</li>
</ul>
<p>✅ <strong>Adjust exponents when</strong>:</p>
<ul>
<li>Default amplification doesn’t match your team’s priorities</li>
<li>Certain patterns (e.g., God Objects) deserve more/less emphasis</li>
<li>You’re tuning the balance between different debt types</li>
</ul>
<p>✅ <strong>Tune thresholds when</strong>:</p>
<ul>
<li>Scores are getting too large (increase max_threshold)</li>
<li>Trivial issues are being amplified (increase min_threshold)</li>
<li>You want to cap extreme outliers (adjust max_threshold)</li>
</ul>
<h3 id="performance-impact-1"><a class="header" href="#performance-impact-1">Performance Impact</a></h3>
<p>Exponential scaling has negligible performance impact:</p>
<ul>
<li><strong>Computation</strong>: Simple <code>powf()</code> operation per item</li>
<li><strong>Overhead</strong>: &lt;1% additional analysis time</li>
<li><strong>Scalability</strong>: Works with parallel processing (no synchronization needed)</li>
<li><strong>Memory</strong>: No additional data structures required</li>
</ul>
<h3 id="see-also-9"><a class="header" href="#see-also-9">See Also</a></h3>
<ul>
<li><a href="#configuration-2">Configuration</a> - Scoring and aggregation configuration</li>
<li><a href="#analysis-guide">Analysis Guide</a> - Detailed metric explanations</li>
<li><a href="../../ARCHITECTURE.html">ARCHITECTURE.md</a> - Technical details of exponential scaling implementation</li>
</ul>
<h2 id="data-flow-scoring-spec-218"><a class="header" href="#data-flow-scoring-spec-218">Data Flow Scoring (Spec 218)</a></h2>
<p>DebtMap includes advanced data flow analysis that adjusts priority scores based on code purity, refactorability, and separation of concerns. This helps prioritize impure, tightly-coupled code over pure, easily-refactorable functions.</p>
<h3 id="why-data-flow-scoring"><a class="header" href="#why-data-flow-scoring">Why Data Flow Scoring?</a></h3>
<p>Traditional complexity metrics treat all complex code equally, but not all complexity is equal:</p>
<p><strong>Pure functions</strong> (no side effects, deterministic):</p>
<ul>
<li>Easy to test in isolation</li>
<li>Simple to refactor and extract</li>
<li>Lower maintenance burden</li>
<li>Less risky to modify</li>
</ul>
<p><strong>Impure functions</strong> (side effects, mutations):</p>
<ul>
<li>Harder to test (require mocking, state management)</li>
<li>Complex to refactor (dependencies, order matters)</li>
<li>Higher maintenance burden</li>
<li>Riskier to modify</li>
</ul>
<p>Data flow scoring reduces the priority of pure code and increases priority for impure code, ensuring refactoring efforts focus where they matter most.</p>
<h3 id="how-data-flow-scoring-works"><a class="header" href="#how-data-flow-scoring-works">How Data Flow Scoring Works</a></h3>
<p>When data flow analysis is enabled, DebtMap analyzes each function’s data flow graph to compute three additional factors:</p>
<ol>
<li><strong>Purity Factor</strong> (0.0-1.0): Classifies functions on a purity spectrum</li>
<li><strong>Refactorability Factor</strong> (1.0-1.5): Based on dead stores and escape analysis</li>
<li><strong>Pattern Factor</strong> (0.7-1.0): Distinguishes data flow from business logic</li>
</ol>
<p>These factors are combined with the standard scoring factors (complexity, coverage, dependencies) to produce a final score that reflects both complexity and refactorability.</p>
<h3 id="purity-spectrum-classification"><a class="header" href="#purity-spectrum-classification">Purity Spectrum Classification</a></h3>
<p>Functions are classified on a spectrum from strictly pure to impure:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Purity Level</th><th>Score Multiplier</th><th>Description</th><th>Example</th></tr>
</thead>
<tbody>
<tr><td>Strictly Pure</td><td>0.0</td><td>No mutations, no I/O, referentially transparent</td><td>Pure math functions</td></tr>
<tr><td>Locally Pure</td><td>0.3</td><td>Pure interface but uses local mutations internally</td><td>Builder pattern with local state</td></tr>
<tr><td>I/O Isolated</td><td>0.6</td><td>I/O operations clearly separated from logic</td><td>Function with I/O at boundaries only</td></tr>
<tr><td>I/O Mixed</td><td>0.9</td><td>I/O mixed with business logic</td><td>Database queries interleaved with calculations</td></tr>
<tr><td>Impure</td><td>1.0</td><td>Mutable state, side effects throughout</td><td>Functions with global state mutations</td></tr>
</tbody>
</table>
</div>
<p><strong>Score Impact</strong>: The purity factor is multiplied with the base score. Lower multipliers reduce priority for purer code.</p>
<p><strong>Example</strong>:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Strictly Pure (multiplier = 0.0) - Priority reduced to near-zero
fn calculate_discount(price: f64, rate: f64) -&gt; f64 {
    price * rate
}

// Impure (multiplier = 1.0) - Full priority maintained
fn apply_discount_and_save(price: f64, rate: f64) -&gt; Result&lt;()&gt; {
    let discount = price * rate;
    DATABASE.save_discount(discount)?;
    CACHE.invalidate()?;
    Ok(())
}
<span class="boring">}</span></code></pre>
<h3 id="refactorability-factor"><a class="header" href="#refactorability-factor">Refactorability Factor</a></h3>
<p>Based on data flow analysis, this factor identifies code that’s hard to refactor:</p>
<p><strong>Calculation</strong>: <code>1.0 + (dead_store_ratio * 0.5)</code></p>
<p><strong>Range</strong>: 1.0 (easily refactorable) to 1.5 (difficult to refactor)</p>
<p><strong>Indicators of Poor Refactorability</strong>:</p>
<ul>
<li><strong>Dead stores</strong>: Variables assigned but never read (indicates complex flow)</li>
<li><strong>Escaped values</strong>: Variables that escape to outer scopes (limits extraction)</li>
<li><strong>Complex dependencies</strong>: Many input/output dependencies</li>
</ul>
<p><strong>Example</strong>:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Easy to refactor (factor = 1.0)
fn calculate_total(items: &amp;[Item]) -&gt; f64 {
    items.iter().map(|i| i.price).sum()
}

// Hard to refactor (factor = 1.5)
fn process_order(order: &amp;mut Order) -&gt; Result&lt;()&gt; {
    let mut temp1 = order.subtotal; // Dead store
    let mut temp2 = 0.0;            // Dead store
    temp1 = order.items.iter().map(|i| i.price).sum();
    order.total = temp1 + order.tax;
    GLOBAL_STATE.update(order.id)?;  // Escaped to global scope
    Ok(())
}
<span class="boring">}</span></code></pre>
<h3 id="pattern-factor"><a class="header" href="#pattern-factor">Pattern Factor</a></h3>
<p>Distinguishes between data flow patterns (simple transformations) and business logic (complex rules):</p>
<p><strong>Calculation</strong>:</p>
<ul>
<li>Data flow patterns: 0.7 (reduced priority)</li>
<li>Business logic: 1.0 (full priority)</li>
</ul>
<p><strong>Data Flow Patterns</strong> (lower priority):</p>
<ul>
<li>Map/filter/reduce operations</li>
<li>Simple transformations</li>
<li>Pipeline operations</li>
</ul>
<p><strong>Business Logic</strong> (higher priority):</p>
<ul>
<li>Complex conditional logic</li>
<li>Business rules and validations</li>
<li>Stateful computations</li>
</ul>
<p><strong>Example</strong>:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Data flow pattern (factor = 0.7) - Lower priority
fn filter_active_users(users: &amp;[User]) -&gt; Vec&lt;&amp;User&gt; {
    users.iter().filter(|u| u.is_active).collect()
}

// Business logic (factor = 1.0) - Full priority
fn calculate_user_discount(user: &amp;User, cart: &amp;Cart) -&gt; f64 {
    if user.is_premium &amp;&amp; cart.total &gt; 100.0 {
        if user.loyalty_points &gt; 1000 {
            0.2
        } else {
            0.1
        }
    } else {
        0.0
    }
}
<span class="boring">}</span></code></pre>
<h3 id="combined-scoring-formula"><a class="header" href="#combined-scoring-formula">Combined Scoring Formula</a></h3>
<p>When data flow scoring is enabled, the final score includes all three factors:</p>
<pre><code>base_score = (complexity_factor * 0.5) + (coverage_factor * 0.4) + (dependency_factor * 0.1)

data_flow_adjusted_score = base_score * purity_factor * refactorability_factor * pattern_factor

final_score = data_flow_adjusted_score * role_multiplier
</code></pre>
<h3 id="configuration-4-1"><a class="header" href="#configuration-4-1">Configuration</a></h3>
<p>Data flow scoring is configured in <code>.debtmap.toml</code>:</p>
<pre><code class="language-toml">[data_flow_scoring]
# Enable data flow-based priority adjustments
enabled = true

# Purity spectrum weights (how much to reduce priority for pure code)
purity_spectrum_weight = 1.0

# Refactorability weight (how much to increase priority for hard-to-refactor code)
refactorability_weight = 1.0

# Pattern weight (how much to reduce priority for simple data flow patterns)
pattern_weight = 1.0
</code></pre>
<p><strong>Configuration Parameters</strong>:</p>
<ul>
<li><strong>enabled</strong>: Enable/disable data flow scoring (default: true)</li>
<li><strong>purity_spectrum_weight</strong>: Weight for purity factor (0.0-2.0, default: 1.0)</li>
<li><strong>refactorability_weight</strong>: Weight for refactorability factor (0.0-2.0, default: 1.0)</li>
<li><strong>pattern_weight</strong>: Weight for pattern factor (0.0-2.0, default: 1.0)</li>
</ul>
<h3 id="examples-4"><a class="header" href="#examples-4">Examples</a></h3>
<p><strong>Example 1: Pure Complex Function (Low Priority)</strong></p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Complex but pure - Priority significantly reduced
fn fibonacci(n: u32) -&gt; u64 {
    // Cyclomatic complexity: 15
    // Cognitive complexity: 25
    // But: Strictly pure (no side effects)

    // Data flow scoring:
    // - Purity factor: 0.0 (strictly pure)
    // - Refactorability factor: 1.0 (easy to extract)
    // - Pattern factor: 1.0 (business logic)
    // Final adjustment: base_score * 0.0 = near-zero priority
}
<span class="boring">}</span></code></pre>
<p><strong>Example 2: Impure Complex Function (High Priority)</strong></p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Complex and impure - Priority maintained or increased
fn update_user_profile(user_id: i64, updates: &amp;Updates) -&gt; Result&lt;()&gt; {
    // Cyclomatic complexity: 15
    // Cognitive complexity: 25
    // Impure: database mutations, cache invalidation

    // Data flow scoring:
    // - Purity factor: 1.0 (impure)
    // - Refactorability factor: 1.4 (dead stores detected)
    // - Pattern factor: 1.0 (business logic)
    // Final adjustment: base_score * 1.4 = 40% priority increase
}
<span class="boring">}</span></code></pre>
<p><strong>Example 3: Data Flow Pattern (Reduced Priority)</strong></p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Simple transformation - Priority reduced
fn map_to_dtos(users: &amp;[User]) -&gt; Vec&lt;UserDTO&gt; {
    // Cyclomatic complexity: 5
    // Simple map operation

    // Data flow scoring:
    // - Purity factor: 0.0 (strictly pure)
    // - Refactorability factor: 1.0 (easy to extract)
    // - Pattern factor: 0.7 (data flow pattern)
    // Final adjustment: base_score * 0.0 = near-zero priority
}
<span class="boring">}</span></code></pre>
<h3 id="score-explanation-output"><a class="header" href="#score-explanation-output">Score Explanation Output</a></h3>
<p>When data flow scoring is enabled, the score explanation includes data flow factors:</p>
<pre><code class="language-json">{
  "function": "process_payment",
  "scoring_details": {
    "complexity_score": 8.5,
    "coverage_score": 9.0,
    "dependency_score": 6.0,
    "base_score": 23.5,
    "purity_factor": 1.0,
    "refactorability_factor": 1.4,
    "pattern_factor": 1.0,
    "final_score": 32.9
  }
}
</code></pre>
<p><strong>Interpretation</strong>:</p>
<ul>
<li>Base score from complexity/coverage/dependencies: 23.5</li>
<li>Purity factor (1.0): Function is impure, full priority maintained</li>
<li>Refactorability factor (1.4): 40% increase due to difficult refactorability</li>
<li>Pattern factor (1.0): Business logic, not just data flow</li>
<li>Final score: 32.9 (after data flow adjustments)</li>
</ul>
<h3 id="when-to-use-data-flow-scoring"><a class="header" href="#when-to-use-data-flow-scoring">When to Use Data Flow Scoring</a></h3>
<p>✅ <strong>Use data flow scoring when</strong>:</p>
<ul>
<li>You have a mix of pure and impure code</li>
<li>You want to focus refactoring on impure, hard-to-test code</li>
<li>You’re working in a functional codebase with many pure functions</li>
<li>You want to avoid false positives from complex but pure algorithms</li>
</ul>
<p>❌ <strong>Disable data flow scoring when</strong>:</p>
<ul>
<li>Your codebase is primarily imperative (all code is impure)</li>
<li>You don’t have data flow analysis available (requires language support)</li>
<li>You want to prioritize all complex code equally regardless of purity</li>
</ul>
<h3 id="interpreting-results-3"><a class="header" href="#interpreting-results-3">Interpreting Results</a></h3>
<p><strong>Before data flow scoring</strong>:</p>
<pre><code>Top Priority Items:
1. fibonacci() - Score: 8.5 (complex algorithm)
2. process_payment() - Score: 8.2 (complex + impure)
3. map_users() - Score: 7.8 (simple transformation)
</code></pre>
<p><strong>After data flow scoring</strong>:</p>
<pre><code>Top Priority Items:
1. process_payment() - Score: 11.5 (complex + impure + hard to refactor)
2. map_users() - Score: 1.2 (pure data flow pattern, deprioritized)
3. fibonacci() - Score: 0.3 (pure algorithm, deprioritized)
</code></pre>
<p><strong>Result</strong>: Impure, hard-to-refactor code rises to the top, while pure functions are deprioritized.</p>
<h3 id="performance-impact-1-1"><a class="header" href="#performance-impact-1-1">Performance Impact</a></h3>
<p>Data flow scoring requires data flow analysis, which has moderate performance impact:</p>
<ul>
<li><strong>Analysis overhead</strong>: ~15-20% additional analysis time</li>
<li><strong>Memory usage</strong>: Proportional to function size (data flow graphs)</li>
<li><strong>Recommended</strong>: Enable for projects &lt;100k LOC, or use selective analysis</li>
</ul>
<h3 id="see-also-1-1"><a class="header" href="#see-also-1-1">See Also</a></h3>
<ul>
<li><a href="#functional-composition-analysis">Functional Analysis</a> - Purity detection and analysis</li>
<li><a href="data-flow-analysis.html">Data Flow Analysis</a> - Data flow graph construction (if available)</li>
<li><a href="#configuration-2">Configuration</a> - Data flow scoring configuration</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="tiered-prioritization-3"><a class="header" href="#tiered-prioritization-3">Tiered Prioritization</a></h1>
<p>Debtmap uses a sophisticated tiered prioritization system to surface critical architectural issues above simple testing gaps. This chapter explains the tier strategy, how to interpret tier classifications, and how to customize tier thresholds for your project.</p>
<h2 id="overview-20"><a class="header" href="#overview-20">Overview</a></h2>
<p>The tiered prioritization system organizes technical debt into four distinct tiers based on impact, urgency, and architectural significance. This prevents “walls of similar-scored items” and ensures critical issues don’t get lost among minor problems.</p>
<p><strong>Two Tier Systems</strong>: Debtmap uses two complementary tier systems:</p>
<ol>
<li><strong>RecommendationTier</strong> (T1-T4): Used internally to classify items based on architectural significance and testing needs</li>
<li><strong>Display Tier</strong> (Critical/High/Moderate/Low): Score-based tiers shown in terminal output, derived from final calculated scores</li>
</ol>
<p>The configuration examples below control the RecommendationTier classification logic, which influences scoring through tier weights. The final display uses score-based tiers for consistency across all output formats.</p>
<h2 id="the-four-tiers"><a class="header" href="#the-four-tiers">The Four Tiers</a></h2>
<h3 id="tier-1-critical-architecture"><a class="header" href="#tier-1-critical-architecture">Tier 1: Critical Architecture</a></h3>
<p><strong>Description</strong>: God Objects, God Modules, excessive complexity requiring immediate architectural attention</p>
<p><strong>Priority</strong>: Must address before adding new features</p>
<p><strong>Weight</strong>: 1.5x (highest priority multiplier)</p>
<p><strong>Impact</strong>: High impact on maintainability and team velocity</p>
<p><strong>Classification Criteria</strong> (src/priority/tiers.rs:183-216):</p>
<p>Debtmap uses sophisticated multi-factor analysis for Tier 1 classification, not just raw cyclomatic complexity. Items qualify for Tier 1 if they meet <strong>any</strong> of these criteria:</p>
<ol>
<li><strong>Critical Patterns</strong>: AsyncMisuse or ErrorSwallowing debt types (always Tier 1)</li>
<li><strong>High Final Score</strong>: final_score &gt; 10.0 after exponential scaling</li>
<li><strong>Extreme Cyclomatic</strong>: Entropy-dampened cyclomatic complexity &gt; 50</li>
<li><strong>High Cognitive Load</strong>: Cognitive complexity &gt;= 20</li>
<li><strong>Deep Nesting</strong>: Nesting depth &gt;= 5 levels</li>
<li><strong>High Weighted Complexity</strong>: complexity_factor &gt; 5.0 (weighted: 30% cyclomatic + 70% cognitive)</li>
</ol>
<p><strong>Examples</strong>:</p>
<ul>
<li>Files with 15+ responsibilities (God Objects)</li>
<li>Modules with 50+ methods (God Modules)</li>
<li>ComplexityHotspot debt items meeting any of the criteria above</li>
<li>Functions with cognitive complexity &gt;= 20 (extreme mental load)</li>
<li>Deeply nested code (5+ nesting levels)</li>
<li>Circular dependencies affecting core modules</li>
</ul>
<p><strong>When to Address</strong>: Immediately, before sprint work begins. These issues compound over time and block progress.</p>
<pre><code class="language-bash"># Focus on Tier 1 items
debtmap analyze . --min-priority high --top 5
</code></pre>
<h3 id="tier-2-complex-untested"><a class="header" href="#tier-2-complex-untested">Tier 2: Complex Untested</a></h3>
<p><strong>Description</strong>: Untested code with high complexity or critical dependencies, plus moderate complexity hotspots not severe enough for Tier 1.</p>
<p><strong>Priority</strong>: Risk of bugs in critical paths</p>
<p><strong>Weight</strong>: 1.0x (standard multiplier)</p>
<p><strong>Action</strong>: Should be tested before refactoring to prevent regressions</p>
<p><strong>Classification Criteria</strong> (src/priority/tiers.rs:257-302):</p>
<p>Items qualify for Tier 2 through <strong>two distinct paths</strong>:</p>
<p><strong>Path 1: Testing Gaps</strong> - Untested code meeting ANY of:</p>
<ul>
<li>Cyclomatic complexity ≥ 15</li>
<li>Total dependencies ≥ 10</li>
<li>Entry point functions with any coverage gap</li>
</ul>
<p><strong>Path 2: Moderate Complexity Hotspots</strong> - ComplexityHotspot items with meaningful but non-extreme complexity, meeting ANY of:</p>
<ul>
<li>Complexity factor &gt;= 2.0 (weighted: 30% cyclomatic + 70% cognitive, scaled 0-10)</li>
<li>Cognitive complexity &gt;= 12 (moderate to high mental load)</li>
<li>Nesting depth &gt;= 3 (meaningful nested control flow)</li>
<li>Entropy-dampened cyclomatic complexity 8-50 (after filtering repetitive patterns)</li>
</ul>
<p><strong>Examples</strong>:</p>
<ul>
<li>Functions with cyclomatic complexity ≥ 15 and 0% coverage</li>
<li>Functions with 10+ dependencies and low test coverage</li>
<li>Business logic entry points without tests</li>
<li>Complex error handling without validation</li>
<li>Moderate complexity hotspots (complexity_factor &gt;= 2.0, cognitive &gt;= 12, or nesting &gt;= 3)</li>
</ul>
<p><strong>When to Address</strong>: Within current sprint. Add tests before making changes.</p>
<pre><code class="language-bash"># See Tier 2 testing gaps
debtmap analyze . --lcov coverage.lcov --min-priority high
</code></pre>
<h3 id="tier-3-testing-gaps"><a class="header" href="#tier-3-testing-gaps">Tier 3: Testing Gaps</a></h3>
<p><strong>Description</strong>: Untested code with moderate complexity</p>
<p><strong>Priority</strong>: Improve coverage to prevent future issues</p>
<p><strong>Weight</strong>: 0.7x (reduced multiplier)</p>
<p><strong>Action</strong>: Add tests opportunistically or during related changes</p>
<p><strong>Examples</strong>:</p>
<ul>
<li>Functions with cyclomatic complexity 10-15 and low coverage</li>
<li>Utility functions without edge case tests</li>
<li>Moderate complexity with partial coverage</li>
</ul>
<p><strong>When to Address</strong>: Next sprint or when touching related code.</p>
<h3 id="tier-4-maintenance"><a class="header" href="#tier-4-maintenance">Tier 4: Maintenance</a></h3>
<p><strong>Description</strong>: Low-complexity issues and code quality improvements</p>
<p><strong>Priority</strong>: Address opportunistically during other work</p>
<p><strong>Weight</strong>: 0.3x (lowest multiplier)</p>
<p><strong>Action</strong>: Fix when convenient, low urgency</p>
<p><strong>Examples</strong>:</p>
<ul>
<li>Simple functions with minor code quality issues</li>
<li>TODO markers in well-tested code</li>
<li>Minor duplication in test code</li>
</ul>
<p><strong>When to Address</strong>: During cleanup sprints or when refactoring nearby code.</p>
<h2 id="configuration-16"><a class="header" href="#configuration-16">Configuration</a></h2>
<p>Tier configuration is optional in <code>.debtmap.toml</code>. If not specified, Debtmap uses the balanced defaults shown below.</p>
<h3 id="default-tier-thresholds"><a class="header" href="#default-tier-thresholds">Default Tier Thresholds</a></h3>
<pre><code class="language-toml">[tiers]
# Tier 2 thresholds (Complex Untested)
t2_complexity_threshold = 15         # Cyclomatic complexity cutoff
t2_dependency_threshold = 10         # Dependency count cutoff

# Tier 3 thresholds (Testing Gaps)
t3_complexity_threshold = 10         # Lower complexity threshold

# Display options
show_t4_in_main_report = false      # Hide Tier 4 from main output (default: false)

# Tier weights (multipliers applied to base scores)
t1_weight = 1.5    # Critical architecture
t2_weight = 1.0    # Complex untested
t3_weight = 0.7    # Testing gaps
t4_weight = 0.3    # Maintenance
</code></pre>
<p>To use tier-based prioritization with custom settings, add the <code>[tiers]</code> section to your <code>.debtmap.toml</code> configuration file:</p>
<pre><code class="language-bash"># Analyze with custom tier configuration
debtmap analyze . --config .debtmap.toml
</code></pre>
<h3 id="tier-preset-configurations"><a class="header" href="#tier-preset-configurations">Tier Preset Configurations</a></h3>
<p>Debtmap provides three built-in tier presets for different project needs:</p>
<p><strong>Balanced (Default)</strong></p>
<pre><code class="language-toml">[tiers]
t2_complexity_threshold = 15
t2_dependency_threshold = 10
t3_complexity_threshold = 10
</code></pre>
<p>Suitable for most projects. Balances detection sensitivity with manageable issue counts.</p>
<p><strong>Strict</strong></p>
<pre><code class="language-toml">[tiers]
t2_complexity_threshold = 10
t2_dependency_threshold = 7
t3_complexity_threshold = 7
</code></pre>
<p>For high-quality codebases or teams with strict quality standards. Flags more items as requiring attention.</p>
<p><strong>Lenient</strong></p>
<pre><code class="language-toml">[tiers]
t2_complexity_threshold = 20
t2_dependency_threshold = 15
t3_complexity_threshold = 15
</code></pre>
<p>For legacy codebases or gradual technical debt reduction. Focuses on the most critical issues first.</p>
<p><strong>Programmatic Access</strong>: These presets are also available as methods when using Debtmap as a library:</p>
<ul>
<li><code>TierConfig::balanced()</code> - Equivalent to the balanced preset above</li>
<li><code>TierConfig::strict()</code> - Equivalent to the strict preset above</li>
<li><code>TierConfig::lenient()</code> - Equivalent to the lenient preset above</li>
</ul>
<p>These methods can be used in Rust code to configure tier settings programmatically without manual TOML configuration.</p>
<h3 id="customizing-tier-thresholds"><a class="header" href="#customizing-tier-thresholds">Customizing Tier Thresholds</a></h3>
<p>You can also create custom threshold configurations tailored to your project:</p>
<pre><code class="language-toml"># Custom thresholds for specific project needs
[tiers]
t2_complexity_threshold = 12
t2_dependency_threshold = 8
t3_complexity_threshold = 8
</code></pre>
<h3 id="tier-weight-customization"><a class="header" href="#tier-weight-customization">Tier Weight Customization</a></h3>
<p>Tier weights are multipliers applied to base debt scores during prioritization. A weight of 1.5 means items in that tier will score 50% higher than equivalent items in a tier with weight 1.0, pushing them higher in priority rankings.</p>
<p>Adjust weights based on your priorities:</p>
<pre><code class="language-toml"># Emphasize testing over architecture
[tiers]
t1_weight = 1.2    # Reduce architecture weight
t2_weight = 1.3    # Increase testing weight
t3_weight = 0.8
t4_weight = 0.3

# Focus on architecture first
[tiers]
t1_weight = 2.0    # Maximize architecture weight
t2_weight = 1.0
t3_weight = 0.5
t4_weight = 0.2
</code></pre>
<h2 id="use-cases-7"><a class="header" href="#use-cases-7">Use Cases</a></h2>
<h3 id="sprint-planning"><a class="header" href="#sprint-planning">Sprint Planning</a></h3>
<p>Use tiered prioritization to allocate work:</p>
<pre><code class="language-bash"># See Tier 1 items for architectural planning
debtmap analyze . --min-priority high --top 5

# See Tier 2/3 for testing sprint work
debtmap analyze . --lcov coverage.lcov --min-priority medium
</code></pre>
<h3 id="code-review-focus"><a class="header" href="#code-review-focus">Code Review Focus</a></h3>
<p>Prioritize review attention based on tiers:</p>
<ul>
<li><strong>Tier 1</strong>: Architectural review required, senior dev attention</li>
<li><strong>Tier 2</strong>: Test coverage validation critical</li>
<li><strong>Tier 3</strong>: Standard review process</li>
<li><strong>Tier 4</strong>: Quick review or automated checks</li>
</ul>
<h3 id="refactoring-strategy"><a class="header" href="#refactoring-strategy">Refactoring Strategy</a></h3>
<pre><code class="language-bash"># Phase 1: Address Tier 1 architectural issues
debtmap analyze . --min-priority high

# Phase 2: Add tests for Tier 2 complex code
debtmap analyze . --lcov coverage.lcov --min-priority high

# Phase 3: Improve Tier 3 coverage
debtmap analyze . --lcov coverage.lcov --min-priority medium
</code></pre>
<h2 id="best-practices-19"><a class="header" href="#best-practices-19">Best Practices</a></h2>
<ol>
<li><strong>Always address Tier 1 before feature work</strong> - Architectural issues compound</li>
<li><strong>Test Tier 2 items before refactoring</strong> - Avoid regressions</li>
<li><strong>Batch Tier 3 items</strong> - Address multiple in one sprint</li>
<li><strong>Defer Tier 4 items</strong> - Only fix during cleanup or when convenient</li>
<li><strong>Track tier distribution over time</strong> - Aim to reduce Tier 1/2 counts</li>
</ol>
<h2 id="interpreting-tier-output"><a class="header" href="#interpreting-tier-output">Interpreting Tier Output</a></h2>
<h3 id="terminal-output-1"><a class="header" href="#terminal-output-1">Terminal Output</a></h3>
<p>Terminal output displays items grouped by <strong>score-based tiers</strong>:</p>
<pre><code>TECHNICAL DEBT ANALYSIS - PRIORITY TIERS

Critical (score &gt;= 90)
  src/services.rs - God Object (score: 127.5)
  src/core/engine.rs - Circular dependency (score: 95.2)

High (score 70-89.9)
  src/processing/transform.rs:145 - UntestableComplexity (score: 85.0)
  src/api/handlers.rs - God Module (score: 78.3)
  ...

Moderate (score 50-69.9)
  src/utils/parser.rs:220 - TestingGap (score: 62.1)
  ...

Low (score &lt; 50)
  [Items with score &lt; 50 appear here]
</code></pre>
<p><strong>Note</strong>: The scores shown reflect tier weight multipliers applied during classification. Items classified as Tier 1 (Critical Architecture) receive a 1.5x weight boost, which often elevates them into the Critical or High score ranges.</p>
<h3 id="json-output-2"><a class="header" href="#json-output-2">JSON Output</a></h3>
<p>JSON output uses the same <strong>score-based priority</strong> levels as terminal output:</p>
<pre><code class="language-json">{
  "summary": {
    "score_distribution": {
      "critical": 2,
      "high": 5,
      "moderate": 12,
      "low": 45
    }
  },
  "items": [
    {
      "type": "File",
      "score": 127.5,
      "priority": "critical",
      "location": {
        "file": "src/services.rs"
      },
      "debt_type": "GodObject"
    },
    {
      "type": "Function",
      "score": 85.0,
      "priority": "high",
      "location": {
        "file": "src/processing/transform.rs",
        "line": 145,
        "function": "process_data"
      },
      "debt_type": "UntestableComplexity"
    }
  ]
}
</code></pre>
<p>The <code>priority</code> field is derived from the <code>score</code> field using these thresholds:</p>
<ul>
<li><code>critical</code>: score &gt;= 90.0</li>
<li><code>high</code>: score &gt;= 70.0</li>
<li><code>moderate</code>: score &gt;= 50.0</li>
<li><code>low</code>: score &lt; 50.0</li>
</ul>
<p><strong>Source</strong>: Priority tier thresholds defined in src/priority/mod.rs:478-484</p>
<p><strong>Note</strong>: While RecommendationTier (T1-T4) classifications exist internally for applying tier weights, they are not included in JSON output. The output shows final calculated scores and their corresponding priority levels.</p>
<h2 id="troubleshooting-20"><a class="header" href="#troubleshooting-20">Troubleshooting</a></h2>
<p><strong>Issue</strong>: Too many Tier 1 items</p>
<p><strong>Solution</strong>: Lower tier weights or increase thresholds temporarily:</p>
<pre><code class="language-toml">[tiers]
t1_weight = 1.2    # Reduce from 1.5
</code></pre>
<p><strong>Issue</strong>: Not enough items in Tier 1</p>
<p><strong>Solution</strong>: Check if god object detection is enabled:</p>
<pre><code class="language-toml">[god_object_detection]
enabled = true
</code></pre>
<p><strong>Issue</strong>: All items in Tier 4</p>
<p><strong>Solution</strong>: Lower minimum thresholds:</p>
<pre><code class="language-toml">[thresholds]
minimum_debt_score = 1.0
minimum_cyclomatic_complexity = 2
</code></pre>
<h2 id="see-also-10"><a class="header" href="#see-also-10">See Also</a></h2>
<ul>
<li><a href="#scoring-strategies">Scoring Strategies</a> - Understanding file-level vs function-level scoring</li>
<li><a href="#configuration-2">Configuration</a> - Complete configuration reference</li>
<li><a href="#analysis-guide">Analysis Guide</a> - Detailed metric explanations and analysis techniques</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="validation-and-quality-gates"><a class="header" href="#validation-and-quality-gates">Validation and Quality Gates</a></h1>
<p>The <code>validate</code> command enforces quality gates in your development workflow, making it ideal for CI/CD integration. Unlike the <code>analyze</code> command which focuses on exploration and reporting, <code>validate</code> checks your codebase against configured thresholds and returns appropriate exit codes for automated workflows.</p>
<h2 id="table-of-contents"><a class="header" href="#table-of-contents">Table of Contents</a></h2>
<ul>
<li><a href="#validate-vs-analyze">Validate vs Analyze</a></li>
<li><a href="#quick-start-4">Quick Start</a></li>
<li><a href="#understanding-density-based-validation">Understanding Density-Based Validation</a></li>
<li><a href="#configuration-setup">Configuration Setup</a></li>
<li><a href="#validation-metrics">Validation Metrics</a></li>
<li><a href="#exit-codes-and-ci-integration">Exit Codes and CI Integration</a></li>
<li><a href="#coverage-integration-4">Coverage Integration</a></li>
<li><a href="#context-aware-validation">Context-Aware Validation</a></li>
<li><a href="#cicd-examples">CI/CD Examples</a></li>
<li><a href="#migrating-from-deprecated-thresholds">Migrating from Deprecated Thresholds</a></li>
<li><a href="#troubleshooting-21">Troubleshooting</a></li>
<li><a href="#best-practices-20">Best Practices</a></li>
</ul>
<h2 id="validate-vs-analyze"><a class="header" href="#validate-vs-analyze">Validate vs Analyze</a></h2>
<p>Understanding when to use each command is crucial:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Aspect</th><th><code>validate</code></th><th><code>analyze</code></th></tr>
</thead>
<tbody>
<tr><td><strong>Purpose</strong></td><td>Enforce quality gates</td><td>Explore and understand debt</td></tr>
<tr><td><strong>Exit Codes</strong></td><td>Returns non-zero on failure</td><td>Always returns 0 (unless error)</td></tr>
<tr><td><strong>Thresholds</strong></td><td>From <code>.debtmap.toml</code> config</td><td>Command-line flags</td></tr>
<tr><td><strong>Use Case</strong></td><td>CI/CD pipelines, pre-commit hooks</td><td>Interactive analysis, reports</td></tr>
<tr><td><strong>Output Focus</strong></td><td>Pass/fail with violation details</td><td>Comprehensive metrics and insights</td></tr>
<tr><td><strong>Configuration</strong></td><td>Requires <code>.debtmap.toml</code></td><td>Works without config file</td></tr>
</tbody>
</table>
</div>
<p><strong>Rule of thumb:</strong> Use <code>validate</code> for automation and <code>analyze</code> for investigation.</p>
<h2 id="quick-start-4"><a class="header" href="#quick-start-4">Quick Start</a></h2>
<ol>
<li>
<p><strong>Initialize configuration:</strong></p>
<pre><code class="language-bash">debtmap init
</code></pre>
</li>
<li>
<p><strong>Edit <code>.debtmap.toml</code> to set thresholds:</strong></p>
<pre><code class="language-toml">[thresholds.validation]
max_debt_density = 50.0              # Debt items per 1000 LOC
max_average_complexity = 10.0        # Average cyclomatic complexity
max_codebase_risk_score = 7.0        # Overall risk level (1-10)
</code></pre>
</li>
<li>
<p><strong>Run validation:</strong></p>
<pre><code class="language-bash">debtmap validate .
</code></pre>
</li>
<li>
<p><strong>Check exit code:</strong></p>
<pre><code class="language-bash">echo $?  # 0 = pass, non-zero = fail
</code></pre>
</li>
</ol>
<h2 id="understanding-density-based-validation"><a class="header" href="#understanding-density-based-validation">Understanding Density-Based Validation</a></h2>
<p>Debtmap uses <strong>density-based metrics</strong> as the primary quality measure. This approach provides several advantages over traditional absolute count metrics.</p>
<h3 id="why-density-matters"><a class="header" href="#why-density-matters">Why Density Matters</a></h3>
<p>Traditional metrics like “maximum 50 high-complexity functions” fail as your codebase grows:</p>
<pre><code>Scenario: Your team adds 10,000 LOC of high-quality code
- Old metric: "max 50 complex functions" → FAILS (now 55 total)
- Density metric: "max 50 per 1000 LOC" → PASSES (density improved)
</code></pre>
<p><strong>Scale-dependent metrics</strong> (absolute counts):</p>
<ul>
<li>Grow linearly with codebase size</li>
<li>Require constant threshold adjustments</li>
<li>Punish healthy growth</li>
<li>Don’t reflect actual code quality</li>
</ul>
<p><strong>Density metrics</strong> (per 1000 LOC):</p>
<ul>
<li>Remain stable as codebase grows</li>
<li>Measure true quality ratios</li>
<li>No adjustment needed for growth</li>
<li>Directly comparable across projects</li>
</ul>
<h3 id="calculating-debt-density"><a class="header" href="#calculating-debt-density">Calculating Debt Density</a></h3>
<pre><code>Debt Density = (Total Debt Items / Total LOC) × 1000
</code></pre>
<p><strong>Example:</strong></p>
<ul>
<li>25 debt items in 5,000 LOC project</li>
<li>Density = (25 / 5000) × 1000 = <strong>5.0 debt items per 1000 LOC</strong></li>
</ul>
<p>This density remains meaningful whether your codebase is 5,000 or 500,000 LOC.</p>
<h3 id="recommended-density-thresholds"><a class="header" href="#recommended-density-thresholds">Recommended Density Thresholds</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Project Type</th><th>max_debt_density</th><th>Rationale</th></tr>
</thead>
<tbody>
<tr><td><strong>New/Greenfield</strong></td><td>20.0</td><td>High quality bar for new code</td></tr>
<tr><td><strong>Active Development</strong></td><td>50.0</td><td>Balanced quality/velocity (default)</td></tr>
<tr><td><strong>Legacy Modernization</strong></td><td>100.0</td><td>Prevent regression during refactoring</td></tr>
<tr><td><strong>Mature/Critical</strong></td><td>30.0</td><td>Maintain quality in stable systems</td></tr>
</tbody>
</table>
</div>
<h2 id="configuration-setup"><a class="header" href="#configuration-setup">Configuration Setup</a></h2>
<h3 id="creating-configuration-file"><a class="header" href="#creating-configuration-file">Creating Configuration File</a></h3>
<p>The <code>debtmap init</code> command generates a <code>.debtmap.toml</code> with sensible defaults:</p>
<pre><code class="language-bash">debtmap init
</code></pre>
<p>This creates:</p>
<pre><code class="language-toml">[thresholds.validation]
# Primary quality metrics (scale-independent)
max_average_complexity = 10.0
max_debt_density = 50.0
max_codebase_risk_score = 7.0

# Optional metrics
min_coverage_percentage = 0.0  # Disabled by default

# Safety net (high ceiling for extreme cases)
max_total_debt_score = 10000
</code></pre>
<h3 id="editing-thresholds"><a class="header" href="#editing-thresholds">Editing Thresholds</a></h3>
<p>Edit the <code>[thresholds.validation]</code> section to match your quality requirements:</p>
<pre><code class="language-toml">[thresholds.validation]
# Enforce stricter quality for new project
max_debt_density = 30.0              # Tighter density requirement
max_average_complexity = 8.0         # Lower complexity tolerance
max_codebase_risk_score = 6.0        # Reduced risk threshold
min_coverage_percentage = 80.0       # Require 80% test coverage
</code></pre>
<h3 id="override-via-command-line"><a class="header" href="#override-via-command-line">Override via Command Line</a></h3>
<p>You can override the density threshold from the command line:</p>
<pre><code class="language-bash"># Temporarily use stricter threshold
debtmap validate . --max-debt-density 40.0
</code></pre>
<h2 id="validation-metrics"><a class="header" href="#validation-metrics">Validation Metrics</a></h2>
<p>Debtmap organizes validation metrics into three categories:</p>
<h3 id="primary-metrics-scale-independent"><a class="header" href="#primary-metrics-scale-independent">Primary Metrics (Scale-Independent)</a></h3>
<p>These are the core quality measures that every project should monitor:</p>
<ol>
<li>
<p><strong><code>max_average_complexity</code></strong> (default: 10.0)</p>
<ul>
<li>Average cyclomatic complexity per function</li>
<li>Measures typical function complexity across codebase</li>
<li>Lower values indicate simpler, more maintainable code</li>
</ul>
<pre><code class="language-toml">max_average_complexity = 10.0
</code></pre>
</li>
<li>
<p><strong><code>max_debt_density</code></strong> (default: 50.0) - <strong>PRIMARY METRIC</strong></p>
<ul>
<li>Debt items per 1000 lines of code</li>
<li>Scale-independent quality measure</li>
<li>Remains stable as codebase grows</li>
</ul>
<pre><code class="language-toml">max_debt_density = 50.0
</code></pre>
</li>
<li>
<p><strong><code>max_codebase_risk_score</code></strong> (default: 7.0)</p>
<ul>
<li>Overall risk level combining complexity, coverage, and criticality</li>
<li>Score ranges from 1 (low risk) to 10 (high risk)</li>
<li>Considers context-aware analysis when enabled</li>
</ul>
<pre><code class="language-toml">max_codebase_risk_score = 7.0
</code></pre>
</li>
</ol>
<h3 id="optional-metrics-1"><a class="header" href="#optional-metrics-1">Optional Metrics</a></h3>
<p>Configure these when you want additional quality enforcement:</p>
<ol start="4">
<li>
<p><strong><code>min_coverage_percentage</code></strong> (default: 0.0 - disabled)</p>
<ul>
<li>Minimum required test coverage percentage</li>
<li>Only enforced when coverage data is provided via <code>--coverage-file</code></li>
<li>Set to 0.0 to disable coverage requirements</li>
</ul>
<pre><code class="language-toml">min_coverage_percentage = 75.0  # Require 75% coverage
</code></pre>
</li>
</ol>
<h3 id="safety-net-metrics"><a class="header" href="#safety-net-metrics">Safety Net Metrics</a></h3>
<p>High ceilings to catch extreme cases:</p>
<ol start="5">
<li>
<p><strong><code>max_total_debt_score</code></strong> (default: 10000)</p>
<ul>
<li>Absolute ceiling on total technical debt</li>
<li>Prevents runaway growth even if density stays low</li>
<li>Rarely triggers in normal operation</li>
</ul>
<pre><code class="language-toml">max_total_debt_score = 10000
</code></pre>
</li>
</ol>
<h3 id="metric-priority"><a class="header" href="#metric-priority">Metric Priority</a></h3>
<p><strong>Validation uses AND logic:</strong> All primary metrics must pass for validation to succeed. If any check fails, the entire validation fails with a non-zero exit code.</p>
<p>When validation fails, fix issues in this order:</p>
<ol>
<li><strong>Critical:</strong> <code>max_debt_density</code> violations (core quality metric)</li>
<li><strong>High:</strong> <code>max_average_complexity</code> violations (function-level quality)</li>
<li><strong>High:</strong> <code>max_codebase_risk_score</code> violations (overall risk)</li>
<li><strong>Medium:</strong> <code>min_coverage_percentage</code> violations (test coverage)</li>
<li><strong>Low:</strong> <code>max_total_debt_score</code> violations (extreme cases only)</li>
</ol>
<p>The priority list above is for remediation order when validation fails, not for which checks are enforced. All configured thresholds are enforced equally.</p>
<h2 id="exit-codes-and-ci-integration"><a class="header" href="#exit-codes-and-ci-integration">Exit Codes and CI Integration</a></h2>
<p>The <code>validate</code> command uses exit codes to signal success or failure:</p>
<h3 id="exit-code-behavior"><a class="header" href="#exit-code-behavior">Exit Code Behavior</a></h3>
<pre><code class="language-bash">debtmap validate .
echo $?
</code></pre>
<p><strong>Exit codes:</strong></p>
<ul>
<li><strong><code>0</code></strong> - Success: All thresholds passed</li>
<li><strong>Non-zero</strong> - Failure: One or more thresholds exceeded or errors occurred</li>
</ul>
<p><strong>Implementation detail:</strong> When validation fails, the command uses <code>anyhow::bail!("Validation failed")</code> to return a non-zero exit code. This ensures the failure propagates correctly to CI/CD systems (src/commands/validate.rs:166).</p>
<h3 id="using-exit-codes-in-ci"><a class="header" href="#using-exit-codes-in-ci">Using Exit Codes in CI</a></h3>
<p>Exit codes integrate naturally with CI/CD systems:</p>
<p><strong>GitHub Actions:</strong></p>
<pre><code class="language-yaml">- name: Validate code quality
  run: debtmap validate .
  # Step fails automatically if exit code is non-zero
</code></pre>
<p><strong>GitLab CI:</strong></p>
<pre><code class="language-yaml">script:
  - debtmap validate .
  # Pipeline fails if exit code is non-zero
</code></pre>
<p><strong>Shell scripts:</strong></p>
<pre><code class="language-bash">#!/bin/bash
if debtmap validate .; then
    echo "✅ Validation passed"
else
    echo "❌ Validation failed"
    exit 1
fi
</code></pre>
<h3 id="understanding-validation-output"><a class="header" href="#understanding-validation-output">Understanding Validation Output</a></h3>
<p>The validate command uses the actual validation output format from src/utils/validation_printer.rs.</p>
<p><strong>Success output:</strong></p>
<pre><code>[OK] Validation PASSED

  Primary Quality Metrics:
    Debt Density: 32.5 per 1K LOC (threshold: 50.0)
       └─ Using 65% of max density (35% headroom)
    Average complexity: 7.2 (threshold: 10.0)
    Codebase risk score: 5.8 (threshold: 7.0)

  Codebase Statistics (informational):
    High complexity functions: 8
    Technical debt items: 42
    Total debt score: 1250 (safety net threshold: 10000)

  All validation checks passed
</code></pre>
<p><strong>Failure output:</strong></p>
<pre><code>[ERROR] Validation FAILED - Some metrics exceed thresholds

  Primary Quality Metrics:
    Debt Density: 65.8 per 1K LOC (threshold: 50.0)
       └─ Using 132% of max density (-32% headroom)
    Average complexity: 12.3 (threshold: 10.0)
    Codebase risk score: 5.2 (threshold: 7.0)

  Codebase Statistics (informational):
    High complexity functions: 25
    Technical debt items: 87
    Total debt score: 2100 (safety net threshold: 10000)

  Failed checks:
    [ERROR] Average complexity: 12.3 &gt; 10.0
    [ERROR] Debt density: 65.8 per 1K LOC &gt; 50.0
</code></pre>
<p>The output format emphasizes:</p>
<ul>
<li><strong>Debt Density as the primary metric</strong> - shown first with percentage usage</li>
<li><strong>Headroom visualization</strong> - shows how much threshold capacity remains</li>
<li><strong>Clear failure indicators</strong> - <code>[ERROR]</code> prefix for failed checks</li>
<li><strong>Informational statistics</strong> - absolute counts shown as context, not validation criteria</li>
</ul>
<h2 id="coverage-integration-4"><a class="header" href="#coverage-integration-4">Coverage Integration</a></h2>
<p>Integrate test coverage data to enable risk-based validation:</p>
<h3 id="generating-coverage-data-2"><a class="header" href="#generating-coverage-data-2">Generating Coverage Data</a></h3>
<p><strong>For Rust projects with <code>cargo-tarpaulin</code>:</strong></p>
<pre><code class="language-bash">cargo tarpaulin --out Lcov --output-dir target/coverage
</code></pre>
<p><strong>For Python projects with <code>pytest-cov</code>:</strong></p>
<pre><code class="language-bash">pytest --cov --cov-report=lcov:coverage/lcov.info
</code></pre>
<p><strong>For JavaScript projects with Jest:</strong></p>
<pre><code class="language-bash">jest --coverage --coverageReporters=lcov
</code></pre>
<h3 id="running-validation-with-coverage"><a class="header" href="#running-validation-with-coverage">Running Validation with Coverage</a></h3>
<pre><code class="language-bash">debtmap validate . --coverage-file target/coverage/lcov.info
</code></pre>
<h3 id="benefits-of-coverage-integration"><a class="header" href="#benefits-of-coverage-integration">Benefits of Coverage Integration</a></h3>
<p>With coverage data, validation gains additional insights:</p>
<ol>
<li><strong>Risk-based prioritization</strong> - Identifies untested complex code</li>
<li><strong>Coverage threshold enforcement</strong> - via <code>min_coverage_percentage</code></li>
<li><strong>Enhanced risk scoring</strong> - Combines complexity + coverage + context</li>
<li><strong>Better failure diagnostics</strong> - Shows which untested areas need attention</li>
</ol>
<h3 id="coverage-enhanced-output"><a class="header" href="#coverage-enhanced-output">Coverage-Enhanced Output</a></h3>
<pre><code class="language-bash">debtmap validate . --coverage-file coverage/lcov.info -vv
</code></pre>
<p>Output includes:</p>
<ul>
<li>Overall coverage percentage</li>
<li>High-risk uncovered functions</li>
<li>Coverage-adjusted risk scores</li>
<li>Prioritized remediation recommendations</li>
</ul>
<h2 id="context-aware-validation"><a class="header" href="#context-aware-validation">Context-Aware Validation</a></h2>
<p>Enable context-aware analysis for deeper risk insights:</p>
<h3 id="available-context-providers"><a class="header" href="#available-context-providers">Available Context Providers</a></h3>
<ol>
<li><strong><code>critical_path</code></strong> - Analyzes call graph to find execution bottlenecks</li>
<li><strong><code>dependency</code></strong> - Identifies highly-coupled modules</li>
<li><strong><code>git_history</code></strong> - Detects frequently-changed code (churn)</li>
</ol>
<h3 id="enabling-context-providers-1"><a class="header" href="#enabling-context-providers-1">Enabling Context Providers</a></h3>
<p><strong>Enable all providers:</strong></p>
<pre><code class="language-bash">debtmap validate . --enable-context
</code></pre>
<p><strong>Select specific providers:</strong></p>
<pre><code class="language-bash">debtmap validate . --enable-context --context-providers critical_path,git_history
</code></pre>
<p><strong>Disable specific providers:</strong></p>
<pre><code class="language-bash">debtmap validate . --enable-context --disable-context dependency
</code></pre>
<h3 id="context-aware-configuration"><a class="header" href="#context-aware-configuration">Context-Aware Configuration</a></h3>
<p>Add context settings to <code>.debtmap.toml</code>:</p>
<pre><code class="language-toml">[analysis]
enable_context = true
context_providers = ["critical_path", "git_history"]
</code></pre>
<p>Then run validation:</p>
<pre><code class="language-bash">debtmap validate .  # Uses config settings
</code></pre>
<h3 id="context-benefits-for-validation"><a class="header" href="#context-benefits-for-validation">Context Benefits for Validation</a></h3>
<p>Context-aware analysis improves risk scoring by:</p>
<ul>
<li>Prioritizing frequently-called functions</li>
<li>Weighting high-churn code more heavily</li>
<li>Identifying architectural bottlenecks</li>
<li>Surfacing critical code paths</li>
</ul>
<h2 id="cicd-examples"><a class="header" href="#cicd-examples">CI/CD Examples</a></h2>
<h3 id="github-actions-2"><a class="header" href="#github-actions-2">GitHub Actions</a></h3>
<p>Complete workflow with coverage generation and validation:</p>
<pre><code class="language-yaml">name: Code Quality Validation

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]

env:
  CARGO_TERM_COLOR: always
  RUST_BACKTRACE: 1

jobs:
  validate:
    name: Technical Debt Validation
    runs-on: ubuntu-latest

    steps:
    - name: Checkout repository
      uses: actions/checkout@v5
      with:
        fetch-depth: 0  # Full history for git context

    - name: Setup Rust
      uses: dtolnay/rust-toolchain@stable
      with:
        components: rustfmt, clippy

    - name: Cache cargo dependencies
      uses: actions/cache@v4
      with:
        path: |
          ~/.cargo/bin/
          ~/.cargo/registry/index/
          ~/.cargo/registry/cache/
          ~/.cargo/git/db/
          target/
        key: ${{ runner.os }}-cargo-${{ hashFiles('**/Cargo.lock') }}
        restore-keys: |
          ${{ runner.os }}-cargo-

    - name: Install cargo-tarpaulin
      run: |
        if ! command -v cargo-tarpaulin &amp;&gt; /dev/null; then
          cargo install cargo-tarpaulin
        fi

    - name: Build debtmap
      run: cargo build --release

    - name: Generate coverage data
      run: cargo tarpaulin --out Lcov --output-dir target/coverage --timeout 300

    - name: Run debtmap validation with coverage
      run: |
        if [ -f "target/coverage/lcov.info" ]; then
          ./target/release/debtmap validate . \
            --coverage-file target/coverage/lcov.info \
            --enable-context \
            --format json \
            --output debtmap-report.json
        else
          echo "Warning: LCOV file not found, running without coverage"
          ./target/release/debtmap validate . \
            --format json \
            --output debtmap-report.json
        fi

    - name: Upload debtmap report
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: debtmap-analysis-artifacts
        path: |
          debtmap-report.json
          target/coverage/lcov.info
        retention-days: 7
</code></pre>
<h3 id="gitlab-ci-2"><a class="header" href="#gitlab-ci-2">GitLab CI</a></h3>
<pre><code class="language-yaml">stages:
  - test
  - quality

variables:
  CARGO_HOME: $CI_PROJECT_DIR/.cargo

debtmap:
  stage: quality
  image: rust:latest

  cache:
    paths:
      - .cargo/
      - target/

  before_script:
    # Install debtmap and coverage tools
    - cargo install debtmap
    - cargo install cargo-tarpaulin

  script:
    # Generate coverage
    - cargo tarpaulin --out Lcov --output-dir coverage

    # Validate with debtmap
    - debtmap validate . --coverage-file coverage/lcov.info -v

  artifacts:
    when: always
    paths:
      - coverage/
    reports:
      coverage_report:
        coverage_format: cobertura
        path: coverage/cobertura.xml
</code></pre>
<h3 id="circleci"><a class="header" href="#circleci">CircleCI</a></h3>
<pre><code class="language-yaml">version: 2.1

jobs:
  validate:
    docker:
      - image: cimg/rust:1.75

    steps:
      - checkout

      - restore_cache:
          keys:
            - cargo-{{ checksum "Cargo.lock" }}

      - run:
          name: Install tools
          command: |
            cargo install debtmap
            cargo install cargo-tarpaulin

      - run:
          name: Generate coverage
          command: cargo tarpaulin --out Lcov

      - run:
          name: Validate code quality
          command: debtmap validate . --coverage-file lcov.info

      - save_cache:
          key: cargo-{{ checksum "Cargo.lock" }}
          paths:
            - ~/.cargo
            - target

workflows:
  version: 2
  quality:
    jobs:
      - validate
</code></pre>
<h2 id="migrating-from-deprecated-thresholds"><a class="header" href="#migrating-from-deprecated-thresholds">Migrating from Deprecated Thresholds</a></h2>
<p>Debtmap version 0.3.0 deprecated scale-dependent absolute count metrics in favor of density-based metrics.</p>
<h3 id="deprecated-metrics"><a class="header" href="#deprecated-metrics">Deprecated Metrics</a></h3>
<p>The following metrics will be <strong>removed in v1.0</strong>:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Deprecated Metric</th><th>Migration Path</th></tr>
</thead>
<tbody>
<tr><td><code>max_high_complexity_count</code></td><td>Use <code>max_debt_density</code></td></tr>
<tr><td><code>max_debt_items</code></td><td>Use <code>max_debt_density</code></td></tr>
<tr><td><code>max_high_risk_functions</code></td><td>Use <code>max_debt_density</code> + <code>max_codebase_risk_score</code></td></tr>
</tbody>
</table>
</div>
<h3 id="migration-example"><a class="header" href="#migration-example">Migration Example</a></h3>
<p><strong>Old configuration (deprecated):</strong></p>
<pre><code class="language-toml">[thresholds.validation]
max_high_complexity_count = 50    # ❌ Scale-dependent
max_debt_items = 100               # ❌ Scale-dependent
max_high_risk_functions = 20       # ❌ Scale-dependent
</code></pre>
<p><strong>New configuration (recommended):</strong></p>
<pre><code class="language-toml">[thresholds.validation]
max_debt_density = 50.0            # ✅ Scale-independent
max_average_complexity = 10.0      # ✅ Quality ratio
max_codebase_risk_score = 7.0      # ✅ Risk level
</code></pre>
<h3 id="calculating-equivalent-density-threshold"><a class="header" href="#calculating-equivalent-density-threshold">Calculating Equivalent Density Threshold</a></h3>
<p>Convert your old absolute thresholds to density:</p>
<pre><code>Old: max_debt_items = 100 in 10,000 LOC codebase
New: max_debt_density = (100 / 10000) × 1000 = 10.0
</code></pre>
<h3 id="deprecation-warnings"><a class="header" href="#deprecation-warnings">Deprecation Warnings</a></h3>
<p>When you run <code>validate</code> with deprecated metrics, you’ll see:</p>
<pre><code>⚠️  DEPRECATION WARNING:
   The following validation thresholds are deprecated:
   - max_high_complexity_count
   - max_debt_items

   These scale-dependent metrics will be removed in v1.0.
   Please migrate to density-based validation:
     - Use 'max_debt_density' instead of absolute counts
     - Density metrics remain stable as your codebase grows
</code></pre>
<h3 id="migration-timeline"><a class="header" href="#migration-timeline">Migration Timeline</a></h3>
<ul>
<li><strong>v0.3.0</strong> - Density metrics introduced, old metrics deprecated</li>
<li><strong>v0.4.0 - v0.9.x</strong> - Deprecation warnings shown</li>
<li><strong>v1.0.0</strong> - Deprecated metrics removed</li>
</ul>
<h2 id="troubleshooting-21"><a class="header" href="#troubleshooting-21">Troubleshooting</a></h2>
<h3 id="debugging-validation-failures"><a class="header" href="#debugging-validation-failures">Debugging Validation Failures</a></h3>
<p>Use verbosity flags to understand why validation failed:</p>
<p><strong>Level 1: Basic details (<code>-v</code>)</strong></p>
<pre><code class="language-bash">debtmap validate . -v
</code></pre>
<p>Shows which thresholds failed, by how much, and timing breakdown:</p>
<ul>
<li>Call graph building time</li>
<li>Trait resolution time</li>
<li>Coverage loading time</li>
<li>Individual analysis phase durations</li>
</ul>
<p><strong>Suppressing timing output:</strong></p>
<pre><code class="language-bash"># Set DEBTMAP_QUIET to disable timing information (src/commands/validate.rs:406)
DEBTMAP_QUIET=1 debtmap validate .
</code></pre>
<p><strong>Level 2: Detailed breakdown (<code>-vv</code>)</strong></p>
<pre><code class="language-bash">debtmap validate . -vv
</code></pre>
<p>Shows everything from <code>-v</code> plus:</p>
<ul>
<li>Score calculation factors and weights</li>
<li>Top violating functions with metrics</li>
<li>Detailed phase timing information</li>
<li>Risk score component breakdown</li>
</ul>
<p><strong>Level 3: Full diagnostic output (<code>-vvv</code>)</strong></p>
<pre><code class="language-bash">debtmap validate . -vvv
</code></pre>
<p>Shows complete debug information:</p>
<ul>
<li>All debt items with full details</li>
<li>Complete risk calculations for each function</li>
<li>All timing information including sub-phases</li>
<li>File-level and function-level analysis data</li>
<li>Context provider outputs (if enabled)</li>
</ul>
<h3 id="common-issues-2"><a class="header" href="#common-issues-2">Common Issues</a></h3>
<p><strong>Issue: Validation fails but output unclear</strong></p>
<pre><code class="language-bash"># Solution: Increase verbosity
debtmap validate . -vv
</code></pre>
<p><strong>Issue: Want to see only the worst problems</strong></p>
<pre><code class="language-bash"># Solution: Use --top flag
debtmap validate . --top 10 -v
</code></pre>
<p><strong>Issue: Output is too verbose for CI logs</strong></p>
<pre><code class="language-bash"># Solution: Use lower verbosity or filter output
debtmap validate . --top 10   # Show only top 10 issues
# or reduce output detail
debtmap validate .            # Default verbosity (clean output)
</code></pre>
<p>At default verbosity (0), validation output is already concise and CI-friendly, showing only metrics and pass/fail status.</p>
<p><strong>Issue: Validation passes locally but fails in CI</strong></p>
<pre><code class="language-bash"># Possible causes:
# 1. Different code (stale local branch)
# 2. Different config file (check .debtmap.toml in CI)
# 3. Missing coverage data (check LCOV generation in CI)

# Debug in CI:
debtmap validate . -vvv  # Maximum verbosity
</code></pre>
<p><strong>Issue: Coverage threshold fails unexpectedly</strong></p>
<pre><code class="language-bash"># Check if coverage file is being read
debtmap validate . --coverage-file coverage/lcov.info -v

# Verify coverage file exists and is valid
ls -lh coverage/lcov.info
</code></pre>
<p><strong>Issue: Context providers causing performance issues</strong></p>
<pre><code class="language-bash"># Disable expensive providers
debtmap validate . --enable-context --disable-context git_history
</code></pre>
<p><strong>Issue: Semantic analysis causing errors or unexpected behavior</strong></p>
<p>Debtmap uses semantic analysis by default, powered by tree-sitter for deep AST (Abstract Syntax Tree) analysis. This provides accurate understanding of code structure, control flow, and complexity patterns.</p>
<p>However, semantic analysis may encounter issues with:</p>
<ul>
<li>Unsupported or experimental language features</li>
<li>Malformed or incomplete syntax</li>
<li>Complex macro expansions</li>
<li>Very large files that timeout during parsing</li>
</ul>
<pre><code class="language-bash"># Solution: Disable semantic analysis with fallback mode
debtmap validate . --semantic-off
</code></pre>
<p>When semantic analysis is disabled with <code>--semantic-off</code>, debtmap falls back to basic syntax analysis, which is faster but less accurate for complexity calculations. Use this flag if:</p>
<ul>
<li>Encountering parsing errors or timeouts</li>
<li>Working with bleeding-edge language features</li>
<li>Need faster validation at the cost of precision</li>
</ul>
<h3 id="validation-report-generation"><a class="header" href="#validation-report-generation">Validation Report Generation</a></h3>
<p>Generate detailed reports for debugging:</p>
<p><strong>JSON format for programmatic analysis:</strong></p>
<pre><code class="language-bash">debtmap validate . --format json --output validation-report.json
cat validation-report.json | jq '.validation_details'
</code></pre>
<p><strong>Markdown format for documentation:</strong></p>
<pre><code class="language-bash">debtmap validate . --format markdown --output validation-report.md
</code></pre>
<p><strong>Terminal format with filtering:</strong></p>
<pre><code class="language-bash">debtmap validate . --format terminal --top 20 -vv
</code></pre>
<h2 id="best-practices-20"><a class="header" href="#best-practices-20">Best Practices</a></h2>
<h3 id="setting-initial-thresholds"><a class="header" href="#setting-initial-thresholds">Setting Initial Thresholds</a></h3>
<p><strong>1. Establish baseline:</strong></p>
<pre><code class="language-bash"># Run analysis to see current metrics
debtmap analyze . --format json &gt; baseline.json
cat baseline.json | jq '.unified_analysis.debt_density'
</code></pre>
<p><strong>2. Set pragmatic thresholds:</strong></p>
<pre><code class="language-toml">[thresholds.validation]
# Start slightly above current values to prevent regression
max_debt_density = 60.0  # Current: 55.0
max_average_complexity = 12.0  # Current: 10.5
</code></pre>
<p><strong>3. Gradually tighten:</strong></p>
<pre><code class="language-toml"># After 1 month of cleanup
max_debt_density = 50.0
max_average_complexity = 10.0
</code></pre>
<h3 id="progressive-threshold-tightening"><a class="header" href="#progressive-threshold-tightening">Progressive Threshold Tightening</a></h3>
<p><strong>Month 1-2: Prevent regression</strong></p>
<pre><code class="language-toml">max_debt_density = 60.0  # Above current baseline
</code></pre>
<p><strong>Month 3-4: Incremental improvement</strong></p>
<pre><code class="language-toml">max_debt_density = 50.0  # Industry standard
</code></pre>
<p><strong>Month 5-6: Quality leadership</strong></p>
<pre><code class="language-toml">max_debt_density = 30.0  # Best-in-class
</code></pre>
<h3 id="project-specific-recommendations"><a class="header" href="#project-specific-recommendations">Project-Specific Recommendations</a></h3>
<p><strong>Greenfield projects:</strong></p>
<pre><code class="language-toml"># Start with high quality bar
max_debt_density = 20.0
max_average_complexity = 8.0
min_coverage_percentage = 80.0
</code></pre>
<p><strong>Active development:</strong></p>
<pre><code class="language-toml"># Balanced quality/velocity
max_debt_density = 50.0
max_average_complexity = 10.0
min_coverage_percentage = 70.0
</code></pre>
<p><strong>Legacy modernization:</strong></p>
<pre><code class="language-toml"># Prevent regression during refactoring
max_debt_density = 100.0
max_average_complexity = 15.0
min_coverage_percentage = 50.0
</code></pre>
<h3 id="pre-commit-hook-integration"><a class="header" href="#pre-commit-hook-integration">Pre-Commit Hook Integration</a></h3>
<p>Add validation as a pre-commit hook:</p>
<pre><code class="language-bash"># .git/hooks/pre-commit
#!/bin/bash
echo "Running debtmap validation..."
if debtmap validate . -v; then
    echo "✅ Validation passed"
    exit 0
else
    echo "❌ Validation failed - commit blocked"
    exit 1
fi
</code></pre>
<p>Make it executable:</p>
<pre><code class="language-bash">chmod +x .git/hooks/pre-commit
</code></pre>
<h3 id="performance-optimization-1"><a class="header" href="#performance-optimization-1">Performance Optimization</a></h3>
<p><strong>Enable parallel processing:</strong>
Validation uses parallel processing by default for fast execution on multi-core systems.</p>
<p><strong>Disable for resource-constrained environments:</strong></p>
<pre><code class="language-bash"># Limit parallelism
debtmap validate . --jobs 2

# Disable completely
debtmap validate . --no-parallel
</code></pre>
<p><strong>Performance characteristics:</strong></p>
<ul>
<li>Parallel call graph construction</li>
<li>Multi-threaded file analysis</li>
<li>Same performance as <code>analyze</code> command</li>
</ul>
<h3 id="monitoring-trends"><a class="header" href="#monitoring-trends">Monitoring Trends</a></h3>
<p>Track validation metrics over time:</p>
<pre><code class="language-bash"># Generate timestamped reports
debtmap validate . --format json --output "reports/validation-$(date +%Y%m%d).json"

# Compare trends
jq -s 'map(.unified_analysis.debt_density)' reports/validation-*.json
</code></pre>
<h3 id="documentation"><a class="header" href="#documentation">Documentation</a></h3>
<p>Document your threshold decisions:</p>
<pre><code class="language-toml"># .debtmap.toml
[thresholds.validation]
# Rationale: Team agreed 50.0 density balances quality and velocity
# Review: Quarterly (next: 2025-04-01)
max_debt_density = 50.0

# Rationale: Enforces single-responsibility principle
# Review: After 3 months of metrics
max_average_complexity = 10.0
</code></pre>
<h2 id="summary-10"><a class="header" href="#summary-10">Summary</a></h2>
<p>The <code>validate</code> command provides automated quality gates for CI/CD integration:</p>
<ul>
<li><strong>Use density-based metrics</strong> for scale-independent quality measurement</li>
<li><strong>Configure in <code>.debtmap.toml</code></strong> for consistent, version-controlled thresholds</li>
<li><strong>Integrate with CI/CD</strong> using exit codes for automated enforcement</li>
<li><strong>Enable coverage and context</strong> for risk-based validation</li>
<li><strong>Migrate from deprecated metrics</strong> to density-based approach</li>
<li><strong>Debug with verbosity flags</strong> when validation fails unexpectedly</li>
<li><strong>Tighten thresholds progressively</strong> as code quality improves</li>
</ul>
<p>Next steps:</p>
<ul>
<li>Review <a href="#configuration-2">Configuration Reference</a> for detailed threshold options</li>
<li>See <a href="#examples-5">Examples</a> for more CI/CD integration patterns</li>
<li>Check <a href="#cli-reference">CLI Reference</a> for complete command documentation</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="metrics-reference"><a class="header" href="#metrics-reference">Metrics Reference</a></h1>
<p>Comprehensive guide to all signals measured by Debtmap and how to interpret them.</p>
<p>Debtmap acts as a <strong>sensor</strong>, providing quantified signals about code complexity and risk. These signals are designed for consumption by AI coding tools and developers alike.</p>
<h2 id="metric-categories-spec-118"><a class="header" href="#metric-categories-spec-118">Metric Categories (Spec 118)</a></h2>
<p>Debtmap distinguishes between two fundamental categories of metrics:</p>
<h3 id="measured-metrics"><a class="header" href="#measured-metrics">Measured Metrics</a></h3>
<p><strong>Definition</strong>: Metrics directly computed from the Abstract Syntax Tree (AST) through precise analysis.</p>
<p>These metrics are:</p>
<ul>
<li><strong>Deterministic</strong>: Same code always produces the same metric value</li>
<li><strong>Precise</strong>: Exact counts from syntax analysis, not estimates</li>
<li><strong>Suitable for thresholds</strong>: Reliable for CI/CD quality gates</li>
<li><strong>Language-specific</strong>: Computed using language parsers (syn for Rust, tree-sitter for others)</li>
</ul>
<p><strong>Measured metrics include:</strong></p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Metric</th><th>Description</th><th>Example</th></tr>
</thead>
<tbody>
<tr><td><code>cyclomatic_complexity</code></td><td>Count of decision points (if, match, while, for, etc.)</td><td>Function with 3 if statements = complexity 4</td></tr>
<tr><td><code>cognitive_complexity</code></td><td>Weighted measure of code understandability</td><td>Nested loops increase cognitive load</td></tr>
<tr><td><code>nesting_depth</code></td><td>Maximum levels of nested control structures</td><td>3 nested if statements = depth 3</td></tr>
<tr><td><code>loc</code></td><td>Lines of code in the function</td><td>Physical line count</td></tr>
<tr><td><code>parameter_count</code></td><td>Number of function parameters</td><td><code>fn foo(a: i32, b: String)</code> = 2</td></tr>
</tbody>
</table>
</div>
<h3 id="estimated-metrics"><a class="header" href="#estimated-metrics">Estimated Metrics</a></h3>
<p><strong>Definition</strong>: Heuristic approximations calculated using formulas, not direct AST measurements.</p>
<p>These metrics are:</p>
<ul>
<li><strong>Heuristic</strong>: Based on mathematical formulas and assumptions</li>
<li><strong>Approximate</strong>: Close estimates, not exact counts</li>
<li><strong>Useful for prioritization</strong>: Help estimate effort and risk</li>
<li><strong>Not suitable for hard thresholds</strong>: Use for relative comparisons, not absolute gates</li>
</ul>
<p><strong>Estimated metrics include:</strong></p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Metric</th><th>Formula</th><th>Purpose</th><th>Example</th></tr>
</thead>
<tbody>
<tr><td><code>est_branches</code></td><td><code>max(nesting, 1) × cyclomatic ÷ 3</code></td><td>Estimate test cases needed for branch coverage</td><td>Complexity 12, nesting 3 → ~12 branches</td></tr>
</tbody>
</table>
</div>
<p><strong>Important</strong>: The <code>est_branches</code> metric was previously called <code>branches</code>. It was renamed in Spec 118 to make it explicit that this is an <strong>estimate</strong>, not a precise count from the AST.</p>
<h2 id="why-the-distinction-matters"><a class="header" href="#why-the-distinction-matters">Why the Distinction Matters</a></h2>
<h3 id="for-code-quality-gates"><a class="header" href="#for-code-quality-gates">For Code Quality Gates</a></h3>
<pre><code class="language-bash"># GOOD: Use measured metrics for CI/CD thresholds
debtmap validate . --threshold-complexity 15

# AVOID: Don't use estimated metrics for hard gates
# (est_branches is not exposed as a threshold flag)
</code></pre>
<p><strong>Rationale</strong>: Measured metrics are deterministic and precise, making them suitable for build-breaking quality gates.</p>
<h3 id="for-prioritization"><a class="header" href="#for-prioritization">For Prioritization</a></h3>
<pre><code class="language-bash"># GOOD: Use est_branches for prioritization
debtmap analyze . --top 10  # Sorts by est_branches (among other factors)

# GOOD: Estimated metrics help understand testing effort
debtmap analyze . --lcov coverage.info --verbose
</code></pre>
<p><strong>Rationale</strong>: Estimated metrics provide useful heuristics for understanding where to focus testing and refactoring efforts.</p>
<h3 id="for-comparison-across-codebases"><a class="header" href="#for-comparison-across-codebases">For Comparison Across Codebases</a></h3>
<ul>
<li><strong>Measured metrics</strong>: Comparable across projects (cyclomatic 10 means the same everywhere)</li>
<li><strong>Estimated metrics</strong>: Project-specific heuristics (est_branches depends on nesting patterns)</li>
</ul>
<h2 id="detailed-metric-descriptions"><a class="header" href="#detailed-metric-descriptions">Detailed Metric Descriptions</a></h2>
<h3 id="cyclomatic-complexity-measured"><a class="header" href="#cyclomatic-complexity-measured">Cyclomatic Complexity (Measured)</a></h3>
<p><strong>What it measures</strong>: The number of linearly independent paths through a function’s control flow.</p>
<p><strong>How it’s calculated</strong>:</p>
<ul>
<li>Start with a base of 1</li>
<li>Add 1 for each decision point:
<ul>
<li><code>if</code>, <code>else if</code></li>
<li><code>match</code> arms</li>
<li><code>while</code>, <code>for</code>, <code>loop</code></li>
<li><code>&amp;&amp;</code>, <code>||</code> in conditions</li>
<li><code>?</code> operator (early return)</li>
</ul>
</li>
</ul>
<p><strong>Example</strong>:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn example(x: i32, y: i32) -&gt; bool {
    if x &gt; 0 {        // +1
        if y &gt; 0 {    // +1
            true
        } else {      // implicit in if/else
            false
        }
    } else if x &lt; 0 { // +1
        false
    } else {
        y == 0        // no additional branches
    }
}
// Cyclomatic complexity = 1 + 3 = 4
<span class="boring">}</span></code></pre>
<p><strong>Thresholds</strong>:</p>
<ul>
<li><strong>1-5</strong>: Simple, easy to test</li>
<li><strong>6-10</strong>: Moderate, manageable complexity</li>
<li><strong>11-20</strong>: Complex, consider refactoring</li>
<li><strong>21+</strong>: Very complex, high maintenance cost</li>
</ul>
<h3 id="cognitive-complexity-measured"><a class="header" href="#cognitive-complexity-measured">Cognitive Complexity (Measured)</a></h3>
<p><strong>What it measures</strong>: How difficult the code is for humans to understand.</p>
<p><strong>How it differs from cyclomatic</strong>:</p>
<ul>
<li>Weights nested structures more heavily (nested if is worse than sequential if)</li>
<li>Ignores shorthand structures (early returns, guard clauses)</li>
<li>Focuses on readability, not just logic paths</li>
</ul>
<p><strong>Example</strong>:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn cyclomatic_low_cognitive_low(status: Status) -&gt; bool {
    match status {  // Cyclomatic: 4, Cognitive: 1
        Status::Active =&gt; true,
        Status::Pending =&gt; false,
        Status::Closed =&gt; false,
        Status::Error =&gt; false,
    }
}

fn cyclomatic_low_cognitive_high(x: i32, y: i32, z: i32) -&gt; bool {
    if x &gt; 0 {
        if y &gt; 0 {      // Nested: +2 cognitive penalty
            if z &gt; 0 {  // Deeply nested: +3 cognitive penalty
                return true;
            }
        }
    }
    false
}
// Cyclomatic: 4, Cognitive: 7 (nesting penalty applied)
<span class="boring">}</span></code></pre>
<p><strong>Thresholds</strong>:</p>
<ul>
<li><strong>1-5</strong>: Easy to understand</li>
<li><strong>6-10</strong>: Moderate mental load</li>
<li><strong>11-15</strong>: Difficult to follow</li>
<li><strong>16+</strong>: Refactor recommended</li>
</ul>
<h3 id="estimated-branches-estimated"><a class="header" href="#estimated-branches-estimated">Estimated Branches (Estimated)</a></h3>
<p><strong>What it estimates</strong>: Approximate number of execution paths that would need test coverage.</p>
<p><strong>Formula</strong>:</p>
<pre><code>est_branches = max(nesting_depth, 1) × cyclomatic_complexity ÷ 3
</code></pre>
<p><strong>Why this formula</strong>:</p>
<ul>
<li><strong>Nesting multiplier</strong>: Deeper nesting creates more combinations</li>
<li><strong>Cyclomatic base</strong>: Higher complexity → more paths</li>
<li><strong>÷ 3 adjustment</strong>: Empirical factor to align with typical branch coverage needs</li>
</ul>
<p><strong>Example scenarios</strong>:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Cyclomatic</th><th>Nesting</th><th>est_branches</th><th>Interpretation</th></tr>
</thead>
<tbody>
<tr><td>3</td><td>1</td><td>1</td><td>Simple linear code</td></tr>
<tr><td>12</td><td>1</td><td>4</td><td>Multiple sequential branches</td></tr>
<tr><td>12</td><td>3</td><td>12</td><td>Nested conditions, many paths</td></tr>
<tr><td>20</td><td>5</td><td>33</td><td>Complex nested logic</td></tr>
</tbody>
</table>
</div>
<p><strong>Use cases</strong>:</p>
<ul>
<li>Estimating test case requirements</li>
<li>Prioritizing untested complex code</li>
<li>Understanding coverage gaps</li>
</ul>
<p><strong>Limitations</strong>:</p>
<ul>
<li><strong>Not a precise count</strong>: This is a heuristic approximation</li>
<li><strong>Don’t use for coverage percentage calculation</strong>: Use actual coverage tools</li>
<li><strong>Varies by code style</strong>: Heavily nested code scores higher</li>
</ul>
<h2 id="terminology-change-spec-118"><a class="header" href="#terminology-change-spec-118">Terminology Change (Spec 118)</a></h2>
<h3 id="before-branches"><a class="header" href="#before-branches">Before: <code>branches</code></a></h3>
<p>Previously, this metric was displayed as <code>branches=X</code>, which was confusing because:</p>
<ol>
<li>Users thought it was a precise count from AST analysis</li>
<li>It was mistaken for cyclomatic complexity (actual branch count)</li>
<li>The estimation nature was not obvious</li>
</ol>
<h3 id="after-est_branches"><a class="header" href="#after-est_branches">After: <code>est_branches</code></a></h3>
<p>Now displayed as <code>est_branches=X</code> to:</p>
<ol>
<li><strong>Make estimation explicit</strong>: “est_” prefix indicates this is approximate</li>
<li><strong>Avoid confusion</strong>: Clearly different from cyclomatic complexity</li>
<li><strong>Set correct expectations</strong>: Users know this is a heuristic, not a measurement</li>
</ol>
<h3 id="migration-guide-1"><a class="header" href="#migration-guide-1">Migration Guide</a></h3>
<p><strong>Terminal Output</strong>:</p>
<ul>
<li>Old: <code>COMPLEXITY: cyclomatic=12, branches=8, cognitive=15</code></li>
<li>New: <code>COMPLEXITY: cyclomatic=12, est_branches=8, cognitive=15</code></li>
</ul>
<p><strong>Code</strong>:</p>
<ul>
<li>Internal variable names updated from <code>branches</code> to <code>est_branches</code></li>
<li>Comments added explaining the estimation formula</li>
</ul>
<p><strong>JSON Output</strong>:</p>
<ul>
<li>No change: The ComplexityMetrics struct does not include this field</li>
<li><code>est_branches</code> is calculated on-demand for display purposes only</li>
</ul>
<h2 id="practical-usage-examples"><a class="header" href="#practical-usage-examples">Practical Usage Examples</a></h2>
<h3 id="example-1-code-quality-gate"><a class="header" href="#example-1-code-quality-gate">Example 1: Code Quality Gate</a></h3>
<pre><code class="language-bash"># Fail build if any function exceeds cyclomatic complexity 15
debtmap validate . --threshold-complexity 15 --max-high 0

# Why: Cyclomatic is measured, precise, and repeatable
</code></pre>
<h3 id="example-2-prioritize-testing-effort"><a class="header" href="#example-2-prioritize-testing-effort">Example 2: Prioritize Testing Effort</a></h3>
<pre><code class="language-bash"># Show top 10 functions by risk (uses est_branches in scoring)
debtmap analyze . --lcov coverage.info --top 10

# Functions with high est_branches and low coverage appear first
</code></pre>
<h3 id="example-3-understanding-test-requirements"><a class="header" href="#example-3-understanding-test-requirements">Example 3: Understanding Test Requirements</a></h3>
<pre><code class="language-bash"># Verbose output shows est_branches for each function
debtmap analyze . --verbose

# Output:
# └─ COMPLEXITY: cyclomatic=12, est_branches=8, cognitive=15, nesting=2
#
# Interpretation: ~8 test cases likely needed for good branch coverage
</code></pre>
<h3 id="example-4-explaining-metrics-to-team"><a class="header" href="#example-4-explaining-metrics-to-team">Example 4: Explaining Metrics to Team</a></h3>
<pre><code class="language-bash"># Display comprehensive metric definitions
debtmap analyze --explain-metrics

# Shows:
# - Measured vs Estimated categories
# - Formulas and thresholds
# - When to use each metric
</code></pre>
<h2 id="metric-selection-guide"><a class="header" href="#metric-selection-guide">Metric Selection Guide</a></h2>
<h3 id="when-to-use-cyclomatic-complexity"><a class="header" href="#when-to-use-cyclomatic-complexity">When to Use Cyclomatic Complexity</a></h3>
<p>✅ <strong>Use for:</strong></p>
<ul>
<li>CI/CD quality gates</li>
<li>Code review guidelines</li>
<li>Consistent cross-project comparison</li>
<li>Identifying refactoring candidates</li>
</ul>
<p>❌ <strong>Don’t use for:</strong></p>
<ul>
<li>Estimating test effort (use est_branches)</li>
<li>Readability assessment (use cognitive complexity)</li>
</ul>
<h3 id="when-to-use-cognitive-complexity"><a class="header" href="#when-to-use-cognitive-complexity">When to Use Cognitive Complexity</a></h3>
<p>✅ <strong>Use for:</strong></p>
<ul>
<li>Readability reviews</li>
<li>Identifying hard-to-maintain code</li>
<li>Onboarding difficulty assessment</li>
</ul>
<p>❌ <strong>Don’t use for:</strong></p>
<ul>
<li>Test coverage planning</li>
<li>Strict quality gates (more subjective than cyclomatic)</li>
</ul>
<h3 id="when-to-use-est_branches"><a class="header" href="#when-to-use-est_branches">When to Use est_branches</a></h3>
<p>✅ <strong>Use for:</strong></p>
<ul>
<li>Estimating test case requirements</li>
<li>Prioritizing test coverage work</li>
<li>Understanding coverage gaps</li>
</ul>
<p>❌ <strong>Don’t use for:</strong></p>
<ul>
<li>CI/CD quality gates (it’s an estimate)</li>
<li>Calculating coverage percentages (use actual coverage data)</li>
<li>Cross-project comparison (formula is heuristic)</li>
</ul>
<h2 id="combining-metrics-for-insights"><a class="header" href="#combining-metrics-for-insights">Combining Metrics for Insights</a></h2>
<h3 id="high-complexity-low-coverage"><a class="header" href="#high-complexity-low-coverage">High Complexity, Low Coverage</a></h3>
<pre><code>cyclomatic=18, est_branches=12, coverage=0%
</code></pre>
<p><strong>Interpretation</strong>: High-risk code needing ~12 test cases for adequate coverage.</p>
<p><strong>Action</strong>: Prioritize writing tests, consider refactoring.</p>
<h3 id="high-cyclomatic-low-cognitive"><a class="header" href="#high-cyclomatic-low-cognitive">High Cyclomatic, Low Cognitive</a></h3>
<pre><code>cyclomatic=15, cognitive=5
</code></pre>
<p><strong>Interpretation</strong>: Many branches, but simple linear logic (e.g., validation checks).</p>
<p><strong>Action</strong>: Acceptable pattern, tests should be straightforward.</p>
<h3 id="low-cyclomatic-high-cognitive"><a class="header" href="#low-cyclomatic-high-cognitive">Low Cyclomatic, High Cognitive</a></h3>
<pre><code>cyclomatic=8, cognitive=18
</code></pre>
<p><strong>Interpretation</strong>: Deeply nested logic, hard to understand despite fewer branches.</p>
<p><strong>Action</strong>: Refactor to reduce nesting, extract functions.</p>
<h3 id="high-est_branches-low-cyclomatic"><a class="header" href="#high-est_branches-low-cyclomatic">High est_branches, Low Cyclomatic</a></h3>
<pre><code>cyclomatic=9, nesting=5, est_branches=15
</code></pre>
<p><strong>Interpretation</strong>: Deep nesting creates many path combinations.</p>
<p><strong>Action</strong>: Flatten nesting, use early returns, extract nested logic.</p>
<h2 id="frequently-asked-questions"><a class="header" href="#frequently-asked-questions">Frequently Asked Questions</a></h2>
<h3 id="q-why-is-est_branches-different-from-cyclomatic-complexity"><a class="header" href="#q-why-is-est_branches-different-from-cyclomatic-complexity">Q: Why is est_branches different from cyclomatic complexity?</a></h3>
<p><strong>A</strong>: Cyclomatic is the <strong>measured</strong> count of decision points. <code>est_branches</code> is an <strong>estimated</strong> number of execution paths, calculated using nesting depth to account for path combinations.</p>
<h3 id="q-can-i-use-est_branches-in-cicd-thresholds"><a class="header" href="#q-can-i-use-est_branches-in-cicd-thresholds">Q: Can I use est_branches in CI/CD thresholds?</a></h3>
<p><strong>A</strong>: No. Use measured metrics (cyclomatic_complexity, cognitive_complexity) for quality gates. <code>est_branches</code> is a heuristic for prioritization, not a precise measurement.</p>
<h3 id="q-why-did-the-metric-name-change-from-branches-to-est_branches"><a class="header" href="#q-why-did-the-metric-name-change-from-branches-to-est_branches">Q: Why did the metric name change from “branches” to “est_branches”?</a></h3>
<p><strong>A</strong>: To make it explicit that this is an <strong>estimate</strong>, not a measured value. Users were confused, thinking it was a precise count from the AST.</p>
<h3 id="q-how-accurate-is-est_branches-for-estimating-test-cases"><a class="header" href="#q-how-accurate-is-est_branches-for-estimating-test-cases">Q: How accurate is est_branches for estimating test cases?</a></h3>
<p><strong>A</strong>: It’s a <strong>rough approximation</strong>. Actual test case requirements depend on:</p>
<ul>
<li>Business logic complexity</li>
<li>Edge cases</li>
<li>Error handling paths</li>
<li>Integration points</li>
</ul>
<p>Use <code>est_branches</code> as a starting point, not an exact requirement.</p>
<h3 id="q-should-i-refactor-code-with-high-est_branches"><a class="header" href="#q-should-i-refactor-code-with-high-est_branches">Q: Should I refactor code with high est_branches?</a></h3>
<p><strong>A</strong>: Not necessarily. High <code>est_branches</code> indicates complex logic that may need thorough testing. If the logic is unavoidable (e.g., state machines, complex business rules), focus on comprehensive test coverage rather than refactoring.</p>
<h2 id="signal-categories-summary"><a class="header" href="#signal-categories-summary">Signal Categories Summary</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Category</th><th>Signals</th><th>Purpose</th></tr>
</thead>
<tbody>
<tr><td>Complexity</td><td>cyclomatic, cognitive, nesting, loc</td><td>How hard code is to understand</td></tr>
<tr><td>Coverage</td><td>line_percent, branch_percent</td><td>How risky changes are</td></tr>
<tr><td>Coupling</td><td>fan_in, fan_out, call_depth</td><td>How changes ripple</td></tr>
<tr><td>Quality</td><td>entropy, purity, dead_code</td><td>False positive reduction</td></tr>
</tbody>
</table>
</div>
<h2 id="using-signals-with-ai"><a class="header" href="#using-signals-with-ai">Using Signals with AI</a></h2>
<p>When piping debtmap output to an AI assistant, signals provide the context needed for intelligent fixes:</p>
<pre><code class="language-bash"># Get structured signals for AI consumption
debtmap analyze . --format llm-markdown --top 5 | claude "Fix the top item"
</code></pre>
<p>The AI uses these signals to:</p>
<ul>
<li>Understand code complexity before reading it</li>
<li>Prioritize which files to examine first</li>
<li>Decide between refactoring vs testing approaches</li>
<li>Estimate the scope of changes needed</li>
</ul>
<h2 id="further-reading-2"><a class="header" href="#further-reading-2">Further Reading</a></h2>
<ul>
<li><a href="#why-debtmap">Why Debtmap?</a> - Sensor model explained</li>
<li><a href="#llm-integration-guide">LLM Integration</a> - AI workflow patterns</li>
<li><a href="#thresholds">Configuration</a> - Threshold customization</li>
<li><a href="#scoring-strategies">Scoring Strategies</a> - How signals combine</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="examples-5"><a class="header" href="#examples-5">Examples</a></h1>
<p>This chapter provides practical, real-world examples of using Debtmap across different project types and workflows. All examples use current CLI syntax verified against the source code.</p>
<blockquote>
<p><strong>Quick Start</strong>: New to Debtmap? Start with <a href="#basic-rust-analysis">Basic Rust Analysis</a> for the simplest introduction, then explore <a href="#coverage-integration-with-cargo-tarpaulin">Coverage Integration</a> for risk-based prioritization.</p>
</blockquote>
<blockquote>
<p><strong>Quick Navigation</strong>: For detailed explanations of all CLI options, see the <a href="#cli-reference">CLI Reference</a> chapter.</p>
</blockquote>
<h2 id="overview-21"><a class="header" href="#overview-21">Overview</a></h2>
<p>This chapter demonstrates:</p>
<ul>
<li><strong>Language-specific analysis</strong>: Rust, Python, JavaScript/TypeScript with their respective testing tools</li>
<li><strong>CI/CD integration</strong>: GitHub Actions, GitLab CI, CircleCI with validation gates</li>
<li><strong>Output formats</strong>: Terminal, JSON, and Markdown with interpretation guidance</li>
<li><strong>Advanced features</strong>: Context-aware analysis, multi-pass processing</li>
<li><strong>Configuration patterns</strong>: Tailored settings for different project types</li>
<li><strong>Progress tracking</strong>: Using the <code>compare</code> command to validate refactoring improvements</li>
</ul>
<p>All examples are copy-paste ready and tested against the current Debtmap implementation.</p>
<h2 id="table-of-contents-1"><a class="header" href="#table-of-contents-1">Table of Contents</a></h2>
<ul>
<li><a href="#analyzing-rust-projects">Analyzing Rust Projects</a></li>
<li><a href="#python-analysis">Python Analysis</a></li>
<li><a href="#javascripttypescript-1">JavaScript/TypeScript</a></li>
<li><a href="#ci-integration-1">CI Integration</a></li>
<li><a href="#output-formats-4">Output Formats</a></li>
<li><a href="#advanced-usage-1">Advanced Usage</a></li>
<li><a href="#configuration-examples-1">Configuration Examples</a></li>
<li><a href="#compare-command">Compare Command</a></li>
</ul>
<h2 id="analyzing-rust-projects"><a class="header" href="#analyzing-rust-projects">Analyzing Rust Projects</a></h2>
<h3 id="basic-rust-analysis"><a class="header" href="#basic-rust-analysis">Basic Rust Analysis</a></h3>
<p>Start with a simple analysis of your Rust project:</p>
<pre><code class="language-bash"># Analyze current directory (path defaults to '.' - both commands are identical)
debtmap analyze

# Same as above with explicit path
debtmap analyze .

# Analyze specific directory
debtmap analyze ./src

# Analyze with custom complexity threshold
debtmap analyze ./src --threshold-complexity 15
</code></pre>
<h3 id="coverage-integration-with-cargo-tarpaulin"><a class="header" href="#coverage-integration-with-cargo-tarpaulin">Coverage Integration with cargo-tarpaulin</a></h3>
<p>Combine complexity analysis with test coverage for risk-based prioritization:</p>
<pre><code class="language-bash"># Generate LCOV coverage data
cargo tarpaulin --out lcov --output-dir target/coverage

# Analyze with coverage data
debtmap analyze . --lcov target/coverage/lcov.info

# Or use the shorter alias
debtmap analyze . --coverage-file target/coverage/lcov.info
</code></pre>
<blockquote>
<p><strong>Note</strong>: <code>--lcov</code> is an alias for <code>--coverage-file</code> - both work identically.</p>
</blockquote>
<p><strong>What this does:</strong></p>
<ul>
<li>Functions with 0% coverage and high complexity get marked as <code>[CRITICAL]</code></li>
<li>Well-tested functions (&gt;80% coverage) are deprioritized</li>
<li>Shows risk reduction potential for each untested function</li>
</ul>
<h3 id="custom-thresholds"><a class="header" href="#custom-thresholds">Custom Thresholds</a></h3>
<p>Configure thresholds to match your project standards:</p>
<pre><code class="language-bash"># Set both complexity and duplication thresholds
debtmap analyze . \
  --threshold-complexity 15 \
  --threshold-duplication 50

# Use preset configurations for quick setup
debtmap analyze . --threshold-preset strict    # Strict standards
debtmap analyze . --threshold-preset balanced  # Default balanced
debtmap analyze . --threshold-preset lenient   # Lenient for legacy code
</code></pre>
<p><strong>Preset configurations:</strong></p>
<ul>
<li><strong>Strict</strong>: Lower thresholds for high quality standards (good for new projects)</li>
<li><strong>Balanced</strong>: Default thresholds suitable for typical projects</li>
<li><strong>Lenient</strong>: Higher thresholds designed for legacy codebases with existing technical debt</li>
</ul>
<p><strong>Preset Threshold Values:</strong></p>
<p>These presets control the minimum complexity levels that trigger debt flagging. Source: <code>src/complexity/threshold_manager.rs:120-148</code></p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Preset</th><th>Min Cyclomatic</th><th>Min Cognitive</th><th>Min Function Lines</th></tr>
</thead>
<tbody>
<tr><td>Strict</td><td>3</td><td>7</td><td>15</td></tr>
<tr><td>Balanced</td><td>5</td><td>10</td><td>20</td></tr>
<tr><td>Lenient</td><td>10</td><td>20</td><td>50</td></tr>
</tbody>
</table>
</div>
<p><strong>Note</strong>: These are minimum thresholds for flagging functions. Functions below these thresholds are considered simple and won’t appear in debt reports.</p>
<h3 id="god-object-detection-3"><a class="header" href="#god-object-detection-3">God Object Detection</a></h3>
<p>Identify classes and modules with too many responsibilities:</p>
<pre><code class="language-bash"># Standard analysis includes god object detection
debtmap analyze .

# Disable god object detection for specific run
debtmap analyze . --no-god-object

# Show detailed module split recommendations (experimental)
debtmap analyze . --show-splits
</code></pre>
<p>God objects are flagged with detailed metrics:</p>
<ul>
<li>Number of methods and fields</li>
<li>Responsibility count (grouped by naming patterns)</li>
<li>God object score (0-100%)</li>
<li>Recommendations for splitting</li>
</ul>
<p><strong>The <code>--show-splits</code> option provides experimental decomposition suggestions:</strong></p>
<pre><code class="language-bash"># Get detailed recommendations for breaking up large modules
debtmap analyze . --show-splits
</code></pre>
<p>This shows suggested module boundaries, responsibility groupings, and how to decompose large files into smaller, focused modules.</p>
<h4 id="purity-weighted-god-object-scoring"><a class="header" href="#purity-weighted-god-object-scoring">Purity-Weighted God Object Scoring</a></h4>
<p>Debtmap uses purity analysis to distinguish functional programming patterns from actual god objects. Enable verbose mode to see purity distribution:</p>
<pre><code class="language-bash"># See purity distribution in god object analysis
debtmap analyze . -v
</code></pre>
<p><strong>Example Output:</strong></p>
<pre><code>GOD OBJECT ANALYSIS: src/core/processor.rs
  Total functions: 107
  PURITY DISTRIBUTION:
    Pure: 70 functions (65%) → complexity weight: 6.3
    Impure: 37 functions (35%) → complexity weight: 14.0
    Total weighted complexity: 20.3
  God object score: 12.0 (threshold: 70.0)
  Status: ✓ Not a god object (functional design)
</code></pre>
<p>This shows:</p>
<ul>
<li><strong>Pure functions</strong> (no side effects, immutable) receive 0.3× weight</li>
<li><strong>Impure functions</strong> (I/O, mutations, side effects) receive 1.0× weight</li>
<li>Functional modules with many pure helpers avoid false positives</li>
<li>Focus shifts to modules with excessive stateful code</li>
</ul>
<p><strong>Why This Matters:</strong></p>
<p>Without purity weighting:</p>
<pre><code>Module with 100 pure helpers → Flagged as god object ❌
</code></pre>
<p>With purity weighting:</p>
<pre><code>Module with 100 pure helpers → Normal (functional design) ✅
Module with 100 impure functions → God object detected ✅
</code></pre>
<p><strong>Compare Two Modules:</strong></p>
<p>Functional module (70 pure, 30 impure):</p>
<pre><code>Pure:    70 × 0.3 = 21.0
Impure:  30 × 1.0 = 30.0
Score: 35.0 → Not a god object ✓
</code></pre>
<p>Procedural module (100 impure):</p>
<pre><code>Impure: 100 × 1.0 = 100.0
Score: 125.0 → God object detected ✗
</code></pre>
<h3 id="filtering-and-focusing"><a class="header" href="#filtering-and-focusing">Filtering and Focusing</a></h3>
<pre><code class="language-bash"># Analyze only Rust files
debtmap analyze . --languages rust

# Focus on architecture issues (god objects, complexity)
debtmap analyze . --filter Architecture

# Focus on testing gaps
debtmap analyze . --filter Testing

# Filter by multiple categories
debtmap analyze . --filter Architecture,Testing

# Show only top 10 issues
debtmap analyze . --top 10

# Show only high-priority items
debtmap analyze . --min-priority high
</code></pre>
<p><strong>Valid filter categories:</strong></p>
<ul>
<li><code>Architecture</code> - God objects, high complexity, structural issues</li>
<li><code>Testing</code> - Test coverage gaps, untested critical code</li>
<li><code>Duplication</code> - Code duplication and similar patterns</li>
<li><code>Maintainability</code> - Long functions, deep nesting, readability issues</li>
</ul>
<h3 id="output-formats-4"><a class="header" href="#output-formats-4">Output Formats</a></h3>
<pre><code class="language-bash"># JSON output for CI integration
debtmap analyze . --format json --output report.json

# Markdown report
debtmap analyze . --format markdown --output DEBT_REPORT.md

# Terminal output (default) - prettified
debtmap analyze .
</code></pre>
<h3 id="multi-pass-analysis-1"><a class="header" href="#multi-pass-analysis-1">Multi-Pass Analysis</a></h3>
<p>For deeper analysis with context awareness:</p>
<pre><code class="language-bash"># Enable context-aware analysis with multiple providers
debtmap analyze . \
  --context \
  --context-providers critical_path,dependency,git_history

# Multi-pass analysis with attribution
debtmap analyze . --multi-pass --attribution
</code></pre>
<h3 id="complete-ci-example"><a class="header" href="#complete-ci-example">Complete CI Example</a></h3>
<p>This is from Debtmap’s own <code>.github/workflows/debtmap.yml</code>:</p>
<pre><code class="language-bash"># 1. Install cargo-tarpaulin
cargo install cargo-tarpaulin

# 2. Build debtmap
cargo build --release

# 3. Generate coverage
cargo tarpaulin --config .tarpaulin.toml --out Lcov --timeout 300

# 4. Run validation with coverage
./target/release/debtmap validate . \
  --coverage-file target/coverage/lcov.info \
  --format json \
  --output debtmap-report.json
</code></pre>
<h2 id="python-analysis"><a class="header" href="#python-analysis">Python Analysis</a></h2>
<h3 id="basic-python-analysis"><a class="header" href="#basic-python-analysis">Basic Python Analysis</a></h3>
<pre><code class="language-bash"># Analyze Python files only
debtmap analyze . --languages python

# Analyze specific Python directory
debtmap analyze src --languages python
</code></pre>
<h3 id="coverage-integration-with-pytest"><a class="header" href="#coverage-integration-with-pytest">Coverage Integration with pytest</a></h3>
<p>Generate coverage and analyze risk:</p>
<pre><code class="language-bash"># Generate LCOV coverage with pytest
pytest --cov --cov-report=lcov

# Analyze with coverage data
debtmap analyze . \
  --languages python \
  --lcov coverage.lcov
</code></pre>
<h3 id="python-specific-patterns"><a class="header" href="#python-specific-patterns">Python-Specific Patterns</a></h3>
<pre><code class="language-bash"># Focus on testing gaps in Python code
debtmap analyze . \
  --languages python \
  --filter Testing

# Find god objects in Python modules
debtmap analyze . \
  --languages python \
  --filter Architecture
</code></pre>
<h3 id="example-configuration-for-python-projects"><a class="header" href="#example-configuration-for-python-projects">Example Configuration for Python Projects</a></h3>
<p>Create <code>.debtmap.toml</code>:</p>
<pre><code class="language-toml">[languages]
enabled = ["python"]

[thresholds]
complexity = 12
max_function_lines = 40

[ignore]
patterns = [
  "**/*_test.py",
  "tests/**",
  ".venv/**",
  "**/__pycache__/**",
]

[god_object]
enabled = true
max_methods = 15
max_responsibilities = 4
</code></pre>
<h2 id="javascripttypescript-1"><a class="header" href="#javascripttypescript-1">JavaScript/TypeScript</a></h2>
<h3 id="analyzing-jsts-projects"><a class="header" href="#analyzing-jsts-projects">Analyzing JS/TS Projects</a></h3>
<pre><code class="language-bash"># Analyze JavaScript and TypeScript
debtmap analyze . --languages javascript,typescript

# TypeScript only
debtmap analyze . --languages typescript
</code></pre>
<h3 id="coverage-integration-with-jest"><a class="header" href="#coverage-integration-with-jest">Coverage Integration with Jest</a></h3>
<pre><code class="language-bash"># Generate LCOV with Jest
jest --coverage --coverageReporters=lcov

# Analyze with coverage
debtmap analyze . \
  --languages javascript,typescript \
  --lcov coverage/lcov.info
</code></pre>
<h3 id="nodejs-project-patterns"><a class="header" href="#nodejs-project-patterns">Node.js Project Patterns</a></h3>
<pre><code class="language-bash"># Exclude node_modules and focus on source
debtmap analyze src --languages javascript,typescript

# With custom complexity thresholds for JS
debtmap analyze . \
  --languages javascript,typescript \
  --threshold-complexity 10
</code></pre>
<h3 id="typescript-configuration-example"><a class="header" href="#typescript-configuration-example">TypeScript Configuration Example</a></h3>
<p>Create <code>.debtmap.toml</code>:</p>
<pre><code class="language-toml">[languages]
enabled = ["typescript", "javascript"]

[thresholds]
complexity = 10
max_function_lines = 50

[ignore]
patterns = [
  "node_modules/**",
  "**/*.test.ts",
  "**/*.spec.ts",
  "dist/**",
  "build/**",
  "**/*.d.ts",
]
</code></pre>
<h3 id="monorepo-analysis"><a class="header" href="#monorepo-analysis">Monorepo Analysis</a></h3>
<pre><code class="language-bash"># Analyze specific package
debtmap analyze packages/api --languages typescript

# Analyze all packages, grouped by category
debtmap analyze packages \
  --languages typescript \
  --group-by-category
</code></pre>
<h2 id="ci-integration-1"><a class="header" href="#ci-integration-1">CI Integration</a></h2>
<h3 id="github-actions-3"><a class="header" href="#github-actions-3">GitHub Actions</a></h3>
<p>Complete workflow example (from <code>.github/workflows/debtmap.yml</code>):</p>
<pre><code class="language-yaml">name: Debtmap

on:
  push:
    branches: [ main, master ]
  pull_request:
    branches: [ main, master ]
  workflow_dispatch:

env:
  CARGO_TERM_COLOR: always
  RUST_BACKTRACE: 1

jobs:
  validate:
    name: Technical Debt Validation
    runs-on: ubuntu-latest

    steps:
    - name: Checkout repository
      uses: actions/checkout@v5
      with:
        fetch-depth: 0

    - name: Setup Rust
      uses: dtolnay/rust-toolchain@stable
      with:
        components: rustfmt, clippy

    - name: Cache cargo dependencies
      uses: actions/cache@v4
      with:
        path: |
          ~/.cargo/bin/
          ~/.cargo/registry/index/
          ~/.cargo/registry/cache/
          ~/.cargo/git/db/
          target/
        key: ${{ runner.os }}-cargo-${{ hashFiles('**/Cargo.lock') }}
        restore-keys: |
          ${{ runner.os }}-cargo-

    - name: Install cargo-tarpaulin
      run: |
        if ! command -v cargo-tarpaulin &amp;&gt; /dev/null; then
          cargo install cargo-tarpaulin
        else
          echo "cargo-tarpaulin already installed"
        fi

    - name: Build debtmap
      run: cargo build --release

    - name: Generate coverage data
      run: cargo tarpaulin --config .tarpaulin.toml --out Lcov --timeout 300

    - name: Run debtmap validation with coverage
      run: |
        if [ -f "target/coverage/lcov.info" ]; then
          ./target/release/debtmap validate . --coverage-file target/coverage/lcov.info --format json --output debtmap-report.json
        else
          echo "Warning: LCOV file not found, running validation without coverage data"
          ./target/release/debtmap validate . --format json --output debtmap-report.json
        fi

    - name: Upload debtmap report and coverage
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: debtmap-analysis-artifacts
        path: |
          debtmap-report.json
          target/coverage/lcov.info
        retention-days: 7
</code></pre>
<h3 id="gitlab-ci-3"><a class="header" href="#gitlab-ci-3">GitLab CI</a></h3>
<pre><code class="language-yaml">debtmap:
  stage: quality
  image: rust:latest
  script:
    # Install debtmap
    - cargo install debtmap

    # Run tests with coverage (generates LCOV format)
    - cargo install cargo-tarpaulin
    - cargo tarpaulin --out Lcov

    # Validate with debtmap (using LCOV format)
    - debtmap validate .
        --coverage-file lcov.info
        --format json
        --output debtmap-report.json
  artifacts:
    paths:
      - lcov.info
      - debtmap-report.json
    expire_in: 1 week
</code></pre>
<h3 id="circleci-1"><a class="header" href="#circleci-1">CircleCI</a></h3>
<pre><code class="language-yaml">version: 2.1

jobs:
  debtmap:
    docker:
      - image: cimg/rust:1.75
    steps:
      - checkout

      - run:
          name: Install debtmap
          command: cargo install debtmap

      - run:
          name: Generate coverage
          command: |
            cargo install cargo-tarpaulin
            cargo tarpaulin --out Lcov

      - run:
          name: Run debtmap
          command: |
            debtmap validate . \
              --coverage-file lcov.info \
              --format json \
              --output debtmap.json

      - store_artifacts:
          path: debtmap.json

workflows:
  version: 2
  build:
    jobs:
      - debtmap
</code></pre>
<h3 id="using-debtmap-validate-for-pr-gates"><a class="header" href="#using-debtmap-validate-for-pr-gates">Using debtmap validate for PR Gates</a></h3>
<pre><code class="language-bash"># Fail build if thresholds are exceeded
debtmap validate . --coverage-file lcov.info

# With custom thresholds
debtmap validate . \
  --coverage-file lcov.info \
  --threshold-complexity 15

# Exit code 0 if passing, 1 if failing
</code></pre>
<h3 id="compare-command-in-ci"><a class="header" href="#compare-command-in-ci">Compare Command in CI</a></h3>
<p>Track technical debt trends over time:</p>
<pre><code class="language-bash"># Generate baseline (on main branch)
debtmap analyze . --format json --output baseline.json

# After PR changes
debtmap analyze . --format json --output current.json

# Compare and fail if regressions detected
debtmap compare \
  --before baseline.json \
  --after current.json \
  --format json
</code></pre>
<h2 id="output-formats-1-1"><a class="header" href="#output-formats-1-1">Output Formats</a></h2>
<h3 id="terminal-output-default"><a class="header" href="#terminal-output-default">Terminal Output (Default)</a></h3>
<p>The default terminal output is prettified with colors and priorities:</p>
<pre><code class="language-bash">debtmap analyze . --lcov coverage.lcov --top 3
</code></pre>
<p>Example output:</p>
<pre><code>════════════════════════════════════════════
    PRIORITY TECHNICAL DEBT FIXES
════════════════════════════════════════════

🎯 TOP 3 RECOMMENDATIONS (by unified priority)

#1 SCORE: 8.9 [CRITICAL]
├─ TEST GAP: ./src/analyzers/rust.rs:38 parse_function()
├─ ACTION: Add 6 unit tests for full coverage
├─ IMPACT: Full test coverage, -3.7 risk
├─ COMPLEXITY: cyclomatic=6, cognitive=8, nesting=2, lines=32
├─ DEPENDENCIES: 0 upstream, 11 downstream
└─ WHY: Business logic with 0% coverage, manageable complexity

📊 TOTAL DEBT SCORE: 4907
📈 OVERALL COVERAGE: 67.12%
</code></pre>
<h3 id="json-output-3"><a class="header" href="#json-output-3">JSON Output</a></h3>
<p>Machine-readable format for CI/CD integration:</p>
<pre><code class="language-bash">debtmap analyze . --format json --output report.json
</code></pre>
<p><strong>Using JSON output programmatically:</strong></p>
<pre><code class="language-bash"># Extract total debt score
debtmap analyze . --format json | jq '.total_debt_score'

# Count critical items
debtmap analyze . --format json | jq '[.items[] | select(.unified_score.final_score &gt;= 8)] | length'

# Get top 5 functions by score
debtmap analyze . --format json | jq '.items | sort_by(-.unified_score.final_score) | .[0:5] | .[].location'

# Extract all test gap items
debtmap analyze . --format json | jq '[.items[] | select(.debt_type == "TestGap")]'
</code></pre>
<p>Structure:</p>
<pre><code class="language-json">{
  "items": [
    {
      "location": {
        "file": "src/main.rs",
        "function": "process_data",
        "line": 42
      },
      "debt_type": "TestGap",
      "unified_score": {
        "complexity_factor": 3.2,
        "coverage_factor": 10.0,
        "dependency_factor": 2.5,
        "role_multiplier": 1.2,
        "final_score": 9.4
      },
      "function_role": "BusinessLogic",
      "recommendation": {
        "action": "Add unit tests",
        "details": "Add 6 unit tests for full coverage",
        "effort_estimate": "2-3 hours"
      },
      "expected_impact": {
        "risk_reduction": 3.9,
        "complexity_reduction": 0,
        "coverage_improvement": 100
      }
    }
  ],
  "overall_coverage": 67.12,
  "total_debt_score": 4907
}
</code></pre>
<h3 id="markdown-report"><a class="header" href="#markdown-report">Markdown Report</a></h3>
<pre><code class="language-bash"># Standard markdown report
debtmap analyze . --format markdown --output DEBT_REPORT.md

# Summary level for executives (minimal detail)
debtmap analyze . --format markdown --detail-level summary --output SUMMARY.md

# Standard level for team review (default)
debtmap analyze . --format markdown --detail-level standard --output DEBT.md

# Comprehensive level for deep analysis
debtmap analyze . --format markdown --detail-level comprehensive --output DETAILED.md

# Debug level for troubleshooting
debtmap analyze . --format markdown --detail-level debug --output DEBUG.md
</code></pre>
<p><strong>Detail levels:</strong></p>
<ul>
<li><strong>summary</strong>: Executive summary with key metrics and top issues only</li>
<li><strong>standard</strong>: Balanced detail suitable for team reviews (default)</li>
<li><strong>comprehensive</strong>: Full details including all debt items and analysis</li>
<li><strong>debug</strong>: Maximum detail including AST information and parser internals</li>
</ul>
<p><strong>Use cases:</strong></p>
<ul>
<li>Summary: Management reports, PR comments</li>
<li>Standard: Regular team reviews</li>
<li>Comprehensive: Deep dives, refactoring planning</li>
<li>Debug: Troubleshooting debtmap behavior</li>
</ul>
<p>Great for documentation or PR comments.</p>
<h2 id="advanced-usage-1"><a class="header" href="#advanced-usage-1">Advanced Usage</a></h2>
<h3 id="design-pattern-detection-1"><a class="header" href="#design-pattern-detection-1">Design Pattern Detection</a></h3>
<p>Detect design patterns in your codebase to understand architectural choices and identify potential overuse of certain patterns:</p>
<pre><code class="language-bash"># Detect specific design patterns
debtmap analyze . --patterns observer,singleton,factory

# Adjust pattern confidence threshold (0.0-1.0, default: 0.7)
debtmap analyze . --pattern-threshold 0.8

# Show uncertain pattern matches with warnings
debtmap analyze . --show-pattern-warnings

# Disable pattern detection entirely
debtmap analyze . --no-pattern-detection
</code></pre>
<p><strong>Available patterns:</strong></p>
<ul>
<li><code>observer</code> - Event listener registrations, callback patterns</li>
<li><code>singleton</code> - Static instance management</li>
<li><code>factory</code> - Object creation methods</li>
<li><code>strategy</code> - Algorithm selection via traits/interfaces</li>
<li><code>callback</code> - Function passing and invocation</li>
<li><code>template_method</code> - Abstract methods with concrete implementations</li>
</ul>
<p><strong>Use cases:</strong></p>
<ul>
<li>Identify architectural patterns in unfamiliar codebases</li>
<li>Detect potential overuse of certain patterns (e.g., too many singletons)</li>
<li>Understand code organization and design decisions</li>
</ul>
<h3 id="dead-code-and-public-api-analysis"><a class="header" href="#dead-code-and-public-api-analysis">Dead Code and Public API Analysis</a></h3>
<p>Control how Debtmap detects public APIs to prevent false positives when analyzing libraries vs CLI tools:</p>
<pre><code class="language-bash"># Analyze with public API awareness (default for libraries)
debtmap analyze . --context

# Disable public API heuristics (useful for CLI tools)
debtmap analyze . --no-public-api-detection

# Adjust public API confidence threshold (default: 0.7)
debtmap analyze . --public-api-threshold 0.8
</code></pre>
<p><strong>When to use:</strong></p>
<ul>
<li><strong>Libraries</strong>: Keep default public API detection to avoid flagging exported functions as unused</li>
<li><strong>CLI tools</strong>: Use <code>--no-public-api-detection</code> since there’s no public API</li>
<li><strong>Mixed projects</strong>: Adjust threshold based on false positive rate</li>
</ul>
<p><strong>What it detects:</strong></p>
<ul>
<li>Functions exported in <code>lib.rs</code> or <code>api.rs</code></li>
<li>Public trait implementations</li>
<li>Functions matching API naming patterns</li>
<li>Prevents false positives for “unused” library functions</li>
</ul>
<h3 id="context-aware-analysis-3"><a class="header" href="#context-aware-analysis-3">Context-Aware Analysis</a></h3>
<p>Enable advanced context providers for more accurate prioritization:</p>
<pre><code class="language-bash"># Enable all context providers for comprehensive analysis
debtmap analyze . \
  --context \
  --context-providers critical_path,dependency,git_history
</code></pre>
<p><strong>Context Providers:</strong></p>
<p><strong>critical_path</strong> - Identifies functions on critical execution paths</p>
<ul>
<li>Analyzes call graph to find frequently-called functions</li>
<li>Prioritizes functions that affect many code paths</li>
<li>Use for: Understanding impact of potential failures</li>
</ul>
<p><strong>dependency</strong> - Analyzes dependency impact and cascade effects</p>
<ul>
<li>Tracks caller/callee relationships</li>
<li>Calculates cascade impact of changes</li>
<li>Use for: Understanding change propagation and refactoring risk</li>
</ul>
<p><strong>git_history</strong> - Tracks change frequency and churn</p>
<ul>
<li>Analyzes git blame and commit history</li>
<li>Identifies frequently-changed functions</li>
<li>Use for: Finding volatile code that needs stabilization</li>
</ul>
<p><strong>Example workflows:</strong></p>
<pre><code class="language-bash"># Find volatile high-complexity code
debtmap analyze . --context --context-providers git_history

# Understand refactoring impact
debtmap analyze . --context --context-providers dependency

# Disable slow provider for faster analysis
debtmap analyze . --context --disable-context git_history
</code></pre>
<h3 id="multi-pass-analysis-1-1"><a class="header" href="#multi-pass-analysis-1-1">Multi-Pass Analysis</a></h3>
<p>Multi-pass analysis is enabled by default for deeper analysis with context awareness:</p>
<pre><code class="language-bash"># Multi-pass analysis is default behavior (no flag needed)
debtmap analyze .

# Multi-pass with attribution tracking
debtmap analyze . --attribution

# Disable multi-pass for single-pass performance mode
debtmap analyze . --no-multi-pass
</code></pre>
<p><strong>When to use <code>--no-multi-pass</code>:</strong></p>
<ul>
<li><strong>Performance-critical CI environments</strong>: Faster analysis for large codebases</li>
<li><strong>Quick validation</strong>: When you need fast feedback</li>
<li><strong>Single-file analysis</strong>: When deep context isn’t needed</li>
</ul>
<p><strong>Performance comparison example:</strong></p>
<pre><code class="language-bash"># Fast single-pass (skip context analysis)
time debtmap analyze . --no-multi-pass

# Default multi-pass (includes context analysis)
time debtmap analyze .
</code></pre>
<p>Multi-pass analysis provides better prioritization by analyzing dependencies and relationships across files, but single-pass mode can be 2-3x faster for large codebases.</p>
<h3 id="aggregation-methods-1"><a class="header" href="#aggregation-methods-1">Aggregation Methods</a></h3>
<pre><code class="language-bash"># Use logarithmic sum for aggregation
debtmap analyze . --aggregation-method logarithmic_sum

# Standard sum (default)
debtmap analyze . --aggregation-method sum
</code></pre>
<h3 id="filtering-and-grouping"><a class="header" href="#filtering-and-grouping">Filtering and Grouping</a></h3>
<pre><code class="language-bash"># Group results by debt category
debtmap analyze . --group-by-category

# Filter specific categories
debtmap analyze . --filter Architecture,Testing

# Show only high-priority items
debtmap analyze . --min-priority high --top 10
</code></pre>
<h3 id="call-graph-debugging"><a class="header" href="#call-graph-debugging">Call Graph Debugging</a></h3>
<p>Debug and validate call graph construction for accurate dependency analysis:</p>
<pre><code class="language-bash"># Enable call graph debugging output
debtmap analyze . --debug-call-graph

# Trace specific function resolution
debtmap analyze . --trace-function my_function --trace-function another_fn

# Validate call graph structure (detect orphans and cycles)
debtmap analyze . --validate-call-graph

# Show detailed caller/callee relationships
debtmap analyze . --show-dependencies
</code></pre>
<p><strong>Use cases:</strong></p>
<p><strong>Troubleshooting resolution failures:</strong></p>
<pre><code class="language-bash"># When a function isn't being analyzed correctly
debtmap analyze . --debug-call-graph --trace-function problematic_function
</code></pre>
<p><strong>Understanding function relationships:</strong></p>
<pre><code class="language-bash"># See who calls what
debtmap analyze . --show-dependencies --top 10
</code></pre>
<p><strong>Validating call graph integrity:</strong></p>
<pre><code class="language-bash"># Detect cycles and orphaned nodes
debtmap analyze . --validate-call-graph
</code></pre>
<p>Output includes:</p>
<ul>
<li>Resolution statistics (success/failure rates)</li>
<li>DFS cycle detection results</li>
<li>Orphan node detection</li>
<li>Cross-module resolution details</li>
</ul>
<h3 id="verbosity-levels-1"><a class="header" href="#verbosity-levels-1">Verbosity Levels</a></h3>
<p>Control the level of diagnostic output for debugging and understanding analysis decisions:</p>
<pre><code class="language-bash"># Level 1: Show main score factors
debtmap analyze . -v

# Level 2: Show detailed calculations
debtmap analyze . -vv

# Level 3: Show all debug information
debtmap analyze . -vvv

# Long form also available
debtmap analyze . --verbose

# Show macro expansion details (Rust)
debtmap analyze . --verbose-macro-warnings --show-macro-stats
</code></pre>
<p><strong>What each level shows:</strong></p>
<ul>
<li><strong>-v</strong>: Score factor breakdowns and purity distribution in god object analysis</li>
<li><strong>-vv</strong>: Detailed calculations, coverage lookups, and metric computations</li>
<li><strong>-vvv</strong>: Full debug information including AST details and parser internals</li>
</ul>
<h3 id="understanding-metrics-2"><a class="header" href="#understanding-metrics-2">Understanding Metrics</a></h3>
<p>Learn how Debtmap calculates complexity metrics and scores:</p>
<pre><code class="language-bash"># Show metric definitions and formulas
debtmap analyze . --explain-metrics
</code></pre>
<p><strong>What it explains:</strong></p>
<p><strong>Measured metrics</strong> (counted from AST):</p>
<ul>
<li><code>cyclomatic_complexity</code> - Decision points (if, match, while, for, etc.)</li>
<li><code>cognitive_complexity</code> - Weighted readability measure</li>
<li><code>nesting_depth</code> - Maximum nested control structure levels</li>
<li><code>loc</code> - Lines of code in function</li>
<li><code>parameter_count</code> - Number of function parameters</li>
</ul>
<p><strong>Estimated metrics</strong> (formula-based approximations):</p>
<ul>
<li><code>est_branches</code> - Estimated execution paths
<ul>
<li>Formula: <code>max(nesting_depth, 1) × cyclomatic_complexity ÷ 3</code></li>
<li>Purpose: Estimate test cases needed for branch coverage</li>
<li>Note: This is an ESTIMATE, not a count from the AST</li>
</ul>
</li>
</ul>
<p><strong>Scoring formulas:</strong></p>
<ul>
<li>Complexity factor calculation</li>
<li>Coverage factor weight</li>
<li>Dependency factor impact</li>
<li>Role multiplier application</li>
<li>Final score aggregation</li>
</ul>
<p><strong>Use –explain-metrics when:</strong></p>
<ul>
<li>First learning debtmap</li>
<li>Questioning why something is flagged</li>
<li>Understanding score differences</li>
<li>Teaching team members about technical debt metrics</li>
</ul>
<h3 id="ast-functional-analysis"><a class="header" href="#ast-functional-analysis">AST Functional Analysis</a></h3>
<p>Enable AST-based functional composition analysis to detect functional programming patterns:</p>
<pre><code class="language-bash"># Enable AST-based functional composition analysis
debtmap analyze . --ast-functional-analysis

# Combine with verbose mode to see purity analysis
debtmap analyze . --ast-functional-analysis -v
</code></pre>
<p><strong>What it detects:</strong></p>
<ul>
<li>Pure functions (no side effects, immutable)</li>
<li>Impure functions (I/O, mutations, side effects)</li>
<li>Function composition patterns</li>
<li>Immutability patterns</li>
</ul>
<p><strong>Benefits:</strong></p>
<ul>
<li>Distinguishes functional patterns from god objects (see purity weighting in God Object Detection section)</li>
<li>Identifies opportunities for better testability</li>
<li>Highlights side effect boundaries</li>
<li>Supports functional programming code reviews</li>
</ul>
<p><strong>Example output with -v:</strong></p>
<pre><code>PURITY DISTRIBUTION:
  Pure: 70 functions (65%) → complexity weight: 6.3
  Impure: 37 functions (35%) → complexity weight: 14.0
  Total weighted complexity: 20.3
</code></pre>
<h3 id="parallel-processing-control"><a class="header" href="#parallel-processing-control">Parallel Processing Control</a></h3>
<p>Control thread count for CPU-bound systems or to limit resource usage in CI environments. By default, Debtmap uses all available cores for optimal performance.</p>
<pre><code class="language-bash"># Use 8 parallel jobs
debtmap analyze . --jobs 8

# Disable parallel processing
debtmap analyze . --no-parallel
</code></pre>
<p><strong>When to adjust:</strong></p>
<ul>
<li><strong>CI environments</strong>: Limit thread count to avoid resource contention with other jobs</li>
<li><strong>CPU-bound systems</strong>: Reduce threads if machine is under load</li>
<li><strong>Large codebases</strong>: Default parallelism provides best performance</li>
<li><strong>Debugging</strong>: Use <code>--no-parallel</code> to simplify sequential execution when troubleshooting</li>
</ul>
<h2 id="configuration-examples-1"><a class="header" href="#configuration-examples-1">Configuration Examples</a></h2>
<h3 id="basic-configuration"><a class="header" href="#basic-configuration">Basic Configuration</a></h3>
<p>Create <code>.debtmap.toml</code>:</p>
<pre><code class="language-toml">[thresholds]
complexity = 15
duplication = 25
max_function_lines = 50
max_nesting_depth = 4

[languages]
enabled = ["rust", "python"]

[ignore]
patterns = [
  "tests/**/*",
  "**/*.test.rs",
  "target/**",
]
</code></pre>
<h3 id="entropy-based-complexity-1"><a class="header" href="#entropy-based-complexity-1">Entropy-Based Complexity</a></h3>
<pre><code class="language-toml">[entropy]
enabled = true
weight = 0.5
use_classification = true
pattern_threshold = 0.7
entropy_threshold = 0.4
branch_threshold = 0.8
max_combined_reduction = 0.3
</code></pre>
<p>This reduces false positives for repetitive code patterns.</p>
<p><strong>Understanding entropy-adjusted output:</strong></p>
<p>When entropy analysis detects repetitive patterns, detailed output (<code>-vv</code>) shows both original and adjusted complexity:</p>
<pre><code class="language-bash">debtmap analyze . -vv --top 5
</code></pre>
<p><strong>Example output for repetitive validation function:</strong></p>
<pre><code>#15 SCORE: 68.2 [HIGH]
├─ COMPLEXITY: cyclomatic=20 (dampened: 14, factor: 0.70), est_branches=40, cognitive=25, nesting=3, entropy=0.30
  - Entropy Impact: 30% dampening (entropy: 0.30, repetition: 95%)
</code></pre>
<p><strong>Interpreting the adjustment:</strong></p>
<ul>
<li><code>cyclomatic=20</code>: Original complexity before entropy adjustment</li>
<li><code>dampened: 14</code>: Adjusted complexity (20 × 0.70 = 14)</li>
<li><code>factor: 0.70</code>: Dampening factor (30% reduction applied)</li>
<li><code>entropy: 0.30</code>: Low entropy indicates repetitive patterns</li>
<li><code>repetition: 95%</code>: High pattern repetition detected</li>
</ul>
<p><strong>When no dampening is applied:</strong></p>
<pre><code>#5 SCORE: 85.5 [CRITICAL]
├─ COMPLEXITY: cyclomatic=15, est_branches=30, cognitive=22, nesting=4
</code></pre>
<p>No “dampened” indicator means the function has diverse logic without repetitive patterns, so the full complexity is used for scoring.</p>
<p>See <a href="#interpreting-entropy-adjusted-output">Entropy Analysis</a> for more details.</p>
<h3 id="custom-scoring-weights"><a class="header" href="#custom-scoring-weights">Custom Scoring Weights</a></h3>
<pre><code class="language-toml">[scoring]
coverage = 0.40      # Test coverage gaps
complexity = 0.40    # Code complexity
dependency = 0.20    # Dependency criticality
</code></pre>
<h3 id="god-object-detection-tuning"><a class="header" href="#god-object-detection-tuning">God Object Detection Tuning</a></h3>
<pre><code class="language-toml">[god_object]
enabled = true

# Purity-based scoring reduces false positives for functional code
# Pure functions (no side effects) get lower weight in god object scoring
purity_weight_pure = 0.3    # Pure function complexity weight (default: 0.3)
purity_weight_impure = 1.0  # Impure function complexity weight (default: 1.0)

# Rust-specific thresholds
[god_object.rust]
max_methods = 25
max_fields = 15
max_traits = 5
max_lines = 400
max_complexity = 50

# Python-specific thresholds
[god_object.python]
max_methods = 20
max_fields = 12
max_lines = 350
max_complexity = 45

# JavaScript/TypeScript-specific thresholds
[god_object.javascript]
max_methods = 20
max_fields = 12
max_lines = 300
max_complexity = 40
</code></pre>
<p><strong>Why purity weighting matters:</strong>
See the Purity-Weighted God Object Scoring section for detailed explanation. In short:</p>
<ul>
<li>Modules with many pure helper functions avoid false god object flags</li>
<li>Focus shifts to modules with excessive stateful/impure code</li>
<li>Functional programming patterns are properly recognized</li>
</ul>
<p><strong>Example:</strong></p>
<ul>
<li>Module with 100 pure functions → Normal (functional design) ✅</li>
<li>Module with 100 impure functions → God object detected ✅</li>
</ul>
<h3 id="external-api-configuration-1"><a class="header" href="#external-api-configuration-1">External API Configuration</a></h3>
<p>For libraries (not CLI tools):</p>
<pre><code class="language-toml">[external_api]
detect_external_api = true

api_functions = [
  "parse",
  "Parser::new",
  "client::connect",
]

api_files = [
  "src/lib.rs",
  "src/api.rs",
  "src/public/*.rs",
]
</code></pre>
<h3 id="complete-multi-language-configuration"><a class="header" href="#complete-multi-language-configuration">Complete Multi-Language Configuration</a></h3>
<pre><code class="language-toml">[thresholds]
complexity = 12
duplication = 30
max_file_lines = 400
max_function_lines = 40
minimum_debt_score = 1.0
minimum_cyclomatic_complexity = 2

[entropy]
enabled = true
weight = 0.5

[scoring]
coverage = 0.40
complexity = 0.40
dependency = 0.20

[languages]
enabled = ["rust", "python", "javascript", "typescript"]

[ignore]
patterns = [
  # Tests
  "tests/**/*",
  "**/*.test.*",
  "**/*_test.*",

  # Build artifacts
  "target/**",
  "dist/**",
  "build/**",
  "node_modules/**",

  # Python
  ".venv/**",
  "**/__pycache__/**",

  # Generated code
  "*.generated.*",
  "*.pb.*",
]

[god_object]
enabled = true
max_methods = 18
max_fields = 12
</code></pre>
<h2 id="compare-command"><a class="header" href="#compare-command">Compare Command</a></h2>
<p>The <code>compare</code> command helps validate that refactoring achieved its goals.</p>
<h3 id="basic-comparison-workflow"><a class="header" href="#basic-comparison-workflow">Basic Comparison Workflow</a></h3>
<pre><code class="language-bash"># 1. Generate baseline before refactoring
debtmap analyze . --format json --output before.json

# 2. Make your code improvements
#    ... refactor, add tests, etc ...

# 3. Generate new analysis
debtmap analyze . --format json --output after.json

# 4. Compare and verify improvements
debtmap compare --before before.json --after after.json
</code></pre>
<h3 id="target-specific-comparison"><a class="header" href="#target-specific-comparison">Target-Specific Comparison</a></h3>
<p>Focus on whether a specific function improved:</p>
<pre><code class="language-bash"># Target format: file:function:line
debtmap compare \
  --before before.json \
  --after after.json \
  --target-location src/main.rs:process_data:100
</code></pre>
<h3 id="using-with-implementation-plans"><a class="header" href="#using-with-implementation-plans">Using with Implementation Plans</a></h3>
<p>Extract target automatically from plan files:</p>
<pre><code class="language-bash"># If IMPLEMENTATION_PLAN.md contains:
# **Target**: src/parser.rs:parse_expression:45

debtmap compare \
  --before before.json \
  --after after.json \
  --plan IMPLEMENTATION_PLAN.md
</code></pre>
<h3 id="output-formats-2-1"><a class="header" href="#output-formats-2-1">Output Formats</a></h3>
<pre><code class="language-bash"># JSON output (default)
debtmap compare --before before.json --after after.json

# Terminal output (explicit)
debtmap compare \
  --before before.json \
  --after after.json \
  --format terminal

# JSON for CI integration (explicit output file)
debtmap compare \
  --before before.json \
  --after after.json \
  --format json \
  --output comparison.json

# Markdown report
debtmap compare \
  --before before.json \
  --after after.json \
  --format markdown \
  --output COMPARISON.md
</code></pre>
<h3 id="interpreting-results-4"><a class="header" href="#interpreting-results-4">Interpreting Results</a></h3>
<p><strong>Target Status:</strong></p>
<ul>
<li><strong>Resolved</strong>: Function no longer appears (complexity reduced below threshold)</li>
<li><strong>Improved</strong>: Metrics improved (complexity down, coverage up)</li>
<li><strong>Unchanged</strong>: No significant change</li>
<li><strong>Regressed</strong>: Metrics got worse</li>
<li><strong>Not Found</strong>: Target not found in baseline</li>
</ul>
<p><strong>Overall Trend:</strong></p>
<ul>
<li><strong>Improving</strong>: More items resolved/improved than regressed</li>
<li><strong>Stable</strong>: No significant changes</li>
<li><strong>Regressing</strong>: New critical debt introduced</li>
</ul>
<p><strong>Example Output:</strong></p>
<pre><code>Target Status: Resolved ✅
- src/parser.rs:parse_expression:45 reduced from complexity 22 to 8
- Coverage improved from 0% to 85%

Overall Trend: Improving
- 3 items resolved
- 2 items improved
- 0 regressions
- Total debt score: 450 → 285 (-37%)
</code></pre>
<h3 id="ci-integration-1-1"><a class="header" href="#ci-integration-1-1">CI Integration</a></h3>
<p>Use in pull request validation:</p>
<pre><code class="language-bash"># In CI script
debtmap compare \
  --before baseline.json \
  --after current.json \
  --format json | jq -e '.overall_trend == "Improving"'

# Exit code 0 if improving, 1 otherwise
</code></pre>
<h2 id="tips-and-best-practices"><a class="header" href="#tips-and-best-practices">Tips and Best Practices</a></h2>
<ol>
<li><strong>Start Simple</strong>: Begin with basic analysis, add coverage later</li>
<li><strong>Use Filters</strong>: Focus on one category at a time (Architecture, Testing)</li>
<li><strong>Iterate</strong>: Run analysis, fix top items, repeat</li>
<li><strong>CI Integration</strong>: Automate validation in your build pipeline</li>
<li><strong>Track Progress</strong>: Use <code>compare</code> command to validate improvements</li>
<li><strong>Configure Thresholds</strong>: Adjust to match your team’s standards</li>
<li><strong>Leverage Coverage</strong>: Always include coverage data for accurate risk assessment</li>
</ol>
<h2 id="next-steps-4"><a class="header" href="#next-steps-4">Next Steps</a></h2>
<ul>
<li><a href="#cli-reference">CLI Reference</a> - Complete CLI documentation</li>
<li><a href="#analysis-guide">Analysis Guide</a> - Understanding analysis results</li>
<li><a href="#configuration-2">Configuration</a> - Advanced configuration options</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="frequently-asked-questions-1"><a class="header" href="#frequently-asked-questions-1">Frequently Asked Questions</a></h1>
<p>Common questions about debtmap’s features, usage, and AI integration.</p>
<h2 id="ai-integration"><a class="header" href="#ai-integration">AI Integration</a></h2>
<h3 id="how-does-debtmap-work-with-ai-coding-assistants"><a class="header" href="#how-does-debtmap-work-with-ai-coding-assistants">How does debtmap work with AI coding assistants?</a></h3>
<p>Debtmap is designed as a <strong>sensor</strong> that provides structured data for AI consumption. Instead of telling you what to fix, it tells AI assistants:</p>
<ol>
<li><strong>Where to look</strong> - Prioritized list of debt items with file locations</li>
<li><strong>What to read</strong> - Context suggestions (callers, callees, tests)</li>
<li><strong>What signals matter</strong> - Complexity, coverage, coupling metrics</li>
</ol>
<p><strong>Example workflow:</strong></p>
<pre><code class="language-bash"># Pipe directly to Claude Code
debtmap analyze . --format llm-markdown --top 3 | claude "Fix the top item"
</code></pre>
<h3 id="what-output-format-is-best-for-ai"><a class="header" href="#what-output-format-is-best-for-ai">What output format is best for AI?</a></h3>
<p>Use <code>--format llm-markdown</code> for AI workflows. This format:</p>
<ul>
<li>Minimizes tokens while maximizing information</li>
<li>Includes context suggestions inline</li>
<li>Uses consistent structure for reliable parsing</li>
<li>Avoids verbose descriptions that waste context window</li>
</ul>
<pre><code class="language-bash">debtmap analyze . --format llm-markdown --top 5
</code></pre>
<h3 id="does-debtmap-provide-fix-suggestions"><a class="header" href="#does-debtmap-provide-fix-suggestions">Does debtmap provide fix suggestions?</a></h3>
<p>No. Debtmap is a <strong>sensor</strong>, not a prescriber. It provides signals (metrics, coverage, coupling) and lets the AI decide how to fix issues.</p>
<p>This design is intentional:</p>
<ul>
<li>AI can consider business context you provide</li>
<li>Different situations require different approaches</li>
<li>Template recommendations are often wrong</li>
</ul>
<h3 id="how-do-i-use-context-suggestions"><a class="header" href="#how-do-i-use-context-suggestions">How do I use context suggestions?</a></h3>
<p>Each debt item includes file ranges the AI should read:</p>
<pre><code>Context:
├─ Primary: src/parser.rs:38-85 (the debt item)
├─ Caller: src/handler.rs:100-120 (usage context)
└─ Test: tests/parser_test.rs:50-75 (expected behavior)
</code></pre>
<p>Tell your AI to read these files before making changes:</p>
<pre><code class="language-bash">debtmap analyze . --format llm-markdown --top 1 | \
  claude "Read the context files first, then fix the top item"
</code></pre>
<h3 id="can-i-integrate-debtmap-with-cursor"><a class="header" href="#can-i-integrate-debtmap-with-cursor">Can I integrate debtmap with Cursor?</a></h3>
<p>Yes. Generate a report file and reference it in Cursor:</p>
<pre><code class="language-bash"># Generate report
debtmap analyze . --format llm-markdown --top 10 &gt; debt-report.md

# In Cursor, use: @debt-report.md Fix the top critical item
</code></pre>
<h2 id="features--capabilities"><a class="header" href="#features--capabilities">Features &amp; Capabilities</a></h2>
<h3 id="whats-the-difference-between-measured-and-estimated-metrics"><a class="header" href="#whats-the-difference-between-measured-and-estimated-metrics">What’s the difference between measured and estimated metrics?</a></h3>
<p><strong>Measured Metrics</strong> - Precise values from AST analysis:</p>
<ul>
<li><code>cyclomatic_complexity</code>: Exact count of decision points</li>
<li><code>cognitive_complexity</code>: Weighted readability measure</li>
<li><code>nesting_depth</code>: Maximum nesting levels</li>
<li><code>loc</code>: Lines of code</li>
</ul>
<p><strong>Estimated Metrics</strong> - Heuristic approximations:</p>
<ul>
<li><code>est_branches</code>: Estimated execution paths (formula-based)</li>
</ul>
<p>Use measured metrics for thresholds and gates. Use estimated metrics for prioritization.</p>
<h3 id="what-is-entropy-based-complexity-analysis"><a class="header" href="#what-is-entropy-based-complexity-analysis">What is entropy-based complexity analysis?</a></h3>
<p>Entropy analysis uses information theory to distinguish between genuinely complex code and repetitive pattern-based code.</p>
<p>A function with 20 identical if/return validation checks has the same cyclomatic complexity as a function with 20 diverse conditional branches. Entropy analysis gives the validation function a much lower effective complexity score because it follows a simple, repetitive pattern.</p>
<p><strong>Result:</strong> 60-75% reduction in false positives compared to traditional complexity metrics.</p>
<h3 id="what-languages-are-supported"><a class="header" href="#what-languages-are-supported">What languages are supported?</a></h3>
<p><strong>Currently supported:</strong></p>
<ul>
<li>Rust - Full support with AST parsing, macro expansion, and trait resolution</li>
</ul>
<p><strong>Planned:</strong></p>
<ul>
<li>Python, JavaScript/TypeScript, Go (after Rust analysis is mature)</li>
</ul>
<h3 id="why-is-debtmap-rust-only-right-now"><a class="header" href="#why-is-debtmap-rust-only-right-now">Why is debtmap Rust-only right now?</a></h3>
<p>We’re taking a focused approach to deliver the best possible Rust code analyzer before expanding. This allows us to:</p>
<ol>
<li>Perfect core algorithms with one language</li>
<li>Build Rust-specific features (macros, traits, lifetimes)</li>
<li>Establish trust in the Rust community</li>
<li>Apply learnings to future languages</li>
</ol>
<h3 id="how-does-coverage-integration-work"><a class="header" href="#how-does-coverage-integration-work">How does coverage integration work?</a></h3>
<p>Debtmap reads LCOV format coverage data and maps it to functions:</p>
<pre><code class="language-bash"># Generate coverage
cargo llvm-cov --lcov --output-path coverage.lcov

# Analyze with coverage
debtmap analyze . --lcov coverage.lcov
</code></pre>
<p>Coverage affects prioritization:</p>
<ul>
<li>Complex function with good coverage = lower priority</li>
<li>Simple function with no coverage = higher priority</li>
<li>High complexity + zero coverage = critical priority</li>
</ul>
<h2 id="usage--configuration"><a class="header" href="#usage--configuration">Usage &amp; Configuration</a></h2>
<h3 id="how-do-i-exclude-test-files-from-analysis"><a class="header" href="#how-do-i-exclude-test-files-from-analysis">How do I exclude test files from analysis?</a></h3>
<p>By default, debtmap excludes common test directories. To customize:</p>
<pre><code class="language-toml"># .debtmap.toml
[analysis]
exclude_patterns = [
    "**/tests/**",
    "**/*_test.rs",
    "**/target/**",
]
</code></pre>
<h3 id="can-i-customize-the-complexity-thresholds"><a class="header" href="#can-i-customize-the-complexity-thresholds">Can I customize the complexity thresholds?</a></h3>
<p>Yes. Configure in <code>.debtmap.toml</code>:</p>
<pre><code class="language-toml">[thresholds]
cyclomatic_complexity = 10
nesting_depth = 3
loc = 200

[tiers]
critical = 8.0
high = 5.0
moderate = 2.0
</code></pre>
<h3 id="does-debtmap-integrate-with-cicd"><a class="header" href="#does-debtmap-integrate-with-cicd">Does debtmap integrate with CI/CD?</a></h3>
<p>Yes. Use the <code>validate</code> command:</p>
<pre><code class="language-bash">debtmap validate . --max-debt-density 10.0
</code></pre>
<p><strong>Exit codes:</strong></p>
<ul>
<li><code>0</code> = validation passed</li>
<li><code>1</code> = validation failed (debt exceeds thresholds)</li>
<li><code>2</code> = analysis error</li>
</ul>
<p><strong>GitHub Actions example:</strong></p>
<pre><code class="language-yaml">- name: Check technical debt
  run: |
    cargo llvm-cov --lcov --output-path coverage.lcov
    debtmap validate . --lcov coverage.lcov --max-debt-density 10.0
</code></pre>
<h3 id="what-if-debtmap-reports-false-positives"><a class="header" href="#what-if-debtmap-reports-false-positives">What if debtmap reports false positives?</a></h3>
<ol>
<li>
<p><strong>Verify entropy analysis is enabled</strong> (default):</p>
<pre><code class="language-toml">[analysis]
enable_entropy_analysis = true
</code></pre>
</li>
<li>
<p><strong>Adjust thresholds</strong> for your project:</p>
<pre><code class="language-toml">[thresholds]
cyclomatic_complexity = 15
</code></pre>
</li>
<li>
<p><strong>Use ignore comments</strong> for specific functions:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// debtmap:ignore - acceptable validation pattern
fn validate_many_fields() { ... }
<span class="boring">}</span></code></pre>
</li>
<li>
<p><strong>Report issues</strong> - If you believe analysis is incorrect, <a href="https://github.com/iepathos/debtmap/issues">open an issue</a> with a code example.</p>
</li>
</ol>
<h3 id="how-accurate-is-the-risk-scoring"><a class="header" href="#how-accurate-is-the-risk-scoring">How accurate is the risk scoring?</a></h3>
<p>Risk scores are <strong>relative prioritization metrics</strong>, not absolute measures. They help you answer “which code should I focus on first?” rather than “exactly how risky is this code?”</p>
<p>Use risk scores for prioritization, but apply your domain knowledge when deciding what to fix.</p>
<h2 id="comparison-with-other-tools"><a class="header" href="#comparison-with-other-tools">Comparison with Other Tools</a></h2>
<h3 id="how-is-debtmap-different-from-sonarqube"><a class="header" href="#how-is-debtmap-different-from-sonarqube">How is debtmap different from SonarQube?</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Aspect</th><th>Debtmap</th><th>SonarQube</th></tr>
</thead>
<tbody>
<tr><td><strong>Output</strong></td><td>Signals for AI</td><td>Recommendations</td></tr>
<tr><td><strong>Speed</strong></td><td>Seconds</td><td>Minutes</td></tr>
<tr><td><strong>Coverage</strong></td><td>Built-in</td><td>Enterprise only</td></tr>
<tr><td><strong>Entropy</strong></td><td>Yes</td><td>No</td></tr>
<tr><td><strong>Setup</strong></td><td>Single binary</td><td>Server required</td></tr>
</tbody>
</table>
</div>
<p><strong>When to use SonarQube:</strong> Multi-language enterprise dashboards.
<strong>When to use debtmap:</strong> AI-assisted Rust development.</p>
<h3 id="should-i-replace-clippy-with-debtmap"><a class="header" href="#should-i-replace-clippy-with-debtmap">Should I replace clippy with debtmap?</a></h3>
<p><strong>No—use both.</strong> They serve different purposes:</p>
<p><strong>clippy:</strong></p>
<ul>
<li>Rust idioms and patterns</li>
<li>Common mistakes</li>
<li>Runs in milliseconds</li>
</ul>
<p><strong>debtmap:</strong></p>
<ul>
<li>Technical debt prioritization</li>
<li>Coverage-based risk</li>
<li>Context for AI</li>
</ul>
<pre><code class="language-bash">cargo clippy -- -D warnings
debtmap analyze . --lcov coverage.lcov --top 10
</code></pre>
<h3 id="how-does-debtmap-compare-to-coverage-tools"><a class="header" href="#how-does-debtmap-compare-to-coverage-tools">How does debtmap compare to coverage tools?</a></h3>
<p>Coverage tools (tarpaulin, llvm-cov) tell you what’s tested. Debtmap tells you which untested code is most risky.</p>
<p><strong>Coverage tools:</strong> “You have 75% coverage”
<strong>Debtmap:</strong> “Function X has 0% coverage and complexity 12—fix this first”</p>
<h2 id="troubleshooting-22"><a class="header" href="#troubleshooting-22">Troubleshooting</a></h2>
<h3 id="analysis-is-slow-on-my-large-codebase"><a class="header" href="#analysis-is-slow-on-my-large-codebase">Analysis is slow on my large codebase</a></h3>
<p><strong>Optimization strategies:</strong></p>
<ol>
<li>
<p><strong>Exclude unnecessary files:</strong></p>
<pre><code class="language-toml">[analysis]
exclude_patterns = ["**/target/**", "**/vendor/**"]
</code></pre>
</li>
<li>
<p><strong>Analyze specific directories:</strong></p>
<pre><code class="language-bash">debtmap analyze src/
</code></pre>
</li>
<li>
<p><strong>Reduce parallelism:</strong></p>
<pre><code class="language-bash">debtmap analyze . --jobs 4
</code></pre>
</li>
</ol>
<h3 id="coverage-data-isnt-being-applied"><a class="header" href="#coverage-data-isnt-being-applied">Coverage data isn’t being applied</a></h3>
<p>Check:</p>
<ol>
<li>LCOV file path is correct</li>
<li>LCOV file contains data: <code>grep -c "^SF:" coverage.lcov</code></li>
<li>Source paths match between LCOV and project</li>
</ol>
<h3 id="debtmap-reports-no-functions-found"><a class="header" href="#debtmap-reports-no-functions-found">Debtmap reports “No functions found”</a></h3>
<p>Check:</p>
<ol>
<li>Project contains Rust files (<code>.rs</code>)</li>
<li>Files aren’t excluded by ignore patterns</li>
<li>No syntax errors: <code>debtmap analyze . -vv</code></li>
</ol>
<h2 id="getting-help-2"><a class="header" href="#getting-help-2">Getting Help</a></h2>
<ul>
<li><strong>Documentation:</strong> <a href="https://iepathos.github.io/debtmap/">debtmap.dev</a></li>
<li><strong>GitHub Issues:</strong> <a href="https://github.com/iepathos/debtmap/issues">Report bugs</a></li>
<li><strong>LLM Integration:</strong> See <a href="#llm-integration-guide">LLM Integration Guide</a></li>
<li><strong>Examples:</strong> See <a href="#examples-5">Examples</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="troubleshooting-23"><a class="header" href="#troubleshooting-23">Troubleshooting</a></h1>
<p>Common issues and solutions for using debtmap effectively.</p>
<h2 id="quick-fixes-for-common-issues"><a class="header" href="#quick-fixes-for-common-issues">Quick Fixes for Common Issues</a></h2>
<p>If you’re experiencing problems, try these first:</p>
<ol>
<li><strong>Analysis is slow</strong>: Adjust threads with <code>--jobs</code>, use <code>--no-multi-pass</code> for faster single-pass analysis, or use <code>--semantic-off</code> for faster fallback mode</li>
<li><strong>Parse errors</strong>: Use <code>--semantic-off</code> for faster fallback mode or exclude problematic files</li>
<li><strong>No output</strong>: Increase verbosity with <code>-v</code> or lower <code>--min-priority</code></li>
<li><strong>Inconsistent results</strong>: Check if coverage file changed or context providers are enabled</li>
</ol>
<h2 id="common-issues-3"><a class="header" href="#common-issues-3">Common Issues</a></h2>
<h3 id="parse-errors"><a class="header" href="#parse-errors">Parse Errors</a></h3>
<p><strong>Problem</strong>: Encountering “Parse error in file:line:column” messages</p>
<p><strong>Causes</strong>:</p>
<ul>
<li>Unsupported language syntax or version</li>
<li>Complex macro expansions (Rust)</li>
<li>Type inference edge cases (Python, TypeScript)</li>
</ul>
<p><strong>Solutions</strong>:</p>
<pre><code class="language-bash"># Try fallback mode without semantic analysis
debtmap --semantic-off

# For Rust macro issues, see detailed warnings
debtmap --verbose-macro-warnings --show-macro-stats

# Exclude specific problematic files
# Add to .debtmap/config.toml:
# exclude = ["path/to/problematic/file.rs"]
</code></pre>
<h3 id="out-of-memory-errors"><a class="header" href="#out-of-memory-errors">Out of Memory Errors</a></h3>
<p><strong>Problem</strong>: Analysis crashes or runs out of memory on large codebases</p>
<p><strong>Solutions</strong>:</p>
<pre><code class="language-bash"># Limit parallel processing
debtmap --jobs 2

# Disable parallel processing entirely
debtmap --no-parallel

# Test with limited files first
debtmap --max-files 100

# Analyze subdirectories separately
debtmap path/to/subset
</code></pre>
<h3 id="performance-issues-1"><a class="header" href="#performance-issues-1">Performance Issues</a></h3>
<p><strong>Problem</strong>: Analysis takes too long to complete</p>
<p><strong>Solutions</strong>:</p>
<pre><code class="language-bash"># Use all available CPU cores
debtmap --jobs 0

# Disable multi-pass analysis for faster single-pass (slightly less detailed)
debtmap --no-multi-pass

# Try faster fallback mode (less accurate)
debtmap --semantic-off

# Use plain output for faster terminal rendering
debtmap --plain

# Or set environment variable to disable multi-pass globally
export DEBTMAP_SINGLE_PASS=1
</code></pre>
<p>See <a href="#performance-tips">Performance Tips</a> for detailed optimization strategies.</p>
<h3 id="file-permission-errors"><a class="header" href="#file-permission-errors">File Permission Errors</a></h3>
<p><strong>Problem</strong>: “File system error” when accessing files</p>
<p><strong>Solutions</strong>:</p>
<ul>
<li>Ensure you have read permissions for all source files</li>
<li>Check that the project directory is accessible</li>
</ul>
<h3 id="git-history-errors"><a class="header" href="#git-history-errors">Git History Errors</a></h3>
<p><strong>Problem</strong>: Errors when using <code>git_history</code> context provider</p>
<p><strong>Causes</strong>:</p>
<ul>
<li>Not running in a git repository</li>
<li>Git history not available for files</li>
<li>Insufficient git permissions</li>
</ul>
<p><strong>Solutions</strong>:</p>
<pre><code class="language-bash"># Disable git_history context provider
debtmap --context --disable-context git_history

# Disable all context providers
debtmap --no-context-aware

# Check if in git repository
git status
</code></pre>
<h3 id="coverage-file-issues"><a class="header" href="#coverage-file-issues">Coverage File Issues</a></h3>
<p><strong>Problem</strong>: Coverage file not being processed or causing errors</p>
<p><strong>Causes</strong>:</p>
<ul>
<li>Non-LCOV format coverage file</li>
<li>Malformed coverage data</li>
<li>Path mismatches between coverage and source files</li>
</ul>
<p><strong>Solutions</strong>:</p>
<pre><code class="language-bash"># Verify coverage file format (must be LCOV)
head coverage.info

# Check coverage file path
debtmap --coverage-file path/to/coverage.info -v

# Ensure paths in coverage file match source paths
# Coverage paths are relative to project root
</code></pre>
<h3 id="debugging-coverage-matching-with-explain-coverage"><a class="header" href="#debugging-coverage-matching-with-explain-coverage">Debugging Coverage Matching with explain-coverage</a></h3>
<p>The <code>explain-coverage</code> command helps diagnose coverage file parsing and function name matching issues.</p>
<p><strong>Use cases</strong>:</p>
<ul>
<li>Coverage data not being applied to functions</li>
<li>Function name mismatches between coverage and source</li>
<li>Verifying coverage file format is correct</li>
<li>Understanding which functions have coverage data</li>
</ul>
<p><strong>Basic Usage</strong>:</p>
<pre><code class="language-bash"># Debug coverage data parsing for a specific function
debtmap explain-coverage . \
  --coverage-file coverage.lcov \
  --function "process_file"

# See detailed function matching diagnostics
debtmap explain-coverage . \
  --coverage-file coverage.lcov \
  --function "process_file" \
  -v

# Narrow search to specific file
debtmap explain-coverage . \
  --coverage-file coverage.lcov \
  --function "calculate_score" \
  --file src/scoring.rs

# Get JSON output for automation
debtmap explain-coverage . \
  --coverage-file coverage.lcov \
  --function "process_file" \
  --format json
</code></pre>
<p><strong>Output Format</strong>:</p>
<p>The command shows:</p>
<ul>
<li>Whether coverage was found for the function</li>
<li>Which matching strategy succeeded (exact_match, suffix_match, method_name_match, etc.)</li>
<li>Coverage percentage if found</li>
<li>All attempted matching strategies (with <code>-v</code>)</li>
<li>Available functions in coverage file that partially match your query</li>
</ul>
<p><strong>Example Output</strong>:</p>
<pre><code>Coverage Detection Explanation
==============================

Function: process_file
File: src/processor.rs

✓ Coverage Found!
  Strategy: suffix_match
  Coverage: 87.5%

Matching Attempts:
------------------
  ✗ exact_match
  ✓ suffix_match
      File: src/processor.rs
      Function: process_file
      Coverage: 87.5%
</code></pre>
<p><strong>Common Issues Diagnosed</strong>:</p>
<p><strong>Q: Function not found in coverage data?</strong></p>
<p>A: The explain-coverage command will show available functions:</p>
<pre><code class="language-bash">debtmap explain-coverage . \
  --coverage-file coverage.lcov \
  --function "my_function" \
  -v

# Output shows:
#   Available Functions in LCOV (143 total):
#   --------------------------------------
#     Functions containing 'my_function':
#       - src/module.rs::my_function_impl
#       - src/other.rs::my_function_v2
</code></pre>
<p><strong>Q: Path mismatches between source and coverage?</strong></p>
<p>A: Use <code>-v</code> to see all path matching strategies:</p>
<pre><code class="language-bash">debtmap explain-coverage . \
  --coverage-file coverage.lcov \
  --function "process_file" \
  --file src/processor.rs \
  -v

# Shows which strategies were tried:
#   ✗ exact_match (src/processor.rs)
#   ✓ suffix_match (./src/processor.rs)
#   - normalized_path_match
</code></pre>
<p><strong>Q: Function names don’t match between coverage and source?</strong></p>
<p>A: The command tries multiple matching strategies:</p>
<ul>
<li><strong>exact_match</strong>: Exact function name and file path</li>
<li><strong>suffix_match</strong>: File path suffix matching</li>
<li><strong>method_name_match</strong>: Method name without module prefix</li>
<li><strong>normalized_path_match</strong>: Normalized path comparison</li>
<li><strong>global_function_name_match</strong>: Search across all files</li>
<li><strong>global_method_name_match</strong>: Method name search across all files</li>
</ul>
<pre><code class="language-bash"># See which strategy would work
debtmap explain-coverage . \
  --coverage-file coverage.lcov \
  --function "MyStruct::method_name" \
  -v
</code></pre>
<p><strong>Integration with Analysis</strong>:</p>
<p>After diagnosing issues with explain-coverage, fix them and re-run analysis:</p>
<pre><code class="language-bash"># Diagnose coverage issues
debtmap explain-coverage . \
  --coverage-file coverage.lcov \
  --function "problematic_function" \
  -v

# Fix coverage paths or function names based on output

# Run full analysis with coverage
debtmap --coverage-file coverage.lcov -v
</code></pre>
<p>See <a href="#explain-coverage-debugging">CLI Reference - explain-coverage</a> for complete flag documentation.</p>
<h3 id="threshold-and-preset-confusion"><a class="header" href="#threshold-and-preset-confusion">Threshold and Preset Confusion</a></h3>
<p><strong>Problem</strong>: Unexpected filtering or priority levels</p>
<p><strong>Solutions</strong>:</p>
<pre><code class="language-bash"># Check what threshold preset does
debtmap --threshold-preset strict --help

# Override specific thresholds
debtmap --min-priority 3

# See all items regardless of thresholds
debtmap --min-priority 0

# Use category filters instead
debtmap --filter "complexity,debt"
</code></pre>
<h3 id="json-format-issues"><a class="header" href="#json-format-issues">JSON Format Issues</a></h3>
<p><strong>Problem</strong>: JSON output parsing errors or unexpected structure</p>
<p><strong>Understanding the Two Formats</strong>:</p>
<p><strong>Legacy format</strong> wraps items in variant-specific objects:</p>
<pre><code class="language-json">{"File": {"path": "src/main.rs", "score": 7.5, ...}}
{"Function": {"name": "parse", "score": 8.2, ...}}
</code></pre>
<p><strong>Unified format</strong> uses consistent structure with <code>type</code> field:</p>
<pre><code class="language-json">{"type": "File", "path": "src/main.rs", "score": 7.5, ...}
{"type": "Function", "name": "parse", "score": 8.2, ...}
</code></pre>
<p>The unified format is <strong>recommended</strong> for parsing and tool integration as it provides a consistent structure across all item types.</p>
<p><strong>Solutions</strong>:</p>
<pre><code class="language-bash"># Use unified JSON format (consistent structure, recommended)
debtmap --format json --output-format unified

# Legacy format (default, uses {File: {...}} structure)
debtmap --format json --output-format legacy

# Validate JSON output
debtmap --format json | jq .

# Write to file for easier inspection
debtmap --format json --output results.json
</code></pre>
<p>See the <a href="#output-formats-3">Output Formats</a> chapter for detailed JSON structure documentation.</p>
<h3 id="context-provider-errors"><a class="header" href="#context-provider-errors">Context Provider Errors</a></h3>
<p><strong>Problem</strong>: Errors with critical_path, dependency, or git_history providers</p>
<p><strong>Solutions</strong>:</p>
<pre><code class="language-bash"># Enable specific providers only
debtmap --context --context-providers critical_path,dependency

# Disable problematic provider
debtmap --context --disable-context git_history

# Disable context-aware filtering
debtmap --no-context-aware

# Check context provider details
debtmap --context -vvv
</code></pre>
<p>See <a href="#context-provider-troubleshooting">Context Provider Troubleshooting</a> for details.</p>
<h2 id="debug-mode"><a class="header" href="#debug-mode">Debug Mode</a></h2>
<p>Use verbosity flags to diagnose issues and understand analysis behavior.</p>
<h3 id="verbosity-levels-2"><a class="header" href="#verbosity-levels-2">Verbosity Levels</a></h3>
<pre><code class="language-bash"># Level 1: Show main score factors
debtmap -v

# Level 2: Show detailed calculations
debtmap -vv

# Level 3: Show all debug information
debtmap -vvv
</code></pre>
<p><strong>What each level shows</strong>:</p>
<ul>
<li><code>-v</code>: Score breakdowns, main contributing factors</li>
<li><code>-vv</code>: Detailed metric calculations, file processing</li>
<li><code>-vvv</code>: Full debug output, context provider details</li>
</ul>
<h3 id="diagnostic-options"><a class="header" href="#diagnostic-options">Diagnostic Options</a></h3>
<pre><code class="language-bash"># Show macro parsing warnings (Rust)
debtmap --verbose-macro-warnings

# Show macro expansion statistics (Rust)
debtmap --show-macro-stats

# Disable semantic analysis (fallback mode)
debtmap --semantic-off

# Validate LOC consistency
debtmap --validate-loc
</code></pre>
<p><strong>Note</strong>: The <code>--explain-score</code> flag has been deprecated in favor of granular verbosity levels (<code>-v</code>, <code>-vv</code>, <code>-vvv</code>). The new verbosity system provides:</p>
<ul>
<li>Better control over output detail level</li>
<li>Consistent debugging across all commands</li>
<li>Progressive information disclosure (level 1 → 2 → 3)</li>
</ul>
<p>Use <code>-v</code> for score breakdowns, <code>-vv</code> for detailed calculations, and <code>-vvv</code> for full debug output.</p>
<h3 id="debugging-score-calculations"><a class="header" href="#debugging-score-calculations">Debugging Score Calculations</a></h3>
<pre><code class="language-bash"># Use verbosity levels to see score breakdown
debtmap -v    # Shows score factors

# See how coverage affects scores
debtmap --coverage-file coverage.info -v

# See how context affects scores
debtmap --context --context-providers critical_path -v
</code></pre>
<h3 id="example-debug-session"><a class="header" href="#example-debug-session">Example Debug Session</a></h3>
<pre><code class="language-bash"># Step 1: Run with verbosity to see what's happening
debtmap -vv

# Step 2: Try without semantic analysis
debtmap --semantic-off -v

# Step 3: Check specific file
debtmap path/to/file.rs -vvv

# Step 4: Validate results
debtmap --validate-loc
</code></pre>
<h2 id="performance-tips"><a class="header" href="#performance-tips">Performance Tips</a></h2>
<p>Optimize debtmap analysis speed and resource usage.</p>
<h3 id="parallel-processing-1"><a class="header" href="#parallel-processing-1">Parallel Processing</a></h3>
<pre><code class="language-bash"># Use all CPU cores (default)
debtmap --jobs 0

# Limit to 4 threads
debtmap --jobs 4

# Disable parallel processing (debugging)
# Note: --no-parallel is equivalent to --jobs 1 (single-threaded)
debtmap --no-parallel
</code></pre>
<p><strong>When to adjust parallelism</strong>:</p>
<ul>
<li><strong>Use <code>--jobs 0</code></strong> (default): Maximum performance on dedicated machine</li>
<li><strong>Use <code>--jobs N</code></strong>: Limit resource usage while other tasks run</li>
<li><strong>Use <code>--no-parallel</code></strong>: Debugging concurrency issues</li>
</ul>
<h3 id="analysis-optimizations"><a class="header" href="#analysis-optimizations">Analysis Optimizations</a></h3>
<pre><code class="language-bash"># Faster: disable multi-pass analysis (single-pass mode)
debtmap --no-multi-pass

# Fast mode: disable semantic analysis
debtmap --semantic-off

# Plain output: faster terminal rendering
debtmap --plain

# Limit files for testing
debtmap --max-files 100

# Analyze subdirectory only
debtmap src/specific/module

# Reduce output with filters
debtmap --min-priority 4 --top 20
</code></pre>
<h3 id="performance-comparison"><a class="header" href="#performance-comparison">Performance Comparison</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Configuration</th><th>Speed</th><th>Accuracy</th></tr>
</thead>
<tbody>
<tr><td>Default (multi-pass)</td><td>Fast</td><td>Highest</td></tr>
<tr><td><code>--no-multi-pass</code></td><td>Faster</td><td>High</td></tr>
<tr><td><code>--semantic-off</code></td><td>Fastest</td><td>Medium</td></tr>
<tr><td><code>--no-parallel</code></td><td>Slowest</td><td>High</td></tr>
<tr><td><code>--jobs 4</code></td><td>Medium</td><td>High</td></tr>
</tbody>
</table>
</div>
<h3 id="monitoring-performance"><a class="header" href="#monitoring-performance">Monitoring Performance</a></h3>
<pre><code class="language-bash"># Time analysis
time debtmap

# Profile with verbosity
debtmap -vv 2&gt;&amp;1 | grep "processed in"
</code></pre>
<h2 id="environment-variables-1"><a class="header" href="#environment-variables-1">Environment Variables</a></h2>
<p>Debtmap supports various environment variables for configuring behavior without command-line flags.</p>
<h3 id="analysis-feature-flags"><a class="header" href="#analysis-feature-flags">Analysis Feature Flags</a></h3>
<pre><code class="language-bash"># Enable context-aware analysis by default
export DEBTMAP_CONTEXT_AWARE=true

# Enable functional analysis by default
export DEBTMAP_FUNCTIONAL_ANALYSIS=true
</code></pre>
<h3 id="automation-and-cicd-variables"><a class="header" href="#automation-and-cicd-variables">Automation and CI/CD Variables</a></h3>
<pre><code class="language-bash"># Enable automation-friendly output (used by Prodigy)
export PRODIGY_AUTOMATION=true

# Enable validation mode (stricter checks)
export PRODIGY_VALIDATION=true
</code></pre>
<h3 id="output-customization"><a class="header" href="#output-customization">Output Customization</a></h3>
<pre><code class="language-bash"># Disable emoji in output
export NO_EMOJI=1

# Force plain text output (no colors)
export NO_COLOR=1
</code></pre>
<h3 id="usage-examples"><a class="header" href="#usage-examples">Usage Examples</a></h3>
<pre><code class="language-bash"># Enable context-aware analysis by default
echo 'export DEBTMAP_CONTEXT_AWARE=true' &gt;&gt; ~/.bashrc

# CI/CD environment setup
export NO_EMOJI=1
export NO_COLOR=1
export PRODIGY_AUTOMATION=true

# Run analysis with environment settings
debtmap

# Override environment with flags
DEBTMAP_CONTEXT_AWARE=false debtmap --context  # Flag takes precedence
</code></pre>
<h3 id="precedence-rules"><a class="header" href="#precedence-rules">Precedence Rules</a></h3>
<p>When both environment variables and CLI flags are present:</p>
<ol>
<li><strong>CLI flags take precedence</strong> over environment variables</li>
<li><strong>Environment variables override</strong> config file defaults</li>
<li><strong>Config file settings override</strong> built-in defaults</li>
</ol>
<h3 id="troubleshooting-environment-variables"><a class="header" href="#troubleshooting-environment-variables">Troubleshooting Environment Variables</a></h3>
<pre><code class="language-bash"># Test with specific environment
env DEBTMAP_CONTEXT_AWARE=true debtmap -v

# See all debtmap-related environment variables
env | grep -i debtmap
env | grep -i prodigy
</code></pre>
<h2 id="context-provider-troubleshooting"><a class="header" href="#context-provider-troubleshooting">Context Provider Troubleshooting</a></h2>
<p>Diagnose and fix issues with context providers (critical_path, dependency, git_history).</p>
<h3 id="enable-context-analysis"><a class="header" href="#enable-context-analysis">Enable Context Analysis</a></h3>
<pre><code class="language-bash"># Enable with default providers
debtmap --context

# Or use explicit flag
debtmap --enable-context

# Specify specific providers
debtmap --context --context-providers critical_path,dependency,git_history
</code></pre>
<h3 id="disable-specific-providers-1"><a class="header" href="#disable-specific-providers-1">Disable Specific Providers</a></h3>
<pre><code class="language-bash"># Disable git_history only
debtmap --context --disable-context git_history

# Disable multiple providers
debtmap --context --disable-context git_history,dependency

# Disable context-aware filtering
debtmap --no-context-aware
</code></pre>
<h3 id="git-history-provider-issues"><a class="header" href="#git-history-provider-issues">Git History Provider Issues</a></h3>
<p><strong>Problem</strong>: “Git history error” when running analysis</p>
<p><strong>Causes</strong>:</p>
<ul>
<li>Not in a git repository</li>
<li>No git history for files</li>
<li>Git not installed or accessible</li>
</ul>
<p><strong>Solutions</strong>:</p>
<pre><code class="language-bash"># Verify in git repository
git status

# Disable git_history provider
debtmap --context --disable-context git_history

# Initialize git repo if needed
git init
</code></pre>
<h3 id="dependency-provider-issues"><a class="header" href="#dependency-provider-issues">Dependency Provider Issues</a></h3>
<p><strong>Problem</strong>: “Dependency error” or incomplete dependency graph</p>
<p><strong>Causes</strong>:</p>
<ul>
<li>Complex import structures</li>
<li>Circular dependencies</li>
<li>Unsupported dependency patterns</li>
</ul>
<p><strong>Solutions</strong>:</p>
<pre><code class="language-bash"># Disable dependency provider
debtmap --context --disable-context dependency

# Try with verbosity to see details
debtmap --context -vvv

# Use without context
debtmap
</code></pre>
<h3 id="critical-path-provider-issues"><a class="header" href="#critical-path-provider-issues">Critical Path Provider Issues</a></h3>
<p><strong>Problem</strong>: Critical path analysis fails or produces unexpected results</p>
<p><strong>Causes</strong>:</p>
<ul>
<li>Invalid call graph</li>
<li>Missing function definitions</li>
<li>Complex control flow</li>
</ul>
<p><strong>Solutions</strong>:</p>
<pre><code class="language-bash"># Disable critical_path provider
debtmap --context --disable-context critical_path

# Try with semantic analysis disabled
debtmap --context --semantic-off

# Debug with verbosity
debtmap --context --context-providers critical_path -vvv
</code></pre>
<h3 id="context-impact-on-scoring"><a class="header" href="#context-impact-on-scoring">Context Impact on Scoring</a></h3>
<p>Context providers add additional risk factors to scoring:</p>
<pre><code class="language-bash"># See context contribution to scores
debtmap --context -v

# Compare with and without context
debtmap --output baseline.json
debtmap --context --output with_context.json
debtmap compare --before baseline.json --after with_context.json
</code></pre>
<h3 id="performance-impact-2"><a class="header" href="#performance-impact-2">Performance Impact</a></h3>
<p>Context analysis adds processing overhead:</p>
<pre><code class="language-bash"># Faster: no context
debtmap

# Slower: with all context providers
debtmap --context --context-providers critical_path,dependency,git_history

# Medium: selective providers
debtmap --context --context-providers dependency
</code></pre>
<h3 id="debug-context-providers"><a class="header" href="#debug-context-providers">Debug Context Providers</a></h3>
<pre><code class="language-bash"># See detailed context provider output
debtmap --context -vvv

# Check which providers are active
debtmap --context -v | grep "context provider"
</code></pre>
<h2 id="advanced-analysis-troubleshooting"><a class="header" href="#advanced-analysis-troubleshooting">Advanced Analysis Troubleshooting</a></h2>
<p>Advanced CLI flags for specialized analysis scenarios.</p>
<h3 id="multi-pass-analysis-2"><a class="header" href="#multi-pass-analysis-2">Multi-Pass Analysis</a></h3>
<p>Multi-pass analysis is <strong>enabled by default</strong> and performs two iterations (raw and normalized) to distinguish logical complexity from formatting artifacts.</p>
<pre><code class="language-bash"># Multi-pass analysis runs by default
debtmap analyze .

# Disable for performance-critical scenarios
debtmap --no-multi-pass

# Or use environment variable
export DEBTMAP_SINGLE_PASS=1
debtmap analyze .
</code></pre>
<p><strong>When to disable (<code>--no-multi-pass</code>)</strong>:</p>
<ul>
<li>Performance-critical CI/CD pipelines</li>
<li>Very large codebases (&gt;100k LOC)</li>
<li>Quick complexity checks during development</li>
<li>When formatting is already standardized</li>
</ul>
<h3 id="attribution-output"><a class="header" href="#attribution-output">Attribution Output</a></h3>
<p><strong>Flag</strong>: <code>--show-attribution</code></p>
<p>Shows attribution information for detected issues.</p>
<pre><code class="language-bash"># Enable attribution output
debtmap --show-attribution

# Combine with verbosity for details
debtmap --show-attribution -v
</code></pre>
<p><strong>Troubleshooting</strong>:</p>
<ul>
<li>Requires git history provider for author information</li>
<li>May slow down analysis</li>
<li>Use <code>--disable-context git_history</code> if causing errors</li>
</ul>
<h3 id="aggregation-methods-2"><a class="header" href="#aggregation-methods-2">Aggregation Methods</a></h3>
<p><strong>Flag</strong>: <code>--aggregation-method &lt;method&gt;</code></p>
<p>Controls how results are aggregated across files.</p>
<pre><code class="language-bash"># Available aggregation methods:
debtmap --aggregation-method weighted_sum  # (default)
debtmap --aggregation-method sum
debtmap --aggregation-method logarithmic_sum
debtmap --aggregation-method max_plus_average
</code></pre>
<p><strong>Common issues</strong>:</p>
<ul>
<li>Different methods produce different result structures</li>
<li>Choose method based on your reporting needs</li>
<li>Use consistent method for comparison over time</li>
</ul>
<h3 id="minimum-problematic-threshold"><a class="header" href="#minimum-problematic-threshold">Minimum Problematic Threshold</a></h3>
<p><strong>Flag</strong>: <code>--min-problematic &lt;number&gt;</code></p>
<p>Sets the minimum score for an item to be considered problematic.</p>
<pre><code class="language-bash"># Default threshold
debtmap --min-problematic 3

# More strict (show more issues)
debtmap --min-problematic 1

# Less strict (show only serious issues)
debtmap --min-problematic 5
</code></pre>
<p><strong>Relationship to other filters</strong>:</p>
<ul>
<li>Works alongside <code>--min-priority</code></li>
<li>Filters at analysis level vs display level</li>
<li>Lower values = more issues shown</li>
</ul>
<h3 id="god-object-detection-4"><a class="header" href="#god-object-detection-4">God Object Detection</a></h3>
<p><strong>Flag</strong>: <code>--no-god-object</code></p>
<p>Disables god object (large class/module) detection.</p>
<p><strong>God Object Types</strong>:</p>
<p>Debtmap distinguishes three types of god objects:</p>
<ol>
<li>
<p><strong>god_class</strong>: Files with excessive complexity excluding test functions</p>
<ul>
<li>Focuses on production code complexity</li>
<li>Ignores test helper functions and test cases</li>
<li>Best indicator of production code quality issues</li>
</ul>
</li>
<li>
<p><strong>god_file</strong>: Files with excessive complexity including all functions</p>
<ul>
<li>Considers both production and test code</li>
<li>Useful for understanding total file complexity</li>
<li>Alias: <code>god_module</code> (same as god_file)</li>
</ul>
</li>
<li>
<p><strong>god_module</strong>: Alias for god_file</p>
<ul>
<li>Module-level view of complexity</li>
<li>Includes all functions regardless of purpose</li>
</ul>
</li>
</ol>
<p><strong>Responsibility Analysis Metrics</strong> (Spec 140):</p>
<p>Modern god object detection includes domain responsibility analysis:</p>
<pre><code class="language-bash"># See detailed god object metrics
debtmap -vv 2&gt;&amp;1 | grep "god_object\|domain"
</code></pre>
<p><strong>Additional Metrics</strong>:</p>
<ul>
<li><strong>domain_count</strong>: Number of distinct responsibility domains in file</li>
<li><strong>domain_diversity</strong>: Measure of how varied the responsibilities are (0.0-1.0)</li>
<li><strong>struct_ratio</strong>: Ratio of structs to total file size</li>
<li><strong>cross_domain_severity</strong>: How badly domains are mixed (0.0-1.0)</li>
<li><strong>module_splits</strong>: Suggested number of modules to split into</li>
</ul>
<p><strong>Configuration</strong>:</p>
<pre><code class="language-toml"># In .debtmap.toml
[god_object]
# Thresholds for god object detection
complexity_threshold = 100
loc_threshold = 500
function_count_threshold = 20

# Responsibility analysis thresholds
domain_diversity_threshold = 0.7  # High diversity = mixed responsibilities
cross_domain_threshold = 0.6       # High value = poor separation
</code></pre>
<p><strong>Usage</strong>:</p>
<pre><code class="language-bash"># Disable god object detection entirely
debtmap --no-god-object

# See god object analysis with responsibility metrics
debtmap -vv

# Check specific file for god object patterns
debtmap path/to/large/file.rs -vv
</code></pre>
<p><strong>When to use</strong>:</p>
<ul>
<li>False positives on framework files</li>
<li>Intentional large aggregator classes</li>
<li>Reducing noise in results</li>
<li>Files that are legitimately large due to generated code</li>
</ul>
<p><strong>Understanding the Metrics</strong>:</p>
<pre><code class="language-bash"># Example output interpretation:
# domain_count = 5          → File handles 5 different concerns
# domain_diversity = 0.8    → Very mixed responsibilities (bad)
# cross_domain_severity = 0.7 → Poor separation of concerns
# module_splits = 3         → Suggest splitting into 3 modules

# High domain_diversity + high cross_domain_severity = strong god object
# Recommended: refactor into separate modules per domain
</code></pre>
<h3 id="detail-level-control"><a class="header" href="#detail-level-control">Detail Level Control</a></h3>
<p><strong>Flag</strong>: <code>--detail-level &lt;level&gt;</code></p>
<p>Controls the level of detail in analysis output.</p>
<pre><code class="language-bash"># Available detail levels:
debtmap --detail-level summary        # High-level overview only
debtmap --detail-level standard       # (default) Balanced detail
debtmap --detail-level comprehensive  # Detailed analysis
debtmap --detail-level debug         # Full debug information
</code></pre>
<p><strong>When to use</strong>:</p>
<ul>
<li><code>summary</code>: Quick overview for large codebases</li>
<li><code>standard</code>: Default, appropriate for most use cases</li>
<li><code>comprehensive</code>: Deep dive into specific issues</li>
<li><code>debug</code>: Troubleshooting analysis behavior</li>
</ul>
<h3 id="aggregation-control"><a class="header" href="#aggregation-control">Aggregation Control</a></h3>
<p><strong>Flags</strong>: <code>--aggregate-only</code>, <code>--no-aggregation</code></p>
<p>Control file-level score aggregation.</p>
<pre><code class="language-bash"># Show only aggregated file-level scores
debtmap --aggregate-only

# Disable file-level aggregation entirely
debtmap --no-aggregation

# Default: show both individual items and file aggregates
debtmap
</code></pre>
<p><strong>Use cases</strong>:</p>
<ul>
<li><code>--aggregate-only</code>: Focus on file-level technical debt</li>
<li><code>--no-aggregation</code>: See individual functions/classes only</li>
<li>Default: Full picture with both levels</li>
</ul>
<h3 id="call-graph-debugging-1"><a class="header" href="#call-graph-debugging-1">Call Graph Debugging</a></h3>
<p><strong>Overview</strong>: Debug call graph generation and analysis for dependency tracking.</p>
<p><strong>Available Flags</strong>:</p>
<pre><code class="language-bash"># Enable call graph debug output
debtmap --debug-call-graph

# Trace specific functions through call graph
debtmap --trace-functions "function_name,another_function"

# Show only call graph statistics (no detailed graph)
debtmap --call-graph-stats-only

# Control debug output format (text or json)
debtmap --debug-call-graph --debug-format text
debtmap --debug-call-graph --debug-format json

# Validate call graph consistency
debtmap --validate-call-graph
</code></pre>
<p><strong>Dependency Control Flags</strong>:</p>
<pre><code class="language-bash"># Show dependency information in results
debtmap --show-dependencies

# Hide dependency information (default in some contexts)
debtmap --no-dependencies

# Limit number of callers shown per function
debtmap --max-callers 10

# Limit number of callees shown per function
debtmap --max-callees 10

# Include external crate calls in call graph
debtmap --show-external

# Include standard library calls in call graph
debtmap --show-std-lib
</code></pre>
<p><strong>Common Issues</strong>:</p>
<p><strong>Q: Call graph shows incomplete or missing relationships?</strong></p>
<p>A: Try these debugging steps:</p>
<pre><code class="language-bash"># Enable debug output to see graph construction
debtmap --debug-call-graph -vv

# Validate the call graph consistency
debtmap --validate-call-graph

# Include external dependencies if relevant
debtmap --show-external --show-std-lib

# Trace specific functions to see their relationships
debtmap --trace-functions "my_function" -vv
</code></pre>
<p><strong>Q: Call graph output is overwhelming?</strong></p>
<p>A: Use filtering options:</p>
<pre><code class="language-bash"># Show only statistics, not the full graph
debtmap --call-graph-stats-only

# Limit callers and callees shown
debtmap --max-callers 5 --max-callees 5

# Hide dependencies from main output
debtmap --no-dependencies

# Export to JSON for external processing
debtmap --debug-call-graph --debug-format json --output call-graph.json
</code></pre>
<p><strong>When to use call graph debugging</strong>:</p>
<ul>
<li>Investigating missing critical path detection</li>
<li>Understanding dependency relationships</li>
<li>Debugging context provider issues</li>
<li>Analyzing architectural coupling</li>
<li>Validating function relationship detection</li>
</ul>
<h3 id="tiered-prioritization-issues"><a class="header" href="#tiered-prioritization-issues">Tiered Prioritization Issues</a></h3>
<p><strong>Overview</strong>: Debtmap uses a 4-tier system to classify and <strong>sort</strong> technical debt items by architectural importance. Tiers affect result ordering but do not multiply scores.</p>
<p><strong>Tier Classification</strong>:</p>
<ul>
<li><strong>Tier 1 (Critical Architecture)</strong>: High complexity, low coverage, high dependencies, entry points, or file-level architectural debt</li>
<li><strong>Tier 2 (Complex Untested)</strong>: Significant complexity or coverage gaps</li>
<li><strong>Tier 3 (Testing Gaps)</strong>: Moderate issues that need attention</li>
<li><strong>Tier 4 (Maintenance)</strong>: Low-priority items, routine maintenance</li>
</ul>
<p><strong>Result Ordering</strong>:
Results are sorted first by tier (T1 &gt; T2 &gt; T3 &gt; T4), then by score within each tier. This ensures architecturally critical items appear at the top regardless of their absolute score.</p>
<p><strong>Note</strong>: Tier weights (1.5×, 1.0×, 0.7×, 0.3×) exist in the configuration but are currently not applied as score multipliers. Tiers control sort order instead.</p>
<p><strong>Configuration</strong>:</p>
<pre><code class="language-toml"># In .debtmap.toml
[tiers]
# Tier 2 requires EITHER high complexity OR high dependencies
t2_complexity_threshold = 15
t2_dependency_threshold = 10

# Tier 3 requires moderate complexity
t3_complexity_threshold = 8

# Control Tier 4 visibility in main report
show_t4_in_main_report = false
</code></pre>
<p><strong>Common Issues</strong>:</p>
<p><strong>Q: Why is my item in Tier 3 instead of Tier 2?</strong></p>
<p>A: Check if it meets Tier 2 thresholds:</p>
<pre><code class="language-bash"># See tier classification with verbosity
debtmap -v

# Check current thresholds
cat .debtmap.toml | grep -A 5 "\[tiers\]"

# Lower thresholds to promote more items to Tier 2
# In .debtmap.toml:
# t2_complexity_threshold = 10  (default: 15)
# t2_dependency_threshold = 5   (default: 10)
</code></pre>
<p><strong>Q: How do I hide Tier 4 items from the main report?</strong></p>
<p>A: Use the <code>show_t4_in_main_report</code> configuration:</p>
<pre><code class="language-toml"># In .debtmap.toml
[tiers]
show_t4_in_main_report = false
</code></pre>
<p>Tier 4 items will still appear in detailed output but won’t clutter the main summary.</p>
<h3 id="file-level-scoring-issues"><a class="header" href="#file-level-scoring-issues">File-Level Scoring Issues</a></h3>
<p><strong>Overview</strong>: Debtmap aggregates function/class scores into file-level scores using configurable aggregation methods.</p>
<p><strong>Note</strong>: The exact aggregation formula depends on the selected method (see <code>--aggregation-method</code> flag). File-level scores combine individual item scores with file-level characteristics.</p>
<p><strong>Aggregation Methods</strong>:</p>
<pre><code class="language-bash"># Weighted sum (default) - considers complexity weights
debtmap --aggregation-method weighted_sum

# Simple sum - adds all function scores
debtmap --aggregation-method sum

# Logarithmic sum - dampens very high scores
debtmap --aggregation-method logarithmic_sum

# Max plus average - highlights worst function + context
debtmap --aggregation-method max_plus_average
</code></pre>
<p><strong>When to use each method</strong>:</p>
<ul>
<li><strong>weighted_sum</strong>: Default, balances individual and collective impact</li>
<li><strong>sum</strong>: When you want raw cumulative debt</li>
<li><strong>logarithmic_sum</strong>: For very large files to prevent score explosion</li>
<li><strong>max_plus_average</strong>: Focus on worst offender while considering overall file health</li>
</ul>
<p><strong>Aggregation Control Flags</strong>:</p>
<pre><code class="language-bash"># Show only aggregated file-level scores
debtmap --aggregate-only

# Disable file-level aggregation entirely
debtmap --no-aggregation

# Default: show both individual items and file aggregates
debtmap
</code></pre>
<p><strong>Troubleshooting High File Scores</strong>:</p>
<p><strong>Q: Why does this file have such a high score?</strong></p>
<p>A: Check contributing factors with verbosity:</p>
<pre><code class="language-bash"># See file-level score breakdown
debtmap path/to/file.rs -vv

# Look for:
# - High function count (density_factor kicks in at 50+)
# - God object detection (1.5× multiplier)
# - Low coverage (high coverage_factor)
# - Large file size (size_factor)
# - Multiple high-complexity functions

# Disable god object detection if false positive
debtmap --no-god-object path/to/file.rs
</code></pre>
<h3 id="functional-analysis-issues"><a class="header" href="#functional-analysis-issues">Functional Analysis Issues</a></h3>
<p><strong>Overview</strong>: Functional analysis detects violations of functional programming principles like impure functions, excessive mutation, and side effects.</p>
<p><strong>Enable Functional Analysis</strong>:</p>
<pre><code class="language-bash"># Enable AST-based functional analysis
debtmap --ast-functional-analysis

# Use different strictness profiles
debtmap --ast-functional-analysis --functional-analysis-profile strict
debtmap --ast-functional-analysis --functional-analysis-profile balanced  # (default)
debtmap --ast-functional-analysis --functional-analysis-profile lenient
</code></pre>
<p><strong>Analysis Profiles</strong>:</p>
<ul>
<li><strong>strict</strong>: Flag most functional violations, enforce pure functions</li>
<li><strong>balanced</strong>: Default, reasonable middle ground for mixed codebases</li>
<li><strong>lenient</strong>: Allow more pragmatic deviations from pure functional style</li>
</ul>
<p><strong>Common Issues</strong>:</p>
<p><strong>Q: Too many false positives for legitimate imperative code?</strong></p>
<p>A: Adjust the profile or disable for specific areas:</p>
<pre><code class="language-bash"># Use lenient profile for pragmatic codebases
debtmap --ast-functional-analysis --functional-analysis-profile lenient

# Disable functional analysis if not using FP style
debtmap  # (functional analysis is opt-in via --ast-functional-analysis)
</code></pre>
<p><strong>Q: What violations does functional analysis detect?</strong></p>
<p>A: Functional analysis flags:</p>
<ul>
<li>Mutation of variables (reassignment)</li>
<li>Side effects in functions (I/O, global state)</li>
<li>Impure functions (non-deterministic behavior)</li>
<li>Excessive mutable state</li>
<li>Missing const/immutability annotations</li>
</ul>
<pre><code class="language-bash"># See detailed functional analysis results
debtmap --ast-functional-analysis -vv

# Focus on functional purity issues
debtmap --ast-functional-analysis --filter "functional"
</code></pre>
<p><strong>When to use functional analysis</strong>:</p>
<ul>
<li>Projects following functional programming principles</li>
<li>Codebases using immutable data structures</li>
<li>When refactoring to reduce side effects</li>
<li>For detecting hidden mutation bugs</li>
<li>In functional-first languages (Rust with functional style)</li>
</ul>
<p><strong>When to disable</strong>:</p>
<ul>
<li>Imperative codebases where mutation is expected</li>
<li>Performance-critical code requiring in-place updates</li>
<li>When false positives overwhelm actual issues</li>
</ul>
<h3 id="pattern-detection-issues"><a class="header" href="#pattern-detection-issues">Pattern Detection Issues</a></h3>
<p><strong>Overview</strong>: Pattern detection identifies repetitive code structures, anti-patterns, and common debt patterns.</p>
<p><strong>Control Pattern Detection</strong>:</p>
<pre><code class="language-bash"># Disable pattern detection entirely
debtmap --no-pattern-detection

# Specify specific patterns to detect
debtmap --patterns "god_object,long_function,complex_conditional"

# Adjust pattern detection sensitivity
debtmap --pattern-threshold 0.8  # Higher = stricter matching (0.0-1.0)

# Show pattern detection warnings
debtmap --show-pattern-warnings
</code></pre>
<p><strong>Common Issues</strong>:</p>
<p><strong>Q: Pattern detection causes too many false positives?</strong></p>
<p>A: Adjust threshold or disable specific patterns:</p>
<pre><code class="language-bash"># Increase threshold for stricter matching (fewer false positives)
debtmap --pattern-threshold 0.9

# Disable pattern detection for exploratory analysis
debtmap --no-pattern-detection

# See which patterns are triggering with warnings
debtmap --show-pattern-warnings -v
</code></pre>
<p><strong>Q: Missing patterns I expect to see?</strong></p>
<p>A: Lower threshold or check pattern names:</p>
<pre><code class="language-bash"># Lower threshold to catch more patterns
debtmap --pattern-threshold 0.6

# Specify patterns explicitly
debtmap --patterns "god_object,long_function,deep_nesting"

# Use verbosity to see pattern detection process
debtmap --show-pattern-warnings -vv
</code></pre>
<p><strong>Detected Patterns</strong>:</p>
<ul>
<li><code>god_object</code>: Classes/modules with too many responsibilities</li>
<li><code>long_function</code>: Functions exceeding length thresholds</li>
<li><code>complex_conditional</code>: Nested or complex branching logic</li>
<li><code>deep_nesting</code>: Excessive indentation depth</li>
<li><code>parameter_overload</code>: Too many function parameters</li>
<li><code>duplicate_code</code>: Repetitive code structures</li>
</ul>
<p><strong>When to adjust pattern threshold</strong>:</p>
<ul>
<li><strong>Higher (0.8-1.0)</strong>: Reduce noise, only flag clear violations</li>
<li><strong>Lower (0.5-0.7)</strong>: Catch subtle patterns, more comprehensive detection</li>
<li><strong>Default (0.7)</strong>: Balanced detection for most codebases</li>
</ul>
<h3 id="public-api-detection-issues"><a class="header" href="#public-api-detection-issues">Public API Detection Issues</a></h3>
<p><strong>Overview</strong>: Public API detection identifies functions and types that form your crate’s public interface, affecting scoring and priority.</p>
<p><strong>Control Public API Detection</strong>:</p>
<pre><code class="language-bash"># Disable public API detection
debtmap --no-public-api-detection

# Adjust public API detection threshold
debtmap --public-api-threshold 0.5  # Lower = more items marked as public (0.0-1.0)
</code></pre>
<p><strong>Common Issues</strong>:</p>
<p><strong>Q: Private functions being marked as public API?</strong></p>
<p>A: Increase the threshold for stricter detection:</p>
<pre><code class="language-bash"># Higher threshold = only clearly public items
debtmap --public-api-threshold 0.8

# Disable public API detection if not useful
debtmap --no-public-api-detection

# See what's being detected as public
debtmap -vv 2&gt;&amp;1 | grep "public API"
</code></pre>
<p><strong>Q: Public functions not being detected?</strong></p>
<p>A: Lower the threshold or check visibility:</p>
<pre><code class="language-bash"># Lower threshold to detect more public items
debtmap --public-api-threshold 0.3

# Verify function is actually public (pub keyword in Rust)
debtmap path/to/file.rs -vv
</code></pre>
<p><strong>How Public API Detection Works</strong>:</p>
<ul>
<li>Checks for <code>pub</code> visibility in Rust</li>
<li>Identifies exported functions in Python (<code>__all__</code>)</li>
<li>Detects exported symbols in JavaScript/TypeScript</li>
<li>Considers call graph entry points</li>
<li>Factors in documentation presence</li>
</ul>
<p><strong>Impact on Scoring</strong>:</p>
<ul>
<li>Public API items get higher priority scores (1.1× multiplier)</li>
<li>Entry point detection uses public API information</li>
<li>Critical path analysis considers public boundaries</li>
</ul>
<p><strong>When to disable</strong>:</p>
<ul>
<li>Internal tools or scripts (no public API)</li>
<li>When API detection causes confusion</li>
<li>Libraries where everything is intentionally public</li>
</ul>
<h3 id="combining-advanced-flags"><a class="header" href="#combining-advanced-flags">Combining Advanced Flags</a></h3>
<pre><code class="language-bash"># Comprehensive analysis with all features (multi-pass is default)
debtmap --attribution --context -vv

# Minimal filtering for exploration
debtmap --min-problematic 1 --min-priority 0 --no-god-object

# Performance-focused analysis (disable multi-pass)
debtmap --no-multi-pass --jobs 8

# Summary view with aggregated scores
debtmap --detail-level summary --aggregate-only
</code></pre>
<h2 id="error-messages-reference"><a class="header" href="#error-messages-reference">Error Messages Reference</a></h2>
<p>Understanding common error messages and how to resolve them.</p>
<h3 id="file-system-errors"><a class="header" href="#file-system-errors">File System Errors</a></h3>
<p><strong>Message</strong>: <code>File system error: Permission denied</code></p>
<p><strong>Meaning</strong>: Cannot read file or directory due to permissions</p>
<p><strong>Solutions</strong>:</p>
<ul>
<li>Check file permissions: <code>ls -la &lt;file&gt;</code></li>
<li>Ensure user has read access</li>
</ul>
<hr>
<p><strong>Message</strong>: <code>File system error: No such file or directory</code></p>
<p><strong>Meaning</strong>: File or directory does not exist</p>
<p><strong>Solutions</strong>:</p>
<ul>
<li>Verify path is correct</li>
<li>Check current working directory: <code>pwd</code></li>
<li>Use absolute paths if needed</li>
<li>Ensure files weren’t moved or deleted</li>
</ul>
<h3 id="parse-errors-1"><a class="header" href="#parse-errors-1">Parse Errors</a></h3>
<p><strong>Message</strong>: <code>Parse error in file.rs:line:column: unexpected token</code></p>
<p><strong>Meaning</strong>: Syntax debtmap cannot parse</p>
<p><strong>Solutions</strong>:</p>
<pre><code class="language-bash"># Try fallback mode
debtmap --semantic-off

# For Rust macros
debtmap --verbose-macro-warnings --show-macro-stats

# Exclude problematic file
# In .debtmap/config.toml:
# exclude = ["path/to/file.rs"]
</code></pre>
<h3 id="analysis-errors"><a class="header" href="#analysis-errors">Analysis Errors</a></h3>
<p><strong>Message</strong>: <code>Analysis error: internal analysis failure</code></p>
<p><strong>Meaning</strong>: Internal error during analysis phase</p>
<p><strong>Solutions</strong>:</p>
<pre><code class="language-bash"># Try fallback mode
debtmap --semantic-off

# Report with debug info
debtmap -vvv 2&gt;&amp;1 | tee error.log

# Isolate problem file
debtmap --max-files 1 path/to/suspected/file
</code></pre>
<h3 id="configuration-errors"><a class="header" href="#configuration-errors">Configuration Errors</a></h3>
<p><strong>Message</strong>: <code>Configuration error: invalid config value</code></p>
<p><strong>Meaning</strong>: Invalid configuration in <code>.debtmap/config.toml</code> or CLI</p>
<p><strong>Solutions</strong>:</p>
<ul>
<li>Check <code>.debtmap/config.toml</code> syntax</li>
<li>Validate TOML format: <code>cat .debtmap/config.toml</code></li>
<li>Review CLI flag values</li>
<li>Check for typos in flag names</li>
</ul>
<h3 id="validation-errors"><a class="header" href="#validation-errors">Validation Errors</a></h3>
<p><strong>Message</strong>: <code>Validation error: threshold validation failed</code></p>
<p><strong>Meaning</strong>: Threshold configuration is invalid</p>
<p><strong>Solutions</strong>:</p>
<ul>
<li>Check threshold values in config</li>
<li>Ensure <code>--min-priority</code> is in valid range (0-10)</li>
<li>Verify threshold preset exists</li>
<li>Use <code>--threshold-preset</code> with valid preset name</li>
</ul>
<h3 id="dependency-errors"><a class="header" href="#dependency-errors">Dependency Errors</a></h3>
<p><strong>Message</strong>: <code>Dependency error: cannot resolve dependency graph</code></p>
<p><strong>Meaning</strong>: Cannot build dependency relationships</p>
<p><strong>Solutions</strong>:</p>
<pre><code class="language-bash"># Disable dependency provider
debtmap --context --disable-context dependency

# Try without context
debtmap

# Debug with verbosity
debtmap -vvv
</code></pre>
<h3 id="concurrency-errors"><a class="header" href="#concurrency-errors">Concurrency Errors</a></h3>
<p><strong>Message</strong>: <code>Concurrency error: parallel processing failure</code></p>
<p><strong>Meaning</strong>: Error during parallel execution</p>
<p><strong>Solutions</strong>:</p>
<pre><code class="language-bash"># Disable parallel processing
debtmap --no-parallel

# Reduce thread count
debtmap --jobs 1

# Report issue with debug output
debtmap -vvv 2&gt;&amp;1 | tee error.log
</code></pre>
<h3 id="unsupported-errors"><a class="header" href="#unsupported-errors">Unsupported Errors</a></h3>
<p><strong>Message</strong>: <code>Unsupported: feature not available for &lt;language&gt;</code></p>
<p><strong>Meaning</strong>: Language or construct not supported</p>
<p><strong>Solutions</strong>:</p>
<ul>
<li>Use supported languages: Rust, Python, JavaScript, TypeScript</li>
<li>Check if language is enabled in config</li>
<li>Some advanced features may not be available for all languages</li>
<li>Try <code>--semantic-off</code> for basic analysis</li>
</ul>
<h3 id="pattern-errors"><a class="header" href="#pattern-errors">Pattern Errors</a></h3>
<p><strong>Message</strong>: <code>Pattern error: invalid glob pattern</code></p>
<p><strong>Meaning</strong>: Invalid glob pattern in configuration or CLI</p>
<p><strong>Solutions</strong>:</p>
<ul>
<li>Check glob pattern syntax</li>
<li>Escape special characters if needed</li>
<li>Test pattern with shell glob: <code>ls &lt;pattern&gt;</code></li>
<li>Use simpler patterns or path prefixes</li>
</ul>
<h2 id="language-specific-issues"><a class="header" href="#language-specific-issues">Language-Specific Issues</a></h2>
<h3 id="rust-1"><a class="header" href="#rust-1">Rust</a></h3>
<p><strong>Macro Expansion Issues</strong></p>
<pre><code class="language-bash"># See macro warnings
debtmap --verbose-macro-warnings

# Show macro statistics
debtmap --show-macro-stats

# Common issue: Complex macros may not expand correctly
# Solution: Use --semantic-off for faster fallback
</code></pre>
<p><strong>Trait and Generic Complexity</strong></p>
<p>Complex trait bounds and generic constraints may affect analysis accuracy:</p>
<pre><code class="language-bash"># Full semantic analysis (default)
debtmap

# Fallback mode for edge cases
debtmap --semantic-off
</code></pre>
<h3 id="python-1"><a class="header" href="#python-1">Python</a></h3>
<p><strong>Type Inference Limitations</strong></p>
<p>Dynamic typing makes some analysis challenging:</p>
<pre><code class="language-bash"># Best effort type inference (default)
debtmap

# Fallback mode if issues
debtmap --semantic-off
</code></pre>
<p><strong>Import Resolution</strong></p>
<p>Complex import structures may not resolve fully:</p>
<ul>
<li>Relative imports usually work</li>
<li>Dynamic imports may not be detected</li>
<li><code>__init__.py</code> packages are supported</li>
</ul>
<h3 id="javascripttypescript-2"><a class="header" href="#javascripttypescript-2">JavaScript/TypeScript</a></h3>
<p><strong>JSX/TSX Parsing</strong></p>
<p>Ensure files have correct extensions:</p>
<ul>
<li><code>.jsx</code> for JavaScript + JSX</li>
<li><code>.tsx</code> for TypeScript + JSX</li>
<li>Configure extensions in <code>.debtmap/config.toml</code> if needed</li>
</ul>
<p><strong>Type Resolution</strong></p>
<p>TypeScript type resolution in complex projects:</p>
<pre><code class="language-bash"># Full type checking (default for .ts files)
debtmap

# Fallback if type issues
debtmap --semantic-off
</code></pre>
<h3 id="mixed-language-projects"><a class="header" href="#mixed-language-projects">Mixed Language Projects</a></h3>
<pre><code class="language-bash"># Analyze all supported languages (default)
debtmap

# Filter specific languages
# In .debtmap/config.toml:
# languages = ["rust", "python"]
</code></pre>
<h3 id="unsupported-language-constructs"><a class="header" href="#unsupported-language-constructs">Unsupported Language Constructs</a></h3>
<p>Some advanced language features may show as “Unsupported”:</p>
<ul>
<li>Rust: Some macro patterns, const generics edge cases</li>
<li>Python: Some metaclass patterns, dynamic code generation</li>
<li>JavaScript: Some advanced AST manipulation</li>
</ul>
<p><strong>Solutions</strong>:</p>
<ul>
<li>Use <code>--semantic-off</code> for basic analysis</li>
<li>Exclude problematic files if needed</li>
<li>Report unsupported patterns as feature requests</li>
</ul>
<h3 id="boilerplate-detection-issues"><a class="header" href="#boilerplate-detection-issues">Boilerplate Detection Issues</a></h3>
<p><strong>Overview</strong>: Boilerplate detection identifies repetitive code patterns that are necessary but contribute to complexity scores, such as trait implementations, error handling, and validation logic.</p>
<p><strong>How Boilerplate Detection Works</strong>:</p>
<p>Debtmap automatically detects common boilerplate patterns:</p>
<ul>
<li><strong>Trait implementations</strong>: Standard trait method implementations (Debug, Display, From, etc.)</li>
<li><strong>Error handling</strong>: Repetitive error conversion and propagation code</li>
<li><strong>Validation functions</strong>: Similar validation logic across multiple functions</li>
<li><strong>Macro-generated code</strong>: Repetitive patterns from macro expansions</li>
<li><strong>Builder patterns</strong>: Setter methods and builder implementations</li>
</ul>
<p><strong>Impact on Scoring</strong>:</p>
<p>Detected boilerplate receives dampened complexity scores to avoid inflating technical debt for necessary repetitive code.</p>
<p><strong>Common Issues</strong>:</p>
<p><strong>Q: Legitimate complex code being marked as boilerplate?</strong></p>
<p>A: Boilerplate detection uses pattern similarity thresholds. If unique logic is being incorrectly dampened:</p>
<pre><code class="language-bash"># See what's being detected as boilerplate
debtmap -vv 2&gt;&amp;1 | grep "boilerplate"

# Check entropy analysis settings (used for boilerplate detection)
# In .debtmap.toml:
# [entropy]
# pattern_threshold = 0.8  # Increase for stricter matching
</code></pre>
<p><strong>Q: Boilerplate code still showing high scores?</strong></p>
<p>A: Some boilerplate patterns may not be recognized. Common cases:</p>
<pre><code class="language-bash"># Trait implementations should be automatically detected
# If not dampened, check that code follows standard patterns

# For custom validation patterns, ensure similarity is high enough
# In .debtmap.toml:
# [entropy]
# pattern_threshold = 0.7  # Lower to catch more patterns
# enabled = true
</code></pre>
<p><strong>Q: How to identify what debtmap considers boilerplate?</strong></p>
<p>A: Use verbose output:</p>
<pre><code class="language-bash"># See boilerplate detection in action
debtmap -vv 2&gt;&amp;1 | grep -i "boilerplate\|pattern\|entropy"

# Check specific file
debtmap path/to/file.rs -vv
</code></pre>
<p><strong>Boilerplate Reduction Strategies</strong>:</p>
<pre><code class="language-toml"># In .debtmap.toml
[entropy]
enabled = true                    # Enable pattern-based dampening
pattern_threshold = 0.7           # Similarity threshold (0.0-1.0)
weight = 0.3                      # Impact on complexity adjustment
min_tokens = 50                   # Minimum size for pattern analysis
</code></pre>
<p><strong>When boilerplate detection helps</strong>:</p>
<ul>
<li>Codebases with many trait implementations</li>
<li>Projects with extensive validation logic</li>
<li>Macro-heavy code (derives, procedural macros)</li>
<li>Builder pattern implementations</li>
<li>Error handling boilerplate</li>
</ul>
<p><strong>When to adjust thresholds</strong>:</p>
<ul>
<li><strong>Increase <code>pattern_threshold</code></strong> (0.8-0.9): If unique code is being dampened</li>
<li><strong>Decrease <code>pattern_threshold</code></strong> (0.5-0.6): If obvious boilerplate isn’t being detected</li>
<li><strong>Disable entropy</strong> (<code>enabled = false</code>): If causing too many false dampening</li>
</ul>
<h3 id="false-positives-1"><a class="header" href="#false-positives-1">False Positives</a></h3>
<p>Reduce false positives for validation functions and repetitive code patterns using entropy analysis:</p>
<p><strong>Enable and Configure Entropy Analysis</strong>:</p>
<pre><code class="language-toml"># In .debtmap.toml
[entropy]
enabled = true                    # Enable entropy-based dampening
weight = 0.3                      # Weight in complexity adjustment (0.0-1.0)
min_tokens = 50                   # Minimum tokens for entropy calculation
pattern_threshold = 0.7           # Pattern similarity threshold (0.0-1.0)
use_classification = true         # Enable advanced token classification
entropy_threshold = 0.5           # Entropy level for dampening (0.0-1.0)
branch_threshold = 0.8            # Branch similarity threshold (0.0-1.0)
max_combined_reduction = 0.5      # Max reduction percentage (0.0-1.0)
</code></pre>
<p><strong>When to Adjust Parameters</strong>:</p>
<ul>
<li><strong>Increase <code>pattern_threshold</code></strong> (e.g., 0.8-0.9): Be more strict, reduce dampening for truly unique code</li>
<li><strong>Decrease <code>entropy_threshold</code></strong> (e.g., 0.3-0.4): Apply dampening more broadly to catch more repetitive patterns</li>
<li><strong>Increase <code>weight</code></strong> (e.g., 0.4-0.5): Make entropy have stronger impact on final scores</li>
<li><strong>Increase <code>min_tokens</code></strong> (e.g., 100): Only apply entropy analysis to larger functions</li>
<li><strong>Increase <code>branch_threshold</code></strong> (e.g., 0.9): Be more strict about branching pattern similarity</li>
</ul>
<p>Entropy analysis can reduce false positives by up to 70% for validation functions, error handling, and other repetitive patterns.</p>
<p><strong>Other False Positive Reduction Strategies</strong>:</p>
<pre><code class="language-bash"># Use context-aware analysis
debtmap --context

# Adjust thresholds
debtmap --threshold-preset lenient

# Disable context-aware filtering if too aggressive
debtmap --no-context-aware
</code></pre>
<h3 id="missing-detections"><a class="header" href="#missing-detections">Missing Detections</a></h3>
<pre><code class="language-bash"># Ensure semantic analysis is enabled
debtmap  # (default, semantic ON)

# Increase verbosity to see what's detected
debtmap -vv

# Check if files are being analyzed
debtmap -v 2&gt;&amp;1 | grep "Processing"
</code></pre>
<h2 id="output-formatting-issues"><a class="header" href="#output-formatting-issues">Output Formatting Issues</a></h2>
<h3 id="choose-output-format"><a class="header" href="#choose-output-format">Choose Output Format</a></h3>
<pre><code class="language-bash"># Terminal format (default, human-readable)
debtmap

# JSON format
debtmap --format json

# Markdown format
debtmap --format markdown
</code></pre>
<h3 id="json-format-options"><a class="header" href="#json-format-options">JSON Format Options</a></h3>
<pre><code class="language-bash"># Legacy format (default): {File: {...}}
debtmap --format json --output-format legacy

# Unified format: consistent structure with 'type' field
debtmap --format json --output-format unified

# Validate JSON
debtmap --format json | jq .

# Write to file
debtmap --format json --output results.json
</code></pre>
<h3 id="plain-output-mode"><a class="header" href="#plain-output-mode">Plain Output Mode</a></h3>
<p>For environments without color/emoji support:</p>
<pre><code class="language-bash"># ASCII-only, no colors, no emoji
debtmap --plain

# Or set environment variable
export NO_EMOJI=1
debtmap
</code></pre>
<h3 id="terminal-color-issues"><a class="header" href="#terminal-color-issues">Terminal Color Issues</a></h3>
<p><strong>Problem</strong>: Colors not rendering or showing escape codes</p>
<p><strong>Solutions</strong>:</p>
<pre><code class="language-bash"># Use plain mode
debtmap --plain

# Check TERM environment variable
echo $TERM

# Set appropriate TERM
export TERM=xterm-256color
</code></pre>
<h3 id="emoji-issues"><a class="header" href="#emoji-issues">Emoji Issues</a></h3>
<p><strong>Problem</strong>: Emojis showing as boxes or ??</p>
<p><strong>Solutions</strong>:</p>
<pre><code class="language-bash"># Disable emojis
debtmap --plain

# Or environment variable
export NO_EMOJI=1
debtmap
</code></pre>
<h3 id="markdown-rendering"><a class="header" href="#markdown-rendering">Markdown Rendering</a></h3>
<p>Ensure viewer supports GitHub-flavored markdown:</p>
<ul>
<li>Tables</li>
<li>Code blocks with syntax highlighting</li>
<li>Task lists</li>
</ul>
<h3 id="write-output-to-file"><a class="header" href="#write-output-to-file">Write Output to File</a></h3>
<pre><code class="language-bash"># JSON to file
debtmap --format json --output results.json

# Markdown to file
debtmap --format markdown --output report.md

# Terminal format to file (preserves colors)
debtmap --output results.txt

# Plain format to file
debtmap --plain --output results.txt
</code></pre>
<h3 id="summary-vs-full-output"><a class="header" href="#summary-vs-full-output">Summary vs Full Output</a></h3>
<pre><code class="language-bash"># Summary mode (compact)
debtmap --summary
debtmap -s

# Full output (default)
debtmap

# Limit number of items
debtmap --top 10       # Top 10 by priority
debtmap --tail 10      # Bottom 10 by priority
</code></pre>
<h3 id="filtering-output"><a class="header" href="#filtering-output">Filtering Output</a></h3>
<pre><code class="language-bash"># Minimum priority level
debtmap --min-priority 5

# Category filters
debtmap --filter "complexity,debt"

# Combine filters
debtmap --min-priority 3 --top 20 --filter complexity
</code></pre>
<h2 id="compare-command-issues"><a class="header" href="#compare-command-issues">Compare Command Issues</a></h2>
<p>The <code>compare</code> command helps track changes in technical debt over time.</p>
<h3 id="basic-usage-3"><a class="header" href="#basic-usage-3">Basic Usage</a></h3>
<p><strong>Note</strong>: The <code>compare</code> command defaults to JSON output format (unlike <code>analyze</code> which defaults to terminal). Use <code>--format terminal</code> or <code>--format markdown</code> if you need different output.</p>
<pre><code class="language-bash"># Save baseline results
debtmap --format json --output before.json

# Make code changes...

# Save new results
debtmap --format json --output after.json

# Compare results (outputs JSON by default)
debtmap compare --before before.json --after after.json

# Compare with terminal output
debtmap compare --before before.json --after after.json --format terminal
</code></pre>
<h3 id="targeted-comparison"><a class="header" href="#targeted-comparison">Targeted Comparison</a></h3>
<p>Use <code>--plan</code> and <code>--target-location</code> for focused debt analysis:</p>
<pre><code class="language-bash"># Compare based on implementation plan
debtmap compare --before before.json --after after.json --plan implementation-plan.json

# Compare specific code location
debtmap compare --before before.json --after after.json \
  --target-location src/main.rs:calculate_score:42

# Combine both for precise tracking
debtmap compare --before before.json --after after.json \
  --plan implementation-plan.json \
  --target-location src/analyzers/complexity.rs:analyze_function:128
</code></pre>
<p><strong>Use cases</strong>:</p>
<ul>
<li><code>--plan</code>: Track debt changes for planned refactoring tasks</li>
<li><code>--target-location</code>: Focus on specific function or code location</li>
<li>Combine for granular technical debt tracking</li>
</ul>
<h3 id="incompatible-format-errors"><a class="header" href="#incompatible-format-errors">Incompatible Format Errors</a></h3>
<p><strong>Problem</strong>: “Incompatible formats” error when comparing files</p>
<p><strong>Causes</strong>:</p>
<ul>
<li>Mixing legacy and unified JSON formats</li>
<li>Files from different debtmap versions</li>
<li>Corrupted JSON files</li>
</ul>
<p><strong>Solutions</strong>:</p>
<pre><code class="language-bash"># Ensure both files use same output format
debtmap --format json --output-format unified --output before.json
# ... make changes ...
debtmap --format json --output-format unified --output after.json
debtmap compare --before before.json --after after.json

# Validate JSON files are well-formed
jq . before.json &gt; /dev/null
jq . after.json &gt; /dev/null
</code></pre>
<h3 id="comparing-across-branches"><a class="header" href="#comparing-across-branches">Comparing Across Branches</a></h3>
<pre><code class="language-bash"># Save baseline on main branch
git checkout main
debtmap --format json --output main.json

# Switch to feature branch
git checkout feature-branch
debtmap --format json --output feature.json

# Compare branches
debtmap compare --before main.json --after feature.json
</code></pre>
<h3 id="missing-files-error"><a class="header" href="#missing-files-error">Missing Files Error</a></h3>
<p><strong>Problem</strong>: “File not found” when running compare</p>
<p><strong>Solutions</strong>:</p>
<ul>
<li>Verify file paths are correct (use absolute paths if needed)</li>
<li>Ensure JSON files weren’t moved or deleted</li>
<li>Check current working directory with <code>pwd</code></li>
</ul>
<h3 id="format-mismatch-issues"><a class="header" href="#format-mismatch-issues">Format Mismatch Issues</a></h3>
<p><strong>Problem</strong>: Compare shows unexpected differences or errors</p>
<p><strong>Solutions</strong>:</p>
<pre><code class="language-bash"># Regenerate both files with same debtmap version
debtmap --format json --output before.json
# ... make changes ...
debtmap --format json --output after.json

# Use same output format for both
debtmap --format json --output-format unified --output before.json
debtmap --format json --output-format unified --output after.json
</code></pre>
<h2 id="validate-command-issues"><a class="header" href="#validate-command-issues">Validate Command Issues</a></h2>
<p>The <code>validate</code> command checks if a codebase meets specified quality thresholds, useful for CI/CD pipelines.</p>
<h3 id="basic-validation"><a class="header" href="#basic-validation">Basic Validation</a></h3>
<pre><code class="language-bash"># Validate codebase passes default thresholds
debtmap validate /path/to/project

# Exit code 0 if passes, non-zero if validation fails
</code></pre>
<h3 id="debt-density-validation"><a class="header" href="#debt-density-validation">Debt Density Validation</a></h3>
<p><strong>Flag</strong>: <code>--max-debt-density &lt;number&gt;</code></p>
<p>Sets the maximum acceptable technical debt per 1000 lines of code.</p>
<pre><code class="language-bash"># Set maximum acceptable debt density (per 1000 LOC)
debtmap validate /path/to/project --max-debt-density 10.0

# Stricter threshold for critical projects
debtmap validate /path/to/project --max-debt-density 5.0

# Lenient threshold for legacy code
debtmap validate /path/to/project --max-debt-density 20.0
</code></pre>
<p><strong>Troubleshooting validation failures</strong>:</p>
<pre><code class="language-bash"># See which files exceed threshold with details
debtmap validate /path/to/project --max-debt-density 10.0 -v

# Get detailed breakdown of debt density calculations
debtmap validate /path/to/project --max-debt-density 10.0 -vv

# Analyze specific files that failed validation
debtmap /path/to/problematic/file.rs -v

# Understand debt density metric
# Debt density = (total_debt_score / total_lines_of_code) × 1000
# Example: 150 debt points across 10,000 LOC = 15.0 debt density
</code></pre>
<p><strong>Interpreting debt density values</strong>:</p>
<ul>
<li><strong>&lt; 5.0</strong>: Excellent code quality</li>
<li><strong>5.0 - 10.0</strong>: Good, manageable technical debt</li>
<li><strong>10.0 - 20.0</strong>: Moderate debt, consider cleanup</li>
<li><strong>&gt; 20.0</strong>: High debt, refactoring recommended</li>
</ul>
<h3 id="cicd-integration-6"><a class="header" href="#cicd-integration-6">CI/CD Integration</a></h3>
<pre><code class="language-bash"># In CI pipeline (fails build if validation fails)
debtmap validate . --max-debt-density 10.0 || exit 1

# With verbose output for debugging
debtmap validate . --max-debt-density 10.0 -v

# Save validation report
debtmap validate . --max-debt-density 10.0 --format json --output validation.json
</code></pre>
<p><strong>Use cases</strong>:</p>
<ul>
<li>Enforce quality gates in CI/CD pipelines</li>
<li>Prevent accumulation of technical debt over time</li>
<li>Track debt density trends across releases</li>
<li>Set different thresholds for different parts of codebase</li>
</ul>
<h2 id="validate-improvement-command-issues"><a class="header" href="#validate-improvement-command-issues">Validate-Improvement Command Issues</a></h2>
<p>The <code>validate-improvement</code> command verifies that code changes actually reduced technical debt, useful for validating refactoring efforts.</p>
<h3 id="basic-usage-1-2"><a class="header" href="#basic-usage-1-2">Basic Usage</a></h3>
<pre><code class="language-bash"># Validate that changes improved the codebase
debtmap validate-improvement \
  --comparison comparison.json \
  --output improvement-report.json

# Set minimum acceptable improvement threshold
debtmap validate-improvement \
  --comparison comparison.json \
  --threshold 5.0 \
  --output improvement-report.json
</code></pre>
<h3 id="command-flags"><a class="header" href="#command-flags">Command Flags</a></h3>
<pre><code class="language-bash"># Specify comparison file from 'debtmap compare' output
debtmap validate-improvement --comparison comparison.json

# Set output file for validation results
debtmap validate-improvement \
  --comparison comparison.json \
  --output improvement-report.json

# Use previous validation for trend analysis
debtmap validate-improvement \
  --comparison comparison.json \
  --previous-validation previous-report.json

# Set minimum improvement threshold (percentage)
debtmap validate-improvement \
  --comparison comparison.json \
  --threshold 10.0  # Require 10% improvement

# Control output format (json, text, markdown)
debtmap validate-improvement \
  --comparison comparison.json \
  --format json

# Quiet mode (exit code only, no output)
debtmap validate-improvement \
  --comparison comparison.json \
  --quiet
</code></pre>
<h3 id="typical-workflow"><a class="header" href="#typical-workflow">Typical Workflow</a></h3>
<pre><code class="language-bash"># Step 1: Save baseline before refactoring
debtmap --format json --output before.json

# Step 2: Make code changes...

# Step 3: Analyze after changes
debtmap --format json --output after.json

# Step 4: Compare results
debtmap compare --before before.json --after after.json \
  --format json --output comparison.json

# Step 5: Validate improvement
debtmap validate-improvement \
  --comparison comparison.json \
  --threshold 5.0 \
  --output validation.json

# Exit code 0 if improvement meets threshold, non-zero otherwise
</code></pre>
<h3 id="common-issues-1-1"><a class="header" href="#common-issues-1-1">Common Issues</a></h3>
<p><strong>Q: Validation fails but I fixed issues - why?</strong></p>
<p>A: Check what the validation is measuring:</p>
<pre><code class="language-bash"># See detailed validation results (without --quiet)
debtmap validate-improvement \
  --comparison comparison.json \
  --format text

# Common reasons for failure:
# - Added new complexity elsewhere while fixing issues
# - Threshold too strict for the changes made
# - Comparison file doesn't reflect latest changes
# - File-level scores increased despite function improvements
</code></pre>
<p><strong>Q: How is improvement calculated?</strong></p>
<p>A: Improvement is measured as percentage reduction in total debt score:</p>
<pre><code class="language-bash"># Formula: improvement = ((before_score - after_score) / before_score) × 100
#
# Example:
# - Before: total score = 100
# - After: total score = 80
# - Improvement: ((100 - 80) / 100) × 100 = 20%

# See detailed breakdown
debtmap validate-improvement \
  --comparison comparison.json \
  --format text -v
</code></pre>
<p><strong>Q: Can I track improvement over multiple refactorings?</strong></p>
<p>A: Yes, use <code>--previous-validation</code> for trend analysis:</p>
<pre><code class="language-bash"># First validation
debtmap validate-improvement \
  --comparison refactor1-comparison.json \
  --output validation1.json

# Second validation references first
debtmap validate-improvement \
  --comparison refactor2-comparison.json \
  --previous-validation validation1.json \
  --output validation2.json

# Shows cumulative improvement trend
</code></pre>
<h3 id="cicd-integration-1-1"><a class="header" href="#cicd-integration-1-1">CI/CD Integration</a></h3>
<pre><code class="language-bash"># In CI pipeline: enforce minimum improvement for refactoring PRs
debtmap validate-improvement \
  --comparison comparison.json \
  --threshold 5.0 \
  --quiet || exit 1

# With output for CI reporting
debtmap validate-improvement \
  --comparison comparison.json \
  --threshold 5.0 \
  --format json \
  --output improvement-report.json

# Archive validation reports for tracking
</code></pre>
<p><strong>Use cases</strong>:</p>
<ul>
<li>Verify refactoring PRs actually reduce debt</li>
<li>Enforce improvement thresholds in code review</li>
<li>Track debt reduction trends over time</li>
<li>Validate that tech debt fixes are effective</li>
<li>Generate improvement metrics for reporting</li>
</ul>
<h3 id="troubleshooting-validation-failures"><a class="header" href="#troubleshooting-validation-failures">Troubleshooting Validation Failures</a></h3>
<pre><code class="language-bash"># Check the comparison file is valid
jq . comparison.json

# Verify before/after files were generated correctly
debtmap --format json --output before.json -v
# ... make changes ...
debtmap --format json --output after.json -v

# Lower threshold if being too strict
debtmap validate-improvement \
  --comparison comparison.json \
  --threshold 1.0  # Accept any improvement

# See detailed improvement breakdown
debtmap validate-improvement \
  --comparison comparison.json \
  --format markdown \
  --output improvement.md
</code></pre>
<h2 id="faq-1"><a class="header" href="#faq-1">FAQ</a></h2>
<h3 id="general-questions"><a class="header" href="#general-questions">General Questions</a></h3>
<p><strong>Q: Why is my analysis slow?</strong></p>
<p>A: Check several factors:</p>
<pre><code class="language-bash"># Use all CPU cores
debtmap --jobs 0

# Disable multi-pass for faster single-pass analysis
debtmap --no-multi-pass

# Try faster fallback mode
debtmap --semantic-off

# Check for large files or complex macros
debtmap -vv
</code></pre>
<p><strong>Q: What does ‘Parse error’ mean?</strong></p>
<p>A: File contains syntax debtmap cannot parse. Solutions:</p>
<ul>
<li>Try <code>--semantic-off</code> for fallback mode</li>
<li>Use <code>--verbose-macro-warnings</code> for Rust macros</li>
<li>Exclude problematic files in <code>.debtmap/config.toml</code></li>
<li>Report parse errors as potential bugs</li>
</ul>
<p><strong>Q: Why do scores differ between runs?</strong></p>
<p>A: Several factors affect scores:</p>
<ul>
<li>Coverage file changed (use <code>--coverage-file</code>)</li>
<li>Context providers enabled/disabled (<code>--context</code>)</li>
<li>Code changes (intended behavior)</li>
<li>Different threshold settings</li>
</ul>
<p><strong>Q: How do I reduce noise in results?</strong></p>
<p>A: Use filtering options:</p>
<pre><code class="language-bash"># Increase minimum priority
debtmap --min-priority 5

# Use threshold preset
debtmap --threshold-preset strict

# Filter categories
debtmap --filter "complexity,debt"

# Limit output
debtmap --top 20
</code></pre>
<h3 id="format-and-output"><a class="header" href="#format-and-output">Format and Output</a></h3>
<p><strong>Q: What’s the difference between legacy and unified JSON?</strong></p>
<p>A: Two JSON output formats:</p>
<ul>
<li><strong>Legacy</strong>: <code>{File: {...}}</code> - nested file-based structure</li>
<li><strong>Unified</strong>: Consistent structure with <code>type</code> field for each item</li>
</ul>
<pre><code class="language-bash"># Legacy (default)
debtmap --format json --output-format legacy

# Unified (recommended for parsing)
debtmap --format json --output-format unified
</code></pre>
<p><strong>Q: Can I analyze partial codebases?</strong></p>
<p>A: Yes, several approaches:</p>
<pre><code class="language-bash"># Limit file count
debtmap --max-files 100

# Analyze specific directory
debtmap src/specific/module

# Use filters in config
# .debtmap/config.toml:
# include = ["src/**/*.rs"]
</code></pre>
<p><strong>Q: How is the 0-10 priority score calculated?</strong></p>
<p>A: Debtmap uses a multiplicative risk-based scoring formula to compute priority scores:</p>
<p><strong>Core Formula</strong>:</p>
<pre><code>Final Score = base_risk × debt_factor × complexity_factor ×
              coverage_penalty × coverage_factor
</code></pre>
<p><strong>Base Risk Calculation</strong>:</p>
<pre><code>complexity_component = (cyclomatic × 0.3 + cognitive × 0.45) / 50.0
coverage_component = (100 - coverage_percentage) / 100.0 × 0.5
base_risk = (complexity_component + coverage_component) × 5.0
</code></pre>
<p><strong>Coverage Penalty</strong> (tiered based on test coverage):</p>
<ul>
<li><strong>&lt; 20% coverage</strong>: 3.0× penalty (critical)</li>
<li><strong>20-40% coverage</strong>: 2.0× penalty (high risk)</li>
<li><strong>40-60% coverage</strong>: 1.5× penalty (moderate risk)</li>
<li><strong>60-80% coverage</strong>: 1.2× penalty (low risk)</li>
<li><strong>≥ 80% coverage</strong>: 0.8× penalty (well tested - reduction)</li>
</ul>
<p><strong>Coverage Factor</strong> (additional reduction for well-tested code):</p>
<ul>
<li><strong>≥ 90% coverage</strong>: 0.8 (20% score reduction)</li>
<li><strong>70-90% coverage</strong>: 0.9 (10% score reduction)</li>
<li><strong>&lt; 70% coverage</strong>: 1.0 (no reduction)</li>
</ul>
<p><strong>Role-Based Adjustments</strong> (Evidence-Based Calculator):</p>
<ul>
<li><strong>Pure logic</strong>: 1.2× (testable, maintainable code)</li>
<li><strong>Entry points</strong>: 1.1× (public API boundaries)</li>
<li><strong>I/O wrappers</strong>: 0.7× (thin delegation layers)</li>
</ul>
<p><strong>Default Weights</strong>:</p>
<ul>
<li>Coverage weight: 0.5</li>
<li>Cyclomatic complexity weight: 0.3</li>
<li>Cognitive complexity weight: 0.45</li>
<li>Debt factor weight: 0.2</li>
</ul>
<p><strong>Example</strong>:</p>
<ul>
<li>Function: cyclomatic=15, cognitive=20, coverage=10%, role=entry_point</li>
<li>Complexity component: (15 × 0.3 + 20 × 0.45) / 50 = 0.27</li>
<li>Coverage component: (100 - 10) / 100 × 0.5 = 0.45</li>
<li>Base risk: (0.27 + 0.45) × 5.0 = 3.6</li>
<li>Coverage penalty: 3.0 (&lt; 20% coverage)</li>
<li>Coverage factor: 1.0 (&lt; 70% coverage)</li>
<li>Debt factor: ~1.2 (moderate debt patterns)</li>
<li>Complexity factor: ~1.3 (pattern-adjusted)</li>
<li>Final score: 3.6 × 1.2 × 1.3 × 3.0 × 1.0 × 1.1 (role) ≈ <strong>18.5</strong> (clamped to 10.0 scale)</li>
</ul>
<pre><code class="language-bash"># See score breakdown with verbosity
debtmap -v

# See detailed factor calculations including all multipliers
debtmap -vv
</code></pre>
<h3 id="coverage-and-testing"><a class="header" href="#coverage-and-testing">Coverage and Testing</a></h3>
<p><strong>Q: How does coverage affect scores?</strong></p>
<p>A: Coverage affects scores through two multiplicative factors in the risk calculation:</p>
<p><strong>1. Coverage Penalty</strong> (tiered multiplier based on test coverage):</p>
<ul>
<li><strong>&lt; 20% coverage</strong>: 3.0× penalty (untested code gets highest priority)</li>
<li><strong>20-40% coverage</strong>: 2.0× penalty</li>
<li><strong>40-60% coverage</strong>: 1.5× penalty</li>
<li><strong>60-80% coverage</strong>: 1.2× penalty</li>
<li><strong>≥ 80% coverage</strong>: 0.8× reduction (well-tested code deprioritized)</li>
</ul>
<p><strong>2. Coverage Factor</strong> (additional reduction for well-tested code):</p>
<ul>
<li><strong>≥ 90% coverage</strong>: 0.8 (20% score reduction)</li>
<li><strong>70-90% coverage</strong>: 0.9 (10% score reduction)</li>
<li><strong>&lt; 70% coverage</strong>: 1.0 (no additional reduction)</li>
</ul>
<p><strong>3. Base Risk Component</strong> (coverage weight: 0.5):</p>
<ul>
<li><code>coverage_component = (100 - coverage_percentage) / 100.0 × 0.5</code></li>
<li>Integrated into base risk calculation</li>
</ul>
<p><strong>Combined Effect</strong>:
Untested complex code (0% coverage) receives maximum penalties (3.0× coverage penalty), while well-tested code (≥90% coverage) receives both the 0.8× coverage penalty and 0.8× coverage factor, resulting in a 0.64× total reduction. This ensures untested code rises to the top of the priority list.</p>
<pre><code class="language-bash"># Use coverage file
debtmap --coverage-file coverage.info

# See coverage impact on scoring
debtmap --coverage-file coverage.info -v

# See detailed coverage penalty and factor breakdown
debtmap --coverage-file coverage.info -vv
</code></pre>
<p>See the FAQ entry “How is the 0-10 priority score calculated?” for complete scoring formula details.</p>
<p><strong>Q: What’s the difference between measured and estimated metrics?</strong></p>
<p>A: Debtmap provides both directly measured metrics and formula-based estimates:</p>
<p><strong>Measured Metrics</strong> (from AST analysis):</p>
<ul>
<li><code>cyclomatic_complexity</code>: Actual count of decision points in code</li>
<li><code>cognitive_complexity</code>: Weighted measure of code understandability</li>
<li><code>nesting_depth</code>: Maximum level of nested blocks</li>
<li><code>loc</code> (lines of code): Actual line count</li>
<li><code>parameters</code>: Number of function parameters</li>
<li><code>return_points</code>: Number of return statements</li>
</ul>
<p><strong>Estimated Metrics</strong> (formula-based):</p>
<ul>
<li><code>est_branches</code>: Estimated branch count for testing effort
<ul>
<li>Formula: <code>max(nesting_depth, 1) × cyclomatic_complexity ÷ 3</code></li>
<li>Not an actual count of branches in the AST</li>
<li>Represents estimated testing complexity/effort</li>
<li>Useful for understanding test coverage needs</li>
</ul>
</li>
</ul>
<pre><code class="language-bash"># See all metrics including estimates
debtmap -vv

# Example output:
# cyclomatic_complexity: 15    (measured from AST)
# cognitive_complexity: 20     (measured from AST)
# nesting_depth: 4             (measured from AST)
# est_branches: 20             (estimated: max(4,1) × 15 ÷ 3 = 20)
</code></pre>
<p><strong>When to trust estimated metrics</strong>:</p>
<ul>
<li>Comparing relative complexity between functions</li>
<li>Estimating testing effort</li>
<li>Understanding potential branching scenarios</li>
</ul>
<p><strong>When to rely on measured metrics</strong>:</p>
<ul>
<li>Precise complexity analysis</li>
<li>Setting hard thresholds</li>
<li>Exact cyclomatic/cognitive complexity values</li>
</ul>
<h3 id="context-and-analysis"><a class="header" href="#context-and-analysis">Context and Analysis</a></h3>
<p><strong>Q: What are context providers?</strong></p>
<p>A: Additional analysis for prioritization:</p>
<ul>
<li><strong>critical_path</strong>: Call graph analysis, entry point distance</li>
<li><strong>dependency</strong>: Dependency relationships and coupling</li>
<li><strong>git_history</strong>: Change frequency and authorship</li>
</ul>
<pre><code class="language-bash"># Enable all
debtmap --context

# Specific providers
debtmap --context --context-providers critical_path,dependency

# See context impact
debtmap --context -v
</code></pre>
<h3 id="results-and-comparison"><a class="header" href="#results-and-comparison">Results and Comparison</a></h3>
<p><strong>Q: Why no output?</strong></p>
<p>A: Check verbosity and filtering:</p>
<pre><code class="language-bash"># Increase verbosity
debtmap -v

# Lower priority threshold
debtmap --min-priority 0

# Check if files were analyzed
debtmap -vv 2&gt;&amp;1 | grep "Processed"

# Ensure not using strict threshold
debtmap --threshold-preset lenient
</code></pre>
<p><strong>Q: How to compare results over time?</strong></p>
<p>A: Use the <code>compare</code> command:</p>
<pre><code class="language-bash"># Save baseline
debtmap --format json --output before.json

# Make changes...

# Analyze again
debtmap --format json --output after.json

# Compare
debtmap compare --before before.json --after after.json
</code></pre>
<p><strong>Q: Why does compare fail with ‘incompatible formats’?</strong></p>
<p>A: The JSON files must use the same output format:</p>
<pre><code class="language-bash"># Use unified format for both
debtmap --format json --output-format unified --output before.json
# ... make changes ...
debtmap --format json --output-format unified --output after.json
debtmap compare --before before.json --after after.json

# Or use legacy format for both (but unified is recommended)
debtmap --format json --output-format legacy --output before.json
debtmap --format json --output-format legacy --output after.json
</code></pre>
<p><strong>Q: How do I compare results from different branches?</strong></p>
<p>A: Generate JSON output on each branch and compare:</p>
<pre><code class="language-bash"># On main branch
git checkout main
debtmap --format json --output main.json

# On feature branch
git checkout feature-branch
debtmap --format json --output feature.json

# Compare (from either branch)
debtmap compare --before main.json --after feature.json
</code></pre>
<p><strong>Q: Can I compare legacy and unified JSON formats?</strong></p>
<p>A: No, both files must use the same format. Regenerate with matching formats:</p>
<pre><code class="language-bash"># Convert both to unified format
debtmap --format json --output-format unified --output before.json
debtmap --format json --output-format unified --output after.json
debtmap compare --before before.json --after after.json
</code></pre>
<h3 id="performance-and-optimization"><a class="header" href="#performance-and-optimization">Performance and Optimization</a></h3>
<p><strong>Q: How many threads should I use?</strong></p>
<p>A: Depends on your machine:</p>
<pre><code class="language-bash"># Use all cores (default, recommended)
debtmap --jobs 0

# Limit to 4 threads (if other work running)
debtmap --jobs 4

# Single threaded (debugging only)
debtmap --no-parallel
</code></pre>
<h2 id="when-to-file-bug-reports"><a class="header" href="#when-to-file-bug-reports">When to File Bug Reports</a></h2>
<p>File a bug report when:</p>
<p>✅ <strong>These are bugs</strong>:</p>
<ul>
<li>Parse errors on valid syntax</li>
<li>Crashes or panics</li>
<li>Incorrect complexity calculations</li>
<li>Concurrency errors</li>
<li>Incorrect error messages</li>
</ul>
<p>❌ <strong>These are not bugs</strong>:</p>
<ul>
<li>Unsupported language constructs (file feature request)</li>
<li>Disagreement with complexity scores (subjective)</li>
<li>Performance on very large codebases (optimization request)</li>
<li>Missing documentation (docs issue, not code bug)</li>
</ul>
<h3 id="how-to-report-issues"><a class="header" href="#how-to-report-issues">How to Report Issues</a></h3>
<ol>
<li><strong>Reproduce with minimal example</strong></li>
<li><strong>Include debug output</strong>: <code>debtmap -vvv 2&gt;&amp;1 | tee error.log</code></li>
<li><strong>Include version</strong>: <code>debtmap --version</code></li>
<li><strong>Include platform</strong>: OS, Rust version if relevant</li>
<li><strong>Include configuration</strong>: <code>.debtmap/config.toml</code> if used</li>
<li><strong>Expected vs actual behavior</strong></li>
</ol>
<h3 id="before-filing"><a class="header" href="#before-filing">Before Filing</a></h3>
<ol>
<li>Check this troubleshooting guide</li>
<li>Try <code>--semantic-off</code> fallback mode</li>
<li>Update to latest version</li>
<li>Search existing issues on GitHub</li>
</ol>
<h2 id="related-documentation-4"><a class="header" href="#related-documentation-4">Related Documentation</a></h2>
<ul>
<li><strong><a href="#configuration-2">Configuration Guide</a></strong>: Configure debtmap behavior</li>
<li><strong><a href="#cli-reference">CLI Reference</a></strong>: Complete CLI flag documentation</li>
<li><strong><a href="#analysis-guide">Analysis Guide</a></strong>: Understanding analysis results</li>
<li><strong><a href="#examples-5">Examples</a></strong>: Practical usage examples</li>
<li><strong><a href="api/index.html">API Documentation</a></strong>: Rust API documentation</li>
</ul>
<h2 id="troubleshooting-checklist"><a class="header" href="#troubleshooting-checklist">Troubleshooting Checklist</a></h2>
<p>When debugging issues, work through this checklist:</p>
<ul>
<li><input disabled="" type="checkbox"> Run with <code>-vv</code> to see detailed output</li>
<li><input disabled="" type="checkbox"> Try <code>--semantic-off</code> to use fallback mode</li>
<li><input disabled="" type="checkbox"> Check file permissions and paths</li>
<li><input disabled="" type="checkbox"> Verify configuration in <code>.debtmap/config.toml</code></li>
<li><input disabled="" type="checkbox"> Test with <code>--max-files 10</code> to isolate issues</li>
<li><input disabled="" type="checkbox"> Try <code>--no-parallel</code> to rule out concurrency</li>
<li><input disabled="" type="checkbox"> Check <code>debtmap --version</code> for updates</li>
<li><input disabled="" type="checkbox"> Review error messages in this guide</li>
<li><input disabled="" type="checkbox"> Search GitHub issues for similar problems</li>
<li><input disabled="" type="checkbox"> Create minimal reproduction case</li>
<li><input disabled="" type="checkbox"> File bug report with debug output</li>
</ul>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>

        <template id=fa-eye><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M288 32c-80.8 0-145.5 36.8-192.6 80.6C48.6 156 17.3 208 2.5 243.7c-3.3 7.9-3.3 16.7 0 24.6C17.3 304 48.6 356 95.4 399.4C142.5 443.2 207.2 480 288 480s145.5-36.8 192.6-80.6c46.8-43.5 78.1-95.4 93-131.1c3.3-7.9 3.3-16.7 0-24.6c-14.9-35.7-46.2-87.7-93-131.1C433.5 68.8 368.8 32 288 32zM432 256c0 79.5-64.5 144-144 144s-144-64.5-144-144s64.5-144 144-144s144 64.5 144 144zM288 192c0 35.3-28.7 64-64 64c-11.5 0-22.3-3-31.6-8.4c-.2 2.8-.4 5.5-.4 8.4c0 53 43 96 96 96s96-43 96-96s-43-96-96-96c-2.8 0-5.6 .1-8.4 .4c5.3 9.3 8.4 20.1 8.4 31.6z"/></svg></span></template>
        <template id=fa-eye-slash><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M38.8 5.1C28.4-3.1 13.3-1.2 5.1 9.2S-1.2 34.7 9.2 42.9l592 464c10.4 8.2 25.5 6.3 33.7-4.1s6.3-25.5-4.1-33.7L525.6 386.7c39.6-40.6 66.4-86.1 79.9-118.4c3.3-7.9 3.3-16.7 0-24.6c-14.9-35.7-46.2-87.7-93-131.1C465.5 68.8 400.8 32 320 32c-68.2 0-125 26.3-169.3 60.8L38.8 5.1zM223.1 149.5C248.6 126.2 282.7 112 320 112c79.5 0 144 64.5 144 144c0 24.9-6.3 48.3-17.4 68.7L408 294.5c5.2-11.8 8-24.8 8-38.5c0-53-43-96-96-96c-2.8 0-5.6 .1-8.4 .4c5.3 9.3 8.4 20.1 8.4 31.6c0 10.2-2.4 19.8-6.6 28.3l-90.3-70.8zm223.1 298L373 389.9c-16.4 6.5-34.3 10.1-53 10.1c-79.5 0-144-64.5-144-144c0-6.9 .5-13.6 1.4-20.2L83.1 161.5C60.3 191.2 44 220.8 34.5 243.7c-3.3 7.9-3.3 16.7 0 24.6c14.9 35.7 46.2 87.7 93 131.1C174.5 443.2 239.2 480 320 480c47.8 0 89.9-12.9 126.2-32.5z"/></svg></span></template>
        <template id=fa-copy><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M502.6 70.63l-61.25-61.25C435.4 3.371 427.2 0 418.7 0H255.1c-35.35 0-64 28.66-64 64l.0195 256C192 355.4 220.7 384 256 384h192c35.2 0 64-28.8 64-64V93.25C512 84.77 508.6 76.63 502.6 70.63zM464 320c0 8.836-7.164 16-16 16H255.1c-8.838 0-16-7.164-16-16L239.1 64.13c0-8.836 7.164-16 16-16h128L384 96c0 17.67 14.33 32 32 32h47.1V320zM272 448c0 8.836-7.164 16-16 16H63.1c-8.838 0-16-7.164-16-16L47.98 192.1c0-8.836 7.164-16 16-16H160V128H63.99c-35.35 0-64 28.65-64 64l.0098 256C.002 483.3 28.66 512 64 512h192c35.2 0 64-28.8 64-64v-32h-47.1L272 448z"/></svg></span></template>
        <template id=fa-play><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M73 39c-14.8-9.1-33.4-9.4-48.5-.9S0 62.6 0 80V432c0 17.4 9.4 33.4 24.5 41.9s33.7 8.1 48.5-.9L361 297c14.3-8.7 23-24.2 23-41s-8.7-32.2-23-41L73 39z"/></svg></span></template>
        <template id=fa-clock-rotate-left><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M75 75L41 41C25.9 25.9 0 36.6 0 57.9V168c0 13.3 10.7 24 24 24H134.1c21.4 0 32.1-25.9 17-41l-30.8-30.8C155 85.5 203 64 256 64c106 0 192 86 192 192s-86 192-192 192c-40.8 0-78.6-12.7-109.7-34.4c-14.5-10.1-34.4-6.6-44.6 7.9s-6.6 34.4 7.9 44.6C151.2 495 201.7 512 256 512c141.4 0 256-114.6 256-256S397.4 0 256 0C185.3 0 121.3 28.7 75 75zm181 53c-13.3 0-24 10.7-24 24V256c0 6.4 2.5 12.5 7 17l72 72c9.4 9.4 24.6 9.4 33.9 0s9.4-24.6 0-33.9l-65-65V152c0-13.3-10.7-24-24-24z"/></svg></span></template>



        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr-ef4e11c1.min.js"></script>
        <script src="mark-09e88c2c.min.js"></script>
        <script src="searcher-c2a407aa.js"></script>

        <script src="clipboard-1626706a.min.js"></script>
        <script src="highlight-abc7f01d.js"></script>
        <script src="book-a0b12cfe.js"></script>

        <!-- Custom JS scripts -->

        <script>
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>


    </div>
    </body>
</html>
